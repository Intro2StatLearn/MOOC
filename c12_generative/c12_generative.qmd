---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Generative Models"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Generative Models - Class 12

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Intro. to Generative Models {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Generated images

![Image generated using DALLÂ·E by OpenAI with prompt "a realistic image of a dog and a cat hugging in front of the eiffel tower"](dalle_cat_dog_hugging.png){fig-align="center" height=400}

::: {.fragment}
How did we get here?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Discriminative models

- We are constanly trying to *model* $E(Y|X = x)$ (regression) or $P(Y|X = x)$ (Bayes classifier):

::: {.fragment}
- Linear:
  - linear regression, ridge, lasso
  - logistic regression
  - SVC, SVR
- Non-linear:
  - KNN
  - Trees, RF, Boosting
  - SVM
:::

::: {.fragment}
::: {.callout-note}
Even our Bias-variance tradeoff analysis assumed a Fixed-$X$ scenario!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Generative models

But for classification: $$P(Y | X) = \frac{P(X, Y)}{P(X)}$$

::: {.incremental}
- [Generative]{style="color:red;"} models focus on modeling the joint distribution $P(X, Y)$, or more specifically:
$$P(Y | X) = \frac{P(X, Y)}{P(X)} = \frac{P(Y)P(X|Y)}{P(X)} \propto P(Y)P(X|Y)$$

- Focus on the mechanism which **generated** the data, not just the data

- Especially useful when $n$ is low
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Generative models

::: {.incremental}
- Even more specifically, if $Y \in \{1, \dots, K\}$:
$$P(Y = k|X = x) = \frac{P(Y = k)P(X = x|Y = k)}{P(X = x)} = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}$$

- We focus on estimating $\pi_k, f_k(x)$
:::

::: {.fragment}
::: {.callout-note}
And if we are so good at estimaing $f_k(x) = P(X = x|Y = k)$ why not generate more!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Detour: multivariate Gaussian {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Linear Discriminant Analysis {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Linear discriminant analysis (LDA)

- Recall:
$$P(Y = k|X = x) = \frac{P(Y = k)P(X = x|Y = k)}{P(X = x)} = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}$$

- $\hat{f}(x_0) = \arg\max_k \pi_k f_k(x)$

::: {.incremental}
- In LDA:
  - $\pi_k$ are priors, or (spoiler): $\hat{\pi}_k = \frac{\sum_{i = 1}^n \mathbb{I}\left[Y_i = k\right]}{n}$
  - $f_k(x)$ are multivariate Gaussian, or: $X | Y = k \sim \mathcal{N}(\mu_k, \Sigma)$
  - Notice the covariance matrix $\Sigma$ is the same $\forall k$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA: What to expect

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Parameters
np.random.seed(42)
n_samples = 100  # Number of samples per class
n_features = 2   # 2D feature space
n_classes = 3    # Number of classes (K=3)

# Class means
means = [np.array([2, 2]), np.array([6, 2]), np.array([4, 6])]

# Common covariance matrix (same for all classes)
cov_matrix = np.array([[1.0, 0.8], [0.8, 1.0]])

# Generate data for each class
X = []
Y = []
for i, mean in enumerate(means):
    X_class = np.random.multivariate_normal(mean, cov_matrix, n_samples)
    X.append(X_class)
    Y.append(np.full(n_samples, i))

# Stack the data
X = np.vstack(X)
Y = np.hstack(Y)

# Plotting the observations with different markers and colors for each class
colors = ['r', 'g', 'b']
markers = ['o', 's', '^']

plt.figure(figsize=(5, 5))
for i in range(n_classes):
    plt.scatter(X[Y == i][:, 0], X[Y == i][:, 1], 
                color=colors[i], marker=markers[i], label=f'Class {i}', edgecolor='k')

# Customize the plot
plt.xlabel('X1')
plt.ylabel('X2')
plt.grid(True)
plt.show()
```

::: {.fragment}
::: {.callout-note}
What do you expect the decision rule(s) to look like?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA: $k-j$ classes decision rule

- See that: $P(Y = k|X = x) > P(Y = j|X = x) \Leftrightarrow \pi_kf_k(x) > \pi_jf_j(x) \Leftrightarrow \log\left[\pi_kf_k(x)\right] > \log\left[\pi_jf_j(x)\right]$

::: {.incremental}
- Assume $\pi_k, \mu_k, \Sigma$ known
- $X | Y = k \sim \mathcal{N}(\mu_k, \Sigma) \Rightarrow f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x - \mu_k)^T\Sigma^{-1}(x-\mu_k)\right)$
- $\log\left[\pi_kf_k(x)\right] = \log(\pi_k) + \log(f_k(x)) = \log(\pi_k) + C -\frac{1}{2}(x - \mu_k)^T\Sigma^{-1}(x-\mu_k)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA: $k-j$ classes decision rule

So, select class $k$ over class $j$ if $\log\left[\pi_kf_k(x)\right] > \log\left[\pi_jf_j(x)\right]$ means:

::: {.fragment}
$$\log(\pi_k) -\frac{1}{2}(x - \mu_k)^T\Sigma^{-1}(x-\mu_k) > \log(\pi_j) -\frac{1}{2}(x - \mu_j)^T\Sigma^{-1}(x-\mu_j)$$
:::
::: {.fragment}
$$\delta_{k > j}(x): x^T\Sigma^{-1}(\mu_k - \mu_j) + \left[-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k +\frac{1}{2}\mu_j^T\Sigma^{-1}\mu_j + \log(\pi_k) - \log(\pi_j)\right]> 0$$
:::

::: {.fragment}
::: {.callout-note}
What shape is $\delta_{k > j}(x)$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA: Estimation

::: {.incremental}
- If we want to fit LDA to data, we need to estimate the parameters: $\mu_1,\ldots,\mu_K,\; \Sigma,\; \pi_1,\ldots,\pi_K.$
- This is naturally done by maximum likelihood from the data:
$$\begin{eqnarray*}
\hat{\pi}_k &=& \frac{\sum_{i=1}^n \mathbb{I}\left[Y_i = k\right]}{n} \;,\; \hat{\mu}_k = \frac{\sum_{\mathbb{I}\left[Y_i = k\right]} x_i}{\sum_{i=1}^n \mathbb{I}\left[Y_i = k\right]}\\
\hat{\Sigma} &=& \frac{1}{n-K} \sum_{k=1}^K \sum_{\mathbb{I}\left[Y_i = k\right]} (x_i-\hat{\mu}_k) (x_i-\hat{\mu}_k)^T
\end{eqnarray*}$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA: Example

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Parameters
np.random.seed(42)
n_samples = 100  # Number of samples per class
n_features = 2   # 2D feature space
n_classes = 3    # Number of classes (K=3)

# True class means
means = [np.array([2, 2]), np.array([6, 2]), np.array([4, 6])]

# Common covariance matrix (same for all classes)
cov_matrix = np.array([[1.0, 0.8], [0.8, 1.0]])

# Generate data for each class
X = []
Y = []
for i, mean in enumerate(means):
    X_class = np.random.multivariate_normal(mean, cov_matrix, n_samples)
    X.append(X_class)
    Y.append(np.full(n_samples, i))

# Stack the data
X = np.vstack(X)
Y = np.hstack(Y)

# Estimate class means and common covariance matrix from data
estimated_means = [X[Y == i].mean(axis=0) for i in range(n_classes)]

K = n_classes
n = n_samples * K

def cov_est(n, K, n_features, estimated_means): 
  # Initialize covariance matrix
  cov_matrix_estimated = np.zeros((n_features, n_features))

  # Compute the unbiased covariance matrix
  for k in range(K):
      class_samples = X[Y == k]
      mean_k = estimated_means[k]
      # Compute (X_i - \hat{\mu}_k) for all i in class g_k
      centered_class_samples = class_samples - mean_k
      # Accumulate the covariance contributions for class g_k
      cov_matrix_estimated += centered_class_samples.T @ centered_class_samples

  # Final covariance matrix (unbiased estimator)
  cov_matrix_estimated /= (n - K)
  
  return cov_matrix_estimated

estimated_cov = cov_est(n, K, n_features, estimated_means)

# Inverse of the estimated covariance matrix
estimated_cov_inv = np.linalg.inv(estimated_cov)

# Function to compute the slope and intercept for decision boundaries
def compute_decision_boundary_params(mean1, mean2, cov_inv):
    w = cov_inv @ (mean1 - mean2)
    b = -0.5 * (mean1 @ cov_inv @ mean1 - mean2 @ cov_inv @ mean2)
    return w, b

# Plotting the observations with different markers and colors for each class
colors = ['r', 'g', 'b']
markers = ['o', 's', '^']

plt.figure(figsize=(5, 5))
plt.xlim(min(X[:, 0])-1, max(X[:, 0])+1)
plt.ylim(min(X[:, 1])-1, max(X[:, 1])+1)

for i in range(n_classes):
    plt.scatter(X[Y == i][:, 0], X[Y == i][:, 1], 
                color=colors[i], marker=markers[i], label=f'Class {i}', edgecolor='k')

# Define the range of x-values for plotting the decision boundaries
x_plot = np.linspace(min(X[:, 0])-1, max(X[:, 0])+1, 100)

# Plot true decision boundaries (from true means and covariance matrix)
true_intersections = []  # To store intersection points
for i in range(n_classes):
    for j in range(i+1, n_classes):
        # Compute the parameters of the true decision boundary
        w_true, b_true = compute_decision_boundary_params(means[i], means[j], np.linalg.inv(cov_matrix))
        # Compute the y-values for the true decision boundary line
        y_plot_true = -(w_true[0] * x_plot + b_true) / w_true[1]
        plt.plot(x_plot, y_plot_true, 'k-', linewidth=2)

        # Find the intersection of these two lines and store it
        intersection_x = -(b_true) / w_true[0]  # Solve for x when y=0
        intersection_y = -(w_true[0] * intersection_x + b_true) / w_true[1]  # Solve for y
        true_intersections.append([intersection_x, intersection_y])

# Find the centroid of the intersection points (approximate the middle)
centroid = np.mean(true_intersections, axis=0)

# Plot estimated decision boundaries (from estimated means and covariance matrix)
for i in range(n_classes):
    for j in range(i+1, n_classes):
        # Compute the parameters of the estimated decision boundary
        w_est, b_est = compute_decision_boundary_params(estimated_means[i], estimated_means[j], estimated_cov_inv)
        # Compute the y-values for the estimated decision boundary line
        y_plot_est = -(w_est[0] * x_plot + b_est) / w_est[1]

        # Plot only up to the intersection point (centroid)
        x_masked = x_plot#[x_plot <= centroid[0]]  # Limit x-values up to the centroid
        y_masked = -(w_est[0] * x_masked + b_est) / w_est[1]
        plt.plot(x_masked, y_masked, 'k--', linewidth=2)

# Highlight the centroid where the lines meet
# plt.scatter(centroid[0], centroid[1], color='black', marker='x', s=100, label='Intersection Point')

# Customize the plot
plt.xlabel('X1')
plt.ylabel('X2')
plt.grid(True)
plt.show()
```
::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Quadratic Discriminant Analysis {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### A common covariance?

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Parameters
np.random.seed(42)
n_samples = 100  # Number of samples per class
n_classes = 2    # Number of classes (K=2)
n_features = 2   # 2D feature space

# Class means
means = [np.array([2, 2]), np.array([4, 6])]

# Different covariance matrices for each class
covariances = [np.array([[1.2, 0.5], [0.5, 1.2]]),
               np.array([[1.5, -0.7], [-0.7, 1.0]])]

# Generate data for each class
X = []
Y = []
for i, (mean, cov) in enumerate(zip(means, covariances)):
    X_class = np.random.multivariate_normal(mean, cov, n_samples)
    X.append(X_class)
    Y.append(np.full(n_samples, i))

# Stack the data
X = np.vstack(X)
Y = np.hstack(Y)

# Plotting the observations with different markers and colors for each class
colors = ['r', 'g', 'b']
markers = ['o', 's', '^']

plt.figure(figsize=(5, 5))
for i in range(n_classes):
    plt.scatter(X[Y == i][:, 0], X[Y == i][:, 1], 
                color=colors[i], marker=markers[i], label=f'Class {i}', edgecolor='k')

# Customize the plot
plt.xlabel('X1')
plt.ylabel('X2')
plt.grid(True)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Quadratic linear analysis (QDA)

::: {.incremental}
- Now assume: $X | Y = k \sim \mathcal{N}(\mu_k,$[$\Sigma_k$]{style="color:red;"} $)$
- Decision rule stays the same:
  - select class $k$ over class $j$ if $\log\left[\pi_kf_k(x)\right] > \log\left[\pi_jf_j(x)\right]$
- $\log\left[\pi_kf_k(x)\right] = \log(\pi_k) + C -\frac{1}{2}(x - \mu_k)^T$[$\Sigma_k^{-1}$]{style="color:red;"}$(x-\mu_k)$ [$-\frac{1}{2}\log(|\Sigma_k|)$]{style="color:red;"}
:::

::: {.fragment}
$$\begin{align}
  \delta_{k > j}(x)&: \\
  &-\frac{1}{2}x^T\left(\Sigma_k^{-1} - \Sigma_j^{-1}\right)x + x^T\left(\Sigma_k^{-1}\mu_k - \Sigma_j^{-1}\mu_j\right) \\
  &+ \left[-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k +\frac{1}{2}\mu_j^T\Sigma^{-1}\mu_j + \log(\pi_k) - \log(\pi_j) + \frac{1}{2}\left(\log(|\Sigma_j| -\log(|\Sigma_k|)\right)\right]> 0
  \end{align}$$
:::

::: {.fragment}
::: {.callout-note}
What shape is $\delta_{k > j}(x)$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### QDA: Estimation

- Estimate $\mu_k, \pi_k$ as in LDA and:
  - $\hat{\Sigma}_k  = \frac{1}{n_k-1} \sum_{\mathbb{I}\left[Y_i = k\right]}  (X_i-\hat{\mu}_k) (X_i-\hat{\mu}_k)^T$

:::: {.columns}
::: {.column}
::: {.fragment}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Parameters
np.random.seed(42)
n_samples = 100  # Number of samples per class
n_classes = 2    # Number of classes (K=2)
n_features = 2   # 2D feature space

# Class means
means = [np.array([2, 2]), np.array([4, 6])]

# Different covariance matrices for each class
covariances = [np.array([[1.2, 0.5], [0.5, 1.2]]),
               np.array([[1.5, -0.7], [-0.7, 1.0]])]

# Generate data for each class
X = []
Y = []
for i, (mean, cov) in enumerate(zip(means, covariances)):
    X_class = np.random.multivariate_normal(mean, cov, n_samples)
    X.append(X_class)
    Y.append(np.full(n_samples, i))

# Stack the data
X = np.vstack(X)
Y = np.hstack(Y)

# Estimate class means and covariances
estimated_means = [X[Y == k].mean(axis=0) for k in range(n_classes)]
estimated_covariances = [np.cov(X[Y == k].T) for k in range(n_classes)]
class_priors = [np.mean(Y == k) for k in range(n_classes)]  # Priors \pi_k

# Function to calculate the QDA discriminant between two classes
def qda_decision_boundary(x, mu_k, mu_j, cov_inv_k, cov_inv_j, log_det_cov_k, log_det_cov_j, pi_k, pi_j):
    diff_cov_inv = cov_inv_k - cov_inv_j
    linear_term = cov_inv_k @ mu_k - cov_inv_j @ mu_j
    constant_term = (
        -0.5 * mu_k.T @ cov_inv_k @ mu_k 
        + 0.5 * mu_j.T @ cov_inv_j @ mu_j
        + np.log(pi_k) - np.log(pi_j)
        + 0.5 * (log_det_cov_j - log_det_cov_k)
    )
    
    # Compute quadratic decision function
    return -0.5 * np.einsum('ij,ij->i', x @ diff_cov_inv, x) + x @ linear_term + constant_term

# Create a grid of points over the feature space
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))
grid = np.c_[xx.ravel(), yy.ravel()]

# Plotting the observations with different markers and colors for each class
colors = ['r', 'g', 'b']
markers = ['o', 's', '^']

plt.figure(figsize=(5, 5))
for i in range(n_classes):
    plt.scatter(X[Y == i][:, 0], X[Y == i][:, 1], 
                color=colors[i], marker=markers[i], label=f'Class {i}', edgecolor='k')

# Plot QDA decision boundaries between each pair of classes
for k in range(n_classes):
    for j in range(k + 1, n_classes):
        # Compute inverse covariances and log determinant of covariances
        cov_inv_k = np.linalg.inv(estimated_covariances[k])
        cov_inv_j = np.linalg.inv(estimated_covariances[j])
        log_det_cov_k = np.log(np.linalg.det(estimated_covariances[k]))
        log_det_cov_j = np.log(np.linalg.det(estimated_covariances[j]))
        
        # Calculate QDA decision boundary values for the grid
        decision_vals = qda_decision_boundary(grid, estimated_means[k], estimated_means[j], 
                                              cov_inv_k, cov_inv_j, log_det_cov_k, log_det_cov_j, 
                                              class_priors[k], class_priors[j])
        
        # Reshape the decision values and plot the contour for QDA boundary
        decision_vals = decision_vals.reshape(xx.shape)
        plt.contour(xx, yy, decision_vals, levels=[0], colors='k', linestyles='--')

# Customize the plot
plt.xlabel('X1')
plt.ylabel('X2')
plt.grid(True)
plt.show()
```
:::
:::
::: {.column}
::: {.fragment}
::: {.callout-note}
How many params are estimated in QDA? LDA?
:::
:::
:::
::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Naive Bayes {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Comparing classifiers {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
