---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Generative Models"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Generative Models - Class 12

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Intro. to Generative Models {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Generated images

![Image generated using DALLÂ·E by OpenAI with prompt "a realistic image of a dog and a cat hugging in front of the eiffel tower"](dalle_cat_dog_hugging.png){fig-align="center" height=400}

::: {.fragment}
How did we get here?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Discriminative models

- We are constanly trying to *model* $E(Y|X = x)$ (regression) or $P(Y|X = x)$ (Bayes classifier):

::: {.fragment}
- Linear:
  - linear regression, ridge, lasso
  - logistic regression
  - SVC, SVR
- Non-linear:
  - KNN
  - Trees, RF, Boosting
  - SVM
:::

::: {.fragment}
::: {.callout-note}
Even our Bias-variance tradeoff analysis assumed a Fixed-$X$ scenario!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Generative models

But for classification: $$P(Y | X) = \frac{P(X, Y)}{P(X)}$$

::: {.incremental}
- [Generative]{style="color:red;"} models focus on modeling the joint distribution $P(X, Y)$, or more specifically:
$$P(Y | X) = \frac{P(X, Y)}{P(X)} = \frac{P(Y)P(X|Y)}{P(X)} \propto P(Y)P(X|Y)$$

- Focus on the mechanism which **generated** the data, not just the data

- Especially useful when $n$ is low
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Generative models

::: {.incremental}
- Even more specifically, if $Y \in \{1, \dots, K\}$:
$$P(Y = k|X = x) = \frac{P(Y = k)P(X = x|Y = k)}{P(X = x)} = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}$$

- We focus on estimating $\pi_k, f_k(x)$
:::

::: {.fragment}
::: {.callout-note}
And if we are so good at estimaing $f_k(x) = P(X = x|Y = k)$ why not generate more!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Detour: multivariate normal {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Univariate normal

- $X \in \mathbb{R} \sim \mathcal{N}(\mu, \sigma^2) \Rightarrow E(X) = \mu; \quad V(X) = \sigma^2 > 0$
- $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)$
- $F(x) = P(X \le x) = \int_{-\infty}^x f(x)dx = \Phi\left(\frac{x - \mu}{\sigma}\right)$

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Parameters for the normal distribution
mu = 0   # mean
sigma = 1  # standard deviation

# Generate x values and corresponding f(x) values for the normal distribution
x = np.linspace(mu - 4*sigma, mu + 4*sigma, 500)
y = norm.pdf(x, mu, sigma)

# Create the plot
plt.figure(figsize=(4, 4))
plt.plot(x, y, color='black')

# # Plot the horizontal line for the 95% confidence interval
# lower_bound = mu - 2*sigma
# upper_bound = mu + 2*sigma
# plt.hlines(0, lower_bound, upper_bound, color='black', linewidth=2)

# # Draw vertical lines at the bounds of the interval
# plt.vlines([lower_bound, upper_bound], ymin=0, ymax=norm.pdf([lower_bound, upper_bound], mu, sigma), color='black', linestyle='dashed')

# Remove plot title and legend as per your preference
plt.gca().set_title('')
plt.gca().legend_ = None

# Display the plot
plt.xlabel('x')
plt.ylabel('f(x)')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Bivariate normal

::: {.incremental}
- $X \in \mathbb{R}^2 \sim \mathcal{BVN}(\mathbf{\mu}, \mathbf{\Sigma}) \Rightarrow E(X) = \mu = \begin{pmatrix}\mu_1 \\ \mu_2\end{pmatrix}; \quad V(X) = \mathbf{\Sigma} = \begin{pmatrix}\sigma_1^2 & \rho\sigma_1\sigma_2 \\ \rho\sigma_1\sigma_2 & \sigma_2^2\end{pmatrix}$
- $\Leftrightarrow X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2), X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2); \quad \rho = \text{Cor}(X_1, X_2) = \frac{\text{Cov}(X_1, X_2)}{\sigma_1\sigma_2}$
- $f(x) = f(x_1, x_2) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \exp\left( -\frac{1}{2(1 - \rho^2)} \left[ \frac{(x_1 - \mu_1)^2}{\sigma_1^2} + \frac{(x_2 - \mu_2)^2}{\sigma_2^2} - \frac{2\rho(x_1 - \mu_1)(x_2 - \mu_2)}{\sigma_1 \sigma_2} \right] \right)$
- $F(x) = P(X_1 \le x_1, X_2 \le x_2) = \int_{-\infty}^{x_1}\int_{-\infty}^{x_2}f(x_1, x_2)dx_1 dx_2$
:::

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

# Parameters for the bivariate normal distribution
mu = np.array([0, 0])  # Means of x1 and x2
sigma1 = 1  # Standard deviation of x1
sigma2 = 1  # Standard deviation of x2
rho = 0.5  # Correlation coefficient

# Covariance matrix
cov = [[sigma1**2, rho*sigma1*sigma2], 
       [rho*sigma1*sigma2, sigma2**2]]

# Generate grid of x1 and x2 values
x1 = np.linspace(-3, 3, 100)
x2 = np.linspace(-3, 3, 100)
X1, X2 = np.meshgrid(x1, x2)
pos = np.dstack((X1, X2))

# Bivariate normal distribution
rv = multivariate_normal(mu, cov)
Z = rv.pdf(pos)

# Create the figure and two subplots side by side
fig = plt.figure(figsize=(7.5, 3.3))

# 3D plot (left)
ax1 = fig.add_subplot(121, projection='3d')
ax1.plot_surface(X1, X2, Z, cmap='viridis', edgecolor='none')
ax1.set_xlabel('$x_1$')
ax1.set_ylabel('$x_2$')
ax1.set_zlabel('$f(x_1, x_2)$')
# ax1.zaxis.set_rotate_label(False) 

# Contour plot (right)
ax2 = fig.add_subplot(122)
contour_levels = [0.5 * np.max(Z), 0.8 * np.max(Z), 0.95 * np.max(Z), 0.99 * np.max(Z), 0.999 * np.max(Z)]
ax2.contour(X1, X2, Z, levels=contour_levels, colors='black')
ax2.set_xlim(-2, 2)
ax2.set_ylim(-2, 2)
ax2.set_xlabel('$x_1$')
ax2.set_ylabel('$x_2$')
ax2.yaxis.set_label_coords(-0.15, 0.35)
ax2.grid(True)

# Remove plot titles and legends as per your preference
ax1.set_title('')
ax2.set_title('')

# Show the plots
plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Multivariate normal

::: {.incremental}
- $X \in \mathbb{R}^p \sim \mathcal{MVN}(\mathbf{\mu}, \mathbf{\Sigma})$
- $E(X) = \mu = \begin{pmatrix}\mu_1 \\ \vdots \\ \mu_p\end{pmatrix}; \quad V(X) = \mathbf{\Sigma} = \begin{pmatrix}\sigma_1^2 & \dots & \rho_{1,p}\sigma_1\sigma_p \\ \vdots & \ddots & \vdots \\ \rho_{1, p}\sigma_{1}\sigma_{p} & \dots & \sigma_p^2\end{pmatrix}$
- $\Leftrightarrow X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2), \dots, X_p \sim \mathcal{N}(\mu_p, \sigma_p^2); \quad \rho_{j,k} = \text{Cor}(X_j, X_k) = \frac{\text{Cov}(X_j, X_k)}{\sigma_j\sigma_k}$
- $f(x) = f(x_1, \dots, x_p) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x-\mu)\right)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Linear Discriminant Analysis {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Linear discriminant analysis (LDA)

- Recall:
$$P(Y = k|X = x) = \frac{P(Y = k)P(X = x|Y = k)}{P(X = x)} = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}$$

- $\hat{f}(x_0) = \arg\max_k \pi_k f_k(x)$

::: {.incremental}
- In LDA:
  - $\pi_k$ are priors, or (spoiler): $\hat{\pi}_k = \frac{\sum_{i = 1}^n \mathbb{I}\left[Y_i = k\right]}{n}$
  - $f_k(x)$ are multivariate Gaussian, or: $X | Y = k \sim \mathcal{N}(\mu_k, \Sigma)$
  - Notice the covariance matrix $\Sigma$ is the same $\forall k$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA: What to expect

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Parameters
np.random.seed(42)
n_samples = 100  # Number of samples per class
n_features = 2   # 2D feature space
n_classes = 3    # Number of classes (K=3)

# Class means
means = [np.array([2, 2]), np.array([6, 2]), np.array([4, 6])]

# Common covariance matrix (same for all classes)
cov_matrix = np.array([[1.0, 0.8], [0.8, 1.0]])

# Generate data for each class
X = []
Y = []
for i, mean in enumerate(means):
    X_class = np.random.multivariate_normal(mean, cov_matrix, n_samples)
    X.append(X_class)
    Y.append(np.full(n_samples, i))

# Stack the data
X = np.vstack(X)
Y = np.hstack(Y)

# Plotting the observations with different markers and colors for each class
colors = ['r', 'g', 'b']
markers = ['o', 's', '^']

plt.figure(figsize=(5, 5))
for i in range(n_classes):
    plt.scatter(X[Y == i][:, 0], X[Y == i][:, 1], 
                color=colors[i], marker=markers[i], label=f'Class {i}', edgecolor='k')

# Customize the plot
plt.xlabel('X1')
plt.ylabel('X2')
plt.grid(True)
plt.show()
```

::: {.fragment}
::: {.callout-note}
What do you expect the decision rule(s) to look like?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA: $k-j$ classes decision rule

- See that: $P(Y = k|X = x) > P(Y = j|X = x) \Leftrightarrow \pi_kf_k(x) > \pi_jf_j(x) \Leftrightarrow \log\left[\pi_kf_k(x)\right] > \log\left[\pi_jf_j(x)\right]$

::: {.incremental}
- Assume $\pi_k, \mu_k, \Sigma$ known
- $X | Y = k \sim \mathcal{N}(\mu_k, \Sigma) \Rightarrow f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x - \mu_k)^T\Sigma^{-1}(x-\mu_k)\right)$
- $\log\left[\pi_kf_k(x)\right] = \log(\pi_k) + \log(f_k(x)) = \log(\pi_k) + C -\frac{1}{2}(x - \mu_k)^T\Sigma^{-1}(x-\mu_k)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA: $k-j$ classes decision rule

So, select class $k$ over class $j$ if $\log\left[\pi_kf_k(x)\right] > \log\left[\pi_jf_j(x)\right]$ means:

::: {.fragment}
$$\log(\pi_k) -\frac{1}{2}(x - \mu_k)^T\Sigma^{-1}(x-\mu_k) > \log(\pi_j) -\frac{1}{2}(x - \mu_j)^T\Sigma^{-1}(x-\mu_j)$$
:::
::: {.fragment}
$$\delta_{k > j}(x): x^T\Sigma^{-1}(\mu_k - \mu_j) + \left[-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k +\frac{1}{2}\mu_j^T\Sigma^{-1}\mu_j + \log(\pi_k) - \log(\pi_j)\right]> 0$$
:::

::: {.fragment}
::: {.callout-note}
What shape is $\delta_{k > j}(x)$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA: Estimation

::: {.incremental}
- If we want to fit LDA to data, we need to estimate the parameters: $\mu_1,\ldots,\mu_K,\; \Sigma,\; \pi_1,\ldots,\pi_K.$
- This is naturally done by maximum likelihood from the data:
$$\begin{eqnarray*}
\hat{\pi}_k &=& \frac{\sum_{i=1}^n \mathbb{I}\left[Y_i = k\right]}{n} \;,\; \hat{\mu}_k = \frac{\sum_{\mathbb{I}\left[Y_i = k\right]} x_i}{\sum_{i=1}^n \mathbb{I}\left[Y_i = k\right]}\\
\hat{\Sigma} &=& \frac{1}{n-K} \sum_{k=1}^K \sum_{\mathbb{I}\left[Y_i = k\right]} (x_i-\hat{\mu}_k) (x_i-\hat{\mu}_k)^T
\end{eqnarray*}$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA: Example

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Parameters
np.random.seed(42)
n_samples = 100  # Number of samples per class
n_features = 2   # 2D feature space
n_classes = 3    # Number of classes (K=3)

# True class means
means = [np.array([2, 2]), np.array([6, 2]), np.array([4, 6])]

# Common covariance matrix (same for all classes)
cov_matrix = np.array([[1.0, 0.8], [0.8, 1.0]])

# Generate data for each class
X = []
Y = []
for i, mean in enumerate(means):
    X_class = np.random.multivariate_normal(mean, cov_matrix, n_samples)
    X.append(X_class)
    Y.append(np.full(n_samples, i))

# Stack the data
X = np.vstack(X)
Y = np.hstack(Y)

# Estimate class means and common covariance matrix from data
estimated_means = [X[Y == i].mean(axis=0) for i in range(n_classes)]

K = n_classes
n = n_samples * K

def cov_est(n, K, n_features, estimated_means): 
  # Initialize covariance matrix
  cov_matrix_estimated = np.zeros((n_features, n_features))

  # Compute the unbiased covariance matrix
  for k in range(K):
      class_samples = X[Y == k]
      mean_k = estimated_means[k]
      # Compute (X_i - \hat{\mu}_k) for all i in class g_k
      centered_class_samples = class_samples - mean_k
      # Accumulate the covariance contributions for class g_k
      cov_matrix_estimated += centered_class_samples.T @ centered_class_samples

  # Final covariance matrix (unbiased estimator)
  cov_matrix_estimated /= (n - K)
  
  return cov_matrix_estimated

estimated_cov = cov_est(n, K, n_features, estimated_means)

# Inverse of the estimated covariance matrix
estimated_cov_inv = np.linalg.inv(estimated_cov)

# Function to compute the slope and intercept for decision boundaries
def compute_decision_boundary_params(mean1, mean2, cov_inv):
    w = cov_inv @ (mean1 - mean2)
    b = -0.5 * (mean1 @ cov_inv @ mean1 - mean2 @ cov_inv @ mean2)
    return w, b

# Plotting the observations with different markers and colors for each class
colors = ['r', 'g', 'b']
markers = ['o', 's', '^']

plt.figure(figsize=(5, 5))
plt.xlim(min(X[:, 0])-1, max(X[:, 0])+1)
plt.ylim(min(X[:, 1])-1, max(X[:, 1])+1)

for i in range(n_classes):
    plt.scatter(X[Y == i][:, 0], X[Y == i][:, 1], 
                color=colors[i], marker=markers[i], label=f'Class {i}', edgecolor='k')

# Define the range of x-values for plotting the decision boundaries
x_plot = np.linspace(min(X[:, 0])-1, max(X[:, 0])+1, 100)

# Plot true decision boundaries (from true means and covariance matrix)
true_intersections = []  # To store intersection points
for i in range(n_classes):
    for j in range(i+1, n_classes):
        # Compute the parameters of the true decision boundary
        w_true, b_true = compute_decision_boundary_params(means[i], means[j], np.linalg.inv(cov_matrix))
        # Compute the y-values for the true decision boundary line
        y_plot_true = -(w_true[0] * x_plot + b_true) / w_true[1]
        plt.plot(x_plot, y_plot_true, 'k-', linewidth=2)

        # Find the intersection of these two lines and store it
        intersection_x = -(b_true) / w_true[0]  # Solve for x when y=0
        intersection_y = -(w_true[0] * intersection_x + b_true) / w_true[1]  # Solve for y
        true_intersections.append([intersection_x, intersection_y])

# Find the centroid of the intersection points (approximate the middle)
centroid = np.mean(true_intersections, axis=0)

# Plot estimated decision boundaries (from estimated means and covariance matrix)
for i in range(n_classes):
    for j in range(i+1, n_classes):
        # Compute the parameters of the estimated decision boundary
        w_est, b_est = compute_decision_boundary_params(estimated_means[i], estimated_means[j], estimated_cov_inv)
        # Compute the y-values for the estimated decision boundary line
        y_plot_est = -(w_est[0] * x_plot + b_est) / w_est[1]

        # Plot only up to the intersection point (centroid)
        x_masked = x_plot#[x_plot <= centroid[0]]  # Limit x-values up to the centroid
        y_masked = -(w_est[0] * x_masked + b_est) / w_est[1]
        plt.plot(x_masked, y_masked, 'k--', linewidth=2)

# Highlight the centroid where the lines meet
# plt.scatter(centroid[0], centroid[1], color='black', marker='x', s=100, label='Intersection Point')

# Customize the plot
plt.xlabel('X1')
plt.ylabel('X2')
plt.grid(True)
plt.show()
```
::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Quadratic Discriminant Analysis {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### A common covariance?

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Parameters
np.random.seed(42)
n_samples = 100  # Number of samples per class
n_classes = 2    # Number of classes (K=2)
n_features = 2   # 2D feature space

# Class means
means = [np.array([2, 2]), np.array([4, 6])]

# Different covariance matrices for each class
covariances = [np.array([[1.2, 0.5], [0.5, 1.2]]),
               np.array([[1.5, -0.7], [-0.7, 1.0]])]

# Generate data for each class
X = []
Y = []
for i, (mean, cov) in enumerate(zip(means, covariances)):
    X_class = np.random.multivariate_normal(mean, cov, n_samples)
    X.append(X_class)
    Y.append(np.full(n_samples, i))

# Stack the data
X = np.vstack(X)
Y = np.hstack(Y)

# Plotting the observations with different markers and colors for each class
colors = ['r', 'g', 'b']
markers = ['o', 's', '^']

plt.figure(figsize=(5, 5))
for i in range(n_classes):
    plt.scatter(X[Y == i][:, 0], X[Y == i][:, 1], 
                color=colors[i], marker=markers[i], label=f'Class {i}', edgecolor='k')

# Customize the plot
plt.xlabel('X1')
plt.ylabel('X2')
plt.grid(True)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Quadratic linear analysis (QDA)

::: {.incremental}
- Now assume: $X | Y = k \sim \mathcal{N}(\mu_k,$[$\Sigma_k$]{style="color:red;"} $)$
- Decision rule stays the same:
  - select class $k$ over class $j$ if $\log\left[\pi_kf_k(x)\right] > \log\left[\pi_jf_j(x)\right]$
- $\log\left[\pi_kf_k(x)\right] = \log(\pi_k) + C -\frac{1}{2}(x - \mu_k)^T$[$\Sigma_k^{-1}$]{style="color:red;"}$(x-\mu_k)$ [$-\frac{1}{2}\log(|\Sigma_k|)$]{style="color:red;"}
:::

::: {.fragment}
$$\begin{align}
  \delta_{k > j}(x)&: \\
  &-\frac{1}{2}x^T\left(\Sigma_k^{-1} - \Sigma_j^{-1}\right)x + x^T\left(\Sigma_k^{-1}\mu_k - \Sigma_j^{-1}\mu_j\right) \\
  &+ \left[-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k +\frac{1}{2}\mu_j^T\Sigma^{-1}\mu_j + \log(\pi_k) - \log(\pi_j) + \frac{1}{2}\left(\log(|\Sigma_j| -\log(|\Sigma_k|)\right)\right]> 0
  \end{align}$$
:::

::: {.fragment}
::: {.callout-note}
What shape is $\delta_{k > j}(x)$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### QDA: Estimation

- Estimate $\mu_k, \pi_k$ as in LDA and:
  - $\hat{\Sigma}_k  = \frac{1}{n_k-1} \sum_{\mathbb{I}\left[Y_i = k\right]}  (X_i-\hat{\mu}_k) (X_i-\hat{\mu}_k)^T$

:::: {.columns}
::: {.column}
::: {.fragment}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Parameters
np.random.seed(42)
n_samples = 100  # Number of samples per class
n_classes = 2    # Number of classes (K=2)
n_features = 2   # 2D feature space

# Class means
means = [np.array([2, 2]), np.array([4, 6])]

# Different covariance matrices for each class
covariances = [np.array([[1.2, 0.5], [0.5, 1.2]]),
               np.array([[1.5, -0.7], [-0.7, 1.0]])]

# Generate data for each class
X = []
Y = []
for i, (mean, cov) in enumerate(zip(means, covariances)):
    X_class = np.random.multivariate_normal(mean, cov, n_samples)
    X.append(X_class)
    Y.append(np.full(n_samples, i))

# Stack the data
X = np.vstack(X)
Y = np.hstack(Y)

# Estimate class means and covariances
estimated_means = [X[Y == k].mean(axis=0) for k in range(n_classes)]
estimated_covariances = [np.cov(X[Y == k].T) for k in range(n_classes)]
class_priors = [np.mean(Y == k) for k in range(n_classes)]  # Priors \pi_k

# Function to calculate the QDA discriminant between two classes
def qda_decision_boundary(x, mu_k, mu_j, cov_inv_k, cov_inv_j, log_det_cov_k, log_det_cov_j, pi_k, pi_j):
    diff_cov_inv = cov_inv_k - cov_inv_j
    linear_term = cov_inv_k @ mu_k - cov_inv_j @ mu_j
    constant_term = (
        -0.5 * mu_k.T @ cov_inv_k @ mu_k 
        + 0.5 * mu_j.T @ cov_inv_j @ mu_j
        + np.log(pi_k) - np.log(pi_j)
        + 0.5 * (log_det_cov_j - log_det_cov_k)
    )
    
    # Compute quadratic decision function
    return -0.5 * np.einsum('ij,ij->i', x @ diff_cov_inv, x) + x @ linear_term + constant_term

# Create a grid of points over the feature space
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))
grid = np.c_[xx.ravel(), yy.ravel()]

# Plotting the observations with different markers and colors for each class
colors = ['r', 'g', 'b']
markers = ['o', 's', '^']

plt.figure(figsize=(5, 5))
for i in range(n_classes):
    plt.scatter(X[Y == i][:, 0], X[Y == i][:, 1], 
                color=colors[i], marker=markers[i], label=f'Class {i}', edgecolor='k')

# Plot QDA decision boundaries between each pair of classes
for k in range(n_classes):
    for j in range(k + 1, n_classes):
        # Compute inverse covariances and log determinant of covariances
        cov_inv_k = np.linalg.inv(estimated_covariances[k])
        cov_inv_j = np.linalg.inv(estimated_covariances[j])
        log_det_cov_k = np.log(np.linalg.det(estimated_covariances[k]))
        log_det_cov_j = np.log(np.linalg.det(estimated_covariances[j]))
        
        # Calculate QDA decision boundary values for the grid
        decision_vals = qda_decision_boundary(grid, estimated_means[k], estimated_means[j], 
                                              cov_inv_k, cov_inv_j, log_det_cov_k, log_det_cov_j, 
                                              class_priors[k], class_priors[j])
        
        # Reshape the decision values and plot the contour for QDA boundary
        decision_vals = decision_vals.reshape(xx.shape)
        plt.contour(xx, yy, decision_vals, levels=[0], colors='k', linestyles='--')

# Customize the plot
plt.xlabel('X1')
plt.ylabel('X2')
plt.grid(True)
plt.show()
```
:::
:::
::: {.column}
::: {.fragment}
::: {.callout-note}
How many params are estimated in QDA? LDA?
:::
:::
:::
::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Naive Bayes {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Naive Bayes

- Recall: $P(Y = k|X = x) = \frac{P(Y = k)P(X = x|Y = k)}{P(X = x)} = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}$

::: {.incremental}
- In Naive Bayes:
  - $\pi_k$ are priors, or: $\hat{\pi}_k = \frac{\sum_{i = 1}^n \mathbb{I}\left[Y_i = k\right]}{n}$
  - Within the $k$-th class, the $p$ predictors are **independent**
  - $f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p)$
    - $f_{kj}(x_j)$ for a continuous feature: $\mathcal{N}(\mu_{jk}, \sigma_{jk}^2)$, $Exp(\lambda_{jk})$, KDE, ...
    - $f_{kj}(x_j)$ for a discrete feature: $\hat{f}_{kj}(x_j)=\begin{cases} 0.2 & \text{if $x_j = 1$} \\ 0.8 & \text{if $x_j = 2$} \end{cases}$
:::

::: {.fragment}
::: {.callout-note}
When is this naive assumption particularly useful?
:::
:::

::: {.fragment}
::: {.callout-note}
How is Naive Bayes = LDA/QDA?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Comparing classifiers {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Example: SAHeart data

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

saheart = pd.read_table("../datasets/SAheart.data", header = 0, sep=',', index_col=0)

saheart_X=pd.get_dummies(saheart.iloc[:, :9]).iloc[:, :9]
saheart_y=saheart.iloc[:, 9]

Xtr, Xte, Ytr, Yte = train_test_split(saheart_X, saheart_y, test_size=0.2, random_state=42)

# Standardize the data for SVMs and Logistic Regression
scaler = StandardScaler()
Xtr = scaler.fit_transform(Xtr)
Xte = scaler.transform(Xte)

# Initialize the models
log_reg = LogisticRegression()
svc_linear = SVC(kernel='linear', probability=True)
lda = LinearDiscriminantAnalysis()
qda = QuadraticDiscriminantAnalysis()
gnb = GaussianNB()

# Train the models on the training data
log_reg.fit(Xtr, Ytr)
svc_linear.fit(Xtr, Ytr)
lda.fit(Xtr, Ytr)
qda.fit(Xtr, Ytr)
gnb.fit(Xtr, Ytr)

# Get predicted probabilities for the test set
log_reg_probs = log_reg.predict_proba(Xte)[:, 1]
svc_linear_probs = svc_linear.predict_proba(Xte)[:, 1]
lda_probs = lda.predict_proba(Xte)[:, 1]
qda_probs = qda.predict_proba(Xte)[:, 1]
gnb_probs = gnb.predict_proba(Xte)[:, 1]

# Compute ROC curves and AUCs
fpr_log, tpr_log, _ = roc_curve(Yte, log_reg_probs)
fpr_svc_linear, tpr_svc_linear, _ = roc_curve(Yte, svc_linear_probs)
fpr_lda, tpr_lda, _ = roc_curve(Yte, lda_probs)
fpr_qda, tpr_qda, _ = roc_curve(Yte, qda_probs)
fpr_gnb, tpr_gnb, _ = roc_curve(Yte, gnb_probs)

auc_log = auc(fpr_log, tpr_log)
auc_svc_linear = auc(fpr_svc_linear, tpr_svc_linear)
auc_lda = auc(fpr_lda, tpr_lda)
auc_qda = auc(fpr_qda, tpr_qda)
auc_gnb = auc(fpr_gnb, tpr_gnb)

# Plot ROC curves
plt.figure(figsize=(5, 5))
plt.plot(fpr_log, tpr_log, label=f'Logistic Regression (AUC = {auc_log:.2f})')
plt.plot(fpr_svc_linear, tpr_svc_linear, label=f'SVC (AUC = {auc_svc_linear:.2f})')
plt.plot(fpr_lda, tpr_lda, label=f'LDA (AUC = {auc_lda:.2f})')
plt.plot(fpr_qda, tpr_qda, label=f'QDA (AUC = {auc_qda:.2f})')
plt.plot(fpr_gnb, tpr_gnb, label=f'NB (AUC = {auc_gnb:.2f})')

# Plot settings
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line (random classifier)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Test ROC Curves for Generative models and Logistic Regression')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()
```
::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### LDA vs. logistic regression

::: {.incremental}
- For $K = 2$ (though we can show this for any $K$), logistic regression:
$$\text{logit}(P(Y = 1|X)) = \log\left(\frac{P(Y = 1|X)}{1 - P(Y = 1|X)}\right) = \log\left(\frac{P(Y = 1|X)}{P(Y = 0|X)}\right) = \beta_0 + x^T\beta$$
- For $K = 2$ (though we can show this for any $K$), LDA:
$$\begin{align}\log\left(\frac{P(Y = 1|X)}{P(Y = 0|X)}\right) &= \log\left(\frac{\pi_1 f_1(x)}{\pi_1 f_0(x)}\right) \\
 &= x^T\Sigma^{-1}(\mu_1 - \mu_0) + \left[-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1 +\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0 + \log(\pi_1) - \log(\pi_0)\right] \\
 &= \alpha_0 + x^T\alpha\end{align}$$
:::


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Similarly for all methods

$\delta_{1 > 0}(x) = \log\left(\frac{P(Y = 1|X = x)}{P(Y = 0|X = x)}\right) =$

| method          | type                             | $\delta_{1 > 0}(x)$                                        |
|-----------------|-------------------------------------------|---------------------------------------------------|
| LR              | discriminative, linear           | $\beta_0 + x^T\beta$                                       |
| LDA             | generative, linear               | $\alpha_0 + x^T\alpha$                                     |
| QDA             | generative, non-linear           | $\gamma_0 + x^T\gamma + x^T\Gamma x$                       |
| NB              | generative, non-linear           | $\tau_0 + \sum_{j = 1}^p \tau_j(x)$                        |

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
