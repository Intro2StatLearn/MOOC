=== 1. הקדמה למודלים גנרטיביים ===

בשיעור הזה נעסוק במודלים גנרטיביים. מה ההבדל בין מודלים גנרטיביים למודלים שלמדנו עד עכשיו? מיד נדע. אבל קודם, נראה שהיום אנחנו עושים את הצעד הראשון בלהבין את אחת המהפכות שבודאי כולם שמעו עליהן, והיא היכולת שלנו לייצר או לג'נרט דאטא חדש, כשסוג הדאטא בו עשינו את הקפיצה המרשימה ביותר הוא: תמונות.

:::

מצד שמאל אנחנו רואים תמונה ספציפית מאוד שיכולנו לייצר כבר בשנות התשעים של המאה הקודמת. באמצעות למידת ההתפלגות מתמונות של אלפי פרצופים, מצאנו דרך לייצר פרצופים אקראיים, או האייגנפייסז. מצד ימין אנחנו רואים כבר את מה שאנחנו יכולים לבצע היום. את התמונה הזאת ביקשתי מדאלי, מחולל התמונות של OpenAI, כשכל מה שאמרתי לו זה תיצור לי בבקשה תמונה ריאליסטית של כלב וחתול מתחבקים על רק מגדל אייפל.

איך הגענו בשלושים שנה מזה לזה? זאת הרי קפיצה מטורפת. אז צר לי לאכזב, עוד לא נוכל לקבל תשובה בקורס שלנו, תצטרכו לקחת קורס מתקדם יותר ברשתות נוירונים. אבל אני טוען שמה שנלמד היום הוא צעד ראשון וחשוב מאוד בכיוון.

:::

למודלים שלמדנו עד היום אפשר לקרוא מודלים דיסקרימינטיביים. אם זה ברגרסיה, הם מנסים למדל את התוחלת המותנית של Y בהינתן הנתונים X. אם זה בקלסיפיקציה, אפשר לפרט יותר ולהגיד שהם מנסים למדל את ההסתברות שY שווה לאיזשהו קלאס בהינתן הנתונים X. כלומר הם מנסים להבחין, טו דיסקרימינייט, בין קלאסים שונים. ואם תיזכרו בתחילת הקורס עשינו את זה כי הבנו שבמובן מסוים אם נדע למדל את ההסתברות המותנית הזאת ונדע איך להביא אותה למקסימום זה קריטריון אופטימלי שנקרא בייס קלסיפייר, לעשות קלסיפיקציה טובה.

אם אלה מודלים ליניאריים, דיברנו על רגרסיה ליניארית כשY רציף, רגרסית רידג', לאסו. אם זה קלסיפיקציה דיברנו על רגרסיה לוגיסטית או סאפורט וקטור קלסיפייר.

אם אלה מודלים לא לינאריים, דיברנו על KNN לקלסיפיקציה ולרגרסיה, על עצי החלטה ואנסמבלים שלהם כמו בוסטינג ורנדום פורסט.

כולם כולם, מניחים שX נתון, הוא קבוע, ומנסים למדל את התוחלת המותנית של Y, רציף או בדיד, על הנתונים האלה. מתוך הנחה, או תקווה, שנתונים שהמודל לא ראה דומים לX שלנו.

אפילו בדיון שלנו על הביאס-וריאנס טריידאוף, הפיתוחים שלנו היו יחסים פשוטים והניחו שX המדגם למידה שלנו הוא נתון. על מה לא נתנו דגש? לא נתנו דגש על ההתפלגות של הנתונים עצמם, של X!

:::

באופן יותר פורמלי, לקלסיפיקציה, אם מעניין אותנו ההסתברות של Y בהינתן X, משפט בייז אומר לנו שהיא שווה להסתברות המשותפת של X וY, חלקי ההסתברות השולית של X.

מודלים גנרטיביים, עושים שיפט לפוקוס בדיוק לאלמנט הזה, ההתפלגות המשותפת של X הנתונים ושל המשתנה התלוי Y. ונרחיב: ההסתברות המשותפת של X ושל Y שווה להסתברות השולית או האפריורית של Y, כפול ההסתברות של X בהינתן שראיתי את Y. ומאחר שתיכף נרצה את ההסתברות הזאת לכל מיני Yים, אנחנו נראה שהמכנה הוא זהה לכולם וניתן הדגש בעיקר למונה. הכי הרבה מידע נקבל אם נצליח למדל את ההתפלגות המשותפת, אבל אם נצליח למדל היטב את שתי ההסתברויות האלה, האפריורית שY שווה לקלאס מסוים והתפלגות הנתונים בקרב התצפיות עם הקלאס הזה, זה אומר שהצלחנו למדל היטב את המטרה ההתחלתית שלנו, ההתפלגות המשותפת, ולבסוף הבייס קלסיפייר. 

גישה שונה לגמרי: לתת פוקוס על המנגנון שמג'נרט את הדאטא, שיוצר אותו, לא רק על הדאטא כנתון, כקבוע.

אנחנו נראה, שהדבר יעיל במיוחד כשהדאטא הוא מועט בכל קלאס.

:::

אפשר להרחיב את הרעיון הכללי אפילו יותר:

נניח שY הוא אחד מK קלאסים, 1 עד K. ההסתברות הפוסטריורית שמעניינת אותנו היא ההסתברות שY שייך דווקא לקלאס הK בהינתן תצפית חדשה שמגיעה. ואנחנו הופכים את הבעיה ושמים דגש על ההסתברות האפריורית של הקלאס הK באופן כללי כפול ההתפלגות של הנתונים שלנו בהינתן שראינו את הקלאס הK. נסמן את ההסתברות האפריורית כpi_k ואת ההסתברות המותנית של X כf_k. וכעת המונה שלנו הוא המכפלה הפשוטה pi_kf_k, והמכנה מנוסחת ההסתברות השלמה הוא סכימה על כל האפשרויות.

אנחנו שמים דגש על אמידה של pi_k ושל f_k, וזה בעצם הדבר היחיד שמבדיל בין שלוש השיטות שנלמד היום. חוץ מזה המטרה זהה בשלושתן.

אז איך מגיעים מכאן לג'נרוט של תמונות? המטרה של חוקרים באמת היתה רגרסיה או קלסיפיקציה, אולם בסופו של דבר הגענו למודלים כל כך חזקים כדי לתאר את f_k, התפלגות הנתונים בהינתן הקלאס. הצלחנו לתאר נתונים כל כך מגוונים ועשירים כמו תמונות. שכיוון מסוים במחקר אמר רגע רגע -- אם אנחנו יודעים לתאר כל כך יפה את ההתפלגות של נתונים של תמונות, למה שלא נייצר עוד? תמונות רנדומליות או תמונות על-פי בקשה, על-פי תיאור.

רובנו משתמשים בתמונות של דאלי עדיין בתור קוריוז, אבל ליכולת הזאת שלנו לג'נרט נתונים יש השלכות אדירות על עתיד המחקר, על פרסום ועל תקשורת. אז בואו נעשה היום את הצעד הראשון להבין איך הגענו למודלים החזקים האלה.

:::

=== 2. מעקף: ההתפלגות הרב נורמלית ===

לפני שנלמד איזשהו מודל גנרטיבי, מייד תשימו לב שהם עושים שימוש נרחב בהתפלגות הנורמלית. ואני לא מדבר רק על התפלגות הפעמון של משתנה יחיד, אני מדבר על ההתפלגות הרב-נורמלית, של וקטור של משתנים, שיש לה המון תכונות נורא יפות. אז כדי ליישר קו, חשוב קודם לוודא שכולנו מכירים את ההתפלגות הרב-נורמלית ומה משמעות הפרמטרים שלה.

:::

נתחיל בהתפלגות הנורמלית למשתנה יחיד: X רציף, יכול לקבל ערכים על כל הישר הממשי, מתפלג נורמלית עם פרמטרים מיו וסיגמה בריבוע. כשאני אומר את זה אני מתכוון להתפלגות סימטרית פעמון סביב תוחלת מיו, עם שונות סיגמה בריבוע.

פונקצית הצפיפות של X ניתנת לפי הנוסחה הזאת, אפשר לוודא שהיא תמיד חיובית ושהאינטגרל מתחת לכל העקומה נותן שטח 1.

והפונקציה הזאת היא כאמור לא הסתברות היא צפיפות, ואם אני רוצה הסתברות אני צריך לחשב שטח מתחת לעקומה או אינטגרל. ההסתברות שX קטן מאיזשהו X קטן היא אינטגרל על הצפיפות, ואין ביטוי סגור לאינטגרל הזה אלא קירוב נומרי שמרוב שהוא חוזר על עצמו בספרות יש לו סימון מיוחד פי, שהוא השטח מתחת להתפלגות הנורמלית סטנדרטית, אחרי שלקחנו את X, חיסרנו ממנו את התוחלת שלו וחילקנו בסטיית התקן.

להתפלגות הזאת יש כל מיני תכונות נורא יפות, למשל אני יודע ש95 אחוז מההסתברות או השטח נופל בצורה סימטרית בין התוחלת פחות שתי סטיות תקן לבין התוחלת ועוד שתי סטיות תקן. ועוד הרבה תכונות אחרות.

:::

ההתפלגות הדו-נורמלית, או בייוריאט נורמל, היא הצעד הבא. הפעם X שלנו הוא שני משתנים, או וקטור של שני משתנים. וכשאני מסמן שX מתפלג דו-נורמלית, עם תוחלת מיו ושונות סיגמה גדולה, אני מתכוון שהתוחלת שלו היא וקטור תוחלות באורך 2 מורכב מהתוחלת של X1 מיו1 והתוחלת של X2 מיו2, והשונות שלו היא מטריצת שונויות. על האלכסון יש את סיגמה בריבוע1 השונות של X1, וסיגמה בריבוע2 השונות של X2. ומחוץ לאלכסון יש את הקווריאנס בין שני המשתנים שאני מסמן כמתאם ביניהם רו כפול מכפלת סטיות התקן, תיכף ניזכר למה.

כשאני כותב דבר כזה, מסתבר שאני אומר משהו משמעותי אפילו יותר: שכל אחד מהאיקסים מתפלג באופן שולי כנורמלי עם התוחלת והשונות שלו, והכיוון ההפוך גם מתקיים. ולמה אני רושם את הקוורינאס שלהם בצורה כזאת? כי רו, מקדם המתאם בין שני המשתנים, שווה לקווריאנס ביניהם חלקי מכפלת סטיות התקן, זה על פי הגדרה. מהסיבה הזאת נהוג לרשום במטריצה רו כפול מכפלת סטיות התקן כדי להדגיש שיש כאן בעצם רק פרמטר אחד חדש שנוסף.

מה פונקצית הצפיפות של X, זאת בעצם פונקצית צפיפות משותפת לשני משתנים שרשומה כאן. אפשר לראות שהמבנה הכללי של הצפיפות הנורמלית למשתנה יחיד שבו אנחנו רואים קבוע כפול אקפסוננט, נשמר. ולמטה אנחנו ממש יכולים לראות את הפונקציה הזאת שיוצרת מעין גבעה פעמונית מעל X1 וX2. בצד ימין אנחנו רואים את הקונטורים של צפיפות שהפונקציה הזאת יוצרת כשמטילים אותה על המישור של X1 וX2. אפשר לראות שמדובר באליפסה. אם תסתכלו על הביטוי שבאקספוננט זה יהיה ברור יותר, ממש אפשר לפשט אותו לאליפסה. ואם המתאם הוא אפס, והשונויות שוות מה נראה? עיגול.

כשאנחנו רוצים לחשב הסתברות משותפת מפונקצית הצפיפות, לדוגמא מה הסיכוי שגם X1 קטן מאיזשהו ערך וX2 קטן מערך אחר, העיקרון של חישוב הסתברות מצפיפות נשמר, זה אינטגרל על כל אחד מהמשתנים עד הערכים המתאימים. רק שעכשיו אנחנו לא מחשבים שטח אלא נפח מתוך לגבעה.

:::

מההתפלגות הדו-נורמלית קל יותר להכליל להתפלגות הרב-נורמלית או המולטיוריאט-נורמל:

איקס שלנו הוא וקטור של משתנים באורך p, שיכול להיות די גדול. התוחלת והשונות שלו מסומנות אותו דבר.

רק שכעת התוחלת היא וקטור תוחלות באורך p, ומטריצת השונויות היא p על p. על האלכסון השונויות השוליות כמו מקודם, ומחוץ לאלכסון הקווריאנסים.

וכמו מקודם, אנחנו אומרים משהו חזק אפילו יותר, כל אחד מהמשתנים מתפלג באופן שולי נורמלית עם התוחלת והשונות שלו, ובין כל זוג משתנים יש קווריאנס או מתאם, ואפשר בצורה קומפקטית לרשום דווקא את המתאם כפרמטר נוסף.

פונקצית הצפיפות היא פונקצית צפיפות משותפת של וקטור של משתנים, והיא עדיין מהווה גבעה רק מעל מישור רב-מימדי, שאני לא יכול לצייר. וגם כאן ניתן לראות את המבנה הכללי של קבוע כפול אקספוננט.

חוץ מזה אני לא רושם את פונקציית ההסתברות המצטברת כי מדובר באינטגרל רב-מימדי על פונקצית הצפיפות אבל מדובר באותו עיקרון.

בואו נראה שהבנו: אם אנחנו רוצים לאמוד את מטריצת השונויות סיגמה, כמה פרמטרים בעצם חופשיים כאן אנחנו צריכים לאמוד? על האלכסון p שונויות, ויש לנו מתאם בין כל זוג משתנים רו-ג'יי-קיי נאמר. וזהו, המטריצה סימטרית, אגב חייבת להיות חיובית, כלומר בסך הכל p ועוד p מעל 2 פרמטרים, סדר גודל של p בריבוע.

:::

=== 3. Linear Discriminant Analysis ===

אחד המודלים הגנרטיביים הקלאסיים הוא ליניאר דיסקרימיננט אנליסיס, או LDA.

:::

ניזכר שוב בעיקרון, אנחנו רוצים לאמד את ההסתברות שY שייך לקלאס מסוים בהינתן התצפית שראינו, ואנחנו הופכים את זה לבעיה של אמידת ההסתברות השולית שY שייך לקלאס הזה, כפול ההסתברות של תצפית כמו שלנו בהינתן הקלאס המסוים.

סימנו את ההסתברות השולית כpi_k ואת ההסתברות המותנית כf_k והבנו שבשורה התחתונה המודל שלנו כשיקבל תצפית חדשה X0 יסווג אותה לקלאס הK שנותן את המקסימום על המכפלה במונה pi_kf_k.

אז מה ייחודי לLDA:

קודם כל את ההסתברויות האפריוריות לכל קלאס אנחנו נאמוד בצורה הכי פשוטה, כל אחת מהן תהיה פשוט השכיחות של הקלאס בנתונים. 

אבל חשוב יותר, אנחנו מניחים שכל אחת ההתפלגויות של כל קלאס f_k, היא התפלגות נורמלית, עם פרמטרים משלה. או X, וקטור של p משתנים בהינתן הקלאס הK, מתפלג רב נורמלית עם וקטור תוחלות משלו מיו-קיי, ומטריצת שונות סיגמה, כמו שלמדנו. אני לא רושם כאן MVN כי ככה נהוג, נורמלי זה נורמלי ואנחנו נבין מההקשר את המימד.

שימו לב, שבLDA אין אינדקס K ליד סיגמה, כלומר אנחנו מניחים מטריצת שונויות משותפת לכל הקלאסים.

האם אלה הנחות ריאליות? בודאי שלא תמיד, למשל אם אחד המשתנים בוקטור הוא בדיד -- איך הוא יכול להיות נורמלי? בפועל אנחנו רואים שLDA די רובסטי לעומת הפרות של ההנחה החזקה הזאת. מכל מקום חשוב להגיד שבעצם פירטתי לכם את כל המודל, השורה התחתונה רשומה כאן, רק חסר לנו לאמוד את K ההתפלגויות על כל הפרמטרים שלהן, את הפרופורציות pi_k, והחיזוי לכל תצפית יהיה הקלאס שעבורו המכפלה pi_kf_k היא המקסימלית.

:::

איך נראים נתונים שתואמים את הציפיות שלנו בLDA? הם נראים משהו כזה.

כאן יש לנו 3 קלאסים, על שני משתנים X1 וX2. ואני רואה שX מתפלג דומה מאוד בכל אחד מהקלאסים, רק במיקום שונה. ולא רק זה, שני המשתנים שלי הם רציפים והתצפיות יוצרות מעין אליפסה, מה שמתאים מאוד כמו שראינו למידול על ידי ההתפלגות הנורמלית. אז התוחלות, כלומר המיקומים, שונות, אבל הפיזור כלומר מטריצת השונויות סיגמה נראה דומה לכל הקלאסים -- אז הנתונים מתאימים למידול על ידי LDA.

איך אתם צופים שייראו קוי הגבול שנותן LDA בין הקלאסים? בדוגמא הזאת לא נראה שצריך משהו מתוחכם יותר מקוי החלטה ליניאריים, נכון? (להדגים) בין הקלאס האדום לכחול, בין הקלאס האדום לירוק, ובין הקלאס הכחול לירוק.

:::

בואו נראה באמת אילו גבולות החלטה נוצרים בין הקלאס הJ לקלאס הK.

למה שנבחר דווקא בקלאס K? כי ההסתברות המותנית שY שייך לקלאס הזה, גדולה יותר מההסתברות שY שייך לקלאס הJ. זה כבר ראינו תלוי רק במונים של נוסחת בייז שלנו שסימנו כpi_k_f_k, ואנחנו נראה שקל יותר להסתכל על לוג המכפלות האלה, כי מדובר בצפיפות הנורמלית ויש שם אקספוננט. והרי מדובר ביחס שקול לחלוטין.

אז אם נניח לרגע שאנחנו יודעים את הפרמטרים הדרושים לנו של ההתפלגות הנורמלית, pi_k, pi_j, mu_k, mu_j וסיגמה.

ניזכר שוב בצפיפות הרב-נורמלית של וקטור המשתנים X בהינתן קלאס K --ככה נראית הצפיפות שלנו f_k.

כלומר אם ניקח לוג על המכפלה pi_kf_k נגיע לביטוי שלפנינו: לוג של pi_k ועוד הביטוי שבאקספוננט ועוד איזשהו קבוע שאני אפילו לא רושם בפירוט כי הוא משותף גם לקלאס הk וגם לקלאס הj.

:::

אז להעדיף את קלאס k על פני j, משמעותו שהלוגריתם של קלאס k גדול מהלוגריתם של קלאס j.

וזה אומר האי-שווין שלפנינו, אני פשוט מעתיק את הביטוי שהגענו אליו לקלאס הK ולקלאס הJ. גם כאן נשים לב שיש ביטויים מיותרים (להדגים), אם נפתח את הסוגריים נקבל בשני האגפים את הביטוי x Sigma-inverse x ואפשר להשמיט אותו משני האגפים.

ואז, נעביר את הביטוי עבור הקלאס הJ לצד שני, נכנס איברים דומים, ונראה שמתקבל האי-שוויון הסופי שרשום לנו כאן. אם הביטוי שלפנינו גדול מאפס, נעדיף את קלאס k על פני קלאס j. אפשר לסמן את זה ככלל דלתא k גדול מj בקיצור. כלל שמאוד קל לחשב לכל תצפית חדשה שמגיעה, שנותן מספר ממשי סקלר שצריך להשוות לאפס.

איזו צורה מקבל גבול ההחלטה שלנו דלתא, לכל תצפית? בכוונה רשמתי אותו ככה. נשים לב שכל הביטוי שבסוגריים מרובעים, לא תלוי בוקטור הנתונים X בכלל. אפשר לחשוב עליו כעל סקלר חותך כלשהו. ואילו וקטור X עובר מכפלה פנימית עם וקטור אחר באורך p, הוקטור סיגמא בהופכי כפול המרחק בין התוחלות. ראינו כבר את הצורה הגיאומטרית הזאת גם ברגרסיה לוגיסטית, גם בSVM - זה מישור-על, הייפרפליין!! כל משתנה מוכפל פי מספר, מוסיפים חותך, ואם הביטוי מעל אפס, כלומר אנחנו מעל למישור, נבחר בקלאס k, ואם קטן מאפס, כלומר מתחת למישור, נבחר בקלאס J. זה אומר שגם האינטואיציה שלנו בדוגמא שראינו היא נכונה, כשX הוא שני משתנים והמישור הוא בעצם קו ליניארי, ועוד נחזור לוודא את זה.

:::

רק חוב קטן יש לנו. בפועל הפרמטרים של ההתפלגויות הנורמליות לכל משתנה ומשתנה לא ידועים לנו ואנחנו צריכים לאמוד אותן. מה זה לאמוד התפלגות? זה לאמוד את הפרמטרים שלה.

אלה הפרמטרים שלנו, יש לנו K וקטורים של תוחלות, כל אחד באורך p משתנים. K פרופורציות pi_k, ומטריצת שונויות בגודל p על p משותפת לקלאסים.

אומדים סטנדרטיים לפרמטרים שלנו מהדאטא, על חלקם גם אפשר להראות שהם אומדי נראות מקסימלית:

לפרופורציות, כמו שראינו זה פשוט שכיחות הקלאסים. לתוחלות, זה פשוט ממוצע תצפיות האיקס בכל קלאס, כלומר ממש סכום הוקטורים חלקי מספר תצפיות שיש בקלאס.

ואומד למטריצת השונויות הוא גם מעין אומד קווריאנס משוקלל על פני כל הקלאסים. כל אלמנט הוא בעצם המרחק של התצפית מהתוחלת שלה בריבוע ואנחנו סוכמים על כל התצפיות.

:::

לסיכום נחזור לדוגמא שלנו שהיא די אידיאלית ונריץ עליה LDA. הקווים השחורים הם כללי ההחלטה על-פי הפרמטרים האמיתיים, על-פיהם נוצר הדאטא, והקווים במקווקו הם כללי ההחלטה על-פי האומדנים לפרמטרים, שנאמדו מתוך הנתונים.

אפשר לראות שהאומדנים טובים מאוד, כלומר הקווים המקווקווים מאוד דומים לקווים הרציפים. ואפשר לראות כמובטח שמדובר בכללי החלטה ליניאריים בין כל שני קלאסים. מכאן אגב השם, לינאר דיסקרימיננט אנליסיס. עכשיו ברור שזו דוגמא אידאלית מדי. בחלק הבא נעשה רילקסציה לפחות להנחה אחת חשודה של LDA -- מטריצת שונות או פיזור זהה לכל הקלאסים? באמת?

:::

=== 4. Quadratic Discriminant Analysis ===

אז הנה דוגמא פשוטה של שני קלאסים ושוב אנחנו בשני משתנים, כשברור שהנחה של מטריצת קווריאנס זהה או פיזור זהה לשני הקלאסים, היא לא הנחה סבירה. בקלאס האדום נראה שיש מתאם מעט חיובי בין האיקסים, בקלאס הירוק נראה שיש מתאם שלילי, כשX2 עולה X1 יורד. יותר מזה, יש משהו קצת חשוד בקו הפרדה ליניארי בין שני הקלאסים, אם כבר היינו מצפים לקו קצת מעוגל (להדגים) משהו כזה. אנחנו נראה שכשאנחנו לא מניחים פיזור זהה בין הקלאסים, זה בדיוק מה שמתקבל.

חשוב להדגיש שזאת לא דוגמה צעצוע, ככה מתנהגים נתונים אמיתיים. אפילו בהשוואה בין חולים ובריאים, על משתנה אחד כמו לחץ דם, ונניח אפילו שהוא מתפלג נורמלית (להדגים). אז הוא גבוה יותר אצל החולים, אבל זה סביר שתהיה לו אותה שונות? לא כל כך, אוכלוסיית הבריאים כנראה מגוונת יותר, ובקרב אוכלוסיית החולים הריכוז סביב לחץ דם גבוה קטן יותר, יש כאן אולי גם אפקט תקרה, לחץ דם סיסטולי לכיוון 180, זה כבר בדרך של התקף לב. זאת דוגמא שממחישה למה לא נרצה להניח פיזור זהה בין הקלאסים.

:::

כשאנחנו לא מניחים מטריצת קווריאנס זהה, המודל נקרא קוואדרטיק דיסקרימיננט אנליסיס או QDA.

וזה בדיוק כל ההבדל, וקטור הנתונים שלנו מתפלג עדיין נורמלית אבל אנחנו מאפשרים לו מטריצת קווריאנס משלו סיגמא-קיי.

כדי לראות את כלל ההחלטה נתחיל מאותה נקודה: אנחנו נעדיף את קלאס K על קלאס J, אם ההסתברות האפוסטריורית לקלאס K בהינתן שראינו את הנתונים גדולה מההסתברות האפוסטריורית לקלאס J. וראינו שזה שקול לזה שלוג המכפלה pi_kf_k גדול מלוג המכפלה pi_jf_j.

הפעם נכתוב את הלוג הזה לכל אחד מהקלאסים בצורה קצת יותר מפורטת, כי יש פחות גורמים שאפשר להתעלם מהם: אנחנו עדיין בלוג ההסתברות האפריורית pi_k ועוד איזשהו קבוע, אחר, ועוד הביטוי שבאקספוננט ההתפלגות הנורמלית, רק שנשים לב שיש בו את סיגמה-קיי בהופכי, ולא סיגמה כללית. פחות חצי לוג הדטרמיננטה של סיגמה-קיי, שזה גורם שקודם הכללנו תחת הקבוע C, כי הוא היה זהה לכל הקלאסים, ועכשיו הוא תלוי במטריצת קווריאנס שונה אז אנחנו לא יכולים להתעלם.

אחרי קצת אלגברה ממש כמו שעשינו קודם בLDA, נגיע לכלל החלטה שאפשר לסמן בדלתא, שקלאס K עדיף על קלאס J, אם הביטוי שרשום כאן, גדול מאפס. אחרת, קלאס J עדיף.

וכמו קודם נשאל איזו מין צורה זו כלל ההחלטה הזה?

כל הביטוי שבסוגרים הוא קבוע אחד גדול. כאן יש לנו את הוקטור איקס במכפלה פנימית כפול וקטור של מקדמים מאוד דומה למה שהיה לנו קודם. והביטוי הראשון הוא המקבילה לביטוי שכולל את איקס בריבוע. יש כאן איקס טרנספוז כפול מטריצה בגודל p על p, כפול איקס. אפשר להראות שזה כמו סכום על קבוע כפול אלמנט Xj כפול אלמנט Xk ואנחנו עוברים על כל האלמנטים. לכן כל המודל נקרא קוואדרטיק.

:::

מבחינת אמידה, הכל נשאר אותו דבר, האמידה של הפרופורציות כמו שהיתה, האמידה של התוחלות בכל קלאס אותו דבר. ההבדל הוא שעכשיו אנחנו צריכים לאמוד מטריצת שונות לכל קלאס וקלאס, כמו בביטוי הזה. אנחנו צריכים K מטריצות כאלה.

וכשאנחנו מממשים את הנוסחה שהגענו אליה ומשרטטים את כלל ההחלטה, אכן מתקבל קו עקום או ריבועי כמו פרבולה, לא-ליניארי, שנראה קצת טבעי יותר לבעיה שלפנינו.

לפני שנעבור למודל האחרון, כמה פרמטרים אנחנו בעצם אומדים כאן ובLDA?

בLDA יש לנו K פרופורציות אבל הן מסתכמות ל1 אז זה K - 1 פרמטרים, ועוד K וקטורי תוחלות שבכל אחד יש p פרמטרים, ועוד מטריצת קווריאנס אחת שכבר אמרנו שמורכבת מp פרמטרים ועוד p מעל 2.

בQDA עד התוחלות הכל נשאר אותו דבר רק שאנחנו צריכים לאמוד K מטריצות כאלה!

מאחר שp הוא בדרך כלל המספר הגדול יותר, תחשבו על 10, 20 משתנים, זה יוצא שבQDA אנחנו אומדים סדר גודל של פי K משתנים! וזה אומר שבשביל שQDA יעבוד, כלומר נקבל אומדנים טובים עם שונות קטנה, אנחנו צריכים סדר גודל של פי K תצפיות. זה משהו לקחת בחשבון אם יש לכם מעט תצפיות, גם אם כלל ההחלטה נראה לא-ליניארי, לא בטוח בכלל שיש לנו מספיק תצפיות כדי לאמוד כלל החלטה מורכב כזה.

:::

=== 5. Naive Bayes ===

המודל הגנרטיבי האחרון שנלמד היום הוא נאיב בייז. נאיב בייז הוא עוד דוגמא למודל שהתפתח במדעי המחשב עקב הצורך לענות על  בעיות ממימד גבוה. והאמת היא שעשינו כבר כל כך הרבה עבודה שייקח לנו מעט מאמץ לתאר אותו.

:::

שוב ניזכר מה אנחנו עושים כאן עם ההסתברות האפוסטריורית לקלאס K בהינתן שראינו את הנתונים, כשאמרנו שכל ההבדל בין המודלים שנראה היום הוא איך אנחנו מפרטים את f_k, ההתפלגות של X בהינתן הקלאס הK.

בנאיב בייז עדיין הpi_k הם אותו דבר אלא אם כן יש לנו הצדקה לתת פריורים אחרים.

ובתוך כל קלאס, אנחנו מניחים הנחה חזקה, שהמשתנים הם בלתי תלויים.

אם המשתנים הם בלתי תלויים, זה אומר שאנחנו יכולים לאמוד את ההתפלגות של כל אחד מהם בצורה נפרדת, ואז ההתפלגות המשותפת היא מכפלת ההתפלגויות. אין צורך להתאים כאן התפלגות משותפת רב-נורמלית, עם המון פרמטרים של מתאם בין כל זוג משתנים וכולי.

יותר מזה, אנחנו יכולים לטפל בצורה טבעית יותר במשתנים רציפים לעומת בדידים. אם המשתנים שלנו רציפים נתאים להם התפלגות רציפה, למשל נורמלית כמו מקודם, כשכל מה שאנחנו צריכים לאמוד זה שני פרמטרים למשתנה ולקלאס, תוחלת ושונות. אבל אז יש לנו עוד אפשרויות אולי טובות יותר, כמו התפלגות אקספוננציאלית ואולי בכלל אומדן אפרמטרי להתפלגות שנשיג עם kernel density estimation או KDE.

ואם המשתנים שלנו בדידים עם מעט ערכים אפשר לאמוד את ההתפלגות שלהם פשוט עם ההתפלגות האמפירית -- השכיחויות של הערכים במדגם שלנו. לדוגמא X הוא משתנה שמקבל שני ערכים 1 או 2, ואנחנו אומדים את ההתפלגות הבדידה הזאת עם השכיחות במדגם הלמידה, 20 אחוז לערך 1 ו80 אחוז לערך 2. מכל מקום זה הכל, אמרנו בעצם את כל מה שצריך. ברגע שהתאמנו התפלגות לכל משתנה, אנחנו צריכים לאמוד את הפרמטרים שלה לכל קלאס, והקלאס שנתאים לתצפית חדשה, כמו מקודם, הוא הקלאס שעבורו pi_k_f_k מגיע למקסימום.

שתי שאלות חשובות על נאיב בייז שאולי גם שאלתם את עצמכם:

אני באמת מאמין שהמשתנים בדאטא הם בלתי תלויים? לדוגמא בדאטא שבו גיל והשכלה מסבירים גובה, אפשר לטעון שגיל בלתי תלוי בהשכלה? לא ממש. אז במה המודל הזה טוב, מתי הוא יביא לביצועים מרשימים? רמזתי לזה קודם, בבעיות עם p גדול מאוד, בעיות ממימד גבוה. זה גם היה השימוש שלו הרבה שנים, לדוגמא בבעיה שצריך לסווג מייל, שהוא רצף של מילים, לספאם ולא-ספאם. כל מילה היא משתנה. וזה אולי לא סביר שכל מילה בלתי תלויה בכל מילה אחרת, אבל יש כל כך הרבה מילים, שנאיב בייז הוא מודל יחסית מאוד מאוד פשוט לתקל בכלל את הבעיה הזאת. אומדים הסתברות לכל מילה ומילה שתופיע, וההסתברות של כל קובץ הטקסט הוא מכפלת כל ההסתברויות. פשוט ועבד לא רע עד לפני כמה שנים.

שאלה שנייה היא האם אין קשר בין נאיב בייז לLDA ולQDA? ברור שיש. אפשר להראות שאם אנחנו מניחים שכל המשתנים בנאיב בייז מתפלגים נורמלית, כל אחד עם שונות משלו, אז זה בדיוק כמו לעשות QDA, ולהניח שמטריצת השונות היא אלכסונית, כלומר אפס משני צידי האלכסון, אין מתאם בין המשתנים, ועל,האלכסון השונות של כל משתנה ומשתנה, בקלאס הK.

ואם המטריצה היא גם אלכסונית וגם לכל קלאס יש את אותם שונויות - זה בדיוק מטריצה שווה לכל הקלאסים, וככה מגיעים מנאיב בייז לLDA. בחלק האחרון נראה קשר אפילו פחות טריוויאלי. הנקודה היא שבטיעונים מתמטיים פשוטים אפשר לראות קשרים בין שיטות שונות שהמוטיבציה להן ולפעמים אפילו השנים בהן הן נוסחו שונות  לחלוטין.

:::

=== 6. השוואת קלסיפיירים ===

בואו נשווה בין השיטות. קודם כל על נתונים אמיתיים של הפציינטים הדרום אפריקאיים שאנחנו מנסים לחזות אם פציינט יחלה במחלת לב או לא, אפשר לראות שהשיטות נותנות תוצאות די דומות אחת לשנייה ולרגרסיה לוגיסטית, כאן ספציפית LDA השיג על מדגם הטסט את הAUC הכי גבוה.

:::

אבל השוואה מעניינת יותר היא השוואה אנליטית. למשל בין רגרסיה לוגיסטית לLDA. אנחנו יודעים ששני המודלים האלה הם ליניאריים, אבל הם אפילו דומים יותר ממה שנראה. נתמקד בשני קלאסים, אבל אפשר להראות שהקשר מתקיים גם עבור כל K וההכללה של רגרסיה לוגיסטית לK גדול מ2.

ברגרסיה לוגיסטית אנחנו ממדלים את הלוגיט של ההסתברות של Y להיות 1 בהינתן הנתונים. אבל מה זה הלוגיט, הלוג של p חלקי 1 פחות p. אבל מה זה לוג של p חלקי 1 פחות p, זה בדיוק הלוג של יחס הסיכויים של ההסתברות להיות 1 חלקי ההסתברות להיות 0. זה מה שאנחנו ממדלים כמודל לינארי, ואנחנו מחפשים חותך ווקטור מקדמים בטא באורך p, שימקסמו קריטריון של נראות מקסימלית.

מה זה לוג יחס הסיכויים בLDA? זה ממש לוג יחס ההסתברויות המותנות שלנו, כלומר מה שאנחנו סימנו כpi_1f_1 לpi_0f_0. אפשר לפתח את זה עוד, לוג של מנה זה הפרש הלוגים.

אבל מה זה הפרש הלוגים? זה בדיוק מה שפיתחנו עם ההתפלגויות הנורמליות! והגענו למסקנה שאנחנו רוצים לחשב לכל תצפית את הביטוי שרשום כאן, שהוא ביטוי סגור, והוא מישור. מתי נחזה דווקא את קלאס 1 על פני קלאס 0? אם נהיה מעל המישור, כלומר הביטוי גדול מאפס, או אם תחזרו אחורה יחס הסיכויים גדול מ1 או ההסתברות החזויה לקלאס 1 היא גדולה מחצי.

מכל מקום מה שרשום כאן כבר הבנו זה חותך, ביטוי מאוד מורכב אבל סקלאר, ועוד מכפלה פנימית של וקטור X כפול וקטור של מקדמים.

אפשר לראות שזה דומה מאוד, שתי השיטות מחפשות כלל החלטה לינארי אבל כזה שממדל את אותו הדבר! ולכן אף על פי שלא נקבל תוצאה זהה כי הקריטריונים שלהם שונים, נקבל תוצאות דומות מאוד. ניתן לראות שאם ההנחות של LDA מתקיימות אז אפשר להגיע בצורה יעילה יותר עם LDA לקו הרצוי כלומר עם הרבה פחות תצפיות.

:::

בצורה דומה אפשר לפתח את הלוגיט או לוג יחס הסיכויים גם לQDA וגם לנאיב בייז.

מה שתקבלו בQDA הוא יחס החלטה קוואדרטי כמו שראינו, ומה שתקבלו בנאיב בייז הוא מה שנקרא מודל אדיטיבי, של חותך ועוד סכום של פונקציות טאו-ג'יי לכל משתנה Xj. והנה קיבלנו עוד מובן של הקשר בין LDA ונאיב בייז, אם הפונקציות האלה הן אותן הפונקציות שמביאות לוקטור המקדמים אלפא של LDA, ניתן לראות בLDA כמקרה פרטי ממש של נאיב בייז.

נסיים את השיעור שלנו על מודלים גנרטיביים כאן. אני מקווה שהמסע שלכם בעולם המרתק הזה לא נגמר, כי כאמור אלה ממש הצעדים הראשונים כדי להבין איך מודלים משוכללים כמו דאלי מייצרים לנו תמונות על פי בקשה, ואפילו סרטונים. למעשה, אני יודע שהמסע שלכם בעולם המודלים הגנרטיביים לא נגמר כי כבר בשיעור הבא יתברר לנו, שאחד המודלים שאנחנו כבר די מכירים ובצורה מאוד נוחה לא הזכרנו כאן, הוא גם מודל גנרטיבי! על כך בשיעור הבא.

:::
