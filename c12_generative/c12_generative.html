<!DOCTYPE html>
<html lang="en"><head>
<script src="../libs/clipboard/clipboard.min.js"></script>
<script src="../libs/quarto-html/tabby.min.js"></script>
<script src="../libs/quarto-html/popper.min.js"></script>
<script src="../libs/quarto-html/tippy.umd.min.js"></script>
<link href="../libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.554">

  <title>Generative Models</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="../slides_quarto.css">
  <link href="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  

    <link rel="icon" href="../Intro2SL_logo.jpg" type="image/jpg"> 

    <link rel="shortcut icon" href="../Intro2SL_logo.jpg" type="image/jpg">

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css">

  </head>

<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section id="section" class="slide level2 logo-slide">
<h2></h2>
</section>
<section id="introduction-to-statistical-learning" class="slide level2 title-slide center">
<h2>Introduction to Statistical Learning</h2>
<h3 id="generative-models---class-12">Generative Models - Class 12</h3>
<h3 id="giora-simchoni">Giora Simchoni</h3>
<h4 id="gsimchonigmail.com-and-add-intro2sl-in-subject"><code>gsimchoni@gmail.com</code> and add <code>#intro2sl</code> in subject</h4>
<h3 id="stat.-and-or-department-tau">Stat. and OR Department, TAU</h3>
</section>
<section id="intro.-to-generative-models" class="slide level2 title-slide center">
<h2>Intro. to Generative Models</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בשיעור הזה נעסוק במודלים גנרטיביים. מה ההבדל בין מודלים גנרטיביים למודלים שלמדנו עד עכשיו? מיד נדע. אבל קודם, נראה שהיום אנחנו עושים את הצעד הראשון בלהבין את אחת המהפכות שבודאי כולם שמעו עליהן, והיא היכולת שלנו לייצר או לג’נרט דאטא חדש, כשסוג הדאטא בו עשינו את הקפיצה המרשימה ביותר הוא: תמונות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="generated-images">Generated images</h3>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="eigenface.png" height="400"></p>
<figcaption><em>Face recognition using eigenfaces</em> by Turk &amp; Pentland (1991)</figcaption>
</figure>
</div>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="dalle_cat_dog_hugging.png" height="400"></p>
<figcaption>Image generated using DALL·E by OpenAI with: “a realistic image of a dog and a cat hugging in front of the eiffel tower”</figcaption>
</figure>
</div>
</div>
</div>
<div class="fragment">
<p>How did we get here?</p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מצד שמאל אנחנו רואים תמונה ספציפית מאוד שיכולנו לייצר כבר בשנות התשעים של המאה הקודמת. באמצעות למידת ההתפלגות מתמונות של אלפי פרצופים, מצאנו דרך לייצר פרצופים אקראיים, או האייגנפייסז. מצד ימין אנחנו רואים כבר את מה שאנחנו יכולים לבצע היום. את התמונה הזאת ביקשתי מדאלי, מחולל התמונות של OpenAI, כשכל מה שאמרתי לו זה תיצור לי בבקשה תמונה ריאליסטית של כלב וחתול מתחבקים על רק מגדל אייפל.</p>
<p>איך הגענו בשלושים שנה מזה לזה? זאת הרי קפיצה מטורפת. אז צר לי לאכזב, עוד לא נוכל לקבל תשובה בקורס שלנו, תצטרכו לקחת קורס מתקדם יותר ברשתות נוירונים. אבל אני טוען שמה שנלמד היום הוא צעד ראשון וחשוב מאוד בכיוון.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="discriminative-models">Discriminative models</h3>
<ul>
<li>We are constanly trying to <em>model</em> <span class="math inline">\(E(Y|X = x)\)</span> (regression) or <span class="math inline">\(P(Y|X = x)\)</span> (Bayes classifier):</li>
</ul>
<div class="fragment">
<ul>
<li>Linear:
<ul>
<li>linear regression, ridge, lasso</li>
<li>logistic regression</li>
</ul></li>
<li>Non-linear:
<ul>
<li>KNN</li>
<li>Trees, RF, Boosting</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Even our Bias-variance tradeoff analysis assumed a Fixed-<span class="math inline">\(X\)</span> scenario!</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>למודלים שלמדנו עד היום אפשר לקרוא מודלים דיסקרימינטיביים. אם זה ברגרסיה, הם מנסים למדל את התוחלת המותנית של Y בהינתן הנתונים X. אם זה בקלסיפיקציה, אפשר לפרט יותר ולהגיד שהם מנסים למדל את ההסתברות שY שווה לאיזשהו קלאס בהינתן הנתונים X. כלומר הם מנסים להבחין, טו דיסקרימינייט, בין קלאסים שונים. ואם תיזכרו בתחילת הקורס עשינו את זה כי הבנו שבמובן מסוים אם נדע למדל את ההסתברות המותנית הזאת ונדע איך להביא אותה למקסימום זה קריטריון אופטימלי שנקרא בייס קלסיפייר, לעשות קלסיפיקציה טובה.</p>
<p>אם אלה מודלים ליניאריים, דיברנו על רגרסיה ליניארית כשY רציף, רגרסית רידג’, לאסו. אם זה קלסיפיקציה דיברנו על רגרסיה לוגיסטית או סאפורט וקטור קלסיפייר.</p>
<p>אם אלה מודלים לא לינאריים, דיברנו על KNN לקלסיפיקציה ולרגרסיה, על עצי החלטה ואנסמבלים שלהם כמו בוסטינג ורנדום פורסט.</p>
<p>כולם כולם, מניחים שX נתון, הוא קבוע, ומנסים למדל את התוחלת המותנית של Y, רציף או בדיד, על הנתונים האלה. מתוך הנחה, או תקווה, שנתונים שהמודל לא ראה דומים לX שלנו.</p>
<p>אפילו בדיון שלנו על הביאס-וריאנס טריידאוף, הפיתוחים שלנו היו יחסים פשוטים והניחו שX המדגם למידה שלנו הוא נתון. על מה לא נתנו דגש? לא נתנו דגש על ההתפלגות של הנתונים עצמם, של X!</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="generative-models">Generative models</h3>
<p>But for classification: <span class="math display">\[P(Y | X) = \frac{P(X, Y)}{P(X)}\]</span></p>
<div>
<ul>
<li class="fragment"><p><span style="color:red;">Generative</span> models focus on modeling the joint distribution <span class="math inline">\(P(X, Y)\)</span>, or more specifically: <span class="math display">\[P(Y | X) = \frac{P(X, Y)}{P(X)} = \frac{P(Y)P(X|Y)}{P(X)} \propto P(Y)P(X|Y)\]</span></p></li>
<li class="fragment"><p>Focus on the mechanism which <strong>generated</strong> the data, not just the data</p></li>
<li class="fragment"><p>Especially useful when <span class="math inline">\(n\)</span> is small</p></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>באופן יותר פורמלי, לקלסיפיקציה, אם מעניין אותנו ההסתברות של Y בהינתן X, משפט בייז אומר לנו שהיא שווה להסתברות המשותפת של X וY, חלקי ההסתברות השולית של X.</p>
<p>מודלים גנרטיביים, עושים שיפט לפוקוס בדיוק לאלמנט הזה, ההתפלגות המשותפת של X הנתונים ושל המשתנה התלוי Y. ונרחיב: ההסתברות המשותפת של X ושל Y שווה להסתברות השולית או האפריורית של Y, כפול ההסתברות של X בהינתן שראיתי את Y. ומאחר שתיכף נרצה את ההסתברות הזאת לכל מיני Yים, אנחנו נראה שהמכנה הוא זהה לכולם וניתן הדגש בעיקר למונה. הכי הרבה מידע נקבל אם נצליח למדל את ההתפלגות המשותפת, אבל אם נצליח למדל היטב את שתי ההסתברויות האלה, האפריורית שY שווה לקלאס מסוים והתפלגות הנתונים בקרב התצפיות עם הקלאס הזה, זה אומר שהצלחנו למדל היטב את המטרה ההתחלתית שלנו, ההתפלגות המשותפת, ולבסוף הבייס קלסיפייר.</p>
<p>גישה שונה לגמרי: לתת פוקוס על המנגנון שמג’נרט את הדאטא, שיוצר אותו, לא רק על הדאטא כנתון, כקבוע.</p>
<p>אנחנו נראה, שהדבר יעיל במיוחד כשהדאטא הוא מועט בכל קלאס.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="classification-using-generative-models">Classification using generative models</h3>
<div>
<ul>
<li class="fragment"><p>Even more specifically, if <span class="math inline">\(Y \in \{1, \dots, K\}\)</span>: <span class="math display">\[P(Y = k|X = x) = \frac{P(Y = k)P(X = x|Y = k)}{P(X = x)} = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}\]</span></p></li>
<li class="fragment"><p>We focus on estimating <span class="math inline">\(\pi_k, f_k(x)\)</span></p></li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>And if we are so good at estimaing <span class="math inline">\(f_k(x) = P(X = x|Y = k)\)</span> why not generate more!</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אפשר להרחיב את הרעיון הכללי אפילו יותר:</p>
<p>נניח שY הוא אחד מK קלאסים, 1 עד K. ההסתברות הפוסטריורית שמעניינת אותנו היא ההסתברות שY שייך דווקא לקלאס הK בהינתן תצפית חדשה שמגיעה. ואנחנו הופכים את הבעיה ושמים דגש על ההסתברות האפריורית של הקלאס הK באופן כללי כפול ההתפלגות של הנתונים שלנו בהינתן שראינו את הקלאס הK. נסמן את ההסתברות האפריורית כpi_k ואת ההסתברות המותנית של X כf_k. וכעת המונה שלנו הוא המכפלה הפשוטה pi_kf_k, והמכנה מנוסחת ההסתברות השלמה הוא סכימה על כל האפשרויות.</p>
<p>אנחנו שמים דגש על אמידה של pi_k ושל f_k, וזה בעצם הדבר היחיד שמבדיל בין שלוש השיטות שנלמד היום. חוץ מזה המטרה זהה בשלושתן.</p>
<p>אז איך מגיעים מכאן לג’נרוט של תמונות? המטרה של חוקרים באמת היתה רגרסיה או קלסיפיקציה, אולם בסופו של דבר הגענו למודלים כל כך חזקים כדי לתאר את f_k, התפלגות הנתונים בהינתן הקלאס. הצלחנו לתאר נתונים כל כך מגוונים ועשירים כמו תמונות. שכיוון מסוים במחקר אמר רגע רגע – אם אנחנו יודעים לתאר כל כך יפה את ההתפלגות של נתונים של תמונות, למה שלא נייצר עוד? תמונות רנדומליות או תמונות על-פי בקשה, על-פי תיאור.</p>
<p>רובנו משתמשים בתמונות של דאלי עדיין בתור קוריוז, אבל ליכולת הזאת שלנו לג’נרט נתונים יש השלכות אדירות על עתיד המחקר, על פרסום ועל תקשורת. אז בואו נעשה היום את הצעד הראשון להבין איך הגענו למודלים החזקים האלה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="detour-multivariate-normal" class="slide level2 title-slide center">
<h2>Detour: multivariate normal</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>לפני שנלמד איזשהו מודל גנרטיבי, מייד תשימו לב שהם עושים שימוש נרחב בהתפלגות הנורמלית. ואני לא מדבר רק על התפלגות הפעמון של משתנה יחיד, אנח מדבר על ההתפלגות הרב-נורמלית, של וקטור של משתנים, שיש לה המון תכונות נורא יפות. אז כדאי ליישר קו, חשוב קודם לוודא שכולנו מכירים את ההתפלגות הרב-נורמלית ומה משמעות הפרמטרים שלה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="univariate-normal">Univariate normal</h3>
<ul>
<li><span class="math inline">\(X \in \mathbb{R} \sim \mathcal{N}(\mu, \sigma^2) \Rightarrow E(X) = \mu; \quad V(X) = \sigma^2 &gt; 0\)</span></li>
<li><span class="math inline">\(f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)\)</span></li>
<li><span class="math inline">\(F(x) = P(X \le x) = \int_{-\infty}^x f(x)dx = \Phi\left(\frac{x - \mu}{\sigma}\right)\)</span></li>
</ul>
<div id="728d194e" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c12_generative_files/figure-revealjs/cell-2-output-1.png" width="374" height="356"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נתחיל בהתפלגות הנורמלית למשתנה יחיד: X רציף, יכול לקבל ערכים על כל הישר הממשי, מתפלג נורמלית עם פרמטרים מיו וסיגמה בריבוע. כשאני אומר את זה אני מתכוון להתפלגות סימטרית פעמון סביב תוחלת מיו, עם שונות סיגמה בריבוע.</p>
<p>פונקצית הצפיפות של X ניתנת לפי הנוסחה הזאת, אפשר לוודא שהיא תמיד חיובית ושהאינטגרל מתחת לכל העקומה נותן שטח 1.</p>
<p>והפונקציה הזאת היא כאמור לא הסתברות היא צפיפות, ואם אני רוצה הסתברות אני צריך לחשב שטח מתחת לעקומה או אינטגרל. ההסתברות שX קטן מאיזשהו X קטן היא אינטגרל על הצפיפות, ואין ביטוי סגור לאינטגרל הזה אלא קירוב נומרי שמרוב שהוא חוזר על עצמו בספרות יש לו סימון מיוחד פי, שהוא השטח מתחת להתפלגות הנורמלית סטנדרטית, אחרי שלקחנו את X, חיסרנו ממנו את התוחלת שלו וחילקנו בסטיית התקן.</p>
<p>להתפלגות הזאת יש כל מיני תכונות נורא יפות, למשל אני יודע ש95 אחוז מההסתברות או השטח נופלת בצורה סימטרית בין התוחלת פחות שתי סטיות תקן לבין התוחלת ועוד שתי סטיות תקן. ועוד הרבה תכונות אחרות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="bivariate-normal">Bivariate normal</h3>
<div>
<ul>
<li class="fragment"><span class="math inline">\(X \in \mathbb{R}^2 \sim \mathcal{BVN}(\mathbf{\mu}, \mathbf{\Sigma}) \Rightarrow E(X) = \mu = \begin{pmatrix}\mu_1 \\ \mu_2\end{pmatrix}; \quad V(X) = \mathbf{\Sigma} = \begin{pmatrix}\sigma_1^2 &amp; \rho\sigma_1\sigma_2 \\ \rho\sigma_1\sigma_2 &amp; \sigma_2^2\end{pmatrix}\)</span></li>
<li class="fragment"><span class="math inline">\(\Leftrightarrow X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2), X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2); \quad \rho = \text{Cor}(X_1, X_2) = \frac{\text{Cov}(X_1, X_2)}{\sigma_1\sigma_2}\)</span></li>
<li class="fragment"><span class="math inline">\(f(x) = f(x_1, x_2) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \exp\left( -\frac{1}{2(1 - \rho^2)} \left[ \frac{(x_1 - \mu_1)^2}{\sigma_1^2} + \frac{(x_2 - \mu_2)^2}{\sigma_2^2} - \frac{2\rho(x_1 - \mu_1)(x_2 - \mu_2)}{\sigma_1 \sigma_2} \right] \right)\)</span></li>
<li class="fragment"><span class="math inline">\(F(x) = P(X_1 \le x_1, X_2 \le x_2) = \int_{-\infty}^{x_2}\int_{-\infty}^{x_1}f(x_1, x_2)dx_1 dx_2\)</span></li>
</ul>
</div>
<div id="783f583f" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c12_generative_files/figure-revealjs/cell-3-output-1.png" width="675" height="307"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ההתפלגות הדו-נורמלית, או בייוריאט נורמל, היא הצעד הבא. הפעם X שלנו הוא שני משתנים, או וקטור של שני משתנים. וכשאני מסמן שX מתפלג דו-נורמלית, עם תוחלת מיו ושונות סיגמה גדולה, אני מתכוון שהתוחלת שלו היא וקטור תוחלות באורך 2 מורכב מהתוחלת של X1 מיו1 והתוחלת של X2 מיו2, והשונות שלו היא מטריצת שונויות. על האלכסון יש את סיגמה בריבוע1 השונות של X1, וסיגמה בריבוע2 השונות של X2. ומחוץ לאלכסון יש את הקווריאנס בין שני המשתנים שאני מסמן כמתאם ביניהם רו כפול מכפלת סטיות התקן, תיכף ניזכר למה.</p>
<p>כשאני כותב דבר כזה מסתבר, אני אומר משהו משמעותי אפילו יותר: שכל אחד מהאיקסים מתפלג באופן שולי כנורמלי עם התוחלת והשונות שלו, והכיוון ההפוך גם מתקיים. ולמה אני רושם את הקוורינאס שלהם בצורה כזאת? כי רו, מקדם המתאם בין שני המשתנים, שווה לקווריאנס ביניהם חלקי מכפלת סטיות התקן, זה על פי הגדרה. מהסיבה הזאת נהוג לרשום במטריצה רו כפול מכפלת סטיות התקן כדי להדגיש שיש כאן בעצם רק פרמטר אחד חדש שנוסף.</p>
<p>מה פונקצית הצפיפות של X, זאת בעצם פונקצית צפיפות משותפת לשני משתנים שרשומה כאן. אפשר לראות שהמבנה הכללי של הצפיפות הנומרלית למשתנה יחיד שבו אנחנו רואים קבוע כפול אקפסוננט, נשמר. ולמטה אנחנו ממש יכולים לראות את הפונקציה הזאת שיוצרת מעין גבעה פעמונית מעל X1 וX2. בצד ימין אנחנו רואים את הקונטורים של צפיפות שהפונקציה הזאת יוצרת כשמטילים אותה על המישור של X1 וX2. אפשר לראות שמדובר באליפסה. אם תסתכלו על הביטוי שבאקספוננט זה יהיה ברור יותר, ממש אפשר לפשט אותו לאליפסה. ואם המתאם הוא אפס, והשונויות שוות מה נראה? עיגול.</p>
<p>כשאנחנו רוצים לחשב הסתברות משותפת מפונקצית הצפיפות, לדוגמא מה הסיכוי שגם X1 קטן מאיזשהו ערך וX2 קטן מערך אחר, העיקרון של חישוב הסתברות מצפיפות נשמר, זה אינטגרל על כל אחד מהמשתנים עד הערכים המתאימים. רק שעכשיו אנחנו לא מחשבים שטח אלא נפח מתוך לגבעה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="multivariate-normal">Multivariate normal</h3>
<div>
<ul>
<li class="fragment"><span class="math inline">\(X \in \mathbb{R}^p \sim \mathcal{MVN}(\mathbf{\mu}, \mathbf{\Sigma})\)</span></li>
<li class="fragment"><span class="math inline">\(E(X) = \mu = \begin{pmatrix}\mu_1 \\ \vdots \\ \mu_p\end{pmatrix}; \quad V(X) = \mathbf{\Sigma} = \begin{pmatrix}\sigma_1^2 &amp; \dots &amp; \rho_{1,p}\sigma_1\sigma_p \\ \vdots &amp; \ddots &amp; \vdots \\ \rho_{1, p}\sigma_{1}\sigma_{p} &amp; \dots &amp; \sigma_p^2\end{pmatrix}\)</span></li>
<li class="fragment"><span class="math inline">\(\Leftrightarrow X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2), \dots, X_p \sim \mathcal{N}(\mu_p, \sigma_p^2); \quad \rho_{j,k} = \text{Cor}(X_j, X_k) = \frac{\text{Cov}(X_j, X_k)}{\sigma_j\sigma_k}\)</span></li>
<li class="fragment"><span class="math inline">\(f(x) = f(x_1, \dots, x_p) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x-\mu)\right)\)</span></li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>How many unique params in <span class="math inline">\(\mathbf{\Sigma}\)</span>?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מההתפלגות הדו-נורמלית קל יותר להכליל להתפלגות הרב-נורמלית או המולטיוריאט-נורמל:</p>
<p>איקס שלנו הוא וקטור של משתנים באורך p, שיכול להיות די גדול. התוחלת והשונות שלו מסומנות אותו דבר.</p>
<p>רק שכעת התוחלת היא וקטור תוחלות באורך p, ומטריצת השונויות היא p על p.&nbsp;על האלכסון השונויות השוליות כמו מקודם, ומחוץ לאלכסון הקווריאנסים.</p>
<p>וכמו מקודם, אנחנו אומרים משהו חזק אפילו יותר, כל אחד מהמשתנים מתפלג באופן שולי נורמלית עם התוחלת והשונות שלו, ובין כל זוג משתנים יש קווריאנס או מתאם, ואפשר בצורה קומפקטית לרשום דווקא את המתאם כפרמטר נוסף.</p>
<p>פונקצית הצפיפות היא פונקצית צפיפות משותפת של וקטור של משתנים, והיא עדיין מהווה גבעה רק מעל מישור רב-מימדי, שאני לא יכול לצייר. וגם כאן ניתן לראות את המבנה הכללי של קבוע כפול אקספוננט.</p>
<p>חוץ מזה אני לא רושם את פונקציית ההסתברות המצטברת כי מדובר באינטגרל רב-מימדי על פונקצית הצפיפות אבל מדובר באותו עיקרון.</p>
<p>בואו נראה שהבנו: אם אנחנו רוצים לאמוד את מטריצת השונויות סיגמה, כמה פרמטרים בעצם חופשיים כאן אנחנו צריכים לאמוד? על האלכסון p שונויות, ויש לנו מתאם בין כל זוג משתנים רו-ג’יי-קיי נאמר. וזהו, המטריצה סימטרית, אגב חייבת להיות חיובית, כלומר בסך הכל p ועוד p מעל 2 פרמטרים, סדר גודל של p בריבוע.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linear-discriminant-analysis" class="slide level2 title-slide center">
<h2>Linear Discriminant Analysis</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אחד המודלים הגנרטיביים הקלאסיים הוא ליניאר דיסקרימיננט אנליסיס, או LDA.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="linear-discriminant-analysis-lda">Linear discriminant analysis (LDA)</h3>
<ul>
<li><p>Recall: <span class="math display">\[P(Y = k|X = x) = \frac{P(Y = k)P(X = x|Y = k)}{P(X = x)} = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}\]</span></p></li>
<li><p><span class="math inline">\(\hat{f}(x_0) = \arg\max_k \pi_k f_k(x_0)\)</span></p></li>
</ul>
<div>
<ul>
<li class="fragment">In LDA:
<ul>
<li class="fragment"><span class="math inline">\(\pi_k\)</span> are priors, or (spoiler): <span class="math inline">\(\hat{\pi}_k = \frac{\sum_{i = 1}^n \mathbb{I}\left[Y_i = k\right]}{n}\)</span></li>
<li class="fragment"><span class="math inline">\(f_k(x)\)</span> are multivariate Gaussian, or: <span class="math inline">\(X | Y = k \sim \mathcal{N}(\mu_k, \Sigma)\)</span></li>
<li class="fragment">Notice the covariance matrix <span class="math inline">\(\Sigma\)</span> is the same <span class="math inline">\(\forall k\)</span></li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ניזכר שוב את העיקרון, אנחנו רוצים לאמד את ההסתברות שY שייך לקלאס מסוים בהינתן התצפית שראינו, ואנחנו הופכים את זה לבעיה של אמידת ההסתברות השולית שY שייך לקלאס הזה, כפול ההסתברות של תצפית כמו שלנו בהינתן הקלאס המסוים.</p>
<p>סימנו את ההסתברות השולית כpi_k ואת ההסתברות המותנית כf_k והבנו שבשורה התחתונה המודל שלנו כשיקבל תצפית חדשה X0 יסווג אותה לקלאס הK שנותן את המקסימום על על המכפלה במונה pi_kf_k.</p>
<p>אז מה ייחודי לLDA:</p>
<p>קודם כל את ההסתברויות האפריוריות לכל קלאס אנחנו נאמוד בצורה הכי פשוטה, כל אחת מהן תהיה פשוט השכיחות של הקלאס בנתונים.</p>
<p>אבל חשוב יותר, אנחנו מניחים שכל אחת ההתפלגויות של כל קלאס f_k, היא התפלגות נורמלית, עם פרמטרים משלה. או X, וקטור של p משתנים בהינתן הקלאס הK, מתפלג רב נורמלית עם וקטור תוחלות משלו מיו-קיי, ומטריצת שונות סיגמה, כמו שלמדנו.</p>
<p>שימו לב, שבLDA אין אינדיקס K ליד סיגמה, כלומר אנחנו מניחים מטריצת שונויות משותפת לכל הקלאסים.</p>
<p>האם אלה הנחות ריאליות? בודאי שלא תמיד, למשל אם אחד המשתנים בוקטור הוא בדיד – איך הוא יכול להיות נורמלי? בפועל אנחנו רואים שLDA די רובסטי לעומת הפרות של ההנחה החזקה הזאת. מכל מקום חשוב להגיד שבעצם פירטתי לכם את כל המודל, השורה התחתונה רשומה כאן, רק חסר לנו לאמוד את K ההתפלגויות על כל הפרמטרים שלהם, את הפרופורציות pi_k, והחיזוי לכל תצפית יהיה הקלאס שעבורו המכפלה pi_kf_k היא המקסימלית.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="lda-what-to-expect">LDA: What to expect</h3>
<div id="cd714a9c" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c12_generative_files/figure-revealjs/cell-4-output-1.png" width="427" height="429"></p>
</figure>
</div>
</div>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>What do you expect the decision rule(s) to look like?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>איך נראים נתונים שתואמים את הציפיות שלנו בLDA? הם נראים משהו כזה.</p>
<p>כאן יש לנו 3 קלאסים, על שני משתנים X1 וX2. ואני רואה שX מתפלג דומה מאוד בכל אחד מהקלאסים, רק במיקום שונה. ולא רק זה, שני המשתנים שלי הם רציפים והתצפיות יוצרות מעין אליפסה, מה שמתאים מאוד כמו שראינו למידול על ידי ההתפלגות הנורמלית. אז התוחלות, כלומר המיקומים, שונות, אבל הפיזור כלומר מטריצת השונויות סיגמה נראה דומה לכל הקלאסים – אז הנתונים מתאימים למידול על ידי LDA.</p>
<p>איך אתם צופים שייראו קוי הגבול שנותן LDA בין הקלאסים? בדוגמא הזאת לא נראה שצריך משהו מתוחכם יותר מקוי החלטה ליניאריים, נכון? (להדגים) בין הקלאס האדום לכחול, בין הקלאס האדום לירוק, ובין הקלאס הכחול לירוק.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="lda-k-j-classes-decision-rule">LDA: <span class="math inline">\(k-j\)</span> classes decision rule</h3>
<ul>
<li>Why choose class <span class="math inline">\(k\)</span> over <span class="math inline">\(j\)</span>? <span class="math inline">\(P(Y = k|X = x) &gt; P(Y = j|X = x) \Leftrightarrow \pi_kf_k(x) &gt; \pi_jf_j(x) \Leftrightarrow \log\left[\pi_kf_k(x)\right] &gt; \log\left[\pi_jf_j(x)\right]\)</span></li>
</ul>
<div>
<ul>
<li class="fragment">Assume <span class="math inline">\(\pi_k, \pi_j, \mu_k, \mu_j, \Sigma\)</span> known</li>
<li class="fragment"><span class="math inline">\(X | Y = k \sim \mathcal{N}(\mu_k, \Sigma) \Rightarrow f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x - \mu_k)^T\Sigma^{-1}(x-\mu_k)\right)\)</span></li>
<li class="fragment"><span class="math inline">\(\log\left[\pi_kf_k(x)\right] = \log(\pi_k) + \log(f_k(x)) = \log(\pi_k) + C -\frac{1}{2}(x - \mu_k)^T\Sigma^{-1}(x-\mu_k)\)</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בואו נראה באמת אילו גבולות החלטה נוצרים בין הקלאס הJ לקלאס הK.</p>
<p>למה שנבחר דווקא בקלאס K? כי ההסתברות המותנית שY שייך לקלאס הזה, גדולה יותר מההסתברות שY שייך לקלאס הJ. זה כבר ראינו תלוי רק במונים של נוסחת בייז שלנו שסימנו כpi_k_f_k, ואנחנו נראה שקל יותר להסתכל על לוג המכפלות האלה, כי מדובר בצפיפות הנורמלית ויש שם אקספוננט. והרי מדובר ביחס שקול לחלוטין.</p>
<p>אז אם נניח לרגע אנחנו יודעים את הפרמטרים הדרושים לנו של ההתפלגות הנורמלית, pi_k, pi_j, mu_k, mu_j וסיגמה.</p>
<p>ניזכר שוב בצפיפות הרב-נורמלית של וקטור המשתנים X בהינתן קלאס K –ככה נראית הצפיפות שלנו f_k.</p>
<p>כלומר אם ניקח לוג על המכפלה pi_kf_k נגיע לביטוי שלפנינו: לוג של pi_k ועוד הביטוי שבאקספוננט ועוד איזשהו קבוע שאני אפילו לא רושם בפירוט כי הוא משותף גם לקלאס הk וגם לקלאס הj.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="lda-k-j-classes-decision-rule-1">LDA: <span class="math inline">\(k-j\)</span> classes decision rule</h3>
<p>So, select class <span class="math inline">\(k\)</span> over class <span class="math inline">\(j\)</span> if <span class="math inline">\(\log\left[\pi_kf_k(x)\right] &gt; \log\left[\pi_jf_j(x)\right]\)</span> means:</p>
<div class="fragment">
<p><span class="math display">\[\log(\pi_k) -\frac{1}{2}(x - \mu_k)^T\Sigma^{-1}(x-\mu_k) &gt; \log(\pi_j) -\frac{1}{2}(x - \mu_j)^T\Sigma^{-1}(x-\mu_j)\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\delta_{k &gt; j}(x): x^T\Sigma^{-1}(\mu_k - \mu_j) + \left[-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k +\frac{1}{2}\mu_j^T\Sigma^{-1}\mu_j + \log(\pi_k) - \log(\pi_j)\right]&gt; 0\]</span></p>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>What shape is <span class="math inline">\(\delta_{k &gt; j}(x)\)</span>?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז להעדיף את קלאס k על פני j, משמעותו שהלוגריתם של קלאס k גדול מהלוגריתם של קלאס j.</p>
<p>וזה אומר האי-שווין שלפנינו. גם כאן נשים לב שיש ביטויים מיותרים (להדגים), אם נפתח את הסוגריים נקבל בשני האגפים את הביטוי x’ואפשר להשמיט אותו משני האגפים.</p>
<p>ואז, נעביר את הביטוי עבור הקלאס הJ לצד שני, נכנס איברים דומים, ונראה שמתקבל האי-שוויון הסופי שרשום לנו כאן. אם הביטוי שלפנינו גדול מאפס, נעדיף את קלאס k על פני קלאס j. אפשר לסמן את זה ככלל דלתא k גדול מj בקיצור. כלל שמאוד קל לחשב לכל תצפית חדשה שמגיעה, שנותן מספר ממשי סקלר שצריך להשוות לאפס.</p>
<p>איזו צורה מקבל גבול ההחלטה שלנו דלתא, לכל תצפית? בכוונה רשמתי אותו ככה. נשים לב שכל הביטוי שבסוגריים מרובעים, לא תלוי בוקטור הנתונים X בכלל. אפשר לחשוב עליו כעל סקלר חותך כלשהו. ואילו וקטור X עובר מכפלה פנימית עם וקטור אחר באורך p, הוקטור סיגמא בהופכי כפול המרחק בין התוחלות. ראינו כבר את הצורה הגיאומטרית הזאת גם ברגרסיה לוגיסטית, גם בSVM - זה מישור-על, הייפרפליין!! כל משתנה מוכפל פי מספר, מוסיפים חותך, ואם הביטוי מעל אפס, כלומר אנחנו מעל למישור, נבחר בקלאס k, ואם קטן מאפס, כלומר מתחת למישור, נבחר בקלאס J. זה אומר שם האינטואיציה שלנו בדוגמא שראינו היא נכונה, כשX הוא שני משתנים והמישור הוא בעצם קו ליניארי, ועוד נחזור לוודא את זה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="lda-estimation">LDA: Estimation</h3>
<div>
<ul>
<li class="fragment">If we want to fit LDA to data, we need to estimate the parameters: <span class="math inline">\(\mu_1,\ldots,\mu_K,\; \Sigma,\; \pi_1,\ldots,\pi_K.\)</span></li>
<li class="fragment">This is naturally done from the data (e.g.&nbsp;by maximum likelihood): <span class="math display">\[\begin{eqnarray*}
\hat{\pi}_k &amp;=&amp; \frac{\sum_{i=1}^n \mathbb{I}\left[Y_i = k\right]}{n} \;,\; \hat{\mu}_k = \frac{\sum_{\mathbb{I}\left[Y_i = k\right]} x_i}{\sum_{i=1}^n \mathbb{I}\left[Y_i = k\right]}\\
\hat{\Sigma} &amp;=&amp; \frac{1}{n-K} \sum_{k=1}^K \sum_{\mathbb{I}\left[Y_i = k\right]} (x_i-\hat{\mu}_k) (x_i-\hat{\mu}_k)^T
\end{eqnarray*}\]</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>רק חוב קטן יש לנו. בפועל הפרמטרים של ההתפלגויות הנורמליות לכל משתנה ומשתנה לא ידועות לנו ואנחנו צריכים לאמוד אותן. מה זה לאמוד התפלגות? זה לאמוד את הפרמטרים שלה.</p>
<p>אלה הפרמטרים שלנו, יש לנו K וקטורים של תוחלות, כל אחד באורך p משתנים. K פרופורציות p_k, ומטריצת שונויות בגודל p על p משותפת לקלאסים.</p>
<p>אומדים סטנדרטיים לפרמטרים שלנו מהדאטא, על חלקם גם אפשר להראות שהם אומדי נראות מקסימלית:</p>
<p>לפרופורציות, כמו שראינו זה פשוט שכיחות הקלאסים. לתוחלות, זה פשוט ממוצע תצפיות האיקס בכל קלאס, כלומר ממש סכום הוקטורים חלקי מספר תצפיות שיש בקלאס.</p>
<p>ואומד למטריצת השונויות הוא גם מעין אומד קווריאנס משוקלל על פני כל הקלאסים. כל אלמנט הוא בעצם המרחק של התצפית מהתוחלת שלה בריבוע ואנחנו סוכמים על כל התצפיות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="lda-example">LDA: Example</h3>
<div id="02987d20" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c12_generative_files/figure-revealjs/cell-5-output-1.png" width="559" height="566"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>לסיכום נחזור לדוגמא שלנו שהיא די אידיאלית ונריץ עליה LDA. הקווים השחורים הם כללי ההחלטה על-פי הפרמטרים האמיתיים, על-פיהם נוצר הדאטא, והקווים במקווקו הם כללי ההחלטה על-פי האומדנים לפרמטרים, שנאמדו מתוך הנתונים.</p>
<p>אפשר לראות שהאומדנים טובים מאוד, כלומר הקווים המקווקווים מאוד דומים לקווים הרציפים. ואפשר לראות כמובטח שמדובר בכללי החלטה ליניאריים בין כל שני קלאסים. מכאן אגב השם, לינאר דיסקרימיננט אנליסיס. עכשיו ברור שזו דוגמא אידאלית מדי. בחלק הבא נעשה רילקסציה לפחות להנחה אחת חשודה של LDA – מטריצת שונות או פיזור זהה לכל הקלאסים? באמת?</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="quadratic-discriminant-analysis" class="slide level2 title-slide center">
<h2>Quadratic Discriminant Analysis</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">

</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="a-common-covariance">A common covariance?</h3>
<div id="bc23217b" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c12_generative_files/figure-revealjs/cell-6-output-1.png" width="427" height="429"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז הנה דוגמא פשוטה של שני קלאסים ושוב אנחנו בשני משתנים, כשברור שהנחה של מטריצת קווריאנס זהה או פיזור זהה לשני הקלאסים, היא לא הנחה סבירה. בקלאס האדום נראה שיש מתאם מעט חיובי בין האיקסים, בקלאס הירוק נראה שיש מתאם שלילי, כשX2 עולה X1 יורד. יותר מזה, יש משהו קצת חשוד בקו הפרדה ליניארי בין שני הקלאסים, אם כבר היינו מצפים לקו קצת מעוגל (להדגים) משהו כזה. אנחנו נראה שכשאנחנו לא מניחים פיזור זהה בין הקלאסים, זה בדיוק מה שמתקבל.</p>
<p>חשוב להדגיש שזאת לא דוגמה צעצוע, ככה מתנהגים נתונים אמיתיים. אפילו בהשוואה בין חולים ובריאים, על משתנה אחד כמו לחץ דם, ונניח אפילו שהוא מתפלג נורמלית (להדגים). אז הוא גבוה יותר אצל החולים, אבל זה סביר שתהיה לו אותה שונות? לא כל כך, אוכלוסיית הבריאים כנראה מגוונת יותר, ובקרב אוכלוסיית החולים הריכוז סביב לחץ דם גבוה קטן יותר, יש כאן אולי גם אפקט תקרה, לחץ דם סיסטולי לכיוון 180, זה כבר בדרך של התקף לב. זאת דוגמא שממחישה למה לא נרצה להניח פיזור זהה בין הקלאסים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="quadratic-linear-analysis-qda">Quadratic linear analysis (QDA)</h3>
<div>
<ul>
<li class="fragment">Now assume: <span class="math inline">\(X | Y = k \sim \mathcal{N}(\mu_k,\)</span><span style="color:red;"><span class="math inline">\(\Sigma_k\)</span></span> <span class="math inline">\()\)</span></li>
<li class="fragment">Decision rule stays the same:
<ul>
<li class="fragment">select class <span class="math inline">\(k\)</span> over class <span class="math inline">\(j\)</span> if <span class="math inline">\(\log\left[\pi_kf_k(x)\right] &gt; \log\left[\pi_jf_j(x)\right]\)</span></li>
</ul></li>
<li class="fragment"><span class="math inline">\(\log\left[\pi_kf_k(x)\right] = \log(\pi_k) + C -\frac{1}{2}(x - \mu_k)^T\)</span><span style="color:red;"><span class="math inline">\(\Sigma_k^{-1}\)</span></span><span class="math inline">\((x-\mu_k)\)</span> <span style="color:red;"><span class="math inline">\(-\frac{1}{2}\log(|\Sigma_k|)\)</span></span></li>
</ul>
</div>
<div class="fragment">
<p><span class="math display">\[\begin{align}
  \delta_{k &gt; j}(x)&amp;: \\
  &amp;-\frac{1}{2}x^T\left(\Sigma_k^{-1} - \Sigma_j^{-1}\right)x + x^T\left(\Sigma_k^{-1}\mu_k - \Sigma_j^{-1}\mu_j\right) \\
  &amp;+ \left[-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k +\frac{1}{2}\mu_j^T\Sigma^{-1}\mu_j + \log(\pi_k) - \log(\pi_j) + \frac{1}{2}\left(\log(|\Sigma_j| -\log(|\Sigma_k|)\right)\right]&gt; 0
  \end{align}\]</span></p>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>What shape is <span class="math inline">\(\delta_{k &gt; j}(x)\)</span>?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כשאנחנו לא מניחים מטריצת קווריאנס זהה, המודל נקרא קוואדרטיק דיקרימיננט אנליסיס או QDA.</p>
<p>וזה בדיוק כל ההבדל, וקטור הנתונים שלנו מתפלג עדיין נורמלית אבל אנחנו מאפשרים לו מטריצת קווריאנס משלו סיגמא-קיי.</p>
<p>כדי לראות את כלל ההחלטה נתחיל מאותה נקודה: אנחנו נעדיף את קלאס K על קלאס J, אם ההסתברות האפוסטריורית לקלאס K בהינתן שראינו את הנתונים גדולה מההסתברות האפוסטריורית לקלאס J. וראינו שזה שקול לזה שלוג המכפלה pi_kf_k גדול מלוג המכפלה pi_jf_j.</p>
<p>הפעם נכתוב את הלוג הזה לכל אחד מהקלאסים בצורה קצת יותר מפורטת, כי יש פחות גורמים שאפשר להתעלם מהם: אנחנו עדיין בלוג ההסתברות האפריורית pi_k ועוד איזשהו קבוע, אחר, ועוד הביטוי שבאקספוננט ההתפלגות הנורמלית, רק שנשים לב שיש בו את סיגמה-קיי בהופכי, ולא סיגמה כללית. פחות חצי לוג הדטרמיננטה של סיגמה-קיי, שזה גורם שקודם הכללנו תחת הקבוע C, כי הוא היה זהה לכל הקלאסים, ועכשיו הוא תלוי במטריצת קווריאנס שונה אז אנחנו לא יכולים להתעלם.</p>
<p>אחרי קצת אלגברה ממש כמו שעשינו קודם בLDA, נגיע לכלל החלטה שאפשר לסמן בדלתא, שקלאס K עדיף על קלאס J, אם הביטוי שרשום כאן, גדול מאפס. אחרת, קלאס J עדיף.</p>
<p>וכמו קודם נשאל איזו מין צורה זו כלל ההחלטה הזה?</p>
<p>כל הביטוי שבסוגרים הוא קבוע אחד גדול. כאן יש לנו את הוקטור איקס במכפלה פנימית כפול וקטור של מקדמים מאוד דומה למה שהיה לנו קודם. והביטוי הראשון הוא המקבילה לביטוי שכולל את איקס בריבוע. יש כאן איקס טרנספוז כפול מטריצה בגודל p על p, כפול איקס. אפשר להראות שזה כמו סכום על קבוע כפול אלמנט Xj כפול אלמנט Xk ואנחנו עוברים על כל האלמנטים. לכן כל המודל נקרא קוואדרטיק.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="qda-estimation">QDA: Estimation</h3>
<ul>
<li>Estimate <span class="math inline">\(\mu_k, \pi_k\)</span> as in LDA and:
<ul>
<li><span class="math inline">\(\hat{\Sigma}_k  = \frac{1}{n_k-1} \sum_{\mathbb{I}\left[Y_i = k\right]}  (X_i-\hat{\mu}_k) (X_i-\hat{\mu}_k)^T\)</span></li>
</ul></li>
</ul>
<div class="columns">
<div class="column">
<div class="fragment">
<div id="4290c25d" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c12_generative_files/figure-revealjs/cell-7-output-1.png" width="427" height="429"></p>
</figure>
</div>
</div>
</div>
</div>
</div><div class="column">
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>How many params are estimated in QDA? LDA?</p>
</div>
</div>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מבחינת אמידה, הכל נשאר אותו דבר, האמידה של הפרופורציות כמו שהיתה, האמידה של התוחלות בכל קלאס אותו דבר. ההבדל הוא שעכשיו אנחנו צריכים לאמוד מטריצת שונות לכל קלאס וקלאס, כמו בביטוי הזה. אנחנו צריכים K מטריצות כאלה.</p>
<p>וכשאנחנו מממשים את הנוסחה שהגענו אליה ומשרטטים את כלל ההחלטה, אכן מתקבל קו עקום או ריבועי כמו פרבולה, לא-ליניארי, שנראה קצת טבעי יותר לבעיה שלפנינו.</p>
<p>לפני שנעבור למודל האחרון, כמה פרמטרים אנחנו בעצם אומדים כאן ובLDA?</p>
<p>בLDA יש לנו K פרופורציות אבל הן מסתכמות ל1 אז זה K - 1 פרמטרים, ועוד K וקטורי תוחלות שבכל אחד יש p פרמטרים, ועוד מטריצת קווריאנס אחת שכבר אמרנו שמורכבת מp פרמטרים ועוד p מעל 2.</p>
<p>בQDA עד התוחלות הכל נשאר אותו דבר רק שאנחנו צריכים לאמוד K מטריצות כאלה!</p>
<p>מאחר שp הוא בדרך כלל המספר הגדול יותר, תחשבו על 10, 20 משתנים, זה יוצא שבQDA אנחנו אומדים סדר גודל של פיK משתנים! וזה אומר שבשביל שQDA יעבוד, כלומר נקבל אומדנים טובים עם שונות קטנה, אנחנו צריכים סדר גודל של פי K תצפיות. זה משהו לקחת בחשבון אם יש לכם מעט תצפיות, גם אם כלל ההחלטה נראה לא-ליניארי, לא בטוח בכלל שיש לנו מספיק תצפיות כדי לאמוד כלל החלטה מורכב כזה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="naive-bayes" class="slide level2 title-slide center">
<h2>Naive Bayes</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>המודל הגנרטיבי האחרון שנלמד היום הוא נאיב בייז. נאיב בייז הוא עוד דוגמא למודל שהתפתח במדעי המחשב עקב הצורך לענות על בעיות ממימד גבוה. והאמת היא שעשינו כבר כל כך הרבה עבודה שייקח לנו מעט מאמץ לתאר אותו.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="naive-bayes-1">Naive Bayes</h3>
<ul>
<li>Recall: <span class="math inline">\(P(Y = k|X = x) = \frac{P(Y = k)P(X = x|Y = k)}{P(X = x)} = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}\)</span></li>
</ul>
<div>
<ul>
<li class="fragment">In Naive Bayes:
<ul>
<li class="fragment"><span class="math inline">\(\pi_k\)</span> are priors, or: <span class="math inline">\(\hat{\pi}_k = \frac{\sum_{i = 1}^n \mathbb{I}\left[Y_i = k\right]}{n}\)</span></li>
<li class="fragment">Within the <span class="math inline">\(k\)</span>-th class, the <span class="math inline">\(p\)</span> predictors are <strong>independent</strong></li>
<li class="fragment"><span class="math inline">\(f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p)\)</span>
<ul>
<li class="fragment"><span class="math inline">\(f_{kj}(x_j)\)</span> for a continuous feature: <span class="math inline">\(\mathcal{N}(\mu_{jk}, \sigma_{jk}^2)\)</span>, <span class="math inline">\(Exp(\lambda_{jk})\)</span>, KDE, …</li>
<li class="fragment"><span class="math inline">\(f_{kj}(x_j)\)</span> for a discrete feature: <span class="math inline">\(\hat{f}_{kj}(x_j)=\begin{cases} 0.2 &amp; \text{if $x_j = 1$} \\ 0.8 &amp; \text{if $x_j = 2$} \end{cases}\)</span></li>
</ul></li>
</ul></li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>We don’t really believe this independence… when is this naive assumption particularly useful?</p>
</div>
</div>
</div>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>How is Naive Bayes = LDA/QDA?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>שוב ניזכר מה אנחנו עושים כאן עם ההסתברות האפוסטריורית לקלאס K בהינתן שראינו את הנתונים, כשאמרנו שכל ההבדל בין המודלים שנראה היום הוא איך אנחנו מפרטים את f_k, ההתפלגות של X בהינתן הקלאס הK.</p>
<p>בנאיב בייז עדיין הpi_k הם אותו דבר אלא אם כן יש לנו הצדקה לתת פריורים אחרים.</p>
<p>ובתוך כל קלאס, אנחנו מניחים הנחה חזקה, שהמשתנים הם בלתי תלויים.</p>
<p>אם המשתנים הם בלתי תלויים, זה אומר שאנחנו יכולים לאמוד את ההתפלגות של כל אחד מהם בצורה נפרדת, ואז ההתפלגות המשותפת היא מכפלת ההתפלגויות. אין צורך להתאים כאן התפלגות משותפת רב-נורמלית, עם המון פרמטרים של מתאים בין כל זוג משתנים וכולי.</p>
<p>יותר מזה, אנחנו יכולים לטפל בצורה טבעית יותר במשתנים רציפים לעומת בדידים. אם המשתנים שלנו רציפים נתאים להם התפלגות רציפה, למשל נורמלית כמו מקודם, כשכל מה שאנחנו צריכים לאמוד זה שני פרמטרים למשתנה, לקלאס, תוחלת ושונות. אבל אז יש לנו עוד אפשרויות אולי טובות יותר, כמו התפלגות אקספוננציאלית ואולי בכלל אומדן אפרמטרי להתפלגות שנשיג עם kernel density estimation או KDE.</p>
<p>ואם המשתנים שלנו בדידים עם מעט ערכים אפשר לאמוד את ההתפלגות שלהם פשוט עם ההתפלגות האמפירית – השכיחויות של הערכים במדגם שלנו. לדוגמא X הוא משתנה שמקבל שני ערכים 1 או 2, ואנחנו אומדים את ההתפלגות הבדידה הזאת עם השכיחות במדגם הלמידה, 20 אחוז לערך 1 ו80 אחוז לערך 2. מכל מקום זה הכל, אמרנו בעצם את כל מה שצריך. ברגע שהתאמנו התפלגות לכל משתנה, אנחנו צריכים לאמוד את הפרמטרים שלה לכל קלאס, והקלאס שנתאים לתצפית חדשה, כמו מקודם, הוא הקלאס שעבורו pi_k_f_k מגיע למקסימום.</p>
<p>שתי שאלות חשובות על נאיב בייז שאולי גם שאלתם את עצמכם:</p>
<p>אני באמת מאמין שהמשתנים בדאטא הם בלתי תלויים? לדוגמא בדאטא שבו גיל והשכלה מסבירים גובה, אפשר לטעון שגיל בלתי תלוי בהשכלה? לא ממש. אז במה המודל הזה טוב, מתי הוא יביא לביצועים מרשימים? רמזתי לזה קודם, בבעיות עם p גדול מאוד, בעיות ממימד גבוה. זה גם היה השימוש שלו הרבה שנים, לדוגמא בבעיה שצריך לסווג מייל, שהוא רצף של מילים, לספאם ולא-ספאם. כל מילה היא משתנה. וזה אולי לא סביר שכל מילה בלתי תלויה בכל מילה אחרת, אבל יש כל כך הרבה מילים, שנאיב בייז הוא מודל יחסית מאוד מאוד פשוט לתקל בכלל את הבעיה הזאת. אומדים הסתברות לכל מילה ומילה שתופיע, וההסתברות של כל קובץ הטקסט הוא מכפלת כל ההסתברויות. פשוט ועבד לא רע עד לפני כמה שנים.</p>
<p>שאלה שנייה היא האם אין קשר בין נאיב בייז לLDA ולQDA? ברור שיש. אפשר להראות שאם אנחנו מניחים שכל המשתנים בנאיב בייז מתפלגים נורמלית, כל אחד עם שונות משלו, אז זה בדיוק כמו לעשות QDA, ולהניח שמטריצת השונות היא אלכסונית, כלומר אפס משני צידי האלכסון, אין מתאם בין המשתנים, ועל,האלכסון השונות של כל משתנה ומשתנה, בקלאס הK.</p>
<p>ואם המטריצה היא גם אלכסונית וגם לכל קלאס יש את אותם שונויות - זה בדיוק מטריצה שווה לכל הקלאסים, וככה מגיעים מנאיב בייז לLDA. בחלק האחרון נראה קשר אפילו פחות טריוויאלי. הנקודה היא שבטיעונים מתמטיים פשוטים אפשר לראות קשרים בין שיטות שונות שהמוטיבציה להן ולפעמים אפילו השנים בהן הן נוסחו שונות לחלוטין.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="comparing-classifiers" class="slide level2 title-slide center">
<h2>Comparing classifiers</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">

</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="example-saheart-data">Example: SAHeart data</h3>
<div id="92b0280a" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c12_generative_files/figure-revealjs/cell-8-output-1.png" width="529" height="449"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בואו נשווה בין השיטות. קודם כל על נתונים אמיתיים של הפציינטים הדרום אפריקאיים שאנחנו מנסים לחזות אם פציינט יחלה במחלת לב או לא, אפשר לראות שהשיטות נותנות תוצאות די דומות אחת לשנייה ולרגרסיה לוגיסטית, כאן ספציפית LDA השיג על מדגם הטסט את הAUC הכי גבוה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="lda-vs.-logistic-regression">LDA vs.&nbsp;logistic regression</h3>
<div>
<ul>
<li class="fragment">For <span class="math inline">\(K = 2\)</span> (though we can show this for any <span class="math inline">\(K\)</span>), logistic regression: <span class="math display">\[\text{logit}(P(Y = 1|X)) = \log\left(\frac{P(Y = 1|X)}{1 - P(Y = 1|X)}\right) = \log\left(\frac{P(Y = 1|X)}{P(Y = 0|X)}\right) = \beta_0 + x^T\beta\]</span></li>
<li class="fragment">For <span class="math inline">\(K = 2\)</span> (though we can show this for any <span class="math inline">\(K\)</span>), LDA: <span class="math display">\[\begin{align}\log\left(\frac{P(Y = 1|X)}{P(Y = 0|X)}\right) &amp;= \log\left(\frac{\pi_1 f_1(x)}{\pi_0 f_0(x)}\right) = \log(\pi_1 f_1(x)) - log(\pi_0 f_0(x)) \\
&amp;= x^T\Sigma^{-1}(\mu_1 - \mu_0) + \left[-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1 +\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0 + \log(\pi_1) - \log(\pi_0)\right] \\
&amp;= \alpha_0 + x^T\alpha\end{align}\]</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אבל השוואה מעניינת יותר היא השוואה אנליטית. למשל בין רגרסיה לוגיסטית לLDA. אנחנו יודעים ששני המודלים האלה הם ליניאריים, אבל הם אפילו דומים יותר ממה שנראה. נתמקד בשני קלאסים, אבל אפשר להראות שהקשר מתקיים גם עבור כל K וההכללה של רגרסיה לוגיסטית לK גדול מ2.</p>
<p>ברגרסיה לוגיסטית אנחנו ממדלים את הלוגיט של ההסתברות של Y להיות 1 בהינתן הנתונים. אבל מה זה הלוגיט, הלוג של p חלקי 1 - p.&nbsp;אבל מה זה לוג של p חלקי 1 פחות p, זה בדיוק הלוג של יחס הסיכויים של ההסתברות להיות 1 חלקי ההסתברות להיות 0. זה מה שאנחנו ממדלים כמודל לינארי, ואנחנו מחפשים חותך ווקטור מקדמים בטא באורך p, שימקסמו קריטריון של נראות מקסימלית.</p>
<p>מה זה לוג יחס הסיכויים בLDA? זה ממש לוג יחס ההסתברויות המותנות שלנו, כלומר מה שאנחנו סימנו כpi_1f_1 לpi_0f_0. אפשר לפתח את זה עוד, לוג של מנה זה הפרש הלוגים.</p>
<p>אבל מה זה הפרש הלוגים? זה בדיוק מה שפיתחנו עם ההתפלגויות הנורמליות! והגענו למסקנה שאנחנו רוצים לחשב לכל תצפית את הביטוי שרשום כאן, שהוא ביטוי סגור, והוא מישור. מתי נחזה דווקא את קלאס 1 על פני קלאס 0? אם נהיה מעל המישור, כלומר הביטוי גדול מאפס, או אם תחזרו אחורה יחס הסיכויים גדול מ1 או ההסתברות החזוי לקלאס 1 היא גדולה מחצי.</p>
<p>מכל מקום מה שרשום כאן כבר הבנו זה חותך, ביטוי מאוד מורכב אבל סקלאר, ועוד מכפלה פנימית של וקטור X כפול וקטור של מקדמים.</p>
<p>אפשר לראות שזה דומה מאוד, שתי השיטות מחפשות כלל החלטה לינארי אבל כזה שממדל את אותו הדבר! ולכן אף על פי שלא נקבל תוצאה זהה כי הקריטריונים שלהם שונים, נקבל תוצאות דומות מאוד. ניתן לראות שאם ההנחות של LDA מתקיימות אז אפשר להגיע בצורה יעילה יותר עם LDA לקו הרצוי כלומר עם הרבה פחות תצפיות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="similarly-for-all-methods">Similarly for all methods</h3>
<p><span class="math inline">\(\delta_{1 &gt; 0}(x) = \log\left(\frac{P(Y = 1|X = x)}{P(Y = 0|X = x)}\right) =\)</span></p>
<table>
<colgroup>
<col style="width: 15%">
<col style="width: 38%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>method</th>
<th>type</th>
<th><span class="math inline">\(\delta_{1 &gt; 0}(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LR</td>
<td>discriminative, linear</td>
<td><span class="math inline">\(\beta_0 + x^T\beta\)</span></td>
</tr>
<tr class="even">
<td>LDA</td>
<td>generative, linear</td>
<td><span class="math inline">\(\alpha_0 + x^T\alpha\)</span></td>
</tr>
<tr class="odd">
<td>QDA</td>
<td>generative, non-linear</td>
<td><span class="math inline">\(\gamma_0 + x^T\gamma + x^T\Gamma x\)</span></td>
</tr>
<tr class="even">
<td>NB</td>
<td>generative, non-linear</td>
<td><span class="math inline">\(\tau_0 + \sum_{j = 1}^p \tau_j(x_j)\)</span></td>
</tr>
</tbody>
</table>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בצורה דומה אפשר לפתח את הלוגיט או לוג יחס הסיכויים גם לQDA וגם לנאיב בייז.</p>
<p>מה שתקבלו בQDA הוא יחס החלטה קוואדרטי כמו שראינו, ומה שתקבלו בנאיב בייז הוא מה שנקרא מודל אדיטיבי, של חותך ועוד סכום של פונקציות טאו-ג’יי לכל משתנה Xj. והנה קיבלנו עוד מובן של הקשר בין LDA ונאיב בייז, אם הפונקציות האלה הן אותן הפונקציות שמביאות לוקטור המקדמים אלפא של LDA, ניתן לראות בLDA כמקרה פרטי ממש של נאיב בייז.</p>
<p>נסיים את השיעור שלנו על מודלים גנרטיביים כאן. אני מקווה שהמסע שלכם בעולם המרתק הזה לא נגמר, כי כאמור אלה ממש הצעדים הראשונים כדי להבין איך מודלים משוכללים כמו דאלי מייצרים לנו תמונות על פי בקשה, ואפילו סרטונים. למעשה, אני יודע שהמסע שלכם בעולם המודלים הגנרטיביים לא נגמר כי כבר בשיעור הבא יתברר לנו, שאחד המודלים שאנחנו כבר די מכירים ובצורה מאוד נוחה לא הזכרנו כאן, הוא גם מודל גנרטיבי! על כך בשיעור הבא.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="quarto-auto-generated-content">
<p><img src="../Intro2SL_logo_white.jpg" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://intro2statlearn.github.io/mooc/" target="_blank">Intro to Statistical Learning</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../libs/revealjs/plugin/search/search.js"></script>
  <script src="../libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>