---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Model Selection - Part A"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Model Selection - Part A - Class 5

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Expected Prediction Error {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עד כאן בקורס, הבנו מה המטרה של למידה סטטיסטית, ראינו את הדוגמא הקלאסית ביותר שהיא המודל הליניארי, אבל חסרה לנו תשובה מעמיקה לשאלה ממש פשוטה: איך אנחנו בוחרים בין מודלים, ואיך נכון לאמוד את הביצועים של מודל סופי. אמנם קיבלנו תחושה כבר שלא נכון להשתמש באותו סט של נתונים לאימון המודל, לבחירה של המודל, להערכה של המודל. אבל בשיעור הזה נפרמל את התחושה הזאת ונדבר לעומק על מספר דרכים שונות לבחור בין מודלים.
:::
:::

---

### Previously with the Bias-Variance Tradeoff

- For regression, take the standard model: $y = f(x) + \varepsilon\;,\;\varepsilon \sim (0,\sigma^2)$

- Modeling approach (e.g. OLS), given training data $T$, gives model $\hat{f}(x)$

::: {.fragment}
- Assume we want to predict at new point $x_0$, and understand our expected (squared) prediction error: 
$$\mathbb{E}_{y_0, T}(y_0 - \hat{f}(x_0))^2 = \text{irreducible error} + \text{squared bias} + \text{variance}$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כדי להגדיר את הקריטריון שלנו אנחנו צריכים להיות מדויקים. מה מבחינתנו נשאר קבוע, מה משתנה מקרי. כי מסתבר שיש יותר מקריטריון אחד.

נתחיל במודל רגרסיה פשוט שבו עסקנו עד כה: Y שווה לאיזושהי פונקציה F של X ועוד רעש מקרי אפסילון, שמתפלג לאו דווקא נורמלית, עם תוחלת אפס ושונות סיגמא בריבוע. אמרנו שאנחנו מקבלים מדגם T בגודל n תצפיות, ומעוניינים לאמוד את F.

ואז פיתחנו את פירוק הביאס-וריאנס, ראינו שעבור תצפית חדשה עם X0 נתון, ואנחנו עושים תוחלת על כל השאר, על-פני מדגם הלמידה שראינו ועל פני הY0 של התצפית החדשה, ההפסד הריבועי מתפרק לטעות אירדוסיבל, היא הסיגמא בריבוע, רעש בסיסי שהמודל שלנו לא מצליח למדל, ועוד הטייה בריבוע, טעות קירוב שנובעת מאופי המודל שלנו, אם נניח שF היא פונקציה מורכבת, אז מודל מורכב יוכל לתאר אותה עם הטיה קטנה, ולמודל פשטני או פשוט לא נכון תהיה הטיה גדולה. וטעות אחרונות של שונות, שנובעת מהתוחלת על-פני הרבה מדגמי למידה, אם נראה מדגם למידה קצת שונה, האם המודל ייראה אחרת לגמרי או שהוא יציב.
:::
:::

---

### Expected Prediction Error

$$\mathbb{E}_{y_0, T}(y_0 - \hat{f}(x_0))^2 = \text{irreducible error} + \text{squared bias} + \text{variance}$$

::: {.incremental}
- Note we treat both the training data $T$ (and hence $\hat{f}$) and the response $y_0$ as random variables in our expectations
- So, more generally we decomposed: $\mathbb{E}_{y_0, T}(L(y_0, \hat{f}(x_0))|X = x_0)$
- [Expected prediction error]{style="color:red;"} is when we average over all $x_0$:
$$Err = \mathbb{E}_{x_0, y_0, T}(L(y_0, \hat{f}(x_0))) = \mathbb{E}_{X}\left[\mathbb{E}_{y_0, T}(L(y_0, \hat{f}(x_0))|X = x_0)\right]$$
- This could also be written as:
$$Err = \mathbb{E}_{x_0, y_0, T}(L(y_0, \hat{f}(x_0))) = \mathbb{E}_{T}\left[\mathbb{E}_{x_0, y_0}(L(y_0, \hat{f}(x_0))|T)\right] = \mathbb{E}\left[Err_T\right]$$
- Some would say the *conditional* $Err_T$ is even more interesting!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז כאן אנחנו מתייחסים למדגם הלמידה שלנו כמשתנה מקרי ולY0 כמשתנה מקרי, אבל לX0 כנתון. כלומר אנחנו מפרקים את הטעות הריבועית אם נראה כל מיני מדגמי למידה אחרים וכל מיני ערכי Y0 אחרים לתצפית הX0 הנתונה הזאת.

באופן יותר כללי נרשום זאת כך, עבור איזשהו הפסד L, הסתכלנו על תוחלת מותנית, תוחלת ההפסד ביחס לT ו-Y0 בהינתן X0.

אבל באופן אינטואיטיבי, רבים היו מגדירים את הטעות שהם מעוניינים בה, כתוחלת שולית, על הכל, כלומר גם על המדגם שראינו, וגם על תצפיות חדשות. או במילים אחרות תוחלת של X על התוחלת המותנית שהסתכלנו עליה. נסמן את התוחלת הזאת בErr, ונקרא לה האקספקטד פרדיקשן ארור. שמות אחרים יכולים להיות ג'נ'רליזיישן ארור, טעות ההכללה או האקספקטד טסט ארור.

אפשר לרשום את הטעות הזאת, את התוחלת הזאת כתוחלת שלמה של תוחלת מותנית אחרת: התוחלת המותנית בהינתן מדגם הלמידה, הזוגות של X ו-Y שמהם למדנו. ואז התוחלת היא רק על תצפיות חדשות X0 וY0. והטעות שלנו היא התוחלת על-פני כל מדגמי הלמידה. נסמן את התוחלת המותנית החדשה כErrT, ואם נחשוב עליה קצת - אולי היא זאת שבכלל מעניין אותנו לאמוד! הרי במציאות אני לא מקבל עוד ועוד מדגמי למידה ומעניין אותי איך מבצע המודל שלי על פני הרבה מדגמי למידה. לא! אני מקבל מדגם למידה אחד ומעניין אותי לדעת את הביצועים של המודל בהינתן המדגם הזה, לכל תצפית חדשה שתבוא.

זאת באמת טעות שמעניין להעריך אבל קשה להעריך אותה. בפועל נראה שהרבה יותר קל לאמוד את הטעות דווקא השולית, תוחלת ההפסד על פני הרבה מדגמי למידה והרבה תצפיות חדשות.
:::
:::

---

### What EPE is for?

::: {.fragment}
1. [Model Selection]{style="color:red;"}: select between a set of models (e.g. one with 5 parameters and the other with 6 parameters) the one with lowest error
2. [Model Assessment]{style="color:red;"}: know how accurate the model would be, estimate the error itself
:::


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשלב הזה ראוי להפריד בין שתי מטרות ראויות:

האחת היא מודל סלקשן, אני רוצה לבחור מתוך סט של מודלים נאמדים את הטוב ביותר, עם המינימום תוחלת הפסד החיזוי. לדוגמא במודל הליניארי לבחור בין מודל עם 5 משתנים או 6.

ומטרה אחרת היא לקחת מודל סופי שנבחר, ולאמוד באמצעותו את תוחלת הפסד החיזוי עצמה, את הביצועים של המודל הסופי שנבחר. מטרה זו נקראת מודל אססמנט או אבליואיישן.
:::
:::

---

### How to estimate EPE?

::: {.fragment}
::: {.callout-note}
What do you mean how, why not training error:
$$\overline{err} = \frac{1}{n}\sum_{i=1}^{n} L(y_i, \hat{f}(x_i))$$
:::
:::

::: {.incremental}
1. Data splitting: Train-Validation-Test
2. Cross Validation
3. The Bootstrap
4. Training error + Optimism
:::


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ושוב נזכיר - מה, אי אפשר לאמוד את תוחלת ההפסד עם ההפסד של מדגם הלמידה? אנחנו נסמן אותו כerr עם קו מעל. התשובה צריכה כבר להיות ברורה לכולם: מאוד מאוד לא נכון לאמוד את השגיאה של מודל על תצפיות שהוא לא למד עליהן, מתצפיות שהוא כן למד עליהן. השגיאה הזאת אופטימית מדי. אם נרצה לבחור למשל את המודל הטוב ביותר לפי המודל שמביא את שגיאת הטריין למינימום, רוב הסיכויים שנבחר במודל מורכב מדי שלא לצורך, שעושה אוברפטינג למדגם הלמידה, מתחיל למדל את האירדוסיבל ארור ולא יכליל טוב כשיגיעו תצפיות חדשות.

אז ניכנס לעומק של ארבע אופציות טובות יותר:

ברמה הבסיסית ביותר יש לנו את האופציה של חלוקה יחידה של הדאטא שלנו לטריין, ולידיישן וטסט.

גירסה קצת יותר יעילה של חלוקה כזאת היא חלוקה של הדאטא לכמה פולדים, מה שנקרא קרוס-ולידיישן.

נדבר גם על הבוטסטרפ, ובסופו של דבר גם על גישה מסורתית יותר שמתאימה במיוחד למדגמים קטנים, שבה אנחנו פשוט מנסים לאמוד עד כמה מדגם הלמידה אופטימי, ולהוסיף את התיקון הזה לטריינינג לוס.
:::
:::

---

## Data splitting {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Data splitting: Train-Validation-Test

```{python}
#| echo: false

import matplotlib.pyplot as plt

# Data splitting proportions
training_proportion = 0.6
validation_proportion = 0.2
test_proportion = 0.2

# Plotting the data splitting strategy
fig, ax = plt.subplots(figsize=(10, 2))

# Plot the full dataset rectangle
rect_full = plt.Rectangle((0, 0), 1, 1, edgecolor='black', facecolor='none')
ax.add_patch(rect_full)

# Plot the training set rectangle
rect_train = plt.Rectangle((0, 0), training_proportion, 1, edgecolor='black', facecolor='lightblue')
ax.add_patch(rect_train)
ax.text(training_proportion / 2, 0.5, 'Training Set\n60%', ha='center', va='center', fontsize=12)

# Plot the validation set rectangle
rect_val = plt.Rectangle((training_proportion, 0), validation_proportion, 1, edgecolor='black', facecolor='lightgreen')
ax.add_patch(rect_val)
ax.text(training_proportion + validation_proportion / 2, 0.5, 'Validation Set\n20%', ha='center', va='center', fontsize=12)

# Plot the test set rectangle
rect_test = plt.Rectangle((training_proportion + validation_proportion, 0), test_proportion, 1, edgecolor='black', facecolor='lightcoral')
ax.add_patch(rect_test)
ax.text(training_proportion + validation_proportion + test_proportion / 2, 0.5, 'Test Set\n20%', ha='center', va='center', fontsize=12)

# Setting plot limits and removing axes
ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.axis('off')

# plt.title('Data Splitting Strategy in Machine Learning')
plt.show()
```

Divide the given sample to 3:

- [Training set]{style="color:red;"}: learn different models
- [Validation set]{style="color:red;"}: decide on final model (model selection, tuning)
- [Test set]{style="color:red;"}: estimate final model's performance (model assessment)

::: {.fragment}
::: {.callout-note}
Why not use the final model's performance on *validation* set as estimate?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל בגישה המתבקשת: לפצל את הדאטא שלנו, לעשות לו ספליטינג.

הגישה הקלאסית אומרת לפצל אותו ל3: טריינינג, ולידיישן וטסט. אין המלצה אחידה שמתאימה לכל דאטא, כמה אחוזים לשים בכל ספליט. כאן אנחנו רואים חלוקה סבירה של 60 אחוז לטריינינג, 20 אחוז לולידיישן ו20 אחוז לטסט.

והפרוצדורה היא כזאת: נאמן את כל המודלים המועמדים על הטריינינג דאטא. נבחן את הביצועים שלהם על הולידיישן דאטא ונבחר את המודל הטוב ביותר לפי זה שמביא את השגיאה של הולידיישן למינימום. זה המודל סלקשן, או טיונינג, כוונון של איזשהו פרמטר לא ידוע במודל. לבסוף, כדי לבצע מודל אססמנט, לאמוד את תוחלת הפסד החיזוי או הביצועים של המודל, ניגש לסט הטסט של נתונים ששמרנו בצד.

בשלב הזה מתבקשת השאלה - למה אנחנו צריכים עוד סט של נתונים כדי לאמוד את השגיאה הסופית של המודל? כרגע הרצנו אותו על הולידיישן סט, אי אפשר להשתמש בשגיאה הזאת כאומד לביצועים של המודל על דאטא שהוא לא ראה? התשובה היא שלא, זה בעייתי. כבר השתמשנו בשגיאה הזאת כדי לקבוע מי המודל עם הביצועים הטובים ביותר. אנחנו לא יכולים להשתמש בה גם לאמוד את שגיאת החיזוי עצמה, כי אנחנו כבר יודעים שאנחנו מסתכלים על השגיאה הכי קטנה. מבחינה מתמטית, אם השגיאת ולידיישן של מודל 1 אומד חסר הטיה לתוחלת השגיאה של מודל 1, והשגיאת ולידיישן של מודל 2 אומד חסר הטיה לתוחלת השגיאה של מודל 2, הפרש השגיאות ולידיישן הוא אומד טוב להפרש התוחלות כלומר לקבוע מי המודל הטוב יותר. אבל המינימום מביניהן הוא לא אומד חסר הטיה למינימום התוחלות.
:::
:::

---

### Data splitting: comments

::: {.incremental}
- Test set is used only once!
- Since larger $n$ should improve model's performance, after model selection:
  - unite {training + validation}, train final model and then assess it on the test set
- No clear guidelines: 60-20-20%, 50-25-25%, ... (depends on $n$ and nature of $f$)
- Disadvantages:
  - Lose half the data? Only if we are rich in data
  - Model assessment highly variable
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כמה הערות:

הטסט סט בשימוש רק פעם אחת! חוקר טוב צריך להיות כן עם עצמו ובשביל אמידת שגיאת החיזוי הסופית צריך להשתמש בדאטא טרי באמת.

הערה נוספת: מקובל אחרי בחירת המודל הסופי לאחד את כל הדאטא שיש לנו, ה60 אחוז של הטריינינג וה20 אחוז של ולידיישן כדי לאמן את המודל פעם נוספת, ואת המודל הזה להעריך על הטסט סט פעם אחרונה.

כאמור, אין הוראה ברורה כמה אחוזים בדיוק לתקצב כל אחד מהספליטים, זה תלוי בגודל המדגם הנתון ובמורכבות היחס F בין X לY שאנחנו מנסים להעריך. אם הוא פשוט סביר שלא צריך המון דאטא לאמן אותו, ואם הוא מורכב צריך כמה שיותר מהדאטא לאמן אותו.

אז זאת הגישה הקלאסית, אבל יש לה שתי בעיות עיקריות: אחת, היא אנחנו מאמנים את כל המודלים המועמדים על 50-60 אחוז מהתצפיות, זה שימוש לא יעיל בנתונים שאפשרי רק אם יש לנו הרבה מהם.

והבעיה השניה היא שאמנם האומד לשגיאת החיזוי חסר הטיה אבל יש לו שונות גבוהה, אנחנו מבססים את האמידה שלנו בכל זאת על סט נתונים אחד ויחיד, אפילו שהוא נדגם מקרית, ובמיוחד עם מודלים מודרנים של למידה סטטיסטית, לא מסוגלים לתת שום אומד לשגיאה, לאיזו סטיית תקן לאומד הסופי הזה של ביצועי המודל.
:::
:::

---

### Single split: need 2nd opinion

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Step 1: Generate synthetic data with a clear cubic relationship
np.random.seed(0)
n = 1000
X = np.random.rand(n, 1) * 10  # Features
beta_0, beta_1, beta_2, beta_3 = 1, 2, 3, 4
noise = np.random.randn(n, 1) * 100
y = beta_0 + beta_1 * X + beta_2 * X**2 + beta_3 * X**3 + noise  # Cubic function with noise

# Define polynomial degrees to test
degrees = range(2, 8)

# Function to train and evaluate polynomial regression models
def polynomial_regression(X_train, y_train, X_val, y_val, degrees):
    validation_mse = []
    for d in degrees:
        poly = PolynomialFeatures(degree=d)
        X_train_poly = poly.fit_transform(X_train)
        X_val_poly = poly.transform(X_val)
        
        model = LinearRegression(fit_intercept=False).fit(X_train_poly, y_train)
        y_val_pred = model.predict(X_val_poly)
        
        mse = mean_squared_error(y_val, y_val_pred)
        validation_mse.append(mse)
    return validation_mse

# Single split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=0)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)

# Validation MSE for single split
single_split_mse = polynomial_regression(X_train, y_train, X_val, y_val, degrees)

# Multiple random splits
num_splits = 10
all_splits_mse = []

for i in range(num_splits):
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=i)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=i)
    mse = polynomial_regression(X_train, y_train, X_val, y_val, degrees)
    all_splits_mse.append(mse)

# Determine common y-axis limits
all_mse_values = [mse for sublist in all_splits_mse for mse in sublist] + single_split_mse
y_min = min(all_mse_values)
y_max = max(all_mse_values)

# Define common y-axis ticks
y_ticks = np.linspace(10000, 18000, num=5)

# Plotting
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Left plot: Single split
axs[0].plot(degrees, single_split_mse, marker='o', label='Single Split')
axs[0].set_title('Validation MSE (Single Split)')
axs[0].set_xlabel('Polynomial Degree')
axs[0].set_ylabel('MSE')
axs[0].set_xticks(degrees)
axs[0].set_ylim(y_min, y_max)
axs[0].set_yticks(y_ticks)
# axs[0].legend()

# Right plot: Multiple splits
for i, mse in enumerate(all_splits_mse):
    axs[1].plot(degrees, mse, marker='o', label=f'Split {i+1}')
axs[1].set_title('Validation MSE (10 Random Splits)')
axs[1].set_xlabel('Polynomial Degree')
# axs[1].set_ylabel('MSE')
axs[1].set_xticks(degrees)
axs[1].set_ylim(y_min, y_max)
axs[1].set_yticks(y_ticks)
# axs[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אנחנו מדגימים את הבעייתיות של סינגל ספליט עם רגרסיה פולינימיאלית, שבה אנחנו מתאימים את Y כפונקציה פולינומיאלית של X, ומתלבטים בדרגת הפולינום הנכונה. מצד שמאל אפשר לראות את הMSE של המודל כתלות בדרגת הפולינום על ולידיישן סט שהמודל לא אומן עליו. מצד ימין אפשר לראות את אותו תרגיל על עשרה ולידיישן סט מקריים כולל את זה שלנו בכחול. יכול להיות שכולם יגיעו לאותה מסקנה מבחינת מודל סלקשן, שהדרגה שמביאה למינימום שגיאה ריבועית היא 3. אבל מבחינת מודל אססמנט, מה הMSE שצפוי לנו על נתונים שהמודל לא ראה, אפשר לראות שונות גדולה מאוד, אם אנחנו מסתכלים רק על טסט סט יחיד והוא לא מאוד גדול, אנחנו לא משקפים את זה. יכול להיות שראינו את אחת העקומות הפסימיות או האופטימיות מבין הספליטים האלה.
:::
:::

---

## Cross Validation {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### $K$-fold Cross Validation

```{python}
#| echo: false

import matplotlib.pyplot as plt

# Cross-validation parameters
k = 5
fold_proportion = 1 / k

# Plotting the cross-validation strategy
fig, axs = plt.subplots(k, 1, figsize=(10, 5))

for i in range(k):
    # Plot the full dataset rectangle
    rect_full = plt.Rectangle((0, 0), 1, 1, edgecolor='black', facecolor='none')
    axs[i].add_patch(rect_full)

    # Plot the folds
    for j in range(k):
        if j == i:
            # Validation set
            fold_rect = plt.Rectangle((j * fold_proportion, 0), fold_proportion, 1, edgecolor='black', facecolor='lightgreen')
            axs[i].add_patch(fold_rect)
            axs[i].text(j * fold_proportion + fold_proportion / 2, 0.5, 'Validation', ha='center', va='center', fontsize=10)
        else:
            # Training set
            fold_rect = plt.Rectangle((j * fold_proportion, 0), fold_proportion, 1, edgecolor='black', facecolor='lightblue')
            axs[i].add_patch(fold_rect)
            axs[i].text(j * fold_proportion + fold_proportion / 2, 0.5, 'Training', ha='center', va='center', fontsize=10)

    # Setting plot limits and removing axes
    axs[i].set_xlim(0, 1)
    axs[i].set_ylim(0, 1)
    axs[i].axis('off')
    axs[i].set_title(f'Fold {i + 1}')

plt.tight_layout()
# plt.suptitle('5-Fold Cross-Validation', y=1.02, fontsize=16)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
קרוס ולידיישן היא שיטה פופולרית מאוד כשאין לנו כל-כך הרבה דאטא, וגם כשאנחנו רוצים להימנע מלהסתמך על מדגם יחיד כשאנחנו באים לבצע אססמנט.

עיקר הפרוצדורה מוצג כאן, אנחנו נחלק את הדאטא לK חלקים שווים שנקראים פולדים או קפלים. כאן K שווה 5.

ונאמן כל אחד מהמודלים המועמדים K פעמים. כל פעם נבחר בפולד אחר להיות הולידיישן סט, כשאנחנו מאמנים את המודל על K פחות 1 הפולדים האחרים.

בסופו של דבר נאסוף את כל השגיאות מכל הn תצפיות כאשר הן שימשו כולידיישן. ומכאן מגיעה היעילות, השגיאה שאנחנו מדווחים היא מ100 אחוז מהמדגם!
:::
:::

---

### $K$-fold Cross Validation

More formally:

- Define a random partition of the $n$ data points into $K$ sets, each of size $n / K$:
$$\kappa: \{1,\dots,n\} \to \{1,\dots,K\}$$
- For $k = 1, \dots, K$ do:
  - Build a model $\hat{f}^{(-k)}$ using all folds except the $k$-th fold (total $n \cdot (K - 1)/ K$ data points)
  - Predict on remaining $k$-th fold: $L_k = \sum_{i \in k\text{th fold}}L(y_i, \hat{f}^{(-k)}(x_i))$
- CV estimate for prediction error: $CV = \frac{1}{n}\sum_{k = 1}^{K} L_k$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בצורה פורמלית יותר נגדיר חלוקה קאפא שממפה כל אחת מn התצפיות לאחד מK הפולדים.

לכל פולד, נאמן את מודל f_hat פחות K, כאשר אנחנו מאמנים על כל הפולדים חוץ מK.

נחזה על הפולד הK שהמודל לא ראה, נקרא לסכום השגיאה על כל התצפיות של הפולד הזה LK.

והשגיאה הסופית שלנו היא איסוף ומיצוע של השגיאה מכל הפולדים, כלומר סכום LK וחלוקה פי n.

נזכיר שפעמים רבות אנחנו רואים גם חישוב קצת אחר, שLK שווה לשגיאה הממוצעת על פני הפולד הK, ואז מדווחים על השגיאה הסופית כממוצע הממוצעים, ואפשר להוסיף לזה גם סטיית תקן או טעות תקן של הממוצע, על פני K הפולדים.
:::
:::

---

### Model selection with CV

- To perform model selection on tuning parameter $\lambda$:
$$L_k(\lambda) = \sum_{i \in k\text{th fold}}L(y_i, \hat{f}_{\lambda}^{(-k)}(x_i)), \quad CV(\lambda) = \frac{1}{n}\sum_{k = 1}^{K} L_k, \quad \hat{\lambda} = \arg\min_\lambda CV(\lambda)$$
- Alternatively use the 1-SE rule (see next slide)
- Optionally, train final model using $\hat{\lambda}$ on all $n$ observations and assess it on a fresh test set

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת, אם אנחנו רוצים לבצע מודל סלקשן, למשל לבחור איזשהו פרמטר למדא של המודל שמביא למינימום שגיאה אפשר לאמן את המודל עבור כמה למדא שונות. ולבחור את הלמדא שמביא למינימום את שגיאת הקרוס ולידיישן.

אם משתמשים בממוצע השגיאה בכל פולד ויש לנו טעות תקן שחישבנו לממוצע הזה על פני K פולדים, סטנדרט ארור, אפשר להשתמש גם בכלל ה1SE, שזה אומר לבחור את הפרמטר למדא שמביא למודל הפשוט ביותר כל עוד השגיאה שלו היא בטווח עד טעות תקן אחת לשגיאה הכי קטנה שהתקבלה. הטענה היא שאסטרטגיה כזאת שלא לבחור את המודל עם השגיאה הכי הכי קטנה אלא המודל הפשוט ביותר שהשגיאה שלו עדיין בתחום סביר, עשויה להקטין את הסיכוי לאוברפיטינג.

מכל מקום, אחרי שבחרנו את המודל הסופי או פרמטר סופי, ואנחנו רוצים לבצע מודל אססמנט. בדיוק מאותן סיבות שהזכרנו, נראה הרבה פעמים איחוד של כל הדאטא, אימון עם הפרמטר או המודל הסופי הנבחר על כל התצפיות, ואמידה של הטעות הסופית הסופית על דאטא שנשאר בצד, כלומר שילוב עם השיטה הקודמת.
:::
:::

---

### What does CV error estimate?

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold

# Step 1: Generate synthetic data with a clear cubic relationship
np.random.seed(0)
n = 1000
out_factor = 100
# X = np.random.rand(n, 1) * 10  # Features
X_out = np.random.rand(n * out_factor, 1) * 10  # Features
beta_0, beta_1, beta_2, beta_3 = 1, 2, 3, 4
# noise = np.random.randn(n, 1) * 100
noise_out = np.random.randn(n * out_factor, 1) * 100
# y = beta_0 + beta_1 * X + beta_2 * X**2 + beta_3 * X**3 + noise  # Cubic function with noise
y_out = beta_0 + beta_1 * X_out + beta_2 * X_out**2 + beta_3 * X_out**3 + noise_out  # Cubic function with noise

# Define polynomial degrees to test
degrees = range(2, 8)

# Function to perform k-fold cross-validation and compute MSE
def cross_val_mse(degrees, k=5):
    kf = KFold(n_splits=k, shuffle=True, random_state=0)
    mse_means = []
    mse_stds = []
    expected_mse = []
    
    for d in degrees:
        mse_folds = []
        poly = PolynomialFeatures(degree=d)
        np.random.seed(d + 100)
        X = np.random.rand(n, 1) * 10  # Features
        noise = np.random.randn(n, 1) * 100
        y = beta_0 + beta_1 * X + beta_2 * X**2 + beta_3 * X**3 + noise  # Cubic function with noise
        X_poly = poly.fit_transform(X)
        X_poly_out = poly.fit_transform(X_out)
        
        for train_index, val_index in kf.split(X):
            X_train, X_val = X_poly[train_index], X_poly[val_index]
            y_train, y_val = y[train_index], y[val_index]
            
            model = LinearRegression(fit_intercept=False).fit(X_train, y_train)
            y_val_pred = model.predict(X_val)
            
            mse_folds.append(mean_squared_error(y_val, y_val_pred))
        
        mse_means.append(np.mean(mse_folds))
        mse_stds.append(np.std(mse_folds) / np.sqrt(k))
        expected_mse.append(mean_squared_error(y_out, model.predict(X_poly_out)))
    
    return mse_means, mse_stds, expected_mse

# Compute MSE for polynomial regression models of varying degrees
mse_means, mse_stds, expected_mse = cross_val_mse(degrees)

# Plotting
plt.figure(figsize=(10, 5))
plt.errorbar(degrees, mse_means, yerr=mse_stds, fmt='-o', label='CV MSE Means with SE', capsize=5)
plt.plot(degrees, expected_mse, '-o', label='Expected Prediction Error')
# plt.title('Cross-Validation to Estimate Expected Prediction Error')
plt.xlabel('Polynomial Degree')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.xticks(degrees)
plt.show()

```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לפני שנמשיך, ראינו שיש כל מיני טעויות שניתן להסתכל עליהן, תלוי מה אנחנו מחליטים לקבע ולמה להתייחס כמשתנה מקרי. אז מסתבר שקרוס ולידיישן הוא אומד די טוב לתוחלת הפסד החיזוי שסימנו כErr, כלומר התוחלת השולית על פני הרבה מדגמים מקריים ותצפיות חדשות מקריות. כאן השתמשנו בדוגמא של הרגרסיה הפולינומיאלית ואנחנו יודעים לחשב אנליטית את השגיאה הזאת שאנחנו מנסים לאמוד. הנקודות הכחולות הן ממוצע הMSE על-פני 5 פולדים, והארור בארז הן פלוס מינוס טעות התקן של הממוצע. כאן המינימום מתקבל עבור פולינום מדרגה 4, אבל אם נשתמש בכלל ה1SE, נבחר במודל הפשוט יותר שעדיין מביא לשגיאה בטווח טעות התקן של השגיאה הטובה ביותר, וזה פולינום מדרגה 3. אפשרות אחרת היא אגב לבצע ממש מבחן סטטיסטי על פני הפולדים, לקבוע האם הפולדים בדרגה 3 מביאים בממוצע לשגיאה שונה באופן מובהק סטטיסטית מהפולדים בדרגה 4.

בכל מקרה אנחנו רואים שבמקרה זה שגיאת הקרוס-ולידיישן היא אומד לא רע עבור תוחלת הפסד החיזוי.
:::
:::

---

### Can $K$-fold CV be biased?

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

S  = np.concatenate(([0.001, 0.01, 0.05], np.linspace(0.1, 0.9, 9), [0.95, 0.99, 0.999]))
c = - np.log(1 - S) / S
plt.rcParams["figure.figsize"] = (4, 4)
plt.plot(c - 0.05, S - 0.2)
plt.xlabel('Size of Training Set')
plt.ylabel('1 - Err')
plt.ylim(0, 1)
plt.xticks([1, 3, 5, 7], labels=[0, 50, 100, 150])
plt.title('Hypothetical Learning Curve')
plt.show()
```

- Assume our model requires at least 50 data points to reach its potential
- What will happen if we do 5-fold CV on a training set that contains exactly 50 observations?
- $CV$ error would be biased, over-estimated

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בדוגמא שלנו היו מספיק תצפיות כדי ששגיאת הקרוס ולידיישן תהיה אומד טוב לתוחלת הפסד החיזוי לתצפיות שהמודל לא ראה. אבל צריך לזכור שלא נפתרו כל בעיותינו, אם הדאטא קטן ומכיל 50 תצפיות, והמודל שלנו צריך לפחות 50 תצפיות כדי לחזות טוב על תצפיות שלא ראה, ואנחנו עושים קרוס ולידיישן עם 5 פולדים - המודל יראה רק 40 תצפיות כל פעם!

ואפשר לדמיין מין עקומת למידה כזאת, היפותטית, שבה אכן מודל צריך 50 תצפיות כדי "להגשים את הפוטנציאל" שלו, ואנחנו תמיד רואים פחות. במקרה כזה המודל יבצע בממוצע פחות טוב, והשגיאה תהיה מוטה כלפי מעלה. אז צריך לקחת את זה לתשומת ליבנו.
:::
:::

---

### Leave-one-out CV (LOOCV)

- So why not $K = n$?
- Train on $n - 1$ observations excluding observation $i$, test on observation $i$

::: {.incremental}
  - Might be computationally intensive (though see Generalized CV (GCV))
  - Test on 1 observations, training sets extremely dependent $\to$ low bias but high variance!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ואם אנחנו כל-כך מודאגים - למה לא לעשות שK שווה לN, כלומר נחלק את הדאטא לN פולדים, כל פעם נאמן על N פחות 1 תצפיות ונבחן את המודל על התצפית שהשמטנו.

אז זה רעיון דווקא מקובל שנקרא ליב-וואן-אאוט קרוס ולידיישן, או LOOCV. באופן כזה אנחנו ממצים את הדאטא בצורה היעילה ביותר, ואנחנו יכולים להיות בטוחים שהשגיאה לא תהיה מוטה, אנחנו משתמשים בכמעט כל הדאטא שברשותנו לאימון ולחיזוי.

אז שתי בעיות:
בעיה ראשונה היא שמבחינה חישובית לאמן את המודל n פעמים יכול לקחת המון זמן ומשאבים. אם אנחנו רוצים גם לאמן D מודלים שונים על כל פולד זה אומר D כפול N פעמים לאמן! צריך להגיד שזאת בעיה בעיקר למודלים מודרניים, אם אנחנו מדברים רק על רגרסיה ליניארית, מסתבר שיש לנו קיצור דרך לחשב את השגיאה של ליב-וואן-אאוט, זה נקרא ג'נרלייזד קרוס ולידיישן, אבל לא נפתח את זה כרגע.

בעיה אחרת, גם אם חישובית אנחנו מצליחים להתגבר על המשימה הזאת, היא שכעת המדגמים שעליהם אנחנו מאמנים את המודלים הם מאוד מאוד תלויים, המודלים יראו תמיד את אותן התצפיות! אז השגיאה לא תהיה מוטה, ניצלנו את כל מה שהיה לנו, אבל השונות תהיה גבוהה, כי אנחנו בעצם חוזרים על אותו מודל כמעט N פעמים ולא מרוויחים שום דבר מהמיצוע שלו!

מבחינה מתמטית, אם נסתכל על נקודת הקיצון שאנחנו מאוד קרובים אליה, של כל המודלים זהים עם איזו שונות טאו בריבוע, ואנחנו ממצעים אותם, אנחנו בעצם מקבלים את אותו מודל מקורי עם שונות טאו בריבוע. מתי נרוויח ממיצוע K מודלים? רק אם הם יהיו כמה שיותר שונים זה מזה, בלתי תלויים, ואז שונות הממוצע שלהם תהיה קטנה פי K.

כלומר בשורה התחתונה חזרנו לנקודת ההתחלה, של להיות קרובים לאמן מודל אחד ויחיד, שיכולה להיות לו שונות גבוהה.
:::
:::

---

### CV - Interim Summary

:::: {.columns}

::: {.column width="33%"}
Single-split:

- $`K = 1`$
- Not efficient use of data
- Highly variable, "need 2nd opinion"
- For large $n$
:::

::: {.column width="33%"}
$K$-fold CV:

- $K = 5, 10$
- More efficient use of data but "Learning Curve"
- Can combine with an additional final test set
- Can become computationally intensive (2+ tuning params, finer grid)
- Probably best compromise
:::

::: {.column width="33%"}
LOOCV:

- $K = n$
- Most efficient use of data, unbiased
- But computational intensive for modern ML techniques, even for 1 param
- High variance
:::

::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז נסכם איפה אנחנו עומדים עד עכשיו עם קרוס ולידיישן:

בקצה האחד שאפשר לקרוא לו K = 1, למרות שזה לא נכון, אנחנו פשוט עושים סינגל ספליט על הנתונים, עם טריינינג סט אחד, ולידיישן סט אחד וטסט סט אחד -- זה מתאים למצב שיש לנו הרבה מאוד נתונים כי זו חלוקה לא יעילה של הדאטא. אנחנו מסתמכים גם על חלוקה אחת ויחידה ולכן הטעות הנאמדת הסופית יכולה להיות בעלת שונות גבוהה, היא לא ממוצע של כמה דעות.

בצד השני של הסקאלה K = N, זה השימוש היעיל ביותר בנתונים כי אנחנו מאמנים כל פעם כמעט על כולם ולא תהיה לנו שום הטייה שמקורה בגודל המדגם. אבל חישובית זה עלול להיות מופרך, אפילו אם כל מה שיש לנו לעשות זה לבחור פרמטר אחד, ובשיטות מודרניות יכולים להיות לנו כמה פרמטרים. ואנחנו מרוויחים ממש מעט מהממוצע הזה כי המודלים תלויים מאוד ביניהם מה שמשאיר את השונות של המודל האחד גבוהה.

כנראה שהפשרה היא איפשהו באמצע. אנחנו רואים בפועל שK שווה 5 או 10 עובד די טוב. זה נכון שעבור מעט תצפיות גם זה יכול להיות בעייתי בגלל הלרנינג קרב ואיבוד דאטא בזמן האימון, זה נכון שאם יש לנו הרבה פרמטרים לכוונון גם זה יכול להיעשות עמוס חישובית, אבל זאת פשרה לא רעה בין הקצוות.
:::
:::

---

## Cross Validation - Common Pitfalls {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
קרוס ולידיישן זאת שיטה שנמצאת בשימוש רחב, ויש למשתמשים נטייה גם לבצע אותה לא נכון. נסיים בשתי בעיות שאנחנו רואים הרבה פעמים בשימוש בשיטה.

:::
:::

---

### CV - Common Pitfalls (I)

- Suppose we have many days of sequential inter-daily data ([time series]{style="color:red;"})
- We do CV and randomly divide the data into parts
- Can we rely on the variance reduction of the parts?
- Did we leak information between the training part and the test part?

::: {.incremental}
- No and Yes!
- we cannot assume the parts are independent, and we probably have an information leak $\to$ we under-estimate the error
- How can we correct this?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בעיה ראשונה יכולה להיות כשבדאטה יש תלות אינהרנטית בין התצפיות, שלא נלקחת בחשבון בעת החלוקה לפולדים. דוגמא טובה לזה יכולה להיות כשהנתונים שלנו עם טיים סרייז, סדרה עתית, כמו למשל נתונים על פני ימים רבים של מחירי מניות, והמודל שלנו נבנה כדי לחזות את המחיר של מניה מחר.

אם נבצע חלוקה לפולדים בצורה נאיבית כמו לכל דאטא טבלאי:

האם אנחנו יכולים לסמוך על ההפחתה בשונות של האומד למדגם אחד, שאנחנו מצפים לקבל בעקבות חזרה על התהליך על פני כמה פולדים?
האם אין לנו זליגת של דאטא כל פעם בין מדגם הלמידה למדגם הולידיישן?

התשובות לשתי השאלות האלה לצערנו הן לא וכן.

יש כאן שתי בעיות: אחת היא תלות בזמן בין ימים, סביר להניח שיש תלות בין תצפיות מימים עוקבים למשל. והתלות הזאת יכולה להמשיך איתנו בחלוקה לפולדים, אנחנו בטוחים שאנחנו מחלקים את הדאטא בצורה מקרית ל5 או 10 מדגמים שונים, אבל בפועל מקבלים תלות בזמן בין מדגמי הלמידה השונים.

בעיה נוספת, במיוחד אם המטרה של מודל כזה היא חיזוי של מה שיקרה בעתיד - אנחנו כרגע גרמנו לו לראות את העתיד! אם בחלוקה לפולדים אנחנו לא מוודאים שפולד הטריינינג תמיד מכיל תצפיות מהעבר שהגיוני שהמודל ראה, ופולד הולידיישן תמיד מכיל תצפיות מהעתיד שהגיוני שהמודל צריך לחזות עליהן -- זה בדיוק זליגה של דאטא בין פולדים. בסופו של דבר המודל "חוזה" על העתיד, על תצפיות שהוא התאמן קרוב מאוד אליהן, והשגיאה שלו תהיה מוטה כלפי מטה, הוא יהיה אופטימי מדי.

לכן, צריך מאוד להיזהר בחלוקה לפולדים כשאנחנו יודעים שהתפקיד של המודל לחזות את העתיד של מניה למשל, או לחזות את הקרינה במרחב במיקומים חדשים על סמך קריאות ממיקומים אחרים. צריך לדאוג להיות עקביים ולשלב את המטרה הטבעית הזאת בחלוקה לפולדים, להתאמן על העבר, ולחזות את העתיד, או להתאמן על מיקומים ידועים ולחזות על מיקומים לא ידועים.
:::
:::

---

### CV - Common Pitfalls (II)

- Suppose we have a data with many variables, can we do the following process to estimate the prediction error:
  - On all the data find a small subset of the strongest variables
  - Perform CV using only this set of variables

::: {.incremental}
- Extremely problematic and overly optimistic!
- Where is the information leakage?
- Similar principle: standardizing the data based on all $n$ observations
- How can we correct this?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בעיה נוספת שאנחנו רואים המון, גם היא ביסודה זליגה או ליקג' של דאטא של הולידיישן אל הדאטא של הטריין:

נניח שיש לנו נתונים עם המון משתנים. המון משתנים ברמה שהמודל לא יכול להתמודד עם כל כך הרבה, ואנחנו מחלליטים קודם כל לעשות סקרינינג של תת-קבוצה של משתנים לפי איזשהו קריטריון, למשל אלה המשתנים שבאופן שולי בעלי הקורלציה הגבוהה ביותר למשתנה התלוי Y -- ורק עכשיו! להתחיל תהליך של קורס ולידיישן על המשתנים הנותרים -- יכול להיות שזה בעייתי?

זה יכול להיות בעייתי מאוד. להחליט החלטות בייסליין כאלה על פני כל המדגם. איפה כאן הדאטא ליקג'? בעובדה שהמודל כבר יודע משהו על התצפיות שתיכף יוגדרו כתצפיות ולידיישן! הוא כבר נחשף אליהן, הוא כבר ביצע החלטה על סמך התצפיות האלה! בעוד שב"חיים האמיתיים" של מודל כזה, לא תהיה לו האפשרות להציץ בתצפיות שהוא עומד לקבל, הוא לא יכול לסמוך על מידע שיש בהן בשום אופן כדי שנקבל אמידה טובה לביצועים שלו.

דוגמא נוספת שאנחנו רואים זה לעשות סטנדרטיזציה של הדאטא, כלומר לחסר את הממוצע מכל העמודות ולחלק בסטיית התקן, ולעשות את זה על סמך כל הN תצפיות שברשותנו, ורק אז להתחיל תהליך של קרוס ולידיישן. בעייתי מאותה סיבה בדיוק, הממוצעים וסטיות התקן האלה כבר חושבו על סמך התצפיות שבטסט, המודל מכיר אותן ועלול לתת שגיאה אופטימית מדי, מוטה כלפי מטה.

איך מתקנים את זה? צריך לכלול את כל הצעדים הבעייתיים האלה בתהליך הקרוס ולידיישן. אם לדוגמא אנחנו חייבים לעשות סקרינינג של משתנים רלוונטים מתוך הרבה משתנים, זה חייב להיות שלב ראשון שאנחנו מבצעים בתוך הקרוס ולידיישן, לכל אחד ממדגמי הלמידה, אחרי שהשמטנו את מדגם הולידיישן. כלומר הקרוס ולידיישן חייב "לבדוק" עבורנו גם את הצעד הזה. יכול להיות שבמדגם למידה אחד ייבחרו 10 משתנים מסוימים ובמדגם למידה אחר ייבחרו 11 משתנים קצת אחרים וזה בסדר! כי זה רעש מקרי שחייבים לקחת בחשבון בתהליך הלמידה.
:::
:::

---

### Use `Pipeline`s

```{python}
#| eval: false

from sklearn.model_selection import KFold, GridSearchCV
from sklearn.pipeline import Pipeline

kf = KFold(n_splits=5, shuffle=True, random_state=0)

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('feature_selection', SelectKBest(score_func=f_regression)),
    ('knn', KNeighborsRegressor())
])

param_grid = {
    'feature_selection__k': range(1, n_features + 1),
    'knn__n_neighbors': [3, 5, 7, 9],
    'knn__metric': ['euclidean', 'manhattan']
}

grid_search = GridSearchCV(pipeline, param_grid, cv=kf, scoring='neg_mean_squared_error')
grid_search.fit(X, y)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מבחינה מעשית, כל תוכנה רצינית שאיתה מריצים מודלים חייבת לכלול איזשהו קונספט של פייפליין, או "מתכון" שאפשר להגדיר, והוא יהיה מורכב מכמה תת-צעדים שכוללים לא רק את ריצת המודל עצמו. ועל הפייפליין הזה נריץ קרוס ולידיישן, לא רק על ריצת המודל עצמו.

כאן לדוגמא אני משתמש בקלאס פייפליין של sklearn. אני מגדיר שהמתכון הזה יהיה מורכב מ3 צעדים: סטנדרטיזציה של המשתנים, בחירה של סאבסט של משתנים, ורק אז שיטה לרגרסיה כמו KNN. אני מגדיר מהו הגריד של פרמטרים שונים שאני רוצים לבחור מתוכו, לדוגמא כאן מהו הK משתנים שאני רוצה לבחור מתוך הP שיש ברשותי, מהו מספר השכנים של KNN ובאיזו מטריקה KNN ישתמש.

ואז אני משתמש בקלאס שנקרא GridSearchCV שמחפש על כל הקומבינציות של ה3 פרמטרים שחשובים לי, מריץ מודל עם כל קומבינציה באמצעות קרוס-ולידיישן עם 5 פולדים. אבל הכי חשוב, כשאני קורא למתודה פיט, הכוונה לפיט על כל הפייפליין הזה, כולל סטנדרטיזציה, וכולל בחירה של סאבסט של משתנים. לא רק ריצה של KNN עצמו. בדרך הזאת אני מוודא שאין לי זליגה של דאטא מהולידיישן סט לתוך הטריינינג והקרוס ולידיישן אכן מודד את התהליך שאני הייתי רוצה לראות על כל הדאטא.

עד כאן במודל סלקשן, בחלק השני נדבר על שתי השיטות שנותרו לנו - הבוטסטרפ, שמזכירה קצת את הקרוס-ולידיישן במובן של אימון על פני מספר מדגמים שונים מהדאטא. והשיטה של תיקון שגיאת הטריינינג שהיא נמוכה בדרך כלל ממה שאנחנו רוצים, באמצעות איזשהו אומד לעד כמה היא נמוכה, או אופטימית.
:::
:::
