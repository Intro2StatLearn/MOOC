---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Model Selection - Part A"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Model Selection - Part A - Class 5

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Expected Prediction Error {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Previously with the Bias-Variance Tradeoff

- For regression, take the standard model: $y = f(x) + \epsilon\;,\;\epsilon \sim (0,\sigma^2)$

- Modeling approach (e.g. OLS), given training data $T$, gives model $\hat{f}(x)$

::: {.fragment}
- Assume we want to predict at new point $x_0$, and understand our expected (squared) prediction error: 
$$\mathbb{E}_{y_0, T}(y_0 - \hat{f}(x_0))^2 = \text{irreducible error} + \text{squared bias} + \text{variance}$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Expected Prediction Error

$$\mathbb{E}_{y_0, T}(y_0 - \hat{f}(x_0))^2 = \text{irreducible error} + \text{squared bias} + \text{variance}$$

::: {.incremental}
- Note we treat both the training data $T$ (and hence $\hat{f}$) and the response $y_0$ as random variables in our expectations
- So, more generally we decomposed: $\mathbb{E}_{y_0, T}(L(y_0, \hat{f}(x_0))|X = x_0)$
- [Expected prediction error]{style="color:red;"} is when we average over all $x_0$:
$$Err = \mathbb{E}_{x_0, y_0, T}(L(y_0, \hat{f}(x_0))) = \mathbb{E}_{X}\left[\mathbb{E}_{y_0, T}(L(y_0, \hat{f}(x_0))|X = x_0)\right]$$
- This could also be written as:
$$Err = \mathbb{E}_{x_0, y_0, T}(L(y_0, \hat{f}(x_0))) = \mathbb{E}_{T}\left[\mathbb{E}_{x_0, y_0}(L(y_0, \hat{f}(x_0))|T)\right] = \mathbb{E}\left[Err_T\right]$$
- Some would say the *conditional* $Err_T$ is even more interesting!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What EPE is for?

::: {.fragment}
1. [Model Selection]{style="color:red;"}: select between a set of models (e.g. one with 5 parameters and the other with 6 parameters) the one with lowest error
2. [Model Assessment]{style="color:red;"}: know how accurate the model would be, estimate the error itself
:::


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### How to estimate EPE?

::: {.fragment}
::: {.callout-note}
What do you mean how, why not training error:
$$\overline{err} = \frac{1}{n}\sum_{i=1}^{n} L(y_i, \hat{f}(x_i))$$
:::
:::

::: {.incremental}
1. Data splitting: Train-Validation-Test
2. Cross Validation
3. The Bootstrap
4. Training error + Optimism
:::


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Data splitting {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Data splitting: Train-Validation-Test

```{python}
#| echo: false

import matplotlib.pyplot as plt

# Data splitting proportions
training_proportion = 0.6
validation_proportion = 0.2
test_proportion = 0.2

# Plotting the data splitting strategy
fig, ax = plt.subplots(figsize=(10, 2))

# Plot the full dataset rectangle
rect_full = plt.Rectangle((0, 0), 1, 1, edgecolor='black', facecolor='none')
ax.add_patch(rect_full)

# Plot the training set rectangle
rect_train = plt.Rectangle((0, 0), training_proportion, 1, edgecolor='black', facecolor='lightblue')
ax.add_patch(rect_train)
ax.text(training_proportion / 2, 0.5, 'Training Set\n60%', ha='center', va='center', fontsize=12)

# Plot the validation set rectangle
rect_val = plt.Rectangle((training_proportion, 0), validation_proportion, 1, edgecolor='black', facecolor='lightgreen')
ax.add_patch(rect_val)
ax.text(training_proportion + validation_proportion / 2, 0.5, 'Validation Set\n20%', ha='center', va='center', fontsize=12)

# Plot the test set rectangle
rect_test = plt.Rectangle((training_proportion + validation_proportion, 0), test_proportion, 1, edgecolor='black', facecolor='lightcoral')
ax.add_patch(rect_test)
ax.text(training_proportion + validation_proportion + test_proportion / 2, 0.5, 'Test Set\n20%', ha='center', va='center', fontsize=12)

# Setting plot limits and removing axes
ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.axis('off')

# plt.title('Data Splitting Strategy in Machine Learning')
plt.show()
```

Divide the given sample to 3:

- [Training set]{style="color:red;"}: learn different models
- [Validation set]{style="color:red;"}: decide on final model (model selection, tuning)
- [Test set]{style="color:red;"}: estimate final model's performance (model assessment)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Data splitting: comments

::: {.incremental}
- Test set is used only once!
- Since larger $n$ should improve model's performance, after model selection:
  - unite {training + validation}, train final model and then assess it on the test set
- No clear guidelines: 60-20-20%, 50-25-25%, ... (depends on $n$ and nature of $f$)
- Disadvantages:
  - Lose half the data? Only if we are rich in data
  - Model assessment highly variable
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Single split: need 2nd opinion

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Step 1: Generate synthetic data with a clear cubic relationship
np.random.seed(0)
n = 1000
X = np.random.rand(n, 1) * 10  # Features
beta_0, beta_1, beta_2, beta_3 = 1, 2, 3, 4
noise = np.random.randn(n, 1) * 100
y = beta_0 + beta_1 * X + beta_2 * X**2 + beta_3 * X**3 + noise  # Cubic function with noise

# Define polynomial degrees to test
degrees = range(2, 8)

# Function to train and evaluate polynomial regression models
def polynomial_regression(X_train, y_train, X_val, y_val, degrees):
    validation_mse = []
    for d in degrees:
        poly = PolynomialFeatures(degree=d)
        X_train_poly = poly.fit_transform(X_train)
        X_val_poly = poly.transform(X_val)
        
        model = LinearRegression(fit_intercept=False).fit(X_train_poly, y_train)
        y_val_pred = model.predict(X_val_poly)
        
        mse = mean_squared_error(y_val, y_val_pred)
        validation_mse.append(mse)
    return validation_mse

# Single split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=0)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)

# Validation MSE for single split
single_split_mse = polynomial_regression(X_train, y_train, X_val, y_val, degrees)

# Multiple random splits
num_splits = 10
all_splits_mse = []

for i in range(num_splits):
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=i)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=i)
    mse = polynomial_regression(X_train, y_train, X_val, y_val, degrees)
    all_splits_mse.append(mse)

# Determine common y-axis limits
all_mse_values = [mse for sublist in all_splits_mse for mse in sublist] + single_split_mse
y_min = min(all_mse_values)
y_max = max(all_mse_values)

# Define common y-axis ticks
y_ticks = np.linspace(10000, 18000, num=5)

# Plotting
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Left plot: Single split
axs[0].plot(degrees, single_split_mse, marker='o', label='Single Split')
axs[0].set_title('Validation MSE (Single Split)')
axs[0].set_xlabel('Polynomial Degree')
axs[0].set_ylabel('MSE')
axs[0].set_xticks(degrees)
axs[0].set_ylim(y_min, y_max)
axs[0].set_yticks(y_ticks)
# axs[0].legend()

# Right plot: Multiple splits
for i, mse in enumerate(all_splits_mse):
    axs[1].plot(degrees, mse, marker='o', label=f'Split {i+1}')
axs[1].set_title('Validation MSE (10 Random Splits)')
axs[1].set_xlabel('Polynomial Degree')
# axs[1].set_ylabel('MSE')
axs[1].set_xticks(degrees)
axs[1].set_ylim(y_min, y_max)
axs[1].set_yticks(y_ticks)
# axs[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Cross Validation {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### $K$-fold Cross Validation

```{python}
#| echo: false

import matplotlib.pyplot as plt

# Cross-validation parameters
k = 5
fold_proportion = 1 / k

# Plotting the cross-validation strategy
fig, axs = plt.subplots(k, 1, figsize=(10, 5))

for i in range(k):
    # Plot the full dataset rectangle
    rect_full = plt.Rectangle((0, 0), 1, 1, edgecolor='black', facecolor='none')
    axs[i].add_patch(rect_full)

    # Plot the folds
    for j in range(k):
        if j == i:
            # Validation set
            fold_rect = plt.Rectangle((j * fold_proportion, 0), fold_proportion, 1, edgecolor='black', facecolor='lightgreen')
            axs[i].add_patch(fold_rect)
            axs[i].text(j * fold_proportion + fold_proportion / 2, 0.5, 'Validation', ha='center', va='center', fontsize=10)
        else:
            # Training set
            fold_rect = plt.Rectangle((j * fold_proportion, 0), fold_proportion, 1, edgecolor='black', facecolor='lightblue')
            axs[i].add_patch(fold_rect)
            axs[i].text(j * fold_proportion + fold_proportion / 2, 0.5, 'Training', ha='center', va='center', fontsize=10)

    # Setting plot limits and removing axes
    axs[i].set_xlim(0, 1)
    axs[i].set_ylim(0, 1)
    axs[i].axis('off')
    axs[i].set_title(f'Fold {i + 1}')

plt.tight_layout()
# plt.suptitle('5-Fold Cross-Validation', y=1.02, fontsize=16)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### $K$-fold Cross Validation

More formally:

- Define a random partition of the $n$ data points into $K$ sets, each of size $n / K$:
$$\kappa: \{1,\dots,n\} \to \{1,\dots,K\}$$
- For $k = 1, \dots, K$ do:
  - Build a model $\hat{f}^{(-k)}$ using all folds except the $k$-th fold (total $n \cdot (K - 1)/ K$ data points)
  - Predict on remaining $k$-th fold: $L_k = \sum_{i \in k\text{th fold}}L(y_i, \hat{f}^{(-k)}(x_i))$
- CV estimate for prediction error: $CV = \frac{1}{n}\sum_{k = 1}^{K} L_k$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Model selection with CV

- To perform model selection on tuning parameter $\lambda$:
$$L_k(\lambda) = \sum_{i \in k\text{th fold}}L(y_i, \hat{f}_{\lambda}^{(-k)}(x_i)), \quad CV(\lambda) = \frac{1}{n}\sum_{k = 1}^{K} L_k, \quad \hat{\lambda} = \arg\min_\lambda CV(\lambda)$$
- Alternatively use the 1-SE rule (see next slide)
- Train final model using $\hat{\lambda}$ and optionally assess it on a fresh test set

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What does CV error estimate?

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold

# Step 1: Generate synthetic data with a clear cubic relationship
np.random.seed(0)
n = 1000
out_factor = 100
# X = np.random.rand(n, 1) * 10  # Features
X_out = np.random.rand(n * out_factor, 1) * 10  # Features
beta_0, beta_1, beta_2, beta_3 = 1, 2, 3, 4
# noise = np.random.randn(n, 1) * 100
noise_out = np.random.randn(n * out_factor, 1) * 100
# y = beta_0 + beta_1 * X + beta_2 * X**2 + beta_3 * X**3 + noise  # Cubic function with noise
y_out = beta_0 + beta_1 * X_out + beta_2 * X_out**2 + beta_3 * X_out**3 + noise_out  # Cubic function with noise

# Define polynomial degrees to test
degrees = range(2, 8)

# Function to perform k-fold cross-validation and compute MSE
def cross_val_mse(degrees, k=5):
    kf = KFold(n_splits=k, shuffle=True, random_state=0)
    mse_means = []
    mse_stds = []
    expected_mse = []
    
    for d in degrees:
        mse_folds = []
        poly = PolynomialFeatures(degree=d)
        np.random.seed(d + 100)
        X = np.random.rand(n, 1) * 10  # Features
        noise = np.random.randn(n, 1) * 100
        y = beta_0 + beta_1 * X + beta_2 * X**2 + beta_3 * X**3 + noise  # Cubic function with noise
        X_poly = poly.fit_transform(X)
        X_poly_out = poly.fit_transform(X_out)
        
        for train_index, val_index in kf.split(X):
            X_train, X_val = X_poly[train_index], X_poly[val_index]
            y_train, y_val = y[train_index], y[val_index]
            
            model = LinearRegression(fit_intercept=False).fit(X_train, y_train)
            y_val_pred = model.predict(X_val)
            
            mse_folds.append(mean_squared_error(y_val, y_val_pred))
        
        mse_means.append(np.mean(mse_folds))
        mse_stds.append(np.std(mse_folds) / np.sqrt(k))
        expected_mse.append(mean_squared_error(y_out, model.predict(X_poly_out)))
    
    return mse_means, mse_stds, expected_mse

# Compute MSE for polynomial regression models of varying degrees
mse_means, mse_stds, expected_mse = cross_val_mse(degrees)

# Plotting
plt.figure(figsize=(10, 5))
plt.errorbar(degrees, mse_means, yerr=mse_stds, fmt='-o', label='CV MSE Means with SE', capsize=5)
plt.plot(degrees, expected_mse, '-o', label='Expected Prediction Error')
# plt.title('Cross-Validation to Estimate Expected Prediction Error')
plt.xlabel('Polynomial Degree')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.xticks(degrees)
plt.show()

```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Can $K$-fold CV be biased?

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

S  = np.concatenate(([0.001, 0.01, 0.05], np.linspace(0.1, 0.9, 9), [0.95, 0.99, 0.999]))
c = - np.log(1 - S) / S
plt.rcParams["figure.figsize"] = (4, 4)
plt.plot(c - 0.05, S - 0.2)
plt.xlabel('Size of Training Set')
plt.ylabel('1 - Err')
plt.ylim(0, 1)
plt.xticks([1, 3, 5, 7], labels=[0, 50, 100, 150])
plt.title('Hypothetical Learning Curve')
plt.show()
```

- Assume our model requires at least 50 data points to reach its potential
- What will happen if we do 5-fold CV on a training set that contains exactly 50 observations?
- $CV$ error would be biased, over-estimated

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Leave-one-out CV (LOOCV)

- So why not $K = n$?
- Train on $n - 1$ observations excluding observation $i$, test on observation $i$

::: {.incremental}
  - Might be computationally intensive (though see Generalized CV (GCV))
  - Test on 1 observations, training sets extremely dependent $\to$ low bias but high variance!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### CV - Interim Summary

:::: {.columns}

::: {.column width="33%"}
Single-split:

- $`K = 1`$
- Not efficient use of data
- Highly variable, "need 2nd opinion"
- For large $n$
:::

::: {.column width="33%"}
$K$-fold CV:

- $K = 5, 10$
- More efficient use of data but "Learning Curve"
- Can combine with an additional final test set
- Can become computationally intensive (2+ tuning params, finer grid)
- Probably best compromise
:::

::: {.column width="33%"}
LOOCV:

- $K = n$
- Most efficient use of data, unbiased
- But computational intensive for modern ML techniques, even for 1 param
- High variance
:::

::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### CV - Common Pitfalls (I)

- Suppose we have many days of sequential inter-daily data ([time series]{style="color:red;"})
- We do CV and randomly divide the data into parts
- Can we rely on the variance reduction of the parts?
- Did we leak information between the training part and the test part?

::: {.incremental}
- No and Yes!
- we cannot assume the parts are independent, and we probably have an information leak $\to$ we under-estimate the error
- How can we correct this?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### CV - Common Pitfalls (II)

- Suppose we have a data with many variables, can we do the following process to estimate the prediction error:
  - On all the data find a small subset of the strongest variables
  - Perform CV using only this set of variables

::: {.incremental}
- Extremely problematic and overly optimistic!
- Where is the information leakage?
- Similar principle: standardizing the data based on all $n$ observations
- How can we correct this?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Use `Pipeline`s

```{python}
#| eval: false

from sklearn.model_selection import KFold, GridSearchCV
from sklearn.pipeline import Pipeline

kf = KFold(n_splits=5, shuffle=True, random_state=0)

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('feature_selection', SelectKBest(score_func=f_regression)),
    ('knn', KNeighborsRegressor())
])

param_grid = {
    'feature_selection__k': range(1, n_features + 1),
    'knn__n_neighbors': [3, 5, 7, 9],
    'knn__metric': ['euclidean', 'manhattan']
}

grid_search = GridSearchCV(pipeline, param_grid, cv=kf, scoring='neg_mean_squared_error')
grid_search.fit(X, y)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
