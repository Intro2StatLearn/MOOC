=== 1. תוחלת טעות החיזוי ===

עד כאן בקורס, הבנו מה המטרה של למידה סטטיסטית, ראינו את הדוגמא הקלאסית ביותר שהיא המודל הליניארי, אבל חסרה לנו תשובה מעמיקה לשאלה ממש פשוטה: איך אנחנו בוחרים בין מודלים, ואיך נכון לאמוד את הביצועים של מודל סופי. אמנם קיבלנו תחושה כבר שלא נכון להשתמש באותו סט של נתונים לאימון המודל, לבחירה של המודל, להערכה של המודל. אבל בשיעור הזה נפרמל את התחושה הזאת ונדבר לעומק על מספר דרכים שונות לבחור בין מודלים.
:::

כדי להגדיר את הקריטריון שלנו אנחנו צריכים להיות מדויקים. מה מבחינתנו נשאר קבוע, מה משתנה מקרי. כי מסתבר שיש יותר מקריטריון אחד.

נתחיל במודל רגרסיה פשוט שבו עסקנו עד כה: Y שווה לאיזושהי פונקציה F של X ועוד רעש מקרי אפסילון, שמתפלג לאו דווקא נורמלית, עם תוחלת אפס ושונות סיגמא בריבוע. אמרנו שאנחנו מקבלים מדגם T בגודל n תצפיות, ומעוניינים לאמוד את F.

ואז פיתחנו את פירוק הביאס-וריאנס, ראינו שעבור תצפית חדשה עם X0 נתון, ואנחנו עושים תוחלת על כל השאר, על-פני מדגם הלמידה שראינו ועל פני הY0 של התצפית החדשה, ההפסד הריבועי מתפרק לטעות אירדוסיבל, היא הסיגמא בריבוע, רעש בסיסי שהמודל שלנו לא מצליח למדל, ועוד הטייה בריבוע, טעות קירוב שנובעת מאופי המודל שלנו, אם נניח שF היא פונקציה מורכבת, אז מודל מורכב יוכל לתאר אותה עם הטיה קטנה, ולמודל פשטני או פשוט לא נכון תהיה הטיה גדולה. וטעות אחרונה של שונות, שנובעת מהתוחלת על-פני הרבה מדגמי למידה, אם נראה מדגם למידה קצת שונה, האם המודל ייראה אחרת לגמרי או שהוא יציב.

:::

אז כאן אנחנו מתייחסים למדגם הלמידה שלנו כמשתנה מקרי ולY0 כמשתנה מקרי, אבל לX0 כנתון. כלומר אנחנו מפרקים את הטעות הריבועית אם נראה כל מיני מדגמי למידה אחרים וכל מיני ערכי Y0 אחרים לתצפית הX0 הנתונה הזאת.

באופן יותר כללי נרשום זאת כך, עבור איזשהו הפסד L, הסתכלנו על תוחלת מותנית, תוחלת ההפסד ביחס לT ו-Y0 בהינתן X0.

אבל באופן אינטואיטיבי, רבים היו מגדירים את הטעות שהם מעוניינים בה, כתוחלת שולית, על הכל, כלומר גם על המדגם שראינו, וגם על תצפיות חדשות. או במילים אחרות תוחלת של X על התוחלת המותנית שהסתכלנו עליה. נסמן את התוחלת הזאת בErr, ונקרא לה האקספקטד פרדיקשן ארור. שמות אחרים יכולים להיות ג'נ'רליזיישן ארור, טעות ההכללה או האקספקטד טסט ארור.

אפשר לרשום את הטעות הזאת, את התוחלת הזאת כתוחלת שלמה של תוחלת מותנית אחרת: התוחלת המותנית בהינתן מדגם הלמידה, הזוגות של X ו-Y שמהם למדנו. ואז התוחלת היא רק על תצפיות חדשות X0 וY0. והטעות שלנו היא התוחלת על-פני כל מדגמי הלמידה. נסמן את התוחלת המותנית החדשה כErrT, ואם נחשוב עליה קצת - אולי היא זאת שבכלל מעניין אותנו לאמוד! הרי במציאות אני לא מקבל עוד ועוד מדגמי למידה ומעניין אותי איך מבצע המודל שלי על פני הרבה מדגמי למידה. לא! אני מקבל מדגם למידה אחד ובהנחה שהמדגם שלי טוב, כלומר מייצג, ואין שיפט בדאטא שהמודל ייבחן עליו, מעניין אותי לדעת את הביצועים של המודל בהינתן המדגם הזה, לכל תצפית חדשה שתבוא.

זאת באמת טעות שמעניין להעריך אבל קשה להעריך אותה. בפועל נראה שהרבה יותר קל לאמוד את הטעות דווקא השולית, תוחלת ההפסד על פני הרבה מדגמי למידה והרבה תצפיות חדשות.

:::

בשלב הזה ראוי להפריד בין שתי מטרות ראויות:

האחת היא מודל סלקשן, אני רוצה לבחור מתוך סט של מודלים נאמדים את הטוב ביותר, עם המינימום תוחלת הפסד החיזוי. לדוגמא במודל הליניארי לבחור בין מודל עם 5 משתנים או 6.

ומטרה אחרת היא לקחת מודל סופי שנבחר, ולאמוד באמצעותו את תוחלת הפסד החיזוי עצמה, את הביצועים של המודל הסופי שנבחר. מטרה זו נקראת מודל אססמנט או אבליואיישן.

:::

ושוב נזכיר - מה, אי אפשר לאמוד את תוחלת ההפסד עם ההפסד של מדגם הלמידה? אנחנו נסמן אותו כerr עם קו מעל. התשובה צריכה כבר להיות ברורה לכולם: מאוד מאוד לא נכון לאמוד את השגיאה של מודל על תצפיות שהוא לא למד עליהן, מתצפיות שהוא כן למד עליהן. השגיאה הזאת אופטימית מדי. אם נרצה לבחור למשל את המודל הטוב ביותר לפי המודל שמביא את שגיאת הטריין למינימום, רוב הסיכויים שנבחר במודל מורכב מדי שלא לצורך, שעושה אוברפטינג למדגם הלמידה, מתחיל למדל את האירדוסיבל ארור ולא יכליל טוב כשיגיעו תצפיות חדשות.

אז ניכנס לעומק של ארבע אופציות טובות יותר:

ברמה הבסיסית ביותר יש לנו את האופציה של חלוקה יחידה של הדאטא שלנו לטריין, ולידיישן וטסט.

גירסה קצת יותר יעילה של חלוקה כזאת היא חלוקה של הדאטא לכמה פולדים, מה שנקרא קרוס-ולידיישן.

נדבר גם על הבוטסטרפ, ובסופו של דבר גם על גישה מסורתית יותר שמתאימה במיוחד למדגמים קטנים, שבה אנחנו פשוט מנסים לאמוד עד כמה מדגם הלמידה אופטימי, ולהוסיף את התיקון הזה לטריינינג לוס.

:::

=== 2. דאטה ספליטינג ===

נתחיל בגישה המתבקשת: לפצל את הדאטא שלנו, לעשות לו ספליטינג.

הגישה הקלאסית אומרת לפצל אותו ל3: טריינינג, ולידיישן וטסט. אין המלצה אחידה שמתאימה לכל דאטא, כמה אחוזים לשים בכל ספליט. כאן אנחנו רואים חלוקה סבירה של 60 אחוז לטריינינג, 20 אחוז לולידיישן ו20 אחוז לטסט.

והפרוצדורה היא כזאת: נאמן את כל המודלים המועמדים על הטריינינג דאטא. נבחן את הביצועים שלהם על הולידיישן דאטא ונבחר את המודל הטוב ביותר לפי זה שמביא את השגיאה של הולידיישן למינימום. זה המודל סלקשן, או טיונינג, כוונון של איזשהו פרמטר לא ידוע במודל. לבסוף, כדי לבצע מודל אססמנט, לאמוד את תוחלת הפסד החיזוי או הביצועים של המודל, ניגש לסט הטסט של נתונים ששמרנו בצד.

בשלב הזה מתבקשת השאלה - למה אנחנו צריכים עוד סט של נתונים כדי לאמוד את השגיאה הסופית של המודל? כרגע הרצנו אותו על הולידיישן סט, אי אפשר להשתמש בשגיאה הזאת כאומד לביצועים של המודל על דאטא שהוא לא ראה? התשובה היא שלא, זה בעייתי. כבר השתמשנו בשגיאה הזאת כדי לקבוע מי המודל עם הביצועים הטובים ביותר. אנחנו לא יכולים להשתמש בה גם לאמוד את שגיאת החיזוי עצמה, כי אנחנו כבר יודעים שאנחנו מסתכלים על השגיאה הכי קטנה. מבחינה מתמטית, אם השגיאת ולידיישן של מודל 1 אומד חסר הטיה לתוחלת השגיאה של מודל 1, והשגיאת ולידיישן של מודל 2 אומד חסר הטיה לתוחלת השגיאה של מודל 2, הפרש השגיאות ולידיישן הוא אומד טוב להפרש התוחלות כלומר לקבוע מי המודל הטוב יותר. אבל המינימום מביניהן הוא לא אומד חסר הטיה למינימום התוחלות.

:::

כמה הערות:

הטסט סט בשימוש רק פעם אחת! חוקר טוב צריך להיות כן עם עצמו ובשביל אמידת שגיאת החיזוי הסופית צריך להשתמש בדאטא טרי באמת.

הערה נוספת: מקובל אחרי בחירת המודל הסופי לאחד את כל הדאטא שיש לנו, ה60 אחוז של הטריינינג וה20 אחוז של ולידיישן כדי לאמן את המודל פעם נוספת, ואת המודל הזה להעריך על הטסט סט פעם אחרונה.

כאמור, אין הוראה ברורה כמה אחוזים בדיוק לתקצב כל אחד מהספליטים, זה תלוי בגודל המדגם הנתון ובמורכבות היחס F בין X לY שאנחנו מנסים להעריך. אם הוא פשוט סביר שלא צריך המון דאטא לאמן אותו, ואם הוא מורכב צריך כמה שיותר מהדאטא לאמן אותו.

אז זאת הגישה הקלאסית, אבל יש לה שתי בעיות עיקריות: אחת היא, שאנחנו מאמנים את כל המודלים המועמדים על 50-60 אחוז מהתצפיות, זה שימוש לא יעיל בנתונים שאפשרי רק אם יש לנו הרבה מהם.

והבעיה השניה היא שאמנם האומד לשגיאת החיזוי חסר הטיה אבל יש לו שונות גבוהה, אנחנו מבססים את האמידה שלנו בכל זאת על סט נתונים אחד ויחיד, אפילו שהוא נדגם מקרית, ובמיוחד עם מודלים מודרנים של למידה סטטיסטית, לא מסוגלים לתת שום אומד לשגיאה של השגיאה, לאיזו סטיית תקן לאומד הסופי הזה של ביצועי המודל.

והערה אחרונה שלא מדוברת הרבה, ונרחיב עליה יותר עוד שני שיעורים: התוצאה על ה test היא אומד בלתי מוטה לביצועי המודל בעולם האמיתי. אבל כדי שזה יהיה נכון, אנחנו חייבים להניח שהעולם שממנו למדנו אינו משתנה עם הזמן או ליתר דיוק שהתפלגות המדגם שניתן לנו מתאימה להתפלגות העולם.

:::

כאן אנחנו מדגימים את הבעייתיות של סינגל ספליט עם רגרסיה פולינימיאלית, שבה אנחנו מתאימים את Y כפונקציה פולינומיאלית של X, ומתלבטים בדרגת הפולינום הנכונה. מצד שמאל אפשר לראות את הMSE של המודל כתלות בדרגת הפולינום על ולידיישן סט שהמודל לא אומן עליו. מצד ימין אפשר לראות את אותו תרגיל על עשרה ולידיישן סט מקריים כולל את זה שלנו בכחול. יכול להיות שכולם יגיעו לאותה מסקנה מבחינת מודל סלקשן, שהדרגה שמביאה למינימום שגיאה ריבועית היא 3. אבל מבחינת מודל אססמנט, מה הMSE שצפוי לנו על נתונים שהמודל לא ראה, אפשר לראות שונות גדולה מאוד, אם אנחנו מסתכלים רק על טסט סט יחיד והוא לא מאוד גדול, אנחנו לא משקפים את זה. יכול להיות שראינו את אחת העקומות הפסימיות או האופטימיות מבין הספליטים האלה.

:::

=== 3. קרוס ולידציה ===

קרוס ולידיישן היא שיטה פופולרית מאוד כשאין לנו כל-כך הרבה דאטא, וגם כשאנחנו רוצים להימנע מלהסתמך על מדגם יחיד כשאנחנו באים לבצע אססמנט.

עיקר הפרוצדורה מוצג כאן, אנחנו נחלק את הדאטא לK חלקים שווים שנקראים פולדים או קפלים. כאן K שווה 5.

ונאמן כל אחד מהמודלים המועמדים K פעמים. כל פעם נבחר בפולד אחר להיות הולידיישן סט, כשאנחנו מאמנים את המודל על K פחות 1 הפולדים האחרים.

בסופו של דבר נאסוף את כל השגיאות מכל הn תצפיות כאשר הן שימשו כולידיישן. ומכאן מגיעה היעילות, השגיאה שאנחנו מדווחים היא מ100 אחוז מהמדגם!

:::

בצורה פורמלית יותר נגדיר חלוקה קאפא שממפה כל אחת מn התצפיות לאחד מK הפולדים.

לכל פולד, נאמן את מודל f_hat פחות K, כאשר אנחנו מאמנים על כל הפולדים חוץ מK.

נחזה על הפולד הK שהמודל לא ראה, נקרא לסכום השגיאה על כל התצפיות של הפולד הזה LK.

והשגיאה הסופית שלנו היא איסוף ומיצוע של השגיאה מכל הפולדים, כלומר סכום LK וחלוקה פי n.

נזכיר שפעמים רבות אנחנו רואים גם חישוב קצת אחר, שLK שווה לשגיאה הממוצעת על פני הפולד הK, ואז מדווחים על השגיאה הסופית כממוצע הממוצעים, ואפשר להוסיף לזה גם סטיית תקן או טעות תקן של הממוצע, על פני K הפולדים.

:::

כעת, אם אנחנו רוצים לבצע מודל סלקשן, למשל לבחור איזשהו פרמטר למדא של המודל שמביא למינימום שגיאה אפשר לאמן את המודל עבור כמה למדא שונות. ולבחור את הלמדא שמביא למינימום את שגיאת הקרוס ולידיישן.

אם משתמשים בממוצע השגיאה בכל פולד ויש לנו טעות תקן שחישבנו לממוצע הזה על פני K פולדים, סטנדרט ארור, אפשר להשתמש גם בכלל ה1SE, שזה אומר לבחור את הפרמטר למדא שמביא למודל הפשוט ביותר כל עוד השגיאה שלו היא בטווח עד טעות תקן אחת לשגיאה הכי קטנה שהתקבלה. הטענה היא שאסטרטגיה כזאת שלא לבחור את המודל עם השגיאה הכי הכי קטנה אלא המודל הפשוט ביותר שהשגיאה שלו עדיין בתחום סביר, עשויה להקטין את הסיכוי לאוברפיטינג.

מכל מקום, אחרי שבחרנו את המודל הסופי או הפרמטר הסופי, אנחנו רוצים לבצע מודל אססמנט. בדיוק מאותן סיבות שהזכרנו, נראה הרבה פעמים איחוד של כל הדאטא, אימון עם הפרמטר או המודל הסופי הנבחר על כל התצפיות, ואמידה של הטעות הסופית הסופית על דאטא שנשאר בצד, כלומר שילוב עם השיטה הקודמת.

:::

לפני שנמשיך, ראינו שיש כל מיני טעויות שניתן להסתכל עליהן, תלוי מה אנחנו מחליטים לקבע ולמה להתייחס כמשתנה מקרי. אז מסתבר שקרוס ולידיישן הוא אומד די טוב לתוחלת הפסד החיזוי שסימנו כErr, כלומר התוחלת השולית על פני הרבה מדגמים מקריים ותצפיות חדשות מקריות. כאן השתמשנו בדוגמא של הרגרסיה הפולינומיאלית ויש לנו דרך שעוד נלמד לחשב אנליטית את השגיאה הזאת שאנחנו מנסים לאמוד. הנקודות הכחולות הן ממוצע הMSE על-פני 5 פולדים, והארור בארז הן פלוס מינוס טעות התקן של הממוצע. כאן המינימום מתקבל עבור פולינום מדרגה 4, אבל אם נשתמש בכלל ה1SE, נבחר במודל הפשוט יותר שעדיין מביא לשגיאה בטווח טעות התקן של השגיאה הטובה ביותר, וזה פולינום מדרגה 3. אפשרות אחרת היא אגב לבצע ממש מבחן סטטיסטי על פני הפולדים, לקבוע האם הפולדים בדרגה 3 מביאים בממוצע לשגיאה שונה באופן מובהק סטטיסטית מהפולדים בדרגה 4.

בכל מקרה אנחנו רואים שבמקרה זה שגיאת הקרוס-ולידיישן היא אומד לא רע עבור תוחלת הפסד החיזוי.

:::

בדוגמא שלנו היו מספיק תצפיות כדי ששגיאת הקרוס ולידיישן תהיה אומד טוב לתוחלת הפסד החיזוי לתצפיות שהמודל לא ראה. אבל צריך לזכור שלא נפתרו כל בעיותינו, אם הדאטא קטן ומכיל 50 תצפיות, והמודל שלנו צריך לפחות 50 תצפיות כדי לחזות טוב על תצפיות שלא ראה, ואנחנו עושים קרוס ולידיישן עם 5 פולדים - המודל יראה רק 40 תצפיות כל פעם!

ואפשר לדמיין מין עקומת למידה כזאת, היפותטית, שבה אכן מודל צריך 50 תצפיות כדי "להגשים את הפוטנציאל" שלו, כדי להגיע לאיזה מינימום אחוז של דיוק אם נאמר אנחנו בקלסיפיקציה -- ואנחנו תמיד רואים פחות מ-50 תצפיות. במקרה כזה המודל יבצע בממוצע פחות טוב, והשגיאה תהיה מוטה כלפי מעלה. אז צריך לקחת את זה לתשומת ליבנו.

:::

ואם אנחנו כל-כך מודאגים - למה לא לעשות שK שווה לN, כלומר נחלק את הדאטא לN פולדים, כל פעם נאמן על N פחות 1 תצפיות ונבחן את המודל על התצפית שהשמטנו.

אז זה רעיון דווקא מקובל שנקרא ליב-וואן-אאוט קרוס ולידיישן, או LOOCV. באופן כזה אנחנו ממצים את הדאטא בצורה היעילה ביותר, ואנחנו יכולים להיות בטוחים שהשגיאה לא תהיה מוטה, אנחנו משתמשים בכמעט כל הדאטא שברשותנו לאימון ולחיזוי.

אז שתי בעיות:
בעיה ראשונה היא שמבחינה חישובית לאמן את המודל n פעמים יכול לקחת המון זמן ומשאבים. אם אנחנו רוצים גם לאמן D מודלים שונים על כל פולד זה אומר D כפול N פעמים לאמן! צריך להגיד שזאת בעיה בעיקר למודלים מודרניים, אם אנחנו מדברים רק על רגרסיה ליניארית, מסתבר שיש לנו קיצור דרך לחשב את השגיאה של ליב-וואן-אאוט, זה נקרא ג'נרלייזד קרוס ולידיישן, אבל לא נפתח את זה כרגע.

בעיה אחרת, גם אם חישובית אנחנו מצליחים להתגבר על המשימה הזאת, היא שכעת המדגמים שעליהם אנחנו מאמנים את המודלים הם מאוד מאוד תלויים, המודלים יראו תמיד את אותן התצפיות! אז השגיאה לא תהיה מוטה, ניצלנו את כל מה שהיה לנו, אבל השונות תהיה גבוהה, כי אנחנו בעצם חוזרים על אותו מודל כמעט N פעמים ולא מרוויחים שום דבר מהמיצוע שלו!

מבחינה מתמטית, אם נסתכל על נקודת הקיצון שאנחנו מאוד קרובים אליה, של כל המודלים זהים עם איזו שונות טאו בריבוע, ואנחנו ממצעים אותם, אנחנו בעצם מקבלים את אותו מודל מקורי עם שונות טאו בריבוע. מתי נרוויח ממיצוע K מודלים? רק אם הם יהיו כמה שיותר שונים זה מזה, בלתי תלויים, ואז שונות הממוצע שלהם תהיה קטנה פי K.

כלומר בשורה התחתונה חזרנו לנקודת ההתחלה, של להיות קרובים לאמן מודל אחד ויחיד, שיכולה להיות לו שונות גבוהה.

:::

אז נסכם איפה אנחנו עומדים עד עכשיו עם קרוס ולידיישן:

בקצה האחד שאפשר לקרוא לו K = 1, למרות שזה לא נכון, אנחנו פשוט עושים סינגל ספליט על הנתונים, עם טריינינג סט אחד, ולידיישן סט אחד וטסט סט אחד -- זה מתאים למצב שיש לנו הרבה מאוד נתונים כי זו חלוקה לא יעילה של הדאטא. אנחנו מסתמכים גם על חלוקה אחת ויחידה ולכן הטעות הנאמדת הסופית יכולה להיות בעלת שונות גבוהה, היא לא ממוצע של כמה דעות.

בצד השני של הסקאלה K = N, זה השימוש היעיל ביותר בנתונים כי אנחנו מאמנים כל פעם כמעט על כולם ולא תהיה לנו שום הטייה שמקורה בגודל המדגם. אבל חישובית זה עלול להיות מופרך, אפילו אם כל מה שיש לנו לעשות זה לבחור פרמטר אחד, ובשיטות מודרניות יכולים להיות לנו כמה פרמטרים. ואנחנו מרוויחים ממש מעט מהממוצע הזה כי המודלים תלויים מאוד ביניהם מה שמשאיר את השונות של המודל האחד גבוהה.

כנראה שהפשרה היא איפשהו באמצע. אנחנו רואים בפועל שK שווה 5 או 10 עובד די טוב. זה נכון שעבור מעט תצפיות גם זה יכול להיות בעייתי בגלל הלרנינג קרב ואיבוד דאטא בזמן האימון, זה נכון שאם יש לנו הרבה פרמטרים לכוונון גם זה יכול להיעשות עמוס חישובית, אבל זאת פשרה לא רעה בין הקצוות.

:::

=== 4. קרוס ולידציה - טעויות נפוצות ===

קרוס ולידיישן זאת שיטה שנמצאת בשימוש רחב, ויש למשתמשים נטייה גם לבצע אותה לא נכון. נסיים בשתי בעיות שאנחנו רואים הרבה פעמים בשימוש בשיטה.

:::

בעיה ראשונה יכולה להיות כשבדאטה יש תלות אינהרנטית בין התצפיות, שלא נלקחת בחשבון בעת החלוקה לפולדים. דוגמא טובה לזה יכולה להיות כשהנתונים שלנו הם טיים סרייז, סדרה עתית, כמו למשל נתונים על פני ימים רבים של מחירי מניות, והמודל שלנו נבנה כדי לחזות את המחיר של מניה מחר.

אם נבצע חלוקה לפולדים בצורה נאיבית כמו לכל דאטא טבלאי:

האם אנחנו יכולים לסמוך על ההפחתה בשונות של האומד למדגם אחד, שאנחנו מצפים לקבל בעקבות חזרה על התהליך על פני כמה פולדים?
האם אין לנו זליגה של דאטא כל פעם בין מדגם הלמידה למדגם הולידיישן?

התשובות לשתי השאלות האלה לצערנו הן לא וכן.

יש כאן שתי בעיות: אחת היא תלות בזמן בין ימים, סביר להניח שיש תלות בין תצפיות מימים עוקבים למשל. והתלות הזאת יכולה להמשיך איתנו בחלוקה לפולדים, אנחנו בטוחים שאנחנו מחלקים את הדאטא בצורה מקרית ל5 או 10 מדגמים שונים, אבל בפועל מקבלים תלות בזמן בין מדגמי הלמידה השונים.

בעיה נוספת, במיוחד אם המטרה של מודל כזה היא חיזוי של מה שיקרה בעתיד - אנחנו כרגע גרמנו לו לראות את העתיד! אם בחלוקה לפולדים אנחנו לא מוודאים שפולד הטריינינג תמיד מכיל תצפיות מהעבר שהגיוני שהמודל ראה, ופולד הולידיישן תמיד מכיל תצפיות מהעתיד שהגיוני שהמודל צריך לחזות עליהן -- זה בדיוק זליגה של דאטא בין פולדים. בסופו של דבר המודל "חוזה" על העתיד, על תצפיות שהוא התאמן קרוב מאוד אליהן, והשגיאה שלו תהיה מוטה כלפי מטה, הוא יהיה אופטימי מדי.

לכן, צריך מאוד להיזהר בחלוקה לפולדים כשאנחנו יודעים שהתפקיד של המודל לחזות את העתיד של מניה למשל, או לחזות את הקרינה במרחב במיקומים חדשים על סמך קריאות ממיקומים אחרים. צריך לדאוג להיות עקביים ולשלב את המטרה הטבעית הזאת בחלוקה לפולדים, להתאמן על העבר, ולחזות את העתיד, או להתאמן על מיקומים ידועים ולחזות על מיקומים לא ידועים.

:::

בעיה נוספת שאנחנו רואים המון, גם היא ביסודה זליגה או ליקג' של דאטא של הולידיישן אל הדאטא של הטריין:

נניח שיש לנו נתונים עם המון משתנים. המון משתנים ברמה שהמודל לא יכול להתמודד עם כל כך הרבה, ואנחנו מחליטים קודם כל לעשות סקרינינג של תת-קבוצה של משתנים לפי איזשהו קריטריון, למשל אלה המשתנים שבאופן שולי בעלי הקורלציה הגבוהה ביותר למשתנה התלוי Y -- ורק עכשיו! להתחיל תהליך של קורס ולידיישן על המשתנים הנותרים -- יכול להיות שזה בעייתי?

זה יכול להיות בעייתי מאוד. להחליט החלטות בייסליין כאלה על פני כל המדגם. איפה כאן הדאטא ליקג'? בעובדה שהמודל כבר יודע משהו על התצפיות שתיכף יוגדרו כתצפיות ולידיישן! הוא כבר נחשף אליהן, הוא כבר ביצע החלטה על סמך התצפיות האלה! בעוד שב"חיים האמיתיים" של מודל כזה, לא תהיה לו האפשרות להציץ בתצפיות שהוא עומד לקבל, הוא לא יכול לסמוך על מידע שיש בהן בשום אופן כדי שנקבל אמידה טובה לביצועים שלו.

דוגמא נוספת שאנחנו רואים זה לעשות סטנדרטיזציה של הדאטא, כלומר לחסר את הממוצע מכל העמודות ולחלק בסטיית התקן, ולעשות את זה על סמך כל הN תצפיות שברשותנו, ורק אז להתחיל תהליך של קרוס ולידיישן. בעייתי מאותה סיבה בדיוק, הממוצעים וסטיות התקן האלה כבר חושבו על סמך התצפיות שבטסט, המודל מכיר אותן ועלול לתת שגיאה אופטימית מדי, מוטה כלפי מטה.

איך מתקנים את זה? צריך לכלול את כל הצעדים הבעייתיים האלה בתהליך הקרוס ולידיישן. אם לדוגמא אנחנו חייבים לעשות סקרינינג של משתנים רלוונטים מתוך הרבה משתנים, זה חייב להיות שלב ראשון שאנחנו מבצעים בתוך הקרוס ולידיישן, לכל אחד ממדגמי הלמידה, אחרי שהשמטנו את מדגם הולידיישן. כלומר הקרוס ולידיישן חייב "לבדוק" עבורנו גם את הצעד הזה. יכול להיות שבמדגם למידה אחד ייבחרו 10 משתנים מסוימים ובמדגם למידה אחר ייבחרו 11 משתנים קצת אחרים וזה בסדר! כי זה רעש מקרי שחייבים לקחת בחשבון בתהליך הלמידה.

:::

מבחינה מעשית, כל תוכנה רצינית שאיתה מריצים מודלים חייבת לכלול איזשהו קונספט של פייפליין, או "מתכון" שאפשר להגדיר, והוא יהיה מורכב מכמה תת-צעדים שכוללים לא רק את ריצת המודל עצמו. ועל הפייפליין הזה נריץ קרוס ולידיישן, לא רק על ריצת המודל עצמו.

כאן לדוגמא אני משתמש בקלאס פייפליין של sklearn. אני מגדיר שהמתכון הזה יהיה מורכב מ3 צעדים: סטנדרטיזציה של המשתנים, בחירה של סאבסט של משתנים, ורק אז שיטה לרגרסיה כמו KNN. אני מגדיר מהו הגריד של פרמטרים שונים שאני רוצה לבחור מתוכו, לדוגמא כאן מהו הK משתנים שאני רוצה לבחור מתוך הP שיש ברשותי, מהו מספר השכנים של KNN ובאיזו מטריקה KNN ישתמש.

ואז אני משתמש בקלאס שנקרא GridSearchCV שמחפש על כל הקומבינציות של ה3 פרמטרים שחשובים לי, מריץ מודל עם כל קומבינציה באמצעות קרוס-ולידיישן עם 5 פולדים. אבל הכי חשוב, כשאני קורא למתודה פיט, הכוונה לפיט על כל הפייפליין הזה, כולל סטנדרטיזציה, וכולל בחירה של סאבסט של משתנים. לא רק ריצה של KNN עצמו. בדרך הזאת אני מוודא שאין לי זליגה של דאטא מהולידיישן סט לתוך הטריינינג והקרוס ולידיישן אכן מודד את התהליך שאני הייתי רוצה לראות על כל הדאטא.

עד כאן במודל סלקשן, בחלק השני נדבר על שתי השיטות שנותרו לנו - הבוטסטרפ, שמזכירה קצת את הקרוס-ולידיישן במובן של אימון על פני מספר מדגמים שונים מהדאטא. והשיטה של תיקון שגיאת הטריינינג שהיא נמוכה בדרך כלל ממה שאנחנו רוצים, באמצעות איזשהו אומד לעד כמה היא נמוכה, או אופטימית.

:::
