---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Decision Trees and Random Forests"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Decision Trees and Random Forests - Class 9

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Intro. to Decision Trees {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשיעור הזה נצלול לעומק אחד המודלים האינטואיטיביים ביותר בלמידה סטטיסטית - עצי החלטה. אנחנו נראה בשיעור הזה שחשוב להכיר היטב עצי החלטה, והם נמצאים בשימוש רב בתעשייה, אבל לא בפני עצמם. נראה שעצי החלטה לכשעצמם הם מודלים די חלשים של למידה, אבל כשמשלבים אותם למודל אחד גדול , בגל מיני דרכים, מקבלים מודל חזק מאוד.
:::
:::

---

### The models we learned so far

::: {.incremental}
- Parameteric, linear, global: linear regression, logistic regression, Ridge, Lasso, PCR
- Non-parametric, non-linear, local: $K$-nearest neighbors
:::

<br></br>

::: {.fragment}
::: {.incremental}
Simpler idea (non-paramteric, non-linear, local):

- Segment predictor space $\mathcal{X}$ to relatively homogenous neighborhoods in $\mathcal{Y}$
- For each region $R_1, \dots, R_M$ predict constant/class $c_m$, s.t.:
$$\hat{f}(X) = \sum_{m = 1}^M c_m\mathbb{I}\left(X \in R_m\right)$$
:::
:::

::: {.fragment}
::: {.callout-note}
Is this not a linear model?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אילו מודלים למדנו עד עכשיו?

רוב המודלים היו מודלים פרמטריים וליניארים. אפשר לקרוא להם גם גלובליים כי על פני כל המרחב של X הם מניחים שיש איזה חוק שממפה בין X לY, שמנוסח על-ידי קבוצה של פרמטרים, שתקף בכל מקום במרחב.

מבחינת מודלים שהם א-פרמטריים, כלומר גמישים יותר, ויודעים למדל גם יחסים לא-ליניאריים, ראינו רק את KNN. KNN נקרא גם מודל לוקאלי, כי החיזוי לכל תצפית תלוי בסביבה שלה, שמשתנה לפי מדגם הלמידה, אין הנחה של איזו פונקציה יחידה על פני כל המרחב. אבל בKNN אנחנו לא מציצים על Y כשאנחנו בונים את השכונות.

מודל אחר אחר שכן מסתכל על Y כשהוא בונה שכונות, זה עצי החלטה. גם כאן המודל א-פרמטרי, יודע למדל יחסים לא-ליניאריים, לוקאלי.

אבל כאן נחלק את המרחב של X לשכונות ככה שY בכל שכונה הוא כמו שיותר הומוגני, או אחיד, או צפוי.

ואחרי שנחלק את המרחב של X לM איזורים R1 עד RM, לכל איזור נחזה את אותו מספר לרגרסיה או קלאס לקלסיפיקציה, Cm. ניתן לרשום זאת עם משתני אינדיקטור כך: החיזוי הסופי לכל תצפית הוא סכום על כל האיזורים, כל תצפית שייכת רק לאיזור m אחד ותקבל חיזוי Cm בהתאם.

נשאלת השאלה, כשאני רושם את זה כך: זה לא מודל ליניארי? הרי אמרנו שעץ החלטה הוא מודל לא-ליניארי. אז במובן מסוים זה נכון, אפשר להסתכל על עצי החלטה כמודלים ליניאריים אבל על הפיצ'רים שמופיעים כאן, פיצ'רים שמגדירים איזורים, די מתוחכמים. ההבנה הזאת אכן מאפשרת הרחבה של המודל של עץ החלטה למודלים מתוחכמים יותר, אבל הם לא בחומר שלנו היום.
:::
:::

---

### What should $R_1, \dots, R_M$ be?

```{python}
#| echo: false

import matplotlib.pyplot as plt
import numpy as np

# Set up the figure and subplots
fig, axs = plt.subplots(1, 3, figsize=(14, 3.5))

# Plot 3: Non-linear segmentation (spiral or circles)
theta = np.linspace(0, 4 * np.pi, 100)
r = np.linspace(0, 1, 100)
x1 = r * np.cos(theta)
x2 = r * np.sin(theta)
axs[2].plot(x1, x2, color='black')
axs[2].plot(-x1, -x2, color='black')

circle1 = plt.Circle((0.3, 0.3), 0.3, color='black', fill=False)
circle2 = plt.Circle((-0.3, -0.3), 0.3, color='black', fill=False)

axs[2].add_artist(circle1)
axs[2].add_artist(circle2)
axs[2].set_xlim([-1, 1])
axs[2].set_ylim([-1, 1])
axs[2].set_xlabel('X1')
axs[2].set_ylabel('X2')
axs[2].set_xticks([-1.0, -0.5, 0.0, 0.5, 1.0])
axs[2].set_yticks([-1.0, -0.5, 0.0, 0.5, 1.0])

# Plot 1: Tilted rectangle segmentation
# Creating a tilted rectangle
rectangle = np.array([[0.2, 0.3], [0.7, 0.3], [0.6, 0.8], [0.1, 0.8], [0.2, 0.3]])
axs[0].plot(rectangle[:, 0], rectangle[:, 1], 'black')
axs[0].set_xlim([0, 1])
axs[0].set_ylim([0, 1])
axs[0].set_xlabel('X1')
axs[0].set_ylabel('X2')

# Plot 2: Complex segmentation with stacked rectangles
# Adding 3-4 rectangles one on top of the other
rect1 = np.array([[0.1, 0.1], [0.4, 0.1], [0.4, 0.4], [0.1, 0.4], [0.1, 0.1]])
rect2 = np.array([[0.3, 0.3], [0.6, 0.3], [0.6, 0.7], [0.3, 0.7], [0.3, 0.3]])
rect3 = np.array([[0.5, 0.5], [0.9, 0.5], [0.9, 0.9], [0.5, 0.9], [0.5, 0.5]])

axs[1].plot(rect1[:, 0], rect1[:, 1], 'black')
axs[1].plot(rect2[:, 0], rect2[:, 1], 'black')
axs[1].plot(rect3[:, 0], rect3[:, 1], 'black')
axs[1].set_xlim([0, 1])
axs[1].set_ylim([0, 1])
axs[1].set_xlabel('X1')
axs[1].set_ylabel('X2')

# Adding labels R1, R2, ... to the relevant regions
# You can adjust the coordinates to place the labels in the desired positions
axs[0].text(0.4, 0.55, 'R1', fontsize=12)
axs[0].text(0.4, 0.15, 'R2', fontsize=12)

# More complex labels for the right-most plot
axs[1].text(0.25, 0.25, 'R1', fontsize=12)
axs[1].text(0.33, 0.33, 'R2', fontsize=12)
axs[1].text(0.43, 0.43, 'R3', fontsize=12)
axs[1].text(0.53, 0.53, 'R4', fontsize=12)
axs[1].text(0.7, 0.7, 'R5', fontsize=12)
axs[1].text(0.6, 0.2, 'R6', fontsize=12)

# Show plot
plt.tight_layout()
plt.show()
```

::: {.fragment}
What should $c_1, \dots, c_M$ be?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
באילו איזורים נבחר? כאן לדוגמה יש לנו מרחב של שני משתנים X1 וX2, ומייד אפשר לראות שאם נרצה להגדיר איזו מטריקת-על שאנחנו רוצים לעשות לה מינימום, ולהתחשב בכל חלוקה אפשרית של מרחבים, מרחב החיפוש שלנו יהיה עצום, וזה גם יכול לגרום כמובן לאוברפיטינג. תיכף נראה שכל אחת מהחלוקות שאנחנו רואים כאן לאיזורים, מאתגרת מאוד בשביל עץ ההחלטה הפשוט שאנחנו נבנה.

ואם אנחנו שואלים כבר לאילו איזורים נחלק את המרחב של X, באותה הזדמנות נשאל גם מה הם החיזויים שניתן לכל איזור ואיזור, Cm?

כאן התשובה דווקא אינטואיטיבית -- ברגרסיה נחזה את הממוצע של Y בכל איזור, ובקלסיפיקציה נחזה את הקלאס שיש לרוב התצפיות באיזור הזה. אבל אנחנו מקדימים את המאוחר, בואו נראה איך נעשית החלוקה לאיזורים בעץ החלטה קלאסי.
:::
:::

---

### How does a doctor think?

```{python}
#| echo: false

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

# Set up the figure and subplots
fig, axs = plt.subplots(1, 3, figsize=(14, 3.5))

# Plot 1: Decision Tree Structure
ax_tree = axs[0]

# Drawing a simple decision tree manually with 3 splits
ax_tree.plot([0.5, 0.5], [1, 0.9], 'k-', lw=2)  # First split
ax_tree.plot([0.5, 0.25], [0.9, 0.7], 'k-', lw=2)  # Left branch
ax_tree.plot([0.5, 0.75], [0.9, 0.7], 'k-', lw=2)  # Right branch
ax_tree.plot([0.25, 0.125], [0.7, 0.4], 'k-', lw=2)  # Left-Left branch
ax_tree.plot([0.25, 0.375], [0.7, 0.4], 'k-', lw=2)  # Left-Right branch
ax_tree.plot([0.375, 0.3], [0.4, 0.1], 'k-', lw=2)  # Left-Left branch
ax_tree.plot([0.375, 0.45], [0.4, 0.1], 'k-', lw=2)  # Left-Right branch

# Adding text for nodes and leaves
ax_tree.text(0.35, 0.92, 'X1 <= 0.5', ha='center', fontsize=12)
ax_tree.text(0.15, 0.72, 'X2 <= 0.5', ha='center', fontsize=12)
ax_tree.text(0.35, 0.42, 'X1 <= 0.25', ha='center', fontsize=12)
ax_tree.text(0.125, 0.33, 'R1', ha='center', fontsize=12)
ax_tree.text(0.3, 0.03, 'R2', ha='center', fontsize=12)
ax_tree.text(0.45, 0.03, 'R3', ha='center', fontsize=12)
ax_tree.text(0.75, 0.63, 'R4', ha='center', fontsize=12)

ax_tree.set_xlim([0, 1])
ax_tree.set_ylim([0, 1])
ax_tree.axis('off')  # No axes for the tree plot

# Plot 2: Decision areas on X1, X2 plane
ax_decision = axs[1]

# Plotting decision boundaries
ax_decision.axvline(x=0.5, color='black')  # First split on X1
ax_decision.axhline(y=0.5, xmin=0, xmax=0.5, color='black')  # Split on X2 for left side
ax_decision.axvline(x=0.25, ymin=0.5, ymax=1.0, color='black')  # Split on X1 for left side

# Adding labels for decision areas
ax_decision.text(0.25, 0.25, 'R1', fontsize=12)
ax_decision.text(0.15, 0.75, 'R2', fontsize=12)
ax_decision.text(0.35, 0.75, 'R3', fontsize=12)
ax_decision.text(0.75, 0.5, 'R4', fontsize=12)

ax_decision.set_xlim([0, 1])
ax_decision.set_ylim([0, 1])
ax_decision.set_xlabel('X1')
ax_decision.set_ylabel('X2')

# Plot 3: 3D plot of Y predictions
ax_3d = fig.add_subplot(133, projection='3d')

# Data for the 3D plot
x_edges = [0, 0.5, 0.75, 1]
y_edges = [0, 0.5, 1]
X, Y = np.meshgrid(x_edges, y_edges)
Z = np.zeros_like(X)

# Assigning average Y values to each region
z_values = [1, 3, 2, 4]  # example Y averages for R1, R2, R3, R4

# Creating the boxes
ax_3d.bar3d(0, 0, 0, 0.5, 0.5, z_values[2], color='gray', edgecolor='black', alpha=0.5)  # R1
ax_3d.bar3d(0.5, 0, 0, 0.5, 1, z_values[0], color='gray', edgecolor='black', alpha=0.5)  # R4
ax_3d.bar3d(0, 0.5, 0, 0.25, 0.5, z_values[3], color='gray', edgecolor='black', alpha=0.5)  # R2
ax_3d.bar3d(0.25, 0.5, 0, 0.25, 0.5, z_values[1], color='gray', edgecolor='black', alpha=0.5)  # R3

ax_3d.set_xlabel('X1')
ax_3d.set_ylabel('X2')
ax_3d.set_zlabel('$\hat{Y}$')

plt.tight_layout()
plt.show()
```

::: {.fragment}
$\Rightarrow$ recursive binary splitting.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הדרך שבה אנחנו נחלק לאיזורים, מנסה לחקות את דרך החשיבה שלנו כבני אדם שנתקלים בבעיה ושואלים את עצמם מה לעשות, או למשל רופאים שצריכים לתת אבחנה לפציינטים חדשים. אנחנו אוהבים לעבוד עם תרשימי זרימה. לדוגמא כאן, לשאול שאלה ראשונה על משתנה X1 האם הוא גדול או קטן-שווה מחצי. זאת כנראה השאלה הכי חשובה, לבירור מיידי, ותיכף נגדיר מה זה הכי חשובה. אם הוא גדול, יכול להיות שהגעתי לשכונה מאוד הומוגנית מבחינת Y ואין לי צורך יותר בעוד שאלות. אני אתן לה שם R4. אם הוא קטן יש לי צורך להמשיך לשאול עוד שאלות, ועכשיו אולי השאלה הכי חשובה היא שאלה על משתנה אחר, X2. וכך הלאה והלאה, עד שאין לי צורך יותר בשאלות ובעצם חילקתי את המרחב לאיזורים שונים R1 עד R4.

ניתן לראות את החלוקה הזאת בתרשים האמצעי, הנה השאלה הראשונה שמחלקת את המרחב לשני חלקים באמצעות קו אנכי בקואורדינטת X1, ואז שאלה שניה על קואורדינטת X2, שנשים לב, שהיא לא מתייחסת בכלל לאיזור R4 ששמנו בצד. היא מחלקת רק את שאר המרחב לשני חלקים, וכך הלאה.

כלומר עץ ההחלטה הקלאסי, שכל פעם מחלק את המרחב הנוכחי לשני חלקים בדיוק, מסוגל לייצר איזורי החלטה מלבניים, רק באמצעות קווים מקבילים לצירים. הוא עושה זאת שוב ושוב בצורה רקורסיבית ולכן השיטה נקראת גם recursive binary splitting.

בתרשים הימני ציירנו נוסף על מרחב X1, X2 גם ציר עומק שמייצג את y_hat, כלומר מה נחזה לכל איזור ואיזור. בעץ הקלאסי אנחנו חוזים ברגרסיה למשל רק מספר אחד לכל איזור כזה, הממוצע של Y, כלומר ככה ייראה נוף ההחלטה שלנו -- מעין קופסאות על קופסאות, או בניינים על בניינים. לא ליניארי מצד אחד - זה לא מישור - אבל גם די נוקשה, כמו שנראה בהמשך.
:::
:::

---

### Decision trees at high level

::: {.incremental}
- Root: start with $\hat{f}(X) = \bar{y}$ or $\max\{\hat{P}(y = 0), \hat{P}(y = 1)\}$ to all observations
- Recursively:
  - Choose $(j, s)$ pair: feature $j$ to split on value $s$: $X_j \le s$ and $X_j > s$
  - Predict $c_m$ for each region $R_1, \dots, R_M$
- Until $STOP$ criterion
:::

<br></br>

::: {.fragment}
Questions:
:::

::: {.incremental}
- How to choose a split at each node of the tree?
- How to fit a value $c_m$ for each region / terminal node / leaf?
- What is $STOP$ criterion?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בהיי-לבל ככה נראה האלגוריתם של בניית עץ קלאסי: מגיע מדגם למידה T בגודל n של זוגות תצפיות X וY.

בשורש העץ אנחנו נחזה עבור כל התצפיות את הממוצע של Y עבור רגרסיה או הקלאס שהוא הרוב עבור קלסיפיקציה.

בצורה רקורסיבית, נבחר תמיד זוג J, S של משתנה XJ וערך S לפצל עליו. הפיצול הוא תמיד בינארי, לצד אחד הולכות כל התצפיות שעבורן XJ קטן-שווה לS, ולצד האחר הולכות כל התצפיות שעבורן XJ גדול מS.

לכל איזור שהגענו אליו נחזה איזשהו ערך C (ממוצע עבור רגרסיה, הקלאס שהוא הרוב עבור קלסיפיקציה). ונמשיך, רקורסיבית.

עד מתי? עד איזשהו תנאי עצירה.

העצים השונים שפותחו לאורך השנים נבדלו ביניהם בעיקר בתשובה שלהם לשלוש השאלות הבאות:

איך לבחור את הפיצול בכל צומת בעץ, כלומר את הקומבינציה של משתנה J וערך S לפצל עליו.

איזה ערך Cm להתאים בכל איזור שהגענו אליו, שנקרא גם עלה של העץ או טרמינל נוד.

וכמובן: מהו תנאי העצירה.

אז אלגוריתמים שונים נראים אולי שונים במקצת בהתאם לתשובות שלהם לשאלות האלה, אבל הגישה שרשומה כאן, בהיי-לבל, לא שונה בין עצי החלטה שונים.
:::
:::

---

## Regression Trees {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתמקד קודם בעצי רגרסיה ונראה שאם אנחנו מבינים אותם היטב המעבר לקלאסיפיקציה די פשוט.
:::
:::

---

### How to split?

::: {.incremental}
- Criterion: Minimize RSS on training.

- Given set of $r$ observations in current node, define for a variable $j$ and possible split point $s$: 
$$L(j,s) = \{i\leq r: x_{ij} \leq s\}\;,\;\; R(j,s) = \{i\leq r: x_{ij} > s\}$$
$$\bar{y}_L =\frac{\sum_{i \in L(j,s)} y_i}{|L(j,s)|}\;,\; \bar{y}_R=\frac{\sum_{i \in R(j,s)} y_i}{|R(j,s)|}$$
$$RSS(j,s) = \sum_{i \in L(j,s)} (y_i - \bar{y}_L)^2 + \sum_{i \in R(j,s)} (y_i - \bar{y}_R)^2$$

- And find the pair $j, s$ which minimize this RSS among all possible pairs
:::

::: {.fragment}
::: {.callout-note}
What is the complexity of this search?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מה הקריטריון הטבעי לנו ממודלים אחרים ברגרסיה? אנחנו רוצים לעשות מינימום על הRSS.

בכל שלב מגיעות נאמר r תצפיות בצומת הנוכחי שאנחנו נמצאים בו. נגדיר את הפיצול על הזוג של משתנה וערך J, S, כחלוקה של r התצפיות לשתי קבוצות: left וright.

בכל קבוצה נסתכל על הממוצע של Y, והRSS הכללי של הפיצול הוא סכום הRSSים של שני האיזורים הנוצרים, כלומר סכום המרחקים הריבועיים של Y מהממוצעים שנוצרים, בצד ימין ובצד שמאל.

זה הקריטריון לעשות עליו מינימום, ואנחנו עוברים ככה על כל הp משתנים שלנו, על כל האחרים האפשריים לעשות עליהם פיצול. הפיצול שנבחר יהיה זה שבנקודה הנוכחית מביא את קריטריון הRSS למינימום.

זה נשמע קצת מסובך לעבור על כל המשתנים, בכל משתנה לעבור על כל הערכים ולחשב מחדש את הממוצעים ואת הRSS. אז הסיבוכיות כאן דווקא לא נוראית כמו שזה נשמע, בפועל אם ממיינים את המשתנים פעם אחרת בתחילת הריצה, ומשתמשים בטריק לחישוב הממוצעים והRSS כל פעם על-ידי שינוי של תצפית אחת שעוברת בין הקבוצות, אפשר לחשב את זה בזמן ריצה O(r*p), שלהרבה בעיות פרקטיות זה לא כל-כך נורא.
:::
:::

---

### How to fit a value $c_m$ at leaves?

- Similar to OLS, we want to estimate $\hat{y}(x) \approx E(y|x)$ 

- We interpret the splitting as finding *homogeneous areas* with similar $y$ values in our data, hence hopefully similar $E(y|x).$

- Consequently, given a leaf (terminal node) $R_m$ with set of observations $Q_m \subseteq \{1,\dots,n\}$, we estimate: 
$$c_m = \bar{y}_{R_m} = \frac{\sum_{i \in Q_m} y_i}{|Q_m|}$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לגבי השאלה ששאלנו, איזה ערך לנבא עבור כל שכונה, ברגרסיה התשובה די פשוטה. השכונות שלנו נוצרו במטרה להיות הומוגניות בY, כלומר כמעט ללא שינוי. אנחנו רוצים למדל את התוחלת המותנית של Y בהינתן X, כשהקריטריון מינימום שלנו הוא השגיאה הריבועית, ואנחנו כבר יודעים איזה אומד עושה מינימום לשגיאה הריבועית וגם אומד חסר-הטיה לתוחלת -- ממוצע המדגם של Y באותה שכונה או עלה שנוצר בתחתית העץ.
:::
:::

---

### When to $STOP$?

::: {.incremental}
- Why stop?
- Bias-variance tradeoff!
  - Tree too shallow --- high bias, underfitting
  - Tree too deep --- high variance, overfitting
:::
::: {.fragment}
Some heuristics:

  - Maximum tree depth (i.e. maximum "questions", a hyperparameter)
  - Minimum improvement in RSS
  - Minimum node size $Q$
:::

::: {.fragment}
::: {.callout-note}
What could be the issue with each of those?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
התשובה לשאלה השלישית של מתי לעצור את גידול העץ, קצת יותר מורכבת.

קודם כל, ניקח צעד אחורה ונשאל -- למה בכלל לעצור? למה לא לתת לעץ לגדול ולגדול, עם עוד שאלות יותר ויותר ספציפיות שיחלקו את המרחב לשכונות יותר ויותר קטנות -- התשובה היא כמובן הביאס-וריאנס טריידאוף.

אם העץ לא עמוק מספיק -- הוא אולי לא מספיק גמיש לתאר את היחס בין X לY, זה נקרא אנדרפיטינג ויש לנו ביאס, הטיה גבוהה.

אם העץ עמוק מדי -- הוא גמיש מדי, הוא מאמין באמונה עיוורת למדגם הלמידה, יש לנו אוברפיטינג ושונות גבוהה מדי.

אז יש כמה היוריסטיקות להחליט מתי לעצור בגידול העץ:

אפשרות אחת היא פשוט לשלוט בעומק העץ עצמו, להגדיר איזשהו היפרפרמטר נוסף של מקסימום עומק, או המקסימום של שאלות שנשאל כל תצפית במורד העץ. אפשר לבחור אותו למשל עם קרוס ולידיישן.

אפשרות אחרת היא לגדל עץ כל עוד אנחנו מקבלים מינימום של שיפור או רווח בRSS, לעומת הRSS בלי הפיצול. פשוט נחשב את הRSS של כל התצפיות שהגיעו לשכונה, את הRSS של הפיצול הכי טוב בנוסחה שהראינו, ונראה אם ההפרש ביניהם שווה את הפיצול.

גישה אחרת יכולה להיות דווקא שליטה בפרמטר של מינימום גודל השכונה או הצומת. למשל אולי נחליט שכל עוד בשכונה יש 10 תצפיות או יותר שווה להמשיך לפצל אותה, כי אנחנו סומכים על ממוצע של 10 תצפיות, אבל מתחת לזה נעצור את הפיצול מהשכונה שאנחנו נמצאים בה עכשיו.

לכל אחת מהגישות האלה יש חסרונות:
החיסרון של שליטה בעומק העץ הוא שצריך לבחור את ההיפרפרמטר הזה.

החיסרון של השיטה של לעצור כשלא משיגים מספיק שיפור בRSS הוא שכל האלגוריתם שלנו הוא גרידי, חמדני, כלומר במיוחד בהתחלה יכול להיות שאנחנו נמצאים בצומת שבה לא נראה שיש דרך לפצל את התצפיות כך שנשיג שיפור דרמטי יחסית לRSS שבו אנחנו נמצאים, אבל אנחנו לא רואים בהמשך שכונות שבהן אם נפצל נשיג שיפור יחסי דרמטי.

והחיסרון של הגישה השלישית של גידול העץ עד מינימום גודל שכונה, הוא שהיא לא מבוססת על הנתונים, היא מלאכותית מדי והיא נוטה לתת עצים עמוקים מדי, בפועל על נתונים אמיתיים.
:::
:::

---

### Cost complexity pruning

::: {.incremental}
- Grow a deep tree $T_0$, e.g. with the size criterion
- [Prune]{style="color:red;"} to tree $T$ with some $\alpha$ penalty on its size:
$$C_\alpha(T) = RSS(T) + \alpha|T|$$
- Choose $T_\alpha$ which gives minimum $C_\alpha(T)$
:::

::: {.fragment}
- For a given $\alpha$ efficient algorithms exist to find the pruning path
- $\alpha$ is chosen with CV
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אחת הגישות הרווחות החל מהמאמר המקורי על עצי החלטה, היא לגדל עץ עמוק ואז להחליט בצורה מושכלת אם שווה היה לקטום אותו איפשהו, מה שנקרא פרונינג.

אנחנו נגדל עץ עמוק מאוד T0, למשל באמצעות אחד הקריטריונים שהזכרנו, או אולי הכי עמוק שאפשר.

נגדיר איזשהו פרמטר עונש על גודל העץ אלפא, כך שעבור על צומת וצומת שבדיעבד פיצלנו, אפשר להסתכל על הRSS שם ולהוסיף לו עונש אלפא כפול גודל העץ, כלומר כמה צמתים או כמה עלים יש בו, בנקודת הזמן הזאת.

העץ שנבחר בסופו של דבר הוא העץ שמשיג את המינימום קריטריון הזה שמסומן כאן כC(alpha), כי עבור כל אלפא נקבל עץ אחר.

זה נראה הרבה עבודה אבל בפועל קיים אלגוריתם יעיל שמחשב את כל המסלול הזה לכל אלפא, ואת אלפא אפשר לבחור עם קרוס-ולידיישן.
:::
:::

---

### Example: credit data

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score, KFold

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

max_depths = range(1, 12)  # Define the range of max_depth values to evaluate
n_splits = 5  # Number of cross-validation folds

train_errors = []
test_errors = []
train_errors_std = []
test_errors_std = []

# Setting up cross-validation
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

for max_depth in max_depths:
    train_scores = []
    test_scores = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        # Initialize the Decision Tree Regressor with the current max_depth
        regressor = DecisionTreeRegressor(max_depth=max_depth)
        regressor.fit(X_train, y_train)
        
        # Evaluate train and test MSE
        train_scores.append(np.mean((regressor.predict(X_train) - y_train) ** 2))
        test_scores.append(np.mean((regressor.predict(X_test) - y_test) ** 2))
    
    # Store mean and standard deviation of MSE across folds
    train_errors.append(np.mean(train_scores))
    test_errors.append(np.mean(test_scores))
    train_errors_std.append(np.std(train_scores))
    test_errors_std.append(np.std(test_scores))

# Plotting the results
plt.errorbar(max_depths, train_errors, yerr=train_errors_std, label='Train MSE')
plt.errorbar(max_depths, test_errors, yerr=test_errors_std, label='Test MSE')

plt.xlabel('max tree depth')
plt.ylabel('MSE')
plt.legend()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נחזור לדוגמא שלנו מהשיעור הקודם של קרדיט דאטא ונגדל עץ החלטה של רגרסיה לחיזוי היתרה של לקוח בבנק.

אנחנו מבצעים כאן קרוס-ולידיישן על פני חמישה פולדים, ורושמים את ממוצע הMSE על פנ הטריין והטסט, כשהפרמטר שבחרנו לשנות כאן הוא עומק העץ. החל מעץ בעל עומק אחד, מה שנקרא גזע או סטאמפ, עד לעץ בעל עומק מקסימלי 11, כלומר יש מקסימום 11 שאלות עד שמגיעים לחיזוי תצפית.

אנחנו כבר רגילים לראות שמדגם הלמידה בשלב מסוים עלול להגיע לטעות חיזוי אפס עם עץ מספיק עמוק, ושמדגם הטסט מראה שמספיקות 8-9 שאלות מקסימום כדי להגיע למינימום MSE על תצפיות שהמודל לא ראה, מעבר לזה אין ירידה בMSE ואולי אפילו קצת עלייה, כלומר אוברפיטינג.
:::
:::

---

### Example: credit data

```{python}
#| echo: false

from sklearn.tree import DecisionTreeRegressor, plot_tree
import matplotlib.pyplot as plt

# Assuming X, y are already defined

# Train the Decision Tree Regressor on all data
regressor = DecisionTreeRegressor(max_depth=3)
regressor.fit(X, y)

# Plot the tree
plt.figure(figsize=(18, 8))  # Increase figsize to give nodes more space
plot_tree(
    regressor, 
    filled=True, 
    feature_names=regressor.feature_names_in_,  # Use your actual feature names
    rounded=True, 
    fontsize=9,
    precision=1  # Control decimal precision in node labels
)

plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
איך נראה העץ שלנו? איך אנחנו מסבירים את המודל שלנו?

עץ בעומק מקסימום 8 קצת קשה להכניס לשקף אחד, אז הכנסנו כאן עץ בעומק 3, שמגיע לשמונה שכונות או 2 בחזקת 3.

בכל פיצול אנחנו רואים את הRSS, את המשתנה שנבחר לפיצול והערך שעליו מפצלים, כמה תצפיות הגיעו לצומת הזאת ומה הממוצע שלהן, כלומר מה היינו חוזים אם כאן היה נגמר העץ.

אפשר לראות שעבור דירוג אשראי גבוה עוברים לחיזוי יתרה בחשבון קצת יותר גבוהה, ועבור דירוג אשראי נמוך יתרה יותר נמוכה. כלומר זאת השאלה הכי חשובה, שגורמת לירידה הכי גדולה בRSS.

בסופו של דבר אפשר לראות בעלים את השכונות השונות שנוצרו, הן גם צבועות בגרדיאנט כזה מחיזוי יתרה נמוכה מאוד בלבן ועד יתרה גבוהה מאוד בחום.

אנחנו רואים כאן כמה המודל אינטואיטיבי מצד אחד, מצד שני נזכיר שהעץ הכי טוב היה עם עומק מקסימלי 8, שזה דבר שכבר קשה יותר לעקוב אחריו.
:::
:::

---

## Classification Trees {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נעבור עכשיו לעצים לקלסיפיקציה לK קלאסים, ונראה שנשאר לנו מעט מאוד לשנות.
:::
:::

---

### How to split?

::: {.incremental}
- Criterion: Minimize [impurity]{style="color:red;"} on training:
- If $\hat{p}_{kL}, \hat{p}_{kR}$ are the left and right empricial probabilities for each class $k$, and $\hat{y}_{L}, \hat{y}_{R}$ are the most common classes:
  - Misclassification error: $$MC(j,s) = \sum_{i \in L(j,s)} \mathbb{I}\{y_i \neq \hat{y}_L\} + \sum_{i \in R(j,s)} \mathbb{I}\{y_i \neq \hat{y}_R\}$$
  - Gini index: $\quad Gini(j,s) = \sum_{k=1}^K n_{L} \hat{p}_{kL}(1- \hat{p}_{kL}) + \sum_{k=1}^K n_{R} \hat{p}_{kR}(1- \hat{p}_{kR})$
  - Cross-entropy: $$CE(j,s) = -\sum_{k=1}^K n_{L} \hat{p}_{kL}\log(\hat{p}_{kL}) - \sum_{k=1}^K n_{R} \hat{p}_{kR}\log(\hat{p}_{kR})$$
- Otherwise the algorithm stays the same (e.g. cost complexity pruning)
:::


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הקריטריון שאנחנו עושים לו מינימיזציה כשאנחנו בוחנים ספליט נקרא באופן כללי אימפיוריטי, כי אנחנו רוצים לבדוק עד כמה החלוקה הופכת את שני הנודים שנוצרים ל"טהורים" כלומר עד כמה הם מכינים רק קלאס אחד של תצפיות.

נסמן בp_hat את שיעור התצפיות מקלאס K בנוד השמאלי ובנוד הימני. ונסמן בy_hat את הקלאס שהוא הרוב בנוד השמאלי ובימני.

אפשרות אחת היא לעשות מינימום למיסקלסיפיקיישן ארור: שגיאת הפיצול היא מספר התצפיות שלא שוות לקלאס הרוב בצד שמאל, ועוד מספר התצפיות שלא שוות לקלאס הרוב בצד ימין.

אפשרות נוספת פופולרית היא מדד הג'יני. ניקח לדוגמא את צד שמאל עם nL תצפיות. אם נחשוב על איזשהו משתנה Z שסופר כמה מהתצפיות האלה שייכות לקלאס k מסוים, כשההסתברות לקלאס k היא p_hatK, אז אפשר למדל משתנה כזה כמשתנה בינומי, והשונות שלו היא הרכיב שרשום כאן: N כפול P כפול 1 פחות P. כלומר אפשר לחשוב על מדד ג'יני כסוכם את השונויות של משתנים בינומיים שסופרים כמה מהתצפיות ה מכל קלאס. תיכף ננסה לתת תחושה איך זה נראה.

אפשרות נוספת היא קרוס-אנתרופי. גם לקרוס אנתרופי יש הצדקה, זה מדד מתחום תורת האינפורמציה שמנסה לכמת כמה קל להעביר מידע על וקטור של נתונים. במקרה שלנו, להעביר וקטור שעבור כל תצפית אומר מאיזה קלאס היא. אם כל התצפיות אחידות מאותו קלאס למשל, צריך להעביר רק נתון אחד, מה הקלאס של התצפית הראשונה, והקרוס אנתרופי יהיה נמוך מאוד. הכי גרוע זה אם כל התצפיות שונות ואז נצטרך להעביר את כל הנתונים של הוקטור, תצפית אחר תצפית, והקרוס-אנתרופי יהיה מאוד גבוה. כלומר זה מדד טוב לאימפיוריטי של הספליט שאנחנו רוצים לבצע.

לוקח זמן להתרגל למדדים האלה, אבל החדשות הטובות הן שחוץ מהשינוי הזה האלגוריתם של בניית עץ שלמדנו נשאר בדיוק אותו דבר. למשל גם הקריטריון לעצירה נשאר אותו דבר, אפשר לעצור רק כשמגיעים לעומק מקסימלי של צמתים, אפשר לעשות פרונינג, המדד שישמש אותנו לקריטריון פרונינג יהיה אחד המדדים שאנחנו רואים כאן של אימפיוריטי, במקום RSS.
:::
:::

---

### On impurity measures

For a single node, $K = 2$ classes, no weighing:

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Define the range of probabilities p from 0 to 1
p = np.linspace(0, 1, 500)

# Calculate the metrics
misclassification_error = np.minimum(p, 1 - p)
gini_index = 2 * p * (1 - p)
cross_entropy = -(p * np.log(p + 1e-10) + (1 - p) * np.log(1 - p + 1e-10))  # Adding a small value to avoid log(0)

# Plot the metrics
plt.figure(figsize=(8, 5))

plt.plot(p, misclassification_error, color='black', label='Misclassification Error')
plt.plot(p, gini_index, color='black', linestyle='--', label='Gini Index')
plt.plot(p, cross_entropy, color='black', linestyle='-.', label='Cross-Entropy')
plt.xlabel('Probability $\hat{P}(Y = 1)$')
plt.ylabel('Impurity metric')
plt.legend()
plt.show()
```

::: {.fragment}
::: {.callout-note}
We mostly use Gini and Cross-entropy to grow the tree, not misclassification error. Why?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בכל-זאת כדי לתת תחושה של ההבדל בין המדדים אפשר לצייר מה הם יגידו כפונקציה של p_hat, הפרופורציה שY יהיה מקלאס אחד לעומת קלאס אחר בבעיה בינארית. כאן המיסקלסיפיקיישן ארור יהיה בעצם המינימום בין p ל-1 פחות p. הג'יני יהיה פעמיים p כפול 1 פחות p. והקרוס-אנתרופי יהיה p כפול לוג p, ועוד 1 פחות p כפול לוג של 1 פחות p, על כל זה מינוס.

הדבר הראשון שבולט הוא ששלושת המדדים מסכימים שהכי גרוע זה אם P הוא חצי, כלומר התצפיות בנוד מסוים מתחלקות חצי חצי בין שני הקלאסים. זה מצב של חוסר ודאות מוחלט. שלושתם מקבלים את המקסימום שלהם בנקודה הזאת.

הדבר השני שבולט הוא שבעוד שלמיסקלסיפיקיישן רייט יש נקודת חוסר רציפות כאן והעונש שלו עולה ויורד בצורה ליניארית, שני המדדים האחרים רציפים ומענישים הרבה יותר פרופורציות נמוכות מדי וגבוהות מדי, כלומר מענישים יותר פרופורציות ברגע שהנוד מתחיל להיות רק קצת לא טהור.

ומסתבר שיש לזה השפעה, אנחנו בדרך כלל ניקח את מדד הג'יני או הקרוס אנתרופי לגידול העץ ולא את המיסקלסיפיקיישן ארור. למה? כי הוא קצת חסר רגישות. אפשר להישאר בעולם של קלספיקציה בינארית ולחשוב למשל על 400 תצפיות שמגיעות לצומת, מתוכן 200 מקלאס 0 ו200 מקלאס 1. נניח שיש שני מועמדים לפיצול, פיצול אחד נותן:
((150, 50), (50, 150))
פיצול אחר נותן:
((100, 200), (100, 0))
מבחינת מיסקלסיפיקיישן ארור, שני הספליטים זהים, 0.25 מהתצפיות לא מסווגות לקלאס הרוב. אבל ברור לנו שהחלוקה השניה היא קצת טובה יותר, היא מצאה דרך ליצור נוד טהור לגמרי, 100 תצפיות או רבע מהנתונים שלגביהם אין ספק. ושני המדדים האחרים ג'יני וקרוס אנתרופי יהיו נמוכים יותר, הם יהיו רגישים לזה ויעדיפו את הספליט השני.

בזה אנחנו מסכמים את ההתייחסות לעצים לקלסיפיקציה, בחלק הבא נראה כמה דברים שעצים לקלסיפיקציה וגם לרגרסיה עושים טוב מאוד, כמה דברים שהם עושים רע מאוד, ואיך בסופו של דבר נשתמש בהם כדי ליצור את אחד המודלים החזקים ביותר בלמידת מכונה.
:::
:::

---

## Trees issues {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מה עצים טובים בו במיוחד? מה הם גרועים בו במיוחד? בחלק הזה נסכם את היתרונות והחסרונות של עצים.
:::
:::

---

### Categorical features

::: {.incremental}
- Ordered categorical variables: treat exactly as continuous
- Unordered categorical variables:
  - All subsets exhaustive search? (two main problems)
  - Grouping for values with too few observations
  - CART approach:
    - For each category $q$ denote $\bar{y}_q$ the average of the observations of class $q$ in the current node.
    - Sort categories in increasing order of  $\bar{y}_q$ : $\bar{y}_{(1)} \leq \bar{y}_{(2)} \leq \ldots \leq \bar{y}_{(Q)}$
    - The optimal split on the training data is guaranteed to be one of the $Q-1$ splits along this list
    - For 2-class classification, we simply replace $\bar{y}_q$ with $\hat{p}_q$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מה לגבי משתנים קטגוריאליים? דיברנו עליהם גם כשהתמקדנו במודל הליניארי, שם העלינו את האפשרות לקודד Q רמות של המשתנה הקטגוריאלי לQ - 1 משתנים בינאריים, קראנו לזה OHE.

אפשר עקרונית לעשות את זה גם בעצים, אבל בעצים יש אפשרות טובה יותר.

קודם כל אני מקווה שברור שהם המשתנה הוא אורדינלי, למשל רמת השכלה, אפשר להתייחס אליו כמשתנה רציף עם Q רמות ולהמשיך כרגיל, לבחון את האפשרות לעשות ספליט בין כל שתי רמות עוקבות.

אם הוא לא אורדינלי, אפשרות אחת היא לעשות חיפוש על פני כל האפשרויות לחלק את Q הקטגוריות לשתי קבוצות. אבל זה בעייתי משתי סיבות: אחת חישובית, מספר האפשרויות שנצטרך לבדוק בכל צומת הוא סדר גודל של 2 בחזקת Q, מה שיכול להתפוצץ מהר מאוד. סיבה אחרת היא סטטיסטית, גם אם היה לנו מחשב על שאין לו בעיה לבדוק 2 בחזקת Q אפשרויות לפיצול רק על המשתנה הזה, הסיכון לאוברפיטינג גדול מאוד. בכל צומת יש סיכוי סביר שבקומבינציה ספציפית של קבוצה מסוימת של קטגוריות עם מעט מאוד תצפיות באחד הנודים אחרי הפיצול, נקבל שכולן מקלאס מסוים -- אבל זה רק במקרה!

אפשרות שתמיד צריך לשקול אותה היא לאחד קטגוריות עם מעט תצפיות לקטגוריה אחת ובכך להוריד את המימד של המשתנה הקטגוריאלי שלנו.

אבל מסתבר שעצים יכולים לעשות דבר נוסף: נחשב עבור כל קטגוריה את הממוצע של Y ונמיין את הממוצעים האלה בסדר עולה. אפשר להוכיח שהספליט הטוב ביותר נמצא בחיפוש רגיל על וקטור הממוצעים הזה, לפי הסדר, כלומר ירדנו שוב לQ - 1 אפשרויות, כמו משתנה קטגוריאלי אורדינלי.

אם הבעיה היא קלסיפיקציה בינארית אפשר לעשות בדיוק אותו דבר על הפרופורציות של אחד הקלאסים, ממוינות בסדר עולה.
:::
:::

---

### Missing data

::: {.incremental}
- Many models' only choices: ignore missing data or impute them (e.g. mean imputation, EM algorithm)
- Two more natural choices in trees:
  - Surrogate splits: for every split $(j, s)$ keep the next most "similar" $(j', s')$ splits
  - If observation $\mathbb{x}_i$ is missing element $x_{ij}$ for split $(j, s)$ --- send it both left and right!
    - Its average (weighted) prediction has a nice interpretation: $\mathbb{E}(y|x_{ij} \text{ is missing})$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
תחום אחר שעצים יכולים להועיל בו מאוד הוא טיפול בערכים חסרים. טיפול בערכים חסרים זה תחום עצום, שאפשר לבלות עליו סמסטר שלם, כמו לחשוב על למה ערכים הם חסרים והאם המנגנון הזה שיצר את החוסר צריך או לא להשפיע על המידול שלנו. רוב המודלים ניצבים בין שתי אפשרויות: להתעלם מתצפיות עם ערכים חסרים באחד המשתנים, מה שנשמע בעייתי מאוד בצדק ויכול להקטין משמעותית את מדגם הלמידה. ואפשרות אחרת זה להציב ערכים במשתנים שבהם יש לנו ערכים חסרים ורק אז להמשיך בלמידה. אפשר להציב את הממוצע או החציון של המשתנה על פני תצפיות שבהן הוא לא חסר. דרך מתוחכמת יותר היא לעשות רגרסיה של המשתנה על פני המשתנים האחרים, זה תיאור קצת פשטני של האלגוריתם EM שנלמד בהמשך.

בעצים יש אפשרויות טבעיות יותר להכיל באופן טבעי תצפיות חסרות:

דרך אחת היא באמצעות סארוגייט ספליטס או פיצולים פונדקאיים: במהלך בניית העץ, לכל פיצול J, S, נשמור גם פיצולים חלופיים על משתנים אחרים, שהם דומים לפיצול המקורי, כלומר הם מחלקים את המדגם בצורה דומה לפיצול המקורי. זה כמובן מנפח קצת על הדיסק את המודל שבסוף נשמר, אבל זה פרקטי מאוד. כשתגיע באימון או בחיזוי תצפית שאין לה ערך במשתנה שמפצל צומת, נעביר אותה בפיצול החלופי הבא שלגביו אין לה ערך חסר.

אפשרות אחרת שאנחנו רואים במימושים של עצים, היא איפה שלתצפית I יש ערך חסר במשתנה J, לשלוח אותה גם ימינה וגם שמאלה. אם זה בזמן חיזוי, החיזוי הסופי שלה יהיה הממוצע של החיזויים בעלים שהיא הגיעה אליהם בסופו של דבר. היתרון כאן הוא שבמקרה כזה החיזוי שלה מקבל פירוש מפתה, זה בעצם התוחלת המותנית של Y כשלקחנו בחשבון שהתצפית חסרה, על פני אפשרויות שונות למילוי הערכים החסרים שלה.
:::
:::

---

### Trees advantages

- Highly interpretable, "neighborhoods" (when tree is not large, but see variance issue)
- Easy to implement (if-else statements)
- Fast (in prediction at least)
- Little pre-processing of predictors needed (continuous, categorical, missing)
- Non-parameteric, non-linear, assumption free
- Feature selection built-in
- Low bias, in general

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסכם את היתרונות של עצים: באופן כללי מדובר במודל די אינטואיטיבי שמחקה חשיבה בתרשים זרימה. אם הוא קטן גם אפשר ממש בכמה צעדים להבין איך החליט מה שהחליט -- למרות שתיכף נסייג את זה.

מלבד זאת, קל מאוד לממש עץ החלטה, בכל שפה שתרצו, זה בסך הכל אוסף של if else.

זה מודל די מהיר לחיזוי עבור תצפיות חדשות, כמו שדיברנו אין צורך בטיפול מיוחד במשתנים כמו סטנדרטיזציה או טיפול במשתנים קטגוריאליים, או טיפול במשתנים עם ערכים חסרים.

המודל א-פרמטרי, בלי הנחות מיוחדות, מסוגל לתאר יחסים לא-ליניאריים, יש לו פיצ'ר סלקשן בילט אין במובן שאם משתנה לא מופיע בעץ פשוט אין לו השפעה.

ואנחנו רואים שעץ הוא מודל עם הטיה קטנה מאוד, תלוי כמובן בעומק שלו, ככל שיהיה עמוק יותר כך ירד הביאס.
:::
:::

---

### Trees disadvantages

::: {.fragment}

```{python}
#| echo: false

import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# Updated plotting function for logistic regression boundary
def plot_logistic_boundary(X, y, model, ax):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, colors=['white', 'lightgreen'])
    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.Paired)

    # Logistic Regression decision boundary
    coef = model.coef_[0]
    intercept = model.intercept_
    x_vals = np.array([x_min, x_max])
    y_vals = -(x_vals * coef[0] + intercept) / coef[1]
    ax.plot(x_vals, y_vals, color="black", linewidth=2)

    # Adjust limits
    ax.set_xlim(x_min, x_max)
    ax.set_ylim(y_min, y_max)

# Updated plotting function for decision tree boundaries
def plot_decision_tree_boundary(X, y, model, ax):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, colors=['white', 'lightgreen'])
    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.Paired)

    # Recursive partition plotting
    def plot_partition(node_id, x_min, x_max, y_min, y_max):
        node = model.tree_.__getstate__()['nodes'][node_id]
        if node['left_child'] == -1:  # Leaf node
            return
        
        # Split on X1 or X2
        if node['feature'] == 0:  # Split on X1
            split_value = node['threshold']
            ax.plot([split_value, split_value], [y_min, y_max], color="black", linewidth=2)
            plot_partition(node['left_child'], x_min, split_value, y_min, y_max)
            plot_partition(node['right_child'], split_value, x_max, y_min, y_max)
        elif node['feature'] == 1:  # Split on X2
            split_value = node['threshold']
            ax.plot([x_min, x_max], [split_value, split_value], color="black", linewidth=2)
            plot_partition(node['left_child'], x_min, x_max, y_min, split_value)
            plot_partition(node['right_child'], x_min, x_max, split_value, y_max)
    
    plot_partition(0, x_min, x_max, y_min, y_max)

    # Adjust limits
    ax.set_xlim(x_min, x_max)
    ax.set_ylim(y_min, y_max)

# Generate a linearly separable dataset
np.random.seed(0)
X_linear = np.random.randn(200, 2)
y_linear = (X_linear[:, 0] + X_linear[:, 1] > 0).astype(int)

# Generate a non-linearly separable dataset (a square in the middle)
X_nonlinear = np.random.uniform(-3, 3, size=(200, 2))
y_nonlinear = np.ones(200)
y_nonlinear[(X_nonlinear[:, 0] > -1) & (X_nonlinear[:, 0] < 1) & 
           (X_nonlinear[:, 1] > -1) & (X_nonlinear[:, 1] < 1)] = 0

# Initialize classifiers
logistic_model_linear = LogisticRegression()
tree_model_linear = DecisionTreeClassifier(max_depth=10)

logistic_model_nonlinear = LogisticRegression(fit_intercept=False)
tree_model_nonlinear = DecisionTreeClassifier(max_depth=5)

# Fit classifiers on the linear dataset
logistic_model_linear.fit(X_linear, y_linear)
tree_model_linear.fit(X_linear, y_linear)

# Fit classifiers on the non-linear dataset
logistic_model_nonlinear.fit(X_nonlinear, y_nonlinear)
tree_model_nonlinear.fit(X_nonlinear, y_nonlinear)

# Create subplots
fig, axs = plt.subplots(2, 2, figsize=(6, 6))

# Top-left: Logistic regression (linear separable)
plot_logistic_boundary(X_linear, y_linear, logistic_model_linear, ax=axs[0, 0])

# Top-right: Decision tree (linear separable)
plot_decision_tree_boundary(X_linear, y_linear, tree_model_linear, ax=axs[0, 1])

# Bottom-left: Logistic regression (non-linear separable)
plot_logistic_boundary(X_nonlinear, y_nonlinear, logistic_model_nonlinear, ax=axs[1, 0])

# Bottom-right: Decision tree (non-linear separable)
plot_decision_tree_boundary(X_nonlinear, y_nonlinear, tree_model_nonlinear, ax=axs[1, 1])

# Remove plot titles and set X, Y axis labels
for ax in axs.flat:
    ax.set_xlabel('X1')
    ax.set_ylabel('X2')

plt.tight_layout()
plt.show()
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז זה נשמע שיש המון יתרונות לעצי החלטה. מה עם חסרונות?

חיסרון אחד שיש בכל מודל שאנחנו לומדים מגיע ממשפט הנו פרי לאנץ', לכל מודל אני יכול למצוא נתונים שמאוד לא מתאימים לו, ובמקרה של עץ החלטה הטענה היא שאני יכול בקלות רבה מדי.

מה שאנחנו רואים כאן זו בעיה של קלסיפיקציה לשני קלאסים, עבור שני פיצ'רים, X1 וX2. מצד שמאל אפשר לראות שהקלאסים הם מה שנקרא ליניארלי ספרבל, אפשר למצוא קו רגרסיה לוגיסטית שיפריד בין הקלאסים בקלות. זה יחס פשוט מאוד ונפוץ מאוד בנתונים אמיתיים. אבל עץ נורא מתקשה עם יחס כזה בין X לY. תראו איזה סלטות באוויר הוא עושה עם ההתעקשות שלו לחלק את המרחב רק עם קווים מקבילים לצירים. (להדגים) ואתם יכולים לדמיין לעצמכם שהעץ הזה עמוק מאוד ודי מסובך.

כדי לא לקפח את העץ ציירנו כאן גם מצב הפוך, של יחס לא ליניארי בין X לקלאס Y, שעץ ממדל בצורה יחסית קלה ורגרסיה לוגיסטית נכשל לגמרי, כל מה שיש לרגרסיה לוגיסטית זה קו ישר.
:::
:::

---

### Trees disadvantages

- Lack of smoothness: rectangular predictor regions are not always a good thing
- Intuitive appeal is misleading: very unstable, sensitive to small changes in data
- Greedy: no guarantee for optimality
- Complexity of prediction limited in no. of leaves! (For a simple CART)
- HIGH VARIANCE $\Rightarrow$ not a competitive model in terms of prediction accuracy!

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
יש עוד הרבה חסרונות לעצי החלטה:

מה שכרגע הדגמנו הוא שהם מודלים מאוד לא חלקים וקשה להם לתאר פונקציות חלקות, עם איזורי ההחלטה המלבניים שלהם.

גם מה שאמרנו קודם על מודל אינטואיטיבי שקל להסביר זה לא מדויק - בפועל עצים הם מודלים לא יציבים במובן שהם רגישים מאוד לשינויים קלים בדאטא אמיתי, לא דאטא מסומלץ.

זה נובע בעיקר מהאופן הגרידי שלהם, שפיצולים ראשונים במעלה העץ יש להם המון השפעה על האפשרויות של המודל להתפצל בהמשך. זה נובע גם מהנוקשות של איזורי החלטה מלבניים.

וזה נובע גם מהמוגבלות של עץ לתאר איזושהי תופעה בשורה התחתונה במספר סופי של עלים. תחשבו על זה שעץ רגרסיה עם עומק מקסימלי של 3 בסופו של דבר נותן לנו מקסימום 8 שכונות, כלומר 8 מספרים אפשריים של חיזוי.

בשורה התחתונה זה עניין של שונות. הביאס אולי נמוך, אבל השונות גבוהה מאוד, מה שמביא לתופעה שבנתונים אמיתיים עץ החלטה פשוט לא מדייק מבחינת חיזוי וכמעט תמיד אפשר למצוא מודלים עם דיוק גבוה יותר.

אז בשביל מה אנחנו לומדים על עצים?! אנחנו לומדים על עצים כי אמנם עץ אחד הוא מודל די מוגבל, אבל יער של עצים, הוא כבר מודל עוצמתי למדי.
:::
:::

---

## Random Forests {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איך משלבים קבוצה של עצים ליער?
:::
:::

---

### Ensemble methods: using trees as subroutines

Instead of a single tree being a model, combine many trees into a model:

1. Bagging and Random Forest: Fit different trees to the data and average them
2. Boosting: Adaptively build a model from adding more and more trees

::: {.fragment}
- We will focus now on Random Forest (also Bagging), later discuss boosting

- Main idea of Random Forest: Take advantage of the instability and high variance of the trees

- Trees are unstable and greedy: if we change the data a little bit, the tree can change a lot

- Now we intentionally change (randomize) the data to get a different tree every time, and average them
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בחלק הזה נלמד על שיטת אנסמבל, שעושה קומבינציה של מודלים חלשים רבים, למודל חזק במיוחד.

נראה בקורס שתי שיטות אנסמבל מבוססות עצים. הראשונה רנדום פורסט, שמבוססת על מיצוע של הרבה עצים שונים, והשנייה בוסטינג שבה אנחנו בונים עץ אחרי עץ בצורה אדפטיבית.

נתחיל ברנדום פורסט. במקום להתאים עץ אחד לנתונים, אנחנו נתאים הרבה. אבל לא נתאים אותם לאותם הנתונים, אחרת אין הבדל. נתאים אותם כל פעם על דאטא קצת אחר, דאטא שעבר רנדומיזציה, בשתי דרכים שונות. לבסוף נמצע את העצים -- החיזוי לכל תצפית יהיה ממוצע שלה על פני הרבה עצים, ונראה שכך נטפל באופן ישיר בבעיות של העץ היחיד.
:::
:::

---

### Reminder: the value of averaging

- Assume $z_i \sim F$ has some distribution with mean $\mu$ and variance $\sigma^2$

- If $z_1,\dots,z_m \sim F$ are independent, then $Var(\bar{z}) = \sigma^2 / m$, so $\bar{z}$ is close to $\mu$ for large $m$ 

::: {.incremental}
- What if $z_1,\dots,z_m$ are dependent?

- Slightly more complex setting: assume $z_1,\dots,z_m$ are *somewhat* dependent $Cov(z_i,z_j) = \rho \sigma^2,\;\rho<1$

- Now we still get some variance reduction from averaging: 
$$Var(\bar{z}) \approx \rho\sigma^2 + (1-\rho)\sigma^2/ m$$

- This is exactly the intuition behind Random Forest
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניזכר בערך המיצוע. מה מיצוע נותן לנו? אנחנו מכירים את זה משונות הממוצע או מהתפלגות הממוצע לפי משפט הגבול המרכזי. אם משתנה מקרי Z_i, מתפלג לפי איזושהי התפלגות F עם תוחלת מיו ושונות סיגמא בריבוע, ואני לוקח m תצפיות כאלה בלתי תלויות, אז השונות של ממוצע המדגם המקרי קטנה פי m. כלומר ככל שm גדול כך הפיזור סביב הממוצע קטן והוא מתקרב לתוחלת האמיתית מיו.

ונניח שהתצפיות הן לא בלתי תלויות. לא רק שהן לא בלתי תלויות, הן תלויות לחלוטין, הן אותה תצפית בדיוק, שחוזרת על עצמה m פעמים. מה יהיה אז הממוצע? התצפית עצמה כמובן. והאם הקטנו את השונות של ההתפלגות המקורית? בכלל לא, נישאר עם השונות המקורית סיגמא בריבוע. כלומר יש כאן איזשהו טווח מתצפיות בלתי תלויות לחלוטין ועד תצפיות תלויות לחלוטין, וההקטנה של סיגמא בריבוע בהתאם.

נסתכל על מצב ביניים, שהתצפיות לא בלתי תלויות לחלוטין אבל גם לא חוזרות על עצמן, המתאם בין זוג תצפיות הוא איזשהו רו שקטן מ1, כלומר הקווריאנס יהיה סיגמא בריבוע כפול רו.

אפשר לראות שכעת שונות ממוצע המדגם היא בקירוב רו סיגמא בריבוע, ועוד 1 מינוס רו כפול סיגמא בריבוע חלקי m. זאת אומרת כשרו שווה ל1, תלות מושלמת, אנחנו נשארים עם סיגמא בריבוע השונות המקורית, וכשרו שווה לאפס, שזה אומר תצפיות בלתי תלויות, נקבל את סיגמא בריבוע חלקי m, שונות מדגם מקרי המוכרת לנו.

זו האינטואיציה שמסבירה למה רנדום פורסט עובד. אם נצליח לקחת עוד ועוד דגימות עם כמה שפחות תלות - במקרה שלנו עוד ועוד עצים, נקטין את השונות המקורית של כל אחת מהן עד פי m. אם הדגימות שלי תלויות חזק אחת בשניה, הרווח שלי מוגבל מפעולת המיצוע. נרצה אם ככה לייצר עצים שיהיו שונים כמה שיותר אחד מהשני כך שנרוויח מהמיצוע שלהם.

:::
:::

---

### Random forest algorithm

- Repeat $B$ times: 
1. Randomize the data (by taking a [bootstrap]{style="color:red;"} sample $b$)
2. Build a tree $T_b(X)$ on the randomized data, also randomize tree building: randomly choose $m$ features to consider at each node

::: {.fragment}
- To predict at new $x_0$, apply each tree and average their predictions: $\hat{f}(x_0) = \frac{1}{B}\sum_{b = 1}^B T_b(x_0)$ or take majority class for classification
:::
::: {.fragment}
- Intuition: trees are different because of randomization, they are like $z_1,...z_n \stackrel{\cdot}{\sim} P(y|x_0)$
:::
::: {.fragment}
- Hence we expect (and indeed see!) that Random Forest gives more accurate predictions of $E(y|x)$ or $P(y=1|x)$ than single trees
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איך נשיג את היער עצים הזה ששונים זה מזה כמה שיותר?

נזריק רנדומיזציה לתהליך: כל עץ יראה דאטא קצת אחר, נהוג לקחת רק חלק מהנתונים, subsample, או מדגם בוטסטראפ, שזה מדגם בגודל m המקורי, עם החזרה. דבר שני שנעשה, תוך כדי בנית העצים על הדאטא הזה, זה בכל צומת נגריל מספר מסוים של משתנים שיהיו מועמדים לפיצול. כלומר אם בעץ המקורי בכל צומת הוא מתחשב בכל המשתנים האפשריים, העצים שלנו עשויים לראות בכל צומת משתנים אחרים לחלוטין.

כעת מגיעה תצפית חדשה לחיזוי. מה זה אומר למצע עצים? זה אומר שנריץ אותה בכל העצים, והחיזוי הסופי שלה יהיה הממוצע שלהם או הקלאס שרוב העצים מצביעים עליו, בשביל קלסיפיקציה.

על האינטואיציה דיברנו בהרחבה, העצים הם כמו תצפיות ממדגם. הם לא יכולים להיות לגמרי בלתי תלויים כי הם בכל זאת מבוססים על אותו דאטא. אבל נדאג שיהיו כמה שפחות תלויים אחד בשני, וככה נרוויח מהמיצוע שלהם. כשאנחנו רואים תצפית חדשה אפשר לחשוב שאנחנו דוגמים עבורה בקירוב מתוך ההתפלגות המותנית של Y בהינתן התצפית החדשה, ולכן הממוצע על פני הרבה עצים או דגימות כאלה, אומד את התוחלת המותנית של Y בהינתן התצפית החדשה.

אילו עצים נגדל? עמוקים או שטוחים? עמוקים כמובן! עצים עמוקים שמסוגלים לתאר יחסים מורכבים כמה שניתן. לעצים כאלה תהיה שונות גבוהה שנקטין עם המיצוע. אם נבחר בעצים שטוחים יותר, נתחיל אולי בטעות פחות גבוהה אבל גם לא נרוויח מספיק מהמיצוע. למה שלא נראה את רנדום פורסט בפעולה על הנתונים שלנו.
:::
:::

---

### Example: credit data

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score, KFold

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])
# Assuming X, y are already defined
n_splits = 5  # Number of cross-validation folds
n_trees = range(1, 101, 10)  # Range of trees to evaluate in Random Forest

train_errors_rf = []
test_errors_rf = []
train_errors_rf_std = []
test_errors_rf_std = []

# Cross-validation setup
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

for n_tree in n_trees:
    train_scores = []
    test_scores = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        # Initialize the Random Forest Regressor
        rf_regressor = RandomForestRegressor(n_estimators=n_tree, random_state=42)
        rf_regressor.fit(X_train, y_train)
        
        # Evaluate train and test MSE
        train_scores.append(np.mean((rf_regressor.predict(X_train) - y_train) ** 2))
        test_scores.append(np.mean((rf_regressor.predict(X_test) - y_test) ** 2))
    
    # Store mean and standard deviation of MSE across folds
    train_errors_rf.append(np.mean(train_scores))
    test_errors_rf.append(np.mean(test_scores))
    train_errors_rf_std.append(np.std(train_scores)/np.sqrt(5))
    test_errors_rf_std.append(np.std(test_scores)/np.sqrt(5))

# Compute train/test MSE for Linear Regression and Single Decision Tree
linear_regressor = LinearRegression()
tree_regressor = DecisionTreeRegressor(max_depth=9)

# Single Decision Tree MSE
train_mse_tree = -cross_val_score(tree_regressor, X, y, cv=kf, scoring='neg_mean_squared_error').mean()
test_mse_tree = train_mse_tree  # Same for train and test in cross-validation

# Plotting the results
# plt.figure(figsize=(12, 8))

# Plot Random Forest MSE vs. Number of Trees
plt.errorbar(n_trees, train_errors_rf, yerr=train_errors_rf_std, label='RF Train MSE')
plt.errorbar(n_trees, test_errors_rf, yerr=test_errors_rf_std, label='RF Test MSE')

# Plot horizontal lines for a Single Decision Tree
plt.axhline(y=train_mse_tree, color='black', linestyle='-.', label='Single Tree MSE')

plt.xlabel('Number of trees')
plt.ylabel('MSE')
plt.ylim(0, 40000)
plt.legend()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Summary of Random Forest 

- Uses advantages of trees, mitigates their shortcomings

- RF trees should be as different as possible from each other: 
    1. Uses the high-variance property of trees
    2. Add randomization: subsampling of training data for each tree; randomizations in tree splitting

- Add diversity by making trees bigger, control variance by averaging, therefore: 
    1. Trees should be deep
    2. Should build and average as many of them as computationally possible

::: {.fragment}
- Great advantages for "big data": highly parallelizable and (almost) hyperparameter free!
:::

::: {.fragment}
- But it is also our first "black box" model
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסכם: שיטת רנדום פורסט משמרת את הגמישות של עצים תוך כדי שהיא מנסה להפחית את החסרון הכי גדול שלם, הנוקשות והשונות הגדולה שלהם.

אנחנו עושים את העצים כמה שיותר שונים זה מזה, על-ידי מדגמי בוטסטראפ ובחירת משתנים שונים כמועמדים לכל פיצול.

ומלבד זה אנחנו דואגים שהעצים יהיו עמוקים כמה שאפשר, כדי שנרוויח כמה שיותר מאפקט המיצוע, מעץ בודד עם איכות חיזוי גרועה להרבה עם איכות חיזוי טובה.

 עקרונית גם אמרנו, שככל שנבנה יותר עצים איכות החיזוי על הטסט סט יכולה רק לקטון, יתרון משמעותי לשיטה, בפועל אנחנו כנראה מוגבלים על-ידי כוח חישוב וגם גודל על הדיסק, כל אחד מהעצים האלה יכול להיות אוביקט די גדול, אלף עצים לשמור על שרתים זה כבר לא סימפטי.

 עוד יתרון שאנחנו פחות עוסקים בקורס הזה אבל הוא קריטי: קל למקבל רנדום פורסט על-פני מספר מחשבים? קל מאוד! כל עץ ברנדום פורסט יכול לגדול באופן בלתי תלוי מהאחרים, לכן אם הנתונים גדולים ועומדת לרשות מדען הנתונים סביבת עבודה מבוזרת, קלאסטר של מספר מחשבים, ניתן להגיע לאימון מהיר מאוד של האלגוריתם. ויתרון אחרון שרמזנו עליו - כמעט בכל שיטה שאנחנו לומדים יש היפרפרמטרים, איזשהם כפתורים שצריך לסובב כדי להתאים את האלגוריתם למקרה שלנו, כמו מספר השכנים בKNN או מטריקת המרחק. בסך הכל ברנדום פורסט אין פרמטרים שיש עליהם סימן שאלה, ברור שאנחנו צריכים כמה שיותר עצים וברור שהם צריכים להיות כמה שיותר עמוקים. זה הופך את רנדום פורסט לאלגוריתם אוף-דה-שלף מאוד פופולרי, כי בלי כיוונון אפשר להגיע מהר לתוצאה מצוינת.

 מצד שני, כרגע ראינו גם את המודל הראשון שלנו שהוא בגדר קופסה שחורה. להגיד איזה משתנה השפיע על התוצאה, האם משתנה מסוים מעלה או מוריד את ההסתברות שהתצפית תהיה מקלאס מסוים -- זה כבר הרבה יותר קשה לעשות ברנדום פורסט. איבדנו את האינטרפרטביליות. יש הרבה דרכים לנסות להסביר מה עושים מודלים של קופסה שחורה. בקורס שלנו אנחנו עוצרים כאן.
:::
:::
