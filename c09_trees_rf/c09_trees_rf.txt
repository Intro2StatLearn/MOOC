=== 1. הקדמה לעצי החלטה ===

בשיעור הזה נצלול לעומק אחד המודלים האינטואיטיביים ביותר בלמידה סטטיסטית - עצי החלטה. אנחנו נראה בשיעור הזה שחשוב להכיר היטב עצי החלטה, והם נמצאים בשימוש רב בתעשייה, אבל לא בפני עצמם. נראה שעצי החלטה לכשעצמם הם מודלים די חלשים של למידה, אבל כשמשלבים אותם למודל אחד גדול , בכל מיני דרכים, מקבלים מודל חזק מאוד.

:::

אילו מודלים למדנו עד עכשיו?

רוב המודלים היו מודלים פרמטריים וליניארים. אפשר לקרוא להם גם גלובליים כי על פני כל המרחב של X הם מניחים שיש איזה חוק שממפה בין X לY, שמנוסח על-ידי קבוצה של פרמטרים, שתקף בכל מקום במרחב.

מבחינת מודלים שהם א-פרמטריים, כלומר גמישים יותר, ויודעים למדל גם יחסים לא-ליניאריים, ראינו רק את KNN. KNN נקרא גם מודל לוקאלי, כי החיזוי לכל תצפית תלוי בסביבה שלה, שמשתנה לפי מדגם הלמידה, אין הנחה של איזו פונקציה יחידה על פני כל המרחב. אבל בKNN אנחנו לא מציצים על Y כשאנחנו בונים את השכונות.

מודל אחר שכן מסתכל על Y כשהוא בונה שכונות, זה עצי החלטה. גם כאן המודל א-פרמטרי, יודע למדל יחסים לא-ליניאריים, לוקאלי.

אבל כאן נחלק את המרחב של X לשכונות ככה שY בכל שכונה הוא כמו שיותר הומוגני, או אחיד, או צפוי.

ואחרי שנחלק את המרחב של X לM איזורים R1 עד RM, לכל איזור נחזה את אותו מספר לרגרסיה או קלאס לקלסיפיקציה, Cm. ניתן לרשום זאת עם משתני אינדיקטור כך: החיזוי הסופי לכל תצפית הוא סכום על כל האיזורים, כל תצפית שייכת רק לאיזור m אחד ותקבל חיזוי Cm בהתאם.

נשאלת השאלה, כשאני רושם את זה כך: זה לא מודל ליניארי? הרי אמרנו שעץ החלטה הוא מודל לא-ליניארי. אז במובן מסוים זה נכון, אפשר להסתכל על עצי החלטה כמודלים ליניאריים אבל על הפיצ'רים שמופיעים כאן, פיצ'רים שמגדירים איזורים, די מתוחכמים. ההבנה הזאת אכן מאפשרת הרחבה של המודל של עץ החלטה למודלים מתוחכמים יותר, אבל הם לא בחומר שלנו היום.

:::

באילו איזורים נבחר? כאן לדוגמה יש לנו מרחב של שני משתנים X1 וX2, ומייד אפשר לראות שאם נרצה להגדיר איזו מטריקת-על שאנחנו רוצים לעשות לה מינימום, ולהתחשב בכל חלוקה אפשרית של מרחבים, מרחב החיפוש שלנו יהיה עצום, וזה גם יכול לגרום כמובן לאוברפיטינג. תיכף נראה שכל אחת מהחלוקות שאנחנו רואים כאן לאיזורים, מאתגרת מאוד בשביל עץ ההחלטה הפשוט שאנחנו נבנה.

ואם אנחנו שואלים כבר לאילו איזורים נחלק את המרחב של X, באותה הזדמנות נשאל גם מה הם החיזויים שניתן לכל איזור ואיזור, Cm?

כאן התשובה דווקא אינטואיטיבית -- ברגרסיה נחזה את הממוצע של Y בכל איזור, ובקלסיפיקציה נחזה את הקלאס שיש לרוב התצפיות באיזור הזה. אבל אנחנו מקדימים את המאוחר, בואו נראה איך נעשית החלוקה לאיזורים בעץ החלטה קלאסי.

:::

הדרך שבה אנחנו נחלק לאיזורים, מנסה לחקות את דרך החשיבה שלנו כבני אדם שנתקלים בבעיה ושואלים את עצמם מה לעשות, או למשל רופאים שצריכים לתת אבחנה לפציינטים חדשים. אנחנו אוהבים לעבוד עם תרשימי זרימה. לדוגמא כאן, לשאול שאלה ראשונה על משתנה X1 האם הוא גדול או קטן-שווה מחצי. זאת כנראה השאלה הכי חשובה, לבירור מיידי, ותיכף נגדיר מה זה הכי חשובה. אם הוא גדול, יכול להיות שהגעתי לשכונה מאוד הומוגנית מבחינת Y ואין לי צורך יותר בעוד שאלות. אני אתן לה שם R4. אם הוא קטן יש לי צורך להמשיך לשאול עוד שאלות, ועכשיו אולי השאלה הכי חשובה היא שאלה על משתנה אחר, X2. וכך הלאה והלאה, עד שאין לי צורך יותר בשאלות ובעצם חילקתי את המרחב לאיזורים שונים R1 עד R4.

ניתן לראות את החלוקה הזאת בתרשים האמצעי, הנה השאלה הראשונה שמחלקת את המרחב לשני חלקים באמצעות קו אנכי בקואורדינטת X1, ואז שאלה שניה על קואורדינטת X2, שנשים לב, שהיא לא מתייחסת בכלל לאיזור R4 ששמנו בצד. היא מחלקת רק את שאר המרחב לשני חלקים, וכך הלאה.

כלומר עץ ההחלטה הקלאסי, שכל פעם מחלק את המרחב הנוכחי לשני חלקים בדיוק, מסוגל לייצר איזורי החלטה מלבניים, רק באמצעות קווים מקבילים לצירים. הוא עושה זאת שוב ושוב בצורה רקורסיבית ולכן השיטה נקראת גם recursive binary splitting.

בתרשים הימני ציירנו נוסף על מרחב X1, X2 גם ציר עומק שמייצג את y_hat, כלומר מה נחזה לכל איזור ואיזור. בעץ הקלאסי אנחנו חוזים ברגרסיה למשל רק מספר אחד לכל איזור כזה, הממוצע של Y, כלומר ככה ייראה נוף ההחלטה שלנו -- מעין קופסאות על קופסאות, או בניינים על בניינים. לא ליניארי מצד אחד - זה לא מישור - אבל גם די נוקשה, כמו שנראה בהמשך.

:::

בהיי-לבל ככה נראה האלגוריתם של בניית עץ קלאסי: מגיע מדגם למידה T בגודל n של זוגות תצפיות X וY.

בשורש העץ אנחנו נחזה עבור כל התצפיות את הממוצע של Y עבור רגרסיה או הקלאס שהוא הרוב עבור קלסיפיקציה.

בצורה רקורסיבית, נבחר תמיד זוג J, S של משתנה XJ וערך S לפצל עליו. הפיצול הוא תמיד בינארי, לצד אחד הולכות כל התצפיות שעבורן XJ קטן-שווה לS, ולצד האחר הולכות כל התצפיות שעבורן XJ גדול מS. כלומר בתור התחלה אנחנו מניחים שהפיצ'רים שלנו הם רציפים או אורדינליים, אחרת קצת קשה לדבר על ערך S לעשות עליו ספליט.

לכל איזור שהגענו אליו נחזה איזשהו ערך C (ממוצע עבור רגרסיה, הקלאס שהוא הרוב עבור קלסיפיקציה). ונמשיך, רקורסיבית.

עד מתי? עד איזשהו תנאי עצירה.

העצים השונים שפותחו לאורך השנים נבדלו ביניהם בעיקר בתשובה שלהם לשלוש השאלות הבאות:

איך לבחור את הפיצול בכל צומת בעץ, כלומר את הקומבינציה של משתנה J וערך S לפצל עליו.

איזה ערך Cm להתאים בכל איזור שהגענו אליו, שנקרא גם עלה של העץ או טרמינל נוד.

וכמובן: מהו תנאי העצירה.

אז אלגוריתמים שונים נראים אולי שונים במקצת בהתאם לתשובות שלהם לשאלות האלה, אבל הגישה שרשומה כאן, בהיי-לבל, לא שונה בין עצי החלטה שונים.

:::

=== 2. עצים לרגרסיה ===

נתמקד קודם בעצי רגרסיה ונראה שאם אנחנו מבינים אותם היטב המעבר לקלאסיפיקציה די פשוט.

:::

מה הקריטריון הטבעי לנו ממודלים אחרים ברגרסיה? אנחנו רוצים לעשות מינימום על הRSS.

בכל שלב מגיעות נאמר r תצפיות בצומת הנוכחי שאנחנו נמצאים בו. נגדיר את הפיצול על הזוג של משתנה וערך J, S, כחלוקה של r התצפיות לשתי קבוצות: left וright.

בכל קבוצה נסתכל על הממוצע של Y, והRSS הכללי של הפיצול הוא סכום הRSSים של שני האיזורים הנוצרים, כלומר סכום המרחקים הריבועיים של Y מהממוצעים שנוצרים, בצד ימין ובצד שמאל.

זה הקריטריון לעשות עליו מינימום, ואנחנו עוברים ככה על כל הp משתנים שלנו, על כל הערכים האפשריים לעשות עליהם פיצול. הפיצול שנבחר יהיה זה שבנקודה הנוכחית מביא את קריטריון הRSS למינימום.

זה נשמע קצת מסובך לעבור על כל המשתנים, בכל משתנה לעבור על כל הערכים ולחשב מחדש את הממוצעים ואת הRSS. אז הסיבוכיות כאן דווקא לא נוראית כמו שזה נשמע, בפועל אם ממיינים את המשתנים פעם אחרת בתחילת הריצה, ומשתמשים בטריק לחישוב הממוצעים והRSS כל פעם על-ידי שינוי של תצפית אחת שעוברת בין הקבוצות, אפשר לחשב את זה בזמן ריצה O(r*p), שלהרבה בעיות פרקטיות זה לא כל-כך נורא.

:::

לגבי השאלה ששאלנו, איזה ערך לנבא עבור כל שכונה, ברגרסיה התשובה די פשוטה. השכונות שלנו נוצרו במטרה להיות הומוגניות בY, כלומר כמעט ללא שינוי. אנחנו רוצים למדל את התוחלת המותנית של Y בהינתן X, כשהקריטריון מינימום שלנו הוא השגיאה הריבועית, ואנחנו כבר יודעים איזה אומד עושה מינימום לשגיאה הריבועית וגם אומד חסר-הטיה לתוחלת -- ממוצע המדגם של Y באותה שכונה או עלה שנוצר בתחתית העץ.

כאן Qm זה פשוט קבוצת התצפיות במדגם הלמידה שהגיעו לעלה m.

:::

התשובה לשאלה השלישית של מתי לעצור את גידול העץ, קצת יותר מורכבת.

קודם כל, ניקח צעד אחורה ונשאל -- למה בכלל לעצור? למה לא לתת לעץ לגדול ולגדול, עם עוד שאלות יותר ויותר ספציפיות שיחלקו את המרחב לשכונות יותר ויותר קטנות -- התשובה היא כמובן הביאס-וריאנס טריידאוף.

אם העץ לא עמוק מספיק -- הוא אולי לא מספיק גמיש לתאר את היחס בין X לY, זה נקרא אנדרפיטינג ויש לנו ביאס, הטיה גבוהה.

אם העץ עמוק מדי -- הוא גמיש מדי, הוא מאמין באמונה עיוורת למדגם הלמידה, יש לנו אוברפיטינג ושונות גבוהה מדי.

אז יש כמה היוריסטיקות להחליט מתי לעצור בגידול העץ:

אפשרות אחת היא פשוט לשלוט בעומק העץ עצמו, להגדיר איזשהו היפרפרמטר נוסף של מקסימום עומק, או המקסימום של שאלות שנשאל כל תצפית במורד העץ. אפשר לבחור אותו למשל עם קרוס ולידיישן.

אפשרות אחרת היא לגדל עץ כל עוד אנחנו מקבלים מינימום של שיפור או רווח בRSS, לעומת הRSS בלי הפיצול. פשוט נחשב את הRSS של כל התצפיות שהגיעו לשכונה, את הRSS של הפיצול הכי טוב בנוסחה שהראינו, ונראה אם ההפרש ביניהם שווה את הפיצול.

גישה אחרת יכולה להיות דווקא שליטה בפרמטר של מינימום גודל השכונה או הצומת. למשל אולי נחליט שכל עוד בשכונה יש 10 תצפיות או יותר שווה להמשיך לפצל אותה, כי אנחנו סומכים על ממוצע של 10 תצפיות, אבל מתחת לזה נעצור את הפיצול מהשכונה שאנחנו נמצאים בה עכשיו.

לכל אחת מהגישות האלה יש חסרונות:
החיסרון של שליטה בעומק העץ הוא שצריך לבחור את ההיפרפרמטר הזה.

החיסרון של השיטה של לעצור כשלא משיגים מספיק שיפור בRSS הוא שכל האלגוריתם שלנו הוא גרידי, חמדני, כלומר במיוחד בהתחלה יכול להיות שאנחנו נמצאים בצומת שבה לא נראה שיש דרך לפצל את התצפיות כך שנשיג שיפור דרמטי יחסית לRSS שבו אנחנו נמצאים, אבל אנחנו לא רואים בהמשך שכונות שבהן אם נפצל נשיג שיפור יחסי דרמטי.

והחיסרון של הגישה השלישית של גידול העץ עד מינימום גודל שכונה, הוא שהיא לא מבוססת על הנתונים, היא מלאכותית מדי והיא נוטה לתת עצים עמוקים מדי, בפועל על נתונים אמיתיים.

:::

אחת הגישות הרווחות החל מהמאמר המקורי על עצי החלטה, היא לגדל עץ עמוק ואז להחליט בצורה מושכלת אם שווה היה לקטום אותו איפשהו, מה שנקרא פרונינג.

אנחנו נגדל עץ עמוק מאוד T0, למשל באמצעות אחד הקריטריונים שהזכרנו, או אולי הכי עמוק שאפשר.

נגדיר איזשהו פרמטר עונש על גודל העץ אלפא, כך שעבור כל צומת וצומת שבדיעבד פיצלנו, אפשר להסתכל על הRSS הכללי שם ולהוסיף לו עונש אלפא כפול גודל העץ, כלומר כמה צמתים או כמה עלים יש בו, בנקודת הזמן הזאת.

העץ שנבחר בסופו של דבר הוא העץ שמשיג את המינימום קריטריון הזה שמסומן כאן כC(alpha), כי עבור כל אלפא נקבל עץ אחר.

זה נראה הרבה עבודה אבל בפועל קיים אלגוריתם יעיל שמחשב את כל המסלול הזה לכל אלפא, ואת אלפא אפשר לבחור עם קרוס-ולידיישן.

:::

נחזור לדוגמא שלנו מהשיעור הקודם של קרדיט דאטא ונגדל עץ החלטה של רגרסיה לחיזוי היתרה של לקוח בבנק.

אנחנו מבצעים כאן קרוס-ולידיישן על פני חמישה פולדים, ורושמים את ממוצע הMSE על פני הטריין והטסט, כשהפרמטר שבחרנו לשנות כאן הוא עומק העץ. החל מעץ בעל עומק אחד, מה שנקרא גזע או סטאמפ, עד לעץ בעל עומק מקסימלי 11, כלומר יש מקסימום 11 שאלות עד שמגיעים לחיזוי תצפית.

אנחנו כבר רגילים לראות שמדגם הלמידה בשלב מסוים עלול להגיע לטעות חיזוי אפס עם עץ מספיק עמוק, ושמדגם הטסט מראה שמספיקות 8-9 שאלות מקסימום כדי להגיע למינימום MSE על תצפיות שהמודל לא ראה, מעבר לזה אין ירידה בMSE ואולי אפילו קצת עלייה, כלומר אוברפיטינג.

:::

איך נראה העץ שלנו? איך אנחנו מסבירים את המודל שלנו?

עץ בעומק מקסימום 8 קצת קשה להכניס לשקף אחד, אז הכנסנו כאן עץ בעומק 3, שמגיע לשמונה שכונות או 2 בחזקת 3.

בכל פיצול אנחנו רואים את המשתנה שנבחר לפיצול והערך שעליו מפצלים, את הRSS, כמה תצפיות הגיעו לצומת הזאת ומה הממוצע שלהן, כלומר מה היינו חוזים אם כאן היה נגמר העץ.

אפשר לראות שעבור דירוג אשראי גבוה עוברים לחיזוי יתרה בחשבון קצת יותר גבוהה, ועבור דירוג אשראי נמוך יתרה יותר נמוכה. כלומר זאת השאלה הכי חשובה, שגורמת לירידה הכי גדולה בRSS.

בסופו של דבר אפשר לראות בעלים את השכונות השונות שנוצרו, הן גם צבועות בגרדיאנט כזה מחיזוי יתרה נמוכה מאוד בלבן ועד יתרה גבוהה מאוד בחום.

אנחנו רואים כאן כמה המודל אינטואיטיבי מצד אחד, מצד שני נזכיר שהעץ הכי טוב היה עם עומק מקסימלי 8, שזה דבר שכבר קשה יותר לעקוב אחריו.

:::

=== 3. עצים לקלסיפיקציה ===

נעבור עכשיו לעצים לקלסיפיקציה לK קלאסים, ונראה שנשאר לנו מעט מאוד לשנות.

:::

הקריטריון שאנחנו עושים לו מינימיזציה כשאנחנו בוחנים ספליט נקרא באופן כללי אימפיוריטי, כי אנחנו רוצים לבדוק עד כמה החלוקה הופכת את שני הנודים שנוצרים ל"טהורים" כלומר עד כמה הם מכילים רק קלאס אחד של תצפיות.

נסמן בp_hat את שיעור התצפיות מקלאס K בנוד השמאלי ובנוד הימני. ונסמן בy_hat את הקלאס שהוא הרוב בנוד השמאלי ובימני.

אפשרות אחת היא לעשות מינימום למיסקלסיפיקיישן ארור: שגיאת הפיצול היא מספר התצפיות שלא שוות לקלאס הרוב בצד שמאל, ועוד מספר התצפיות שלא שוות לקלאס הרוב בצד ימין.

אפשרות נוספת פופולרית היא מדד הג'יני. ניקח לדוגמא את צד שמאל עם nL תצפיות. אם נחשוב על איזשהו משתנה Z שסופר כמה מהתצפיות האלה שייכות לקלאס k מסוים, כשההסתברות לקלאס k היא p_hatK, אז אפשר למדל משתנה כזה כמשתנה בינומי, והשונות שלו היא הרכיב שרשום כאן: N כפול P כפול 1 פחות P. כלומר אפשר לחשוב על מדד ג'יני כסוכם את השונויות של משתנים בינומיים שסופרים כמה מהתצפיות הן מכל קלאס, ונרצה להקטין את השונויות האלה. תיכף ננסה לתת תחושה איך זה נראה.

אפשרות נוספת היא קרוס-אנתרופי. גם לקרוס אנתרופי יש הצדקה, זה מדד מתחום תורת האינפורמציה שמנסה לכמת כמה קל להעביר מידע על וקטור של נתונים. במקרה שלנו, להעביר וקטור שעבור כל תצפית אומר מאיזה קלאס היא. אם כל התצפיות אחידות מאותו קלאס למשל, צריך להעביר רק נתון אחד, מה הקלאס של התצפית הראשונה, והקרוס אנתרופי יהיה נמוך מאוד. הכי גרוע זה אם כל התצפיות שונות ואז נצטרך להעביר את כל הנתונים של הוקטור, תצפית אחר תצפית, והקרוס-אנתרופי יהיה מאוד גבוה. לכן זה מדד טוב לאימפיוריטי של הספליט שאנחנו רוצים לבצע.

לוקח זמן להתרגל למדדים האלה, אבל החדשות הטובות הן שחוץ מהשינוי הזה האלגוריתם של בניית עץ שלמדנו נשאר בדיוק אותו דבר. למשל גם הקריטריון לעצירה נשאר אותו דבר, אפשר לעצור רק כשמגיעים לעומק מקסימלי של צמתים, אפשר לעשות פרונינג, המדד שישמש אותנו לקריטריון פרונינג יהיה אחד המדדים שאנחנו רואים כאן של אימפיוריטי, במקום RSS.

:::

בכל-זאת כדי לתת תחושה של ההבדל בין המדדים אפשר לצייר מה הם יגידו כפונקציה של p_hat, הפרופורציה שY יהיה מקלאס אחד לעומת קלאס אחר בבעיה בינארית. כאן המיסקלסיפיקיישן ארור יהיה בעצם המינימום בין p ל-1 פחות p. הג'יני יהיה פעמיים p כפול 1 פחות p. והקרוס-אנתרופי יהיה p כפול לוג p, ועוד 1 פחות p כפול לוג של 1 פחות p, על כל זה מינוס.

הדבר הראשון שבולט הוא ששלושת המדדים מסכימים שהכי גרוע זה אם P הוא חצי, כלומר התצפיות בנוד מסוים מתחלקות חצי חצי בין שני הקלאסים. זה מצב של חוסר ודאות מוחלט. שלושתם מקבלים את המקסימום שלהם בנקודה הזאת.

הדבר השני שבולט הוא שבעוד שלמיסקלסיפיקיישן רייט יש נקודת חוסר רציפות כאן והעונש שלו עולה ויורד בצורה ליניארית, שני המדדים האחרים רציפים ומענישים הרבה יותר פרופורציות נמוכות מדי וגבוהות מדי, כלומר מענישים יותר פרופורציות ברגע שהנוד מתחיל להיות רק קצת לא טהור.

ומסתבר שיש לזה השפעה, אנחנו בדרך כלל ניקח את מדד הג'יני או הקרוס אנתרופי לגידול העץ ולא את המיסקלסיפיקיישן ארור. למה? כי הוא קצת חסר רגישות. אפשר להישאר בעולם של קלספיקציה בינארית ולחשוב למשל על 400 תצפיות שמגיעות לצומת, מתוכן 200 מקלאס 0 ו200 מקלאס 1. נניח שיש שני מועמדים לפיצול, פיצול אחד נותן:
((150, 50), (50, 150))
פיצול אחר נותן:
((100, 200), (100, 0))
מבחינת מיסקלסיפיקיישן ארור, שני הספליטים זהים, 0.25 מהתצפיות לא מסווגות לקלאס הרוב. אבל ברור לנו שהחלוקה השניה היא קצת טובה יותר, היא מצאה דרך ליצור נוד טהור לגמרי, 100 תצפיות או רבע מהנתונים שלגביהם אין ספק. ושני המדדים האחרים ג'יני וקרוס אנתרופי יהיו נמוכים יותר, הם יהיו רגישים לזה ויעדיפו את הספליט השני.

בזה אנחנו מסכמים את ההתייחסות לעצים לקלסיפיקציה, בחלק הבא נראה כמה דברים שעצים לקלסיפיקציה וגם לרגרסיה עושים טוב מאוד, כמה דברים שהם עושים רע מאוד, ואיך בסופו של דבר נשתמש בהם כדי ליצור את אחד המודלים החזקים ביותר בלמידת מכונה.

:::

=== 4. נושאים חשובים בעצים ===

מה עצים טובים בו במיוחד? מה הם גרועים בו במיוחד? בחלק הזה נסכם את היתרונות והחסרונות של עצים.

:::

מה לגבי משתנים קטגוריאליים? דיברנו עליהם גם כשהתמקדנו במודל הליניארי, שם העלינו את האפשרות לקודד Q רמות של המשתנה הקטגוריאלי לQ - 1 משתנים בינאריים, קראנו לזה OHE.

אפשר עקרונית לעשות את זה גם בעצים, אבל בעצים יש אפשרות טובה יותר.

קודם כל אני מקווה שברור שאם המשתנה הוא אורדינלי, למשל רמת השכלה, אפשר להתייחס אליו כמשתנה רציף עם Q רמות ולהמשיך כרגיל, לבחון את האפשרות לעשות ספליט בין כל שתי רמות עוקבות.

אם הוא לא אורדינלי, אפשרות אחת היא לעשות חיפוש על פני כל האפשרויות לחלק את Q הקטגוריות לשתי קבוצות. אבל זה בעייתי משתי סיבות: אחת חישובית, מספר האפשרויות שנצטרך לבדוק בכל צומת הוא סדר גודל של 2 בחזקת Q, מה שיכול להתפוצץ מהר מאוד. סיבה אחרת היא סטטיסטית, גם אם היה לנו מחשב-על שאין לו בעיה לבדוק 2 בחזקת Q אפשרויות לפיצול רק על המשתנה הזה, הסיכון לאוברפיטינג גדול מאוד. בכל צומת יש סיכוי סביר שבקומבינציה ספציפית של קבוצה מסוימת של קטגוריות עם מעט מאוד תצפיות באחד הנודים אחרי הפיצול, נקבל שכולן מקלאס מסוים -- אבל זה רק במקרה!

אפשרות שתמיד צריך לשקול אותה היא לאחד קטגוריות עם מעט תצפיות לקטגוריה אחת ובכך להוריד את המימד של המשתנה הקטגוריאלי שלנו.

אבל מסתבר שעצים יכולים לעשות דבר נוסף: נחשב עבור כל קטגוריה את הממוצע של Y ונמיין את הממוצעים האלה בסדר עולה. אפשר להוכיח שהספליט הטוב ביותר נמצא בחיפוש רגיל על וקטור הממוצעים הזה, לפי הסדר. כלומר נחפש מבין Q פחות אחת האפשרויות האלה איזה ממוצע יתן לנו ירידה בקריטריון RSS הכי גדולה, כך שכל התצפיות עם הקטגוריות לשמאלו ילכו שמאלה, וכל התצפיות עם הקטגוריות לימינו ילכו ימינה. ונשים לב, שירדנו שוב לQ - 1 אפשרויות, כמו משתנה קטגוריאלי אורדינלי.

אם הבעיה היא קלסיפיקציה בינארית אפשר לעשות בדיוק אותו דבר על הפרופורציות של אחד הקלאסים, ממוינות בסדר עולה.

:::

תחום אחר שעצים יכולים להועיל בו מאוד הוא טיפול בערכים חסרים. טיפול בערכים חסרים זה תחום עצום, שאפשר לבלות עליו סמסטר שלם, כמו לחשוב על למה ערכים הם חסרים והאם המנגנון הזה שיצר את החוסר צריך או לא להשפיע על המידול שלנו. רוב המודלים ניצבים בין שתי אפשרויות: להתעלם מתצפיות עם ערכים חסרים באחד המשתנים, מה שנשמע בעייתי מאוד בצדק ויכול להקטין משמעותית את מדגם הלמידה. ואפשרות אחרת זה להציב ערכים במשתנים שבהם יש לנו ערכים חסרים ורק אז להמשיך בלמידה. אפשר להציב את הממוצע או החציון של המשתנה על פני תצפיות שבהן הוא לא חסר. דרך מתוחכמת יותר היא לעשות רגרסיה של המשתנה על פני המשתנים האחרים, זה תיאור קצת פשטני של האלגוריתם EM שבודאי תיתקלו בו בקורסים מתקדמים.

בעצים יש אפשרויות טבעיות יותר להכיל באופן טבעי תצפיות חסרות:

דרך אחת היא באמצעות סארוגייט ספליטס או פיצולים פונדקאיים: במהלך בניית העץ, לכל פיצול J, S, נשמור גם פיצולים חלופיים על משתנים אחרים, שהם דומים לפיצול המקורי, כלומר הם מחלקים את המדגם בצורה דומה לפיצול המקורי. זה כמובן מנפח קצת על הדיסק את המודל שבסוף נשמר, אבל זה פרקטי מאוד. כשתגיע באימון או בחיזוי תצפית שאין לה ערך במשתנה שמפצל צומת, נעביר אותה בפיצול החלופי הבא שלגביו אין לה ערך חסר.

אפשרות אחרת שאנחנו רואים במימושים של עצים, היא איפה שלתצפית I יש ערך חסר במשתנה J, לשלוח אותה גם ימינה וגם שמאלה. אם זה בזמן חיזוי, החיזוי הסופי שלה יהיה הממוצע של החיזויים בעלים שהיא הגיעה אליהם בסופו של דבר. היתרון כאן הוא שבמקרה כזה החיזוי שלה מקבל פירוש מעניין, זה בעצם התוחלת המותנית של Y כשלקחנו בחשבון שהתצפית חסרה, על פני אפשרויות שונות למילוי הערכים החסרים שלה.

:::

נסכם את היתרונות של עצים: באופן כללי מדובר במודל די אינטואיטיבי שמחקה חשיבה בתרשים זרימה. אם הוא קטן גם אפשר ממש בכמה צעדים להבין איך החליט מה שהחליט -- למרות שתיכף נסייג את זה.

מלבד זאת, קל מאוד לממש עץ החלטה, בכל שפה שתרצו, זה בסך הכל אוסף של if else.

זה מודל די מהיר לחיזוי עבור תצפיות חדשות, כמו שדיברנו אין צורך בטיפול מיוחד במשתנים כמו סטנדרטיזציה או טיפול במשתנים קטגוריאליים, או טיפול במשתנים עם ערכים חסרים.

המודל א-פרמטרי, בלי הנחות מיוחדות, מסוגל לתאר יחסים לא-ליניאריים, יש לו פיצ'ר סלקשן בילט אין במובן שאם משתנה לא מופיע בעץ פשוט אין לו השפעה.

ואנחנו רואים שעץ הוא מודל עם הטיה קטנה מאוד, תלוי כמובן בעומק שלו, ככל שיהיה עמוק יותר כך ירד הביאס.

:::

אז זה נשמע שיש המון יתרונות לעצי החלטה. מה עם חסרונות?

חיסרון אחד שיש בכל מודל שאנחנו לומדים מגיע ממשפט הנו פרי לאנץ', לכל מודל אני יכול למצוא נתונים שמאוד לא מתאימים לו, ובמקרה של עץ החלטה הטענה היא שאני יכול לעשות את זה בקלות רבה מדי.

מה שאנחנו רואים כאן זו בעיה של קלסיפיקציה לשני קלאסים, עבור שני פיצ'רים, X1 וX2. למעלה אפשר לראות שהקלאסים הם מה שנקרא ליניארלי ספרבל, אפשר למצוא קו רגרסיה לוגיסטית שיפריד בין הקלאסים בקלות. זה יחס פשוט מאוד ונפוץ מאוד בנתונים אמיתיים. אבל עץ נורא מתקשה עם יחס כזה בין X לY. תראו איזה סלטות באוויר הוא עושה עם ההתעקשות שלו לחלק את המרחב רק עם קווים מקבילים לצירים. (להדגים) ואתם יכולים לדמיין לעצמכם שהעץ הזה עמוק מאוד ודי מסובך.

כדי לא לקפח את העץ ציירנו כאן גם מצב הפוך, של יחס לא ליניארי בין X לקלאס Y, שעץ ממדל בצורה יחסית קלה ורגרסיה לוגיסטית נכשל לגמרי, כל מה שיש לרגרסיה לוגיסטית זה קו ישר.

:::

יש עוד הרבה חסרונות לעצי החלטה:

מה שכרגע הדגמנו הוא שהם מודלים מאוד לא חלקים וקשה להם לתאר פונקציות חלקות, עם איזורי ההחלטה המלבניים שלהם.

גם מה שאמרנו קודם על מודל אינטואיטיבי שקל להסביר זה לא מדויק - בפועל עצים הם מודלים לא יציבים במובן שהם רגישים מאוד לשינויים קלים בדאטא אמיתי, לא דאטא מסומלץ.

זה נובע בעיקר מהאופי הגרידי שלהם, שפיצולים ראשונים במעלה העץ יש להם המון השפעה על האפשרויות של המודל להתפצל בהמשך. זה נובע גם מהנוקשות של איזורי החלטה מלבניים.

וזה נובע גם מהמוגבלות של עץ לתאר איזושהי תופעה בשורה התחתונה במספר סופי של עלים. תחשבו על זה שעץ רגרסיה עם עומק מקסימלי של 3 בסופו של דבר נותן לנו מקסימום 8 שכונות, כלומר 8 מספרים אפשריים של חיזוי.

בשורה התחתונה זה עניין של שונות. הביאס אולי נמוך, אבל השונות גבוהה מאוד, מה שמביא לתופעה שבנתונים אמיתיים עץ החלטה פשוט לא מדייק מבחינת חיזוי וכמעט תמיד אפשר למצוא מודלים עם דיוק גבוה יותר.

אז בשביל מה אנחנו לומדים על עצים?! אנחנו לומדים על עצים כי אמנם עץ אחד הוא מודל די מוגבל, אבל יער של עצים, הוא כבר מודל עוצמתי למדי.

:::

=== 5. רנדום פורסט ===

אז איך משלבים קבוצה של עצים ליער?

:::

בחלק הזה נלמד על שיטת אנסמבל, שעושה קומבינציה של מודלים חלשים רבים, למודל חזק במיוחד.

נראה בקורס שתי שיטות אנסמבל מבוססות עצים. הראשונה רנדום פורסט, שמבוססת על מיצוע של הרבה עצים שונים, והשנייה בוסטינג שבה אנחנו בונים עץ אחרי עץ בצורה אדפטיבית.

נתחיל ברנדום פורסט. במקום להתאים עץ אחד לנתונים, אנחנו נתאים הרבה. אבל לא נתאים אותם לאותם הנתונים, אחרת אין הבדל. נתאים אותם כל פעם על דאטא קצת אחר, דאטא שעבר רנדומיזציה, בשתי דרכים שונות. לבסוף נמצע את העצים -- החיזוי לכל תצפית יהיה ממוצע שלה על פני הרבה עצים, ונראה שכך נטפל באופן ישיר בבעיות של העץ היחיד.

:::

ניזכר בערך המיצוע. מה מיצוע נותן לנו? אנחנו מכירים את זה משונות הממוצע או מהתפלגות הממוצע לפי משפט הגבול המרכזי. אם משתנה מקרי Z_i, מתפלג לפי איזושהי התפלגות F עם תוחלת מיו ושונות סיגמא בריבוע, ואני לוקח m תצפיות כאלה בלתי תלויות, אז השונות של ממוצע המדגם המקרי קטנה פי m. כלומר ככל שm גדול כך הפיזור סביב הממוצע קטן והוא מתקרב לתוחלת האמיתית מיו.

ונניח שהתצפיות הן לא בלתי תלויות. לא רק שהן לא בלתי תלויות, הן תלויות לחלוטין, הן אותה תצפית בדיוק, שחוזרת על עצמה m פעמים. מה יהיה אז הממוצע? התצפית עצמה כמובן. והאם הקטנו את השונות של ההתפלגות המקורית? בכלל לא, נישאר עם השונות המקורית סיגמא בריבוע. כלומר יש כאן איזשהו טווח מתצפיות בלתי תלויות לחלוטין ועד תצפיות תלויות לחלוטין, וההקטנה של סיגמא בריבוע בהתאם.

נסתכל על מצב ביניים, שהתצפיות לא בלתי תלויות לחלוטין אבל גם לא חוזרות על עצמן, המתאם בין זוג תצפיות הוא איזשהו רו שקטן מ1, כלומר הקווריאנס יהיה סיגמא בריבוע כפול רו.

אפשר לראות שכעת שונות ממוצע המדגם היא בקירוב רו סיגמא בריבוע, ועוד 1 מינוס רו כפול סיגמא בריבוע חלקי m. זאת אומרת כשרו שווה ל1, תלות מושלמת, אנחנו נשארים עם סיגמא בריבוע השונות המקורית, וכשרו שווה לאפס, שזה אומר תצפיות בלתי תלויות, נקבל את סיגמא בריבוע חלקי m, שונות מדגם מקרי המוכרת לנו.

זו האינטואיציה שמסבירה למה רנדום פורסט עובד. אם נצליח לקחת עוד ועוד דגימות עם כמה שפחות תלות - במקרה שלנו עוד ועוד עצים, נקטין את השונות המקורית של כל אחת מהן עד פי m. אם הדגימות שלי תלויות חזק אחת בשניה, הרווח שלי מוגבל מפעולת המיצוע. נרצה אם ככה לייצר עצים שיהיו שונים כמה שיותר אחד מהשני כך שנרוויח מהמיצוע שלהם.

:::

אז איך נשיג את היער עצים הזה ששונים זה מזה כמה שיותר?

נזריק רנדומיזציה לתהליך: כל עץ יראה דאטא קצת אחר, נהוג לקחת רק חלק מהנתונים, subsample, או מדגם בוטסטראפ, שזה מדגם בגודל n המקורי, עם החזרה. דבר שני שנעשה, תוך כדי בנית העצים על הדאטא הזה, זה בכל צומת נגריל מספר מסוים של משתנים שיהיו מועמדים לפיצול. כלומר אם בעץ המקורי בכל צומת הוא מתחשב בכל המשתנים האפשריים, העצים שלנו עשויים לראות בכל צומת משתנים אחרים לחלוטין.

כעת מגיעה תצפית חדשה לחיזוי. מה זה אומר למצע עצים? זה אומר שנריץ אותה בכל העצים, והחיזוי הסופי שלה יהיה הממוצע שלהם או הקלאס שרוב העצים מצביעים עליו, בשביל קלסיפיקציה.

על האינטואיציה דיברנו בהרחבה, העצים הם כמו תצפיות ממדגם. הם לא יכולים להיות לגמרי בלתי תלויים כי הם בכל זאת מבוססים על אותו דאטא. אבל נדאג שיהיו כמה שפחות תלויים אחד בשני, וככה נרוויח מהמיצוע שלהם. כשאנחנו רואים תצפית חדשה אפשר לחשוב שאנחנו דוגמים עבורה בקירוב מתוך ההתפלגות המותנית של Y בהינתן התצפית החדשה, ולכן הממוצע על פני הרבה עצים או דגימות כאלה, אומד את התוחלת המותנית של Y בהינתן התצפית החדשה.

אילו עצים נגדל? עמוקים או שטוחים? עמוקים כמובן! עצים עמוקים שמסוגלים לתאר יחסים מורכבים כמה שניתן. לעצים כאלה תהיה שונות גבוהה שנקטין עם המיצוע. אם נבחר בעצים שטוחים יותר, נתחיל אולי בטעות פחות גבוהה אבל גם לא נרוויח מספיק מהמיצוע. למה שלא נראה את רנדום פורסט בפעולה על הנתונים שלנו.

:::

שוב על נתוני הקרדיט שלנו, הפעם בציר האיקס יש לנו את מספר העצים שרנדום פורסט משתמש בו. אנחנו רואים את התכונה היפה של רנדום פורסט שבממוצע שגיאת הטסט יכולה רק לרדת, כלומר כדאי לקחת כמה שיותר עצים שאפשר לעבוד איתם אחר כך על הדיסק.

אבל הכי חשוב אנחנו רואים את ההפחתה בMSE של הטסט ברנדום פורסט, לעומת הMSE של עץ בודד, שהוא אומד די מוגבל כמו שאמרנו.

:::

נסכם: שיטת רנדום פורסט משמרת את הגמישות של עצים תוך כדי שהיא מנסה להפחית את החסרון הכי גדול שלם, הנוקשות והשונות הגדולה שלהם.

אנחנו עושים את העצים כמה שיותר שונים זה מזה, על-ידי מדגמי בוטסטראפ ובחירת משתנים שונים כמועמדים לכל פיצול.

ומלבד זה אנחנו דואגים שהעצים יהיו עמוקים כמה שאפשר, כדי שנרוויח כמה שיותר מאפקט המיצוע, מעץ בודד עם איכות חיזוי גרועה להרבה עם איכות חיזוי טובה.

 עקרונית גם אמרנו, שככל שנבנה יותר עצים איכות החיזוי על הטסט סט יכולה רק לקטון, יתרון משמעותי לשיטה, בפועל אנחנו כנראה מוגבלים על-ידי כוח חישוב וגם גודל על הדיסק, כל אחד מהעצים האלה יכול להיות אוביקט די גדול, אלף עצים לשמור על שרתים זה כבר לא סימפטי.

 עוד יתרון שאנחנו פחות עוסקים בקורס הזה אבל הוא קריטי: קל למקבל רנדום פורסט על-פני מספר מחשבים? קל מאוד! כל עץ ברנדום פורסט יכול לגדול באופן בלתי תלוי מהאחרים, לכן אם הנתונים גדולים ועומדת לרשות מדען הנתונים סביבת עבודה מבוזרת, קלאסטר של מספר מחשבים, ניתן להגיע לאימון מהיר מאוד של האלגוריתם. ויתרון אחרון שרמזנו עליו - כמעט בכל שיטה שאנחנו לומדים יש היפרפרמטרים, איזשהם כפתורים שצריך לסובב כדי להתאים את האלגוריתם למקרה שלנו, כמו מספר השכנים בKNN או מטריקת המרחק. בסך הכל ברנדום פורסט אין פרמטרים שיש עליהם סימן שאלה, ברור שאנחנו צריכים כמה שיותר עצים וברור שהם צריכים להיות כמה שיותר עמוקים. זה הופך את רנדום פורסט לאלגוריתם אוף-דה-שלף מאוד פופולרי, כי בלי כיוונון אפשר להגיע מהר לתוצאה מצוינת.

 מצד שני, כרגע ראינו גם את המודל הראשון שלנו שהוא בגדר קופסה שחורה. להגיד איזה משתנה השפיע על התוצאה, האם משתנה מסוים מעלה או מוריד את ההסתברות שהתצפית תהיה מקלאס מסוים -- זה כבר הרבה יותר קשה לעשות ברנדום פורסט. איבדנו את האינטרפרטביליות. יש הרבה דרכים לנסות להסביר מה עושים מודלים של קופסה שחורה. בקורס שלנו אנחנו עוצרים כאן.

:::
