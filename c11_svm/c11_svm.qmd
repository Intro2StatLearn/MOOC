---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "SVM"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Support Vector Machines - Class 11

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Maximum margin classifier {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ביחידה הזאת נלמד על אלגוריתם נוסף שנוסח במקור עבור בעיות של קלאסיפיקציה בינארית: הספורט וקטור משינז, או SVM בקיצור.

כמו אדאבוסט גם SVM הגיע מקהילת מדעי המחשב לבעיה ספציפית, והורחב לאחר מכן לבעיות כלליות יותר כמו רגרסיה, וכך נלמד אותו. אולם בניגוד לאדאבוסט בSVM אנחנו מגיעים בסופו של דבר לבעית אופטימיזציה שקצת קשה להסביר בדיוק כיצד פותרים אותה בלי כלים מאופטימיזציה קמורה או חקר ביצועים, לכן לא ניכנס לדרך הפתרון המדויקת. נטען עם זאת שהגענו לבעיה פשוטה יחסית באופטימיזציה שניתנת לפתרון באמצעות תוכנה סטנדרטית.

כמו תמיד, אנחנו מתחילים עם הנחה, ולאט לאט מפירים אותה עד שנגיע לפרוצדורה הסופית.
:::
:::

---

### A big assumption

- Suppose $y \in \{-1, 1\}$
- Suppose the classes can be separated by a hyperplane, e.g. for data $T = \{(x_1, y_1), \dots, (x_n, y_n)\}$:

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets

# Generate synthetic 2D data for the left plot
np.random.seed(0)
X_2D, y = datasets.make_blobs(n_samples=100, centers=2, random_state=3)
y = 2*y - 1  # Convert to {-1, 1}

# Generate synthetic 3D data for the right plot
X_3D = np.c_[X_2D, np.random.randn(X_2D.shape[0])]  # Adding a 3rd random coordinate

# Create figure and axes for side-by-side plots
fig = plt.figure(figsize=(10, 5))

# Left: 2D plot without hyperplane or support vectors
ax1 = fig.add_subplot(1, 2, 1)
ax1.scatter(X_2D[y == -1][:, 0], X_2D[y == -1][:, 1], s=20, c='red', marker='x', label='-1')
ax1.scatter(X_2D[y == 1][:, 0], X_2D[y == 1][:, 1], s=20, c='blue', marker='o', label='1')
ax1.grid(True)

# Right: 3D plot without hyperplane
ax2 = fig.add_subplot(1, 2, 2, projection='3d')
ax2.scatter(X_3D[y == -1][:, 0], X_3D[y == -1][:, 1], X_3D[y == -1][:, 2], s=20, c='red', marker='x', label='-1')
ax2.scatter(X_3D[y == 1][:, 0], X_3D[y == 1][:, 1], X_3D[y == 1][:, 2], s=20, c='blue', marker='o', label='1')

# Set labels for 3D plot
ax2.set_xlabel('X1')
ax2.set_ylabel('X2')
ax2.set_zlabel('X3')

# Show plot
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
וההנחה שאנחנו מתחילים היא הנחה גדולה. אנחנו מתחילים כאמור עם Y בינארי שיכול להיות 1 או מינוס 1, ועם מדגם למידה בגודל n.

וההנחה היא שהנתונים שלנו במרחב הX הם לינארלי ספרבל, כלומר ניתן למצוא קו או מישור או מישור-על שמפריד בין התצפיות שהן 1 לבין התצפיות שהן מינוס 1. זאת הנחה כמובן לא סבירה אלא אם המימד של X ממש גבוה. אנחנו עושים אותה במודע ולאחר מכן נאפשר בכל זאת חפיפה בין הקלאסים ואולי אפילו גבול לא לינארי.
:::
:::

---

### Separating hyperplanes (I)

- $\beta_0, \beta_1, \dots, \beta_p$ define a $p$-dimensional plane for all points $x \in \mathbb{R}^p$ satisfying:
$$\beta_0 + \beta_1x_1 + \dots + \beta_px_p = \beta_0 + x^T\beta = 0$$
- We say $\beta^* = (\beta_1, \dots, \beta_p) / \|\beta\|$ is normal to the hyperplane

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Coefficients for the line: beta_0 + beta_1*X1 + beta_2*X2 = 0
beta_0 = -4
beta_1 = 2
beta_2 = 3

# Create figure
plt.figure(figsize=(3, 3))

# Generate X1 values and compute X2 from the line equation
x1_vals = np.linspace(-5, 5, 100)
x2_vals = -(beta_0 + beta_1 * x1_vals) / beta_2

# Plot the line
plt.plot(x1_vals, x2_vals, 'k--', label=r'$\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0$')

# Plot the normal vector from the origin
plt.quiver(0, 0, beta_1, beta_2, angles='xy', scale_units='xy', scale=1, color='r')

# Annotate the line and the normal vector
plt.text(-3.4, -1.0, r'$-4 + 2 X_1 + 3 X_2 = 0$', fontsize=12)
plt.text(beta_1 / 2, beta_2 / 2-0.5, r'$\beta = (2, 3)$', fontsize=12, color='r')

# Set axis limits and labels
plt.xlim(-4, 5)
plt.ylim(-4, 5)
plt.xlabel('X1')
plt.ylabel('X2')
plt.axhline(0, color='black',linewidth=0.5)
plt.axvline(0, color='black',linewidth=0.5)

# Grid and equal aspect ratio
plt.grid(True)
plt.gca().set_aspect('equal', adjustable='box')

# Show the plot
plt.show()
```

::: {.fragment}
::: {.callout-note}
What would be a natural decision rule for separating $y \in \{-1, 1\}$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אם אנחנו מדברים על מישור-על או הייפרפליין שמפריד, בואו ניזכר מה ההגדרה של מישור.

ההגדרה של מישור היא באמצעות הנקודות שמקיימות את המשוואה הליניארית שיש לנו כאן: חותך בטא-אפס ועוד מקדמים כפול משתנים, כל זה שווה לאפס. ניתן לכתוב זאת גם באמצעות המכפלה הפנימית של וקטור המקדמים ללא החותך בטא בוקטור הקואורדינטות X. כלומר מישור הוא מעין לוח של כל הנקודות שמקיימות את המשוואה. כאן למשל מצויר מישור בדו-מימד שהוא בעצם קו, של כל הנקודות שמקיימות -4 + 2x1 + 3x2 = 0.

ניזכר גם שוקטור שמאונך למישור, שיוצר איתו זווית של 90 מעלות והנורמה שלו 1, נקרא נורמל למישור. ואפשר לראות שמעצם ההגדרה של וקטור המקדמים בטא, בטא הוא וקטור נורמל למישור, הוא אורתוגונלי אליו, או יותר נכון בטא-כוכב שעבר נרמול. זה יהיה חשוב לנו תיכף. כלומר אפשר להגדיר מישור באמצעות וקטור בטא, ועוד חותך. מישור בטא זה מישור שאנכי לוקטור בטא, כשהחותך בטא אפס אומר לנו כמה להזיז אותו בכיוון בטא.

מכל מקום, אם אפשר למצוא מישור כזה שמפריד בין תצפיות 1 ומינוס 1, מה יהיה כלל החלטה טבעי לחיזוי תצפית חדשה? אם התצפית מעל למישור, כלומר בטא-אפס ועוד X בטא גדול מאפס, אז נחזה 1. ואם התצפית מתחת למישור, כלומר קטן מאפס, נחזה מינוס 1. לשם אנחנו חותרים, זה אכן יהיה כלל ההחלטה שלנו עוד רגע.
:::
:::

---

### Separating hyperplanes (II)

::: {.incremental}
- In other words, a hyperplane is "separating" iff:
$$y_i \cdot (\beta_0 + x_i^T\beta) > 0 \quad \forall i$$

- And a natural decision rule for separating hyperplanes:
$$\hat{f}(x_0) = \text{sign}\left[\hat{\beta}_0 + x_0^T\hat{\beta}\right]$$
:::

::: {.fragment}
:::: {.columns}
::: {.column}
::: {.callout-note}
But *if* there exists a separating hyperplane, how many are there?
:::
:::
::: {.column}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets

# Generate synthetic 2D data for the left plot
np.random.seed(0)
X_2D, y = datasets.make_blobs(n_samples=100, centers=2, random_state=3)
y = 2*y - 1  # Convert to {-1, 1}

# Create figure
plt.figure(figsize=(3.5, 3.5))

# Left: 2D plot without hyperplane or support vectors
plt.scatter(X_2D[y == -1][:, 0], X_2D[y == -1][:, 1], s=20, c='red', marker='x', label='-1')
plt.scatter(X_2D[y == 1][:, 0], X_2D[y == 1][:, 1], s=20, c='blue', marker='o', label='1')
plt.grid(True)
plt.xlabel('X1')
plt.ylabel('X2')

# Show plot
plt.show()
```
:::
::::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
במילים אחרות, כרגע גם מצאנו הגדרה של מהו מישור מפריד, separating. מישור מפריד הוא מישור שעבור מדגם הלמידה שלנו, בכל נקודה (להדגים), אם היא 1 אז היא מעל המישור, ואם היא מינוס 1 אז היא מתחת למישור. ואת זה אנחנו יכולים לרשום בצורה קומפקטית ככה: המכפלה של Y במישור חייבת להיות גדולה מאפס לכל התצפיות.

וכאמור אם נמצא מישור כזה, או אומדים לוקטור בטא ולחותך, זה בסוף יהיה כלל ההחלטה שלנו לכל תצפית חדשה X0: הסיין של הבדיקה הזאת האם הנקודה מעל או מתחת למישור.

אז לפני שנמצא את המישור, צריך להכיר בבעיה של מה שהגדרנו: אם יש מישור כזה, הוא יחיד? לא, (להדגים) יש הרבה מישורים כאלה, למעשה יש אינסוף. ואז נשאל -- איזה מישור הכי טוב למטרה שלנו.
:::
:::

---

### Maximum margin classifier

- Intuitively, we would like the hyperplane that is "fartherst" from points on both sides

::: {.fragment}
- That is, the hyperplane that maximizes the [margin]{style="color:red;"}:
  - the minimum distance of training points to the hyperplane

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC

# Generate synthetic data
np.random.seed(0)
X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=3)
y = 2 * y - 1  # Convert to {-1, 1}

# Train an SVM classifier with a linear kernel (maximum margin hyperplane)
clf_max_margin = SVC(kernel='linear', C=1e10)
clf_max_margin.fit(X, y)

# Get the coefficients and intercept for the hyperplane
w = clf_max_margin.coef_[0]
b = clf_max_margin.intercept_[0]

# Function to plot the hyperplane and margins
def plot_hyperplane(ax, w, b, X, y, title, delta_h=1, delta_l=1):
    # Plot the points
    ax.scatter(X[y == -1][:, 0], X[y == -1][:, 1], s=20, c='red', marker='x', label='-1')
    ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], s=20, c='blue', marker='o', label='1')

    # Limits
    ax.set_xlim(min(X[:, 0]) - 1, max(X[:, 0]) + 1)
    ax.set_ylim(min(X[:, 1]) - 1, max(X[:, 1]) + 1)

    # Plot the decision boundary (hyperplane)
    x_plot = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
    y_plot = -(w[0] * x_plot + b) / w[1]
    ax.plot(x_plot, y_plot, 'k--')  # Black dashed line for the hyperplane
    
    # Plot the margin lines
    margin = 1 / np.linalg.norm(w)
    y_margin_1 = -(w[0] * x_plot + (b - delta_h)) / w[1]
    y_margin_2 = -(w[0] * x_plot + (b + delta_l)) / w[1]
    ax.plot(x_plot, y_margin_1, 'k:', linewidth=1)  # Upper margin line
    ax.plot(x_plot, y_margin_2, 'k:', linewidth=1)  # Lower margin line
    
    # Title
    ax.set_title(title)

# Create figure and axes for side-by-side plots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))

# Left plot: Maximum margin hyperplane
plot_hyperplane(ax1, w, b, X, y, 'Maximum margin hyperplane')

w1 = w + np.array([-0.2, 0.5])
b1 = b - 1.55

# Right plot: Different hyperplane
plot_hyperplane(ax2, w1, b1, X, y, 'Different separating hyperplane', delta_h=0.575, delta_l=0.575)

# Show the plot
plt.tight_layout()
plt.show()
```

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בצורה אינטואיטיבית, נגיד, שהיינו רוצים את המישור, שהוא הכי רחוק מהתצפיות של מדגם הלמידה. כלומר לכל כיוון נרצה את המישור שהוא כמה שיותר באמצע, סביר להניח שזה כלל ההחלטה שיעבור הכללה הכי טובה לתצפיות שהמודל לא ראה.

דרך אחרת להגיד אותו דבר, היא המישור שעושה מקסימום לרווח בין שני הקלאסים, למה שאנחנו קוראים מרג'ין. ולכן לקלסיפייר הנאיבי שנפתח קוראים מקסימום מרג'ין קלסיפייר.

כאן למשל יש שני מישורים שמביאים לשני מרג'ינים, שניהם מפרידים לנו בין הקלאסים. המרג'ין השמאלי אינטואיטיבית נראה טוב יותר כי הוא פשוט עבה יותר, למעשה הוא המרג'ין המקסימלי שאפשר להגיע אליו עם מדגם הלמידה הזה.

נשים לב עוד שלכל נקודה יש מרחק אנכי אל המישור. המרג'ין מוגדר על ידי הנקודות עם המרחק המינימלי למישור. ואנחנו רוצים לעשות מקסימום למרחק הזה, כלומר זאת בעית מקסימום למינימום מרחק.
:::
:::

---

### Maximum margin criterion

::: {.incremental}
- So far we have:  
$\max_{\beta, \beta_0}\text{"minimum distance"} \quad s.t. \space y_i (\beta_0 + x_i^T\beta) > 0 \quad \forall i$

- The distance between any point $x$ to the hyperplane is: $\frac{|\beta_0 + x^T\beta|}{\|\beta\|_2}$
  - because this distance is the length of a vector proportional to the normal $\beta$:
:::

::: {.fragment}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

# Coefficients for the line: beta_0 + beta_1*X1 + beta_2*X2 = 0
beta_0 = -4
beta_1 = 2
beta_2 = 3

# Create figure
plt.figure(figsize=(4, 4))

# Generate X1 values and compute X2 from the line equation
x1_vals = np.linspace(-5, 5, 100)
x2_vals = -(beta_0 + beta_1 * x1_vals) / beta_2

# Plot the line
plt.plot(x1_vals, x2_vals, 'k--', label=r'$\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0$')

# Plot the normal vector from the origin
plt.quiver(0, 0, beta_1, beta_2, angles='xy', scale_units='xy', scale=1, color='r')

# Plot the normal vector from projection of a point x to x
plt.quiver(-2, 2.666, 0.5 * beta_1, 0.5 * beta_2, angles='xy', scale_units='xy', scale=1, color='b')
right1 = mpatches.Rectangle((-2, 2.666), -0.4, 0.4, fc=[0,0,0,0], ec='b', zorder=0, lw=.7, angle=-35)
plt.gca().add_patch(right1)

# Annotate the line, the point and the normal vector
plt.text(-3.4, -1.0, r'$-4 + 2 X_1 + 3 X_2 = 0$', fontsize=12)
plt.text(-1, 4, r'$x$', fontsize=12, color='b')
plt.text(beta_1 / 2, beta_2 / 2-0.5, r'$\beta = (2, 3)$', fontsize=12, color='r')

# Set axis limits and labels
plt.xlim(-4, 5)
plt.ylim(-4, 5)
plt.xlabel('X1')
plt.ylabel('X2')
plt.axhline(0, color='black',linewidth=0.5)
plt.axvline(0, color='black',linewidth=0.5)

# Grid and equal aspect ratio
plt.grid(True)
plt.gca().set_aspect('equal', adjustable='box')

# Show the plot
plt.show()
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איך אנחנו מנסחים את הקריטריון הזה בצורה שגם נוכל להשיג אותו?

הבנו שאנחנו רוצים את המקסימום של "מינימום מרחק", הבנו שהפרמטרים שלנו הם מה שמגדיר את המישור עצמו, וקטור המקדמים של משוואת המישור, או הנורמל למישור ועוד חותך. והבנו שיש לנו אילוץ שנובע מההנחה שלנו, זה חייב להיות מישור מפריד, מה שמתבטא באילוץ שרשום כאן לכל תצפית i. כל שנשאר לנו זה לפרמל מה זה מינימום מרחק.

אז אם אתם יודעים מלימודי המתמטיקה שלכם, יש לנו נוסחה של מרחק של כל נקודה למישור שמוגדר על-ידי וקטור המקדמים בטא: זה בעצם המכפלה הפנימית בין הנקודה לוקטור המקדמים, ועוד חותך, בערך מוחלט, חלקי הנורמה של וקטור המקדמים.

אפשר להוכיח את זה אבל יש כאן משהו מאוד אינטואיטיבי: הנה נקודה X ואני רוצה לבודק מה המרחק האנכי שלה למישור נתון. אפשר לראות שהמרחק הזה הוא אורך של וקטור שהוא מקביל לוקטור בטא הנורמל. ולכן האורך פרופורציונלי לאורך וקטור בטא הנורמל. כדי להשיג את המקטע הזה אנחנו מטילים את הנקודה X על בטא, והטלה פירושה המכפלה הפנימית בין שני הוקטורים, שמופיעה בביטוי שלנו.

בפרט, אם נדרוש שוקטור בטא יהיה בעל אורך 1, וקטור יחידה, אז המרחק של כל נקודה למישור הוא המכפלה של הקואורדינטות שלה בוקטור המקדמים של המישור ועוד חותך. אם זה שווה לאפס הנקודה מקיימת את משוואת המישור, היא על המישור, אם זה גדול מאפס היא מעליו, ואם זה קטן מאפס היא מתחתיו ואנחנו לוקחים את הערך המוחלט.
:::
:::

---

### Maximum margin criterion

::: {.incremental}
- So for any $M > 0$ a compact way of writing the criterion is:
$$\max_{\beta, \beta_0}M \quad s.t. \space \|\beta\| = 1 \text{ and } \quad y_i (\beta_0 + x_i^T\beta) \ge M \quad \forall i$$

- All the points are at least a distance $M$ from the decision boundary defined by $\beta, \beta_0$, and we seek $\beta, \beta_0$ that get the largest such $M$

- Equivalently if we insist on setting $\|\beta\| = 1/M$ we can write:
$$\min_{\beta, \beta_0}\|\beta\| \quad s.t. \space y_i (\beta_0 + x_i^T\beta) \ge 1 \quad \forall i$$

  - because $M = 1/\|\beta\|$ and maximizing $M$ is minimizing $\|\beta\|$

- This is a convex optimization problem (quadratic criterion if we write $\|\beta\|^2$, linear inequality constraints), many efficient solvers exist
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז נסמן את המרחק הזה, את המרג'ין בM.

דרך לסכם את כל מה שדרשנו עד עכשיו היא לעשות מקסימום על M, ככה שוקטור המקדמים שלנו יהיה וקטור יחידה, יהיה באמת נורמל למישור. ועבור כל תצפית i במדגם הלמידה, נחשב את המרחק שלה מהמישור עם הביטוי שמצאנו, ונדרוש שהמרחק הזה יהיה לפחות M. כלומר M הוא המרחק המינימלי. אבל מה זה מרחק, אם הנקודה מעל המישור הY שלה הוא 1, אז מכפילים את המרחק פי 1, אין בעיה והביטוי נשאר כפי שהוא, ואם היא מתחת למישור הY שלה הוא מינוס 1 ולכן ההכפלה בו מביאה אותנו גם כן למרחק חיובי.

או במילים אחרות: כל הנקודות חייבות להיות במרחק לפחות M מהמישור שמוגדר על-ידי הנורמל בטא והחותך בטא-אפס, ואנחנו מחפשים בטא ובטא-אפס שיביאו למקסימום את המרחק הזה, את המרג'ין.

נשים לב שאפשר היה עקרונית לקבע את הנורמה של בטא על כל ערך, ואם נחליט שהנורמה של בטא לא חייבת להיות 1 אלא 1 חלקי M, אפשר להיפטר גם מהפרמטר M בבעית האופטימיזציה שלנו. כעת כדי להגדיל את M כמה שיותר צריך להקטין את הנורמה של וקטור בטא כמה שיותר. וככה אנחנו גם נפטרים מM וגם הופכים את הבעיה שלנו לבעית מינימום.

זאת הנקודה שבה הבטחתי שאנחנו עוצרים ולא נכנסים לדיון על אופטימיזציה, רק מצביעים על כך שאפשר לעשות מינימום לנורמה הריבועית (להדגים) ויש לנו בעית אופטימיזציה קמורה יחסית פשוטה: הפונקציה שלנו קמורה ואנחנו עושים מינימום, כלומר מובטח לנו שנמצא מינימום תחת ההנחה שלנו של ספרביליות, והאילוצים הם אי-שוויונות ליניאריים בבטא, אפשר לפתור את זה בכל אלגוריתם או תוכנה לאופטימיזציה סטנדרטית.
:::
:::

---

### Support vectors

:::: {.columns}
::: {.column}
Back to the maximum margin classifier:

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC

# Generate synthetic data
np.random.seed(0)
X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=3)
y = 2 * y - 1  # Convert to {-1, 1}

# Train an SVM classifier with a linear kernel (maximum margin hyperplane)
clf_max_margin = SVC(kernel='linear', C=1e10)
clf_max_margin.fit(X, y)

# Get the coefficients and intercept for the hyperplane
w = clf_max_margin.coef_[0]
b = clf_max_margin.intercept_[0]

# Function to plot the hyperplane and margins
def plot_hyperplane(ax, clf, w, b, X, y, title, delta_h=1, delta_l=1):
    # Plot the points
    ax.scatter(X[y == -1][:, 0], X[y == -1][:, 1], s=20, c='red', marker='x', label='-1')
    ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], s=20, c='blue', marker='o', label='1')

    # Limits
    ax.set_xlim(min(X[:, 0]) - 1, max(X[:, 0]) + 1)
    ax.set_ylim(min(X[:, 1]) - 1, max(X[:, 1]) + 1)

    # Plot the decision boundary (hyperplane)
    x_plot = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
    y_plot = -(w[0] * x_plot + b) / w[1]
    ax.plot(x_plot, y_plot, 'k--')  # Black dashed line for the hyperplane
    
    # Plot the margin lines
    margin = 1 / np.linalg.norm(w)
    y_margin_1 = -(w[0] * x_plot + (b - delta_h)) / w[1]
    y_margin_2 = -(w[0] * x_plot + (b + delta_l)) / w[1]
    ax.plot(x_plot, y_margin_1, 'k:', linewidth=1)  # Upper margin line
    ax.plot(x_plot, y_margin_2, 'k:', linewidth=1)  # Lower margin line
    
    # Highlight the support vectors
    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=30, facecolors='none', edgecolors='black')

    # Title
    ax.set_title(title)

# Create figure and axes for side-by-side plots
fig, ax = plt.subplots(figsize=(4, 4))

# Left plot: Maximum margin hyperplane
plot_hyperplane(ax, clf_max_margin, w, b, X, y, '')

# Show the plot
plt.tight_layout()
plt.show()
```

:::

::: {.column}
::: {.incremental}
- The final classifier depends only on support vectors but was reached given all the training data
- Advantages/disadvantages to a classifier that only depends on few observations
- What would logistic regression do?
- What if there is no separable hyperplane?
- And as usual: this is very specific to binary classification, can we generalize?
:::
:::
::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אם נסתכל שוב על המוטיבציה שלנו -- אנחנו רוצים למצוא את המישור שממקסם את המרג'ין בין שני הקלאסים - רמזנו לזה כבר קודם אבל כעת אנחנו אומרים במפורש, ניתן לראות שהמרג'ין הזה ברגע שנמצא, הוא מושפע רק מקבוצה קטנה של תצפיות. אם הן יזוזו, גם המישור והמרג'ין יושפעו. אבל אם כל אחת מהתצפיות האחרות יזוזו? בתנאי שיישארו בצד הנכון של המישור ולא ייכנסו פנימה? לא תהיה לזה השפעה על המישור והמרג'ין. לכן הנקודות שבעצם מגדירות את המרג'ין הסופי נקראות נקודות או וקטורים של תמיכה support vectors. ומכאן השם של הבעיה הסופית שנלמד ששומרת על העקרון הזה, סאפורט וקטור מאשינז. למעשה, ניתן להגדיר את כל הבעיה בדרך של מציאת הנקודות שיהיו הסאפורט וקטורז למרג'ין המקסימלי.

כמה הערות לסיום:

קודם כל, חשוב להדגיש שאמנם החיזוי הסופי מושפע רק מהסאפורט וקטורז, ועוד נראה ביטוי מתמטי לזה. אבל זה לא שזרקנו את שאר המדגם, השתמשנו בו כדי להגיע אליהם, כמו שאמרתי אפשר לנסח את כל הבעיה כחיפוש על הנקודות, לא מציאת נורמל למישור.

דבר שני, עם כל מה שלמדתם אתם כבר יכולים לתאר לכם מה יכולים להיות היתרונות והחסרונות בקלסיפייר כזה, שמבוסס רק על סט קטן של תצפיות: למשל הוא מושפע פחות מתצפיות חריגות. קראנו לזה מודל חסין או רובסטי מפני תצפיות חריגות. מצד שני אנחנו תיכף נראה שאמנם המודל לא מושפע מתצפיות שרחוקות מהמרג'ין, אבל הוא מאוד מושפע מתצפיות שקרובות למרג'ין. נזיז תצפית אחת מהסאפורט וקטורז בכיוון קצת לא צפוי והמודל יכול להשתנות בצורה גסה.

שאלה חשובה שיש עליה הרבה מחקר היא מה בעצם ההבדל מרגרסיה לוגיסטית? הרי גם רגרסיה לוגיסטית אם הדאטא הוא ספרבילי אמור למצוא מישור מפריד דומה מאוד -- וזה אכן מה שקורה, אם תריצו רגרסיה לוגיסטית על המדגם שכאן תקבלו משוואה שמאוד מזכירה את משוואת המישור שהמקסימום מרג'ין קלסיפייר מוצא. ולרגרסיה לוגיסטית יש גם פירוש הסתברותי, ופונקצית הפסד הסתברותית.

שתי השאלות שנדון בהן כעת הן המתבקשות ביותר: אחת, מה עם הדאטא הוא לא ספרבילי? הרי אין מישור למצוא. והשנייה היא האם יש דרך להכליל את הרעיון היצירתי הזה מקלסיפיקציה בינארית לכמה קלאסים ולרגרסיה עם Y רציף.
:::
:::

---

## Support vector classifier {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
רילקסציה ראשונה לקריטריון שלנו -- אנחנו רוצים שזה יעבוד גם במקרה שיש קצת חפיפה. כלומר לא נניח יותר שהדאטא הוא ספרבילי. זה יביא אותנו סוף סוף למודל שנקרא סאפורט וקטור קלסיפייר.
:::
:::

---

### Not just the non-separable case

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC

# Generate synthetic data
np.random.seed(0)
X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=3)
y = 2 * y - 1  # Convert to {-1, 1}

# Train an SVM classifier with a linear kernel (maximum margin hyperplane)
clf_max_margin = SVC(kernel='linear', C=1e10)
clf_max_margin.fit(X, y)

X1 = np.concatenate([X, np.array([[-2.5, 4]])], axis=0)
y1 = np.concatenate([y, np.array([1])], axis=0)

clf_narrow_margin = SVC(kernel='linear', C=1e10)
clf_narrow_margin.fit(X1, y1)

X2 = np.concatenate([X, np.array([[-1, 3], [-3, 2]])], axis=0)
y2 = np.concatenate([y, np.array([1, -1])], axis=0)

# Function to plot the hyperplane and margins
def plot_hyperplane(ax, clf, X, y, title, delta_h=1, delta_l=1):
    # Plot the points
    ax.scatter(X[y == -1][:, 0], X[y == -1][:, 1], s=20, c='red', marker='x', label='-1')
    ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], s=20, c='blue', marker='o', label='1')

    # Limits
    ax.set_xlim(min(X[:, 0]) - 1, max(X[:, 0]) + 1)
    ax.set_ylim(min(X[:, 1]) - 1, max(X[:, 1]) + 1)
    if clf is not None:
        # Get the coefficients and intercept for the hyperplane
        w = clf.coef_[0]
        b = clf.intercept_[0]
        # Plot the decision boundary (hyperplane)
        x_plot = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
        y_plot = -(w[0] * x_plot + b) / w[1]
        ax.plot(x_plot, y_plot, 'k--')  # Black dashed line for the hyperplane
        
        # Plot the margin lines
        margin = 1 / np.linalg.norm(w)
        y_margin_1 = -(w[0] * x_plot + (b - delta_h)) / w[1]
        y_margin_2 = -(w[0] * x_plot + (b + delta_l)) / w[1]
        ax.plot(x_plot, y_margin_1, 'k:', linewidth=1)  # Upper margin line
        ax.plot(x_plot, y_margin_2, 'k:', linewidth=1)  # Lower margin line
        
        # Highlight the support vectors
        ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=30, facecolors='none', edgecolors='black')

    # Title
    ax.set_title(title)

# Create figure and axes for side-by-side plots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 3))

# Left plot: Maximum margin hyperplane
plot_hyperplane(ax1, clf_max_margin, X, y, '')

# Middle plot: Maximum margin hyperplane
plot_hyperplane(ax2, clf_narrow_margin, X1, y1, '')

# Right plot: Non-separable
plot_hyperplane(ax3, None, X2, y2, '')

# Show the plot
plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כבר הזכרנו ונדגים את זה, שהבעיה היא לא רק שיש חפיפה ואין אפשרות למצוא מישור מפריד, כמו בדאטא בתרשים בצד ימין. הבעיה היא גם אם כן אפשר למצוא מישור מפריד אבל המרג'ין שנוצר הוא כל כך צר שזה נראה מאולץ. או במילים אחרות נוצר לנו מצב של אוברפיטינג, ואולי נרצה להתחשב קצת פחות בכל נקודה ונקודה. אני יוצר מצב כזה בדאטא בתרשים האמצעי, על-ידי הוספה של נקודה כחולה אחת ויחידה בצורה כזו. המודל משתנה בצורה דרסטית! איך קראנו לזה? מודל לא יציב, עם שונות מאוד גבוהה.
:::
:::

---

### Support vectors classifier

- Let us relax our constraints:
  - Allow observations to be inside the margin
  - Or even on the wrong side of the hyperplane!
  - With a "budget" $C$ for these violations (or penalty)

::: {.incremental}
- We do this with the help of "slack variables": $\epsilon = (\epsilon_1, \dots, \epsilon_n)$
- The new optimization problem:
$$\max_{\beta, \beta_0, \epsilon}M \quad s.t. \space \|\beta\| = 1, \quad y_i (\beta_0 + x_i^T\beta) \ge M(1-\epsilon_i), \quad \epsilon_i \ge 0, \quad \sum_{i = 1}^n \epsilon_i \le C$$

- The final prediction is still:
$$\hat{f}(x_0) = \text{sign}\left[\hat{\beta}_0 + x_0^T\hat{\beta}\right]$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אנחנו נרשה לחלק מהתצפיות להיות גם בתוך המרג'ין. 

או אפילו: לתצפיות מסוימות להיות בצד הלא נכון של המישור!

ואנחנו נעשה את זה באמצעות פרמטר C שיהיה תקציב לחריגות כאלה, מאוד מזכיר את מה שעשינו ברגרסיה עם רגולריזציה כמו רידג' ולאסו.

אנחנו עושים את זה עם אוסף של n פמרטרים לn תצפיות, שאנחנו קוראים להם משתני סלאק, מלשון to cut some slack. נסמן אותם בוקטור אפסילון.

וכעת אנחנו עושים מקסימום על M המרחק המינימלי של התצפיות אל המישור, ככה שוקטור המקדמים בטא הוא בנורמה אחת כלומר הוא נורמל, והמרחק גדול או שווה לא לM אלא לM כפול קצת פחות מ1, כי הוא מוכפל פי 1 מינוס אפסילון i לכל תצפית. אנחנו כמובן גם חייבים לוודא שהאפסילונים הם חיוביים, וכאמור נחסום את הסכום שלהם באמצעות איזשהו פרמטר תקציב C שעכשיו אנחנו חייבים לספק, ותיכף נרחיב על זה. נשים לב גם שהמקסימום על המרג'ין הוא עכשיו גם על עוד וקטור של משתני סלאק אפסילון, כלומר נצטרך להגיע להחלטה כזאת לכל תצפית ותצפית.

ולמרות הכל, בשורה התחתונה המודל שנוצר בסאפורט וקטור קלאסיפייר, הוא בעל צורה זהה למודל של מקסימום מרג'ין קלסיפייר: אנחנו בסופו של דבר חוזים 1 אם תצפית מעל המישור ומינוס 1 אם היא מתחת.
:::
:::

---

### SVC: slack variables

$\max_{\beta, \beta_0, \epsilon}M \quad s.t. \space \|\beta\| = 1, \quad y_i (\beta_0 + x_i^T\beta) \ge M(1-\epsilon_i), \quad \epsilon_i \ge 0, \quad \sum_{i = 1}^n \epsilon_i \le C$

::: {.incremental}
- $\epsilon_i$ is the amount by which observation $x_i$ "violates" the margin:
  - If $\epsilon_i = 0$ then $x_i$ is on the correct side of the margin
  - If $\epsilon_i > 0$ then $x_i$ is on the wrong side of the margin
  - If $\epsilon_i > 1$ then $x_i$ is on the wrong side of the hyperplane!
:::

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC

# Generate synthetic data
np.random.seed(0)
X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=3)
y = 2 * y - 1  # Convert to {-1, 1}

# Train an SVM classifier with a linear kernel (maximum margin hyperplane)
clf_max_margin = SVC(kernel='linear', C=1e10)
clf_max_margin.fit(X, y)

X1 = np.concatenate([X, np.array([[-2.5, 4]])], axis=0)
y1 = np.concatenate([y, np.array([1])], axis=0)

clf_narrow_margin = SVC(kernel='linear', C=1)
clf_narrow_margin.fit(X1, y1)

X2 = np.concatenate([X, np.array([[-1, 3], [-3, 2]])], axis=0)
y2 = np.concatenate([y, np.array([1, -1])], axis=0)

clf_nonsep_margin = SVC(kernel='linear', C=1)
clf_nonsep_margin.fit(X2, y2)

# Function to plot the hyperplane and margins
def plot_hyperplane(ax, clf, X, y, title, delta_h=1, delta_l=1):
    # Plot the points
    ax.scatter(X[y == -1][:, 0], X[y == -1][:, 1], s=20, c='red', marker='x', label='-1')
    ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], s=20, c='blue', marker='o', label='1')

    # Limits
    ax.set_xlim(min(X[:, 0]) - 1, max(X[:, 0]) + 1)
    ax.set_ylim(min(X[:, 1]) - 1, max(X[:, 1]) + 1)
    if clf is not None:
        # Get the coefficients and intercept for the hyperplane
        w = clf.coef_[0]
        b = clf.intercept_[0]
        # Plot the decision boundary (hyperplane)
        x_plot = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
        y_plot = -(w[0] * x_plot + b) / w[1]
        ax.plot(x_plot, y_plot, 'k--')  # Black dashed line for the hyperplane
        
        # Plot the margin lines
        margin = 1 / np.linalg.norm(w)
        y_margin_1 = -(w[0] * x_plot + (b - delta_h)) / w[1]
        y_margin_2 = -(w[0] * x_plot + (b + delta_l)) / w[1]
        ax.plot(x_plot, y_margin_1, 'k:', linewidth=1)  # Upper margin line
        ax.plot(x_plot, y_margin_2, 'k:', linewidth=1)  # Lower margin line
        
        # Highlight the support vectors
        ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=30, facecolors='none', edgecolors='black')

    # Title
    ax.set_title(title)

# Create figure and axes for side-by-side plots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 3))

# Left plot: Maximum margin hyperplane
plot_hyperplane(ax1, clf_max_margin, X, y, '')

# Middle plot: Maximum margin hyperplane
plot_hyperplane(ax2, clf_narrow_margin, X1, y1, '')

# Right plot: Non-separable
plot_hyperplane(ax3, clf_nonsep_margin, X2, y2, '')

# Show the plot
plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז בואו נדבר קצת על הסלאק וריאבלז האלה:

בתרשים השמאלי, אין בהם צורך. כל התצפיות במרחק M לפחות, ולכן כל האפסילונים שווים אפס.

בתרשים האמצעי אנחנו מאפשרים שתי הפרות: יש שתי נקודות שנמצאות בתוך המרג'ין, אחת כחולה וחת אדומה, כלומר יש שני אפסילונים חיוביים, והשאר אפס. יש לנו כאן דוגמא לדאטא שהוא ספרבילי אבל אם לא נאפשר קצת סלאק, נקבל את המרג'ין הצר והמאולץ שקיבלנו קודם, וכעת מתאפשר מרג'ין הרבה יותר יציב.

בתרשים הימני שהוא כבר המקרה הלא-ספרבילי, יש לנו כמה משתני סלאק שהם גדולים מאפס. כאן יש לנו אפילו נקודות שהן בצד הלא נכון של המישור (להדגים). בשביל הנקודות האלה האפסילונים שנמצאו הם גדולים מ1, הם יוצאים מעבר לחצי המרג'ין M.

אז נסכם, האפסילונים הם בסופו של דבר הכמות שבה התצפיות "מפירות" את המרג'ין:

אם אפסילון שווה אפס, התצפית בצד הנכון של המרג'ין, אין צורך בסלאק.

אם אפסילון גדול מאפס, היא בצד הלא נכון של המרג'ין. היא או עליו, או ממש מעבר למישור, היא בצד הלא נכון.

אם אפסילון גדול מ1 אז בודאות התצפית בצד הלא נכון של המישור, אולי על המרג'ין, אולי אפילו מחוצה לו.

והדרך שלנו לשלוט בכמה סלאק אנחנו מאפשרים היא דרך פרמטר C, אז נדבר עליו גם כן.
:::
:::

---

### SVC: the $C$ parameter

$\max_{\beta, \beta_0, \epsilon}M \quad s.t. \space \|\beta\| = 1, \quad y_i (\beta_0 + x_i^T\beta) \ge M(1-\epsilon_i), \quad \epsilon_i \ge 0, \quad \sum_{i = 1}^n \epsilon_i \le C$

::: {.incremental}
- $C$ is the amount by which the margin may be "violated":
  - If $C = 0$ then $\epsilon = \mathbf{0}$, back to maximum margin classifier
  - If $C > 0$ no more than $C$ observations can be on the wrong side of the hyperplane
:::

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC

# Generate synthetic data
np.random.seed(0)
X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=3)
y = 2 * y - 1  # Convert to {-1, 1}

X2 = np.concatenate([X, np.array([[-1, 3], [-3, 2]])], axis=0)
y2 = np.concatenate([y, np.array([1, -1])], axis=0)

clf_nonsep_margin10 = SVC(kernel='linear', C=1)
clf_nonsep_margin10.fit(X2, y2)

clf_nonsep_margin1 = SVC(kernel='linear', C=0.1)
clf_nonsep_margin1.fit(X2, y2)

clf_nonsep_margin01 = SVC(kernel='linear', C=0.01)
clf_nonsep_margin01.fit(X2, y2)

# Function to plot the hyperplane and margins
def plot_hyperplane(ax, clf, X, y, title, delta_h=1, delta_l=1):
    # Plot the points
    ax.scatter(X[y == -1][:, 0], X[y == -1][:, 1], s=20, c='red', marker='x', label='-1')
    ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], s=20, c='blue', marker='o', label='1')

    # Limits
    ax.set_xlim(min(X[:, 0]) - 1, max(X[:, 0]) + 1)
    ax.set_ylim(min(X[:, 1]) - 1, max(X[:, 1]) + 1)
    if clf is not None:
        # Get the coefficients and intercept for the hyperplane
        w = clf.coef_[0]
        b = clf.intercept_[0]
        # Plot the decision boundary (hyperplane)
        x_plot = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
        y_plot = -(w[0] * x_plot + b) / w[1]
        ax.plot(x_plot, y_plot, 'k--')  # Black dashed line for the hyperplane
        
        # Plot the margin lines
        margin = 1 / np.linalg.norm(w)
        y_margin_1 = -(w[0] * x_plot + (b - delta_h)) / w[1]
        y_margin_2 = -(w[0] * x_plot + (b + delta_l)) / w[1]
        ax.plot(x_plot, y_margin_1, 'k:', linewidth=1)  # Upper margin line
        ax.plot(x_plot, y_margin_2, 'k:', linewidth=1)  # Lower margin line
        
        # Highlight the support vectors
        ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=30, facecolors='none', edgecolors='black')

    # Title
    ax.set_title(title)

# Create figure and axes for side-by-side plots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 3))

# Left plot: Small (large SVC) C
plot_hyperplane(ax1, clf_nonsep_margin10, X2, y2, 'C = 0.1')

# Middle plot: Middle (middle SVC) C
plot_hyperplane(ax2, clf_nonsep_margin1, X2, y2, 'C = 1')

# Right plot: Large (small SVC) C
plot_hyperplane(ax3, clf_nonsep_margin01, X2, y2, 'C = 10')

# Show the plot
plt.tight_layout()
plt.show()
```

::: {.fragment}
::: {.callout-note}
How to choose $C$? What is the relation between $C$ and the bias-variance tradeoff?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אם C הוא אפס, יש לנו אפס סבלנות להפרה של המרג'ין, כל האפסילונים יהיו אפס, בעצם אנחנו חוזרים למקסימום מרג'ין קלסיפייר.

אם C גדול מאפס אנחנו כן מאפשרים סלאק, ויותר מזה C הוא חסם על מספר התצפיות שאנחנו מוכנים שיעברו את המישור, כי כל תצפית כזאת תקבל אפסילון יותר מ1.

כאן יש לנו את אותו סט של נתונים שחוזר על עצמו עם C שונים. אפשר לראות כשC התקציב נורא קטן אין כמעט הקצאה להפרה של המרג'ין, מקבל מרג'ין צר מאוד שמתאים את עצמו מאוד לדאטא, עם מעט סאפורט וקטורז.
מנגד, כשC גדול מאוד, מקבלים מרג'ין עבה, שפחות נצמד לדאטא, ויש למודל הרבה מאוד סאפורט וקטורז שמשפיעים עליו.

אז איך נבחר איזה C לתת למודל? פשוט: נתייחס לC כאל היפרפרמטר, כאל איזה כפתור שאפשר לסובב וצריך לבדוק כמה לסובב אותו באמצעות שיטות שלמדנו כמו קרוס ולידיישן. וברור שכאן אנחנו רואים קשר מיידי לביאס-וריאנס טריידאוף. מומלץ לעצור רגע את ההקלטה ולחשוב לבד איך C משפיע על הטריידאוף.

עצרתם? אוקי, אז כמו שאפשר לראות כאן ככל שC גדול יותר, המרג'ין גדול ויציב כי הוא מתחשב בהרבה סאפורט וקטורז, במילים אחרות השונות נמוכה. אבל, הביאס יכול לגדול כי המישור שמצאנו כבר לא מתאים את עצמו לקו המאוד ספציפי שמקבלים בין הקלאסים. מצד שני כשC קטן המרג'ין מאוד צר, כל שינוי במעט סאפורט וקטורז שיש לו יכול מאוד לערער אותו, כלומר שונות גבוהה. אבל הביאס קטן כי אנחנו מתקרבים לקו הספציפי שמתאים עצמו הכי לנתונים.
:::
:::

---

### SVC: equivalent forms

- As with maximum margin classifier, if we insist on setting $\|\beta\| = 1/M$ we can write:
$$\min_{\beta, \beta_0, \epsilon}\|\beta\| \quad s.t. \space y_i (\beta_0 + x_i^T\beta) \ge 1-\epsilon_i, \quad \epsilon_i \ge 0, \quad \sum_{i = 1}^n \epsilon_i \le C \quad \forall i$$

- In fact, it is more common to see the equivalent form:
$$\min_{\beta, \beta_0, \epsilon}\frac{1}{2}\|\beta\|^2 + C \sum_{i = 1}^n \epsilon_i \quad s.t. \space y_i (\beta_0 + x_i^T\beta) \ge 1-\epsilon_i, \quad \epsilon_i \ge 0 \quad \forall i$$

- In which case, notice the $C$ hyperparameter is now a *penalty* (this is also more similar to how sklearn sees it)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסיים את הדיון בסאפורט וקטור קלסיפייר בזה שנזכיר שבדרך כלל אנחנו רואים צורה מעט שונה של הקריטריון.

קודם כל גם כאן ניתן להיפטר מM עצמו, חצי העובי של המרג'ין, אם פשוט קובעים שהנורמה של וקטור המקדמים בטא היא 1 חלקי M. העניין הוא שבקריטריון בלי M קצת קשה להבין את התפקיד של הסלאק וריאבלז, האפסילונים.

ולמעשה, הרבה פעמים תפגשו בכלל את הקריטריון של SVC בצורה כזאת: מינימום על הנורמה הריבועית של וקטור המקדמים בטא, כדי להפוך את הבעיה לקמורה, וC לא מופיע כפרמטר תקציב אלא כפרמטר עונש על סכום האפסילונים.

זה חשוב להבין את זה כי ככה גם המימוש בsklearn, ככל שC גדול אנחנו מענישים יותר את הסלאק וריאבלז ונראה מרג'ינים צרים יותר, וככל שהוא קטן אנחנו מענישים פחות את הסכום שלהם, נראה הרבה הפרות של המרג'ינים שיהיו עבים יותר.

עד כאן על סאפורט וקטור קלסיפיירז. ביחידה הבאה נוותר גם על דרישה הזאת שמישור מפריד בין הקלאסים, אלא כל יריעה אחרת, ונגיע לפרוצדורה הסופית שנקראת סאפורט וקטור מאשינז.
:::
:::

---

## Support vector machines (SVM) {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז מה הופך סאפורט וקטור קלסיפיירז לסאפורט וקטור מאשינז?
:::
:::

---

### The non-linear case

Recall the Bayes decision boundary example from our Bias-Variance discussion:

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Define a more complex logistic function for P(Y = 1 | X1, X2) to create a wavy decision boundary
def logistic_function(x1, x2):
    return 1 / (1 + np.exp(- (np.sin(x1) + np.cos(x2) + 0.5 * x1 * x2 - 1)))

# Create a grid of X1 and X2 values
x1 = np.linspace(-3, 3, 300)
x2 = np.linspace(-3, 3, 300)
X1, X2 = np.meshgrid(x1, x2)

# Compute P(Y = 1 | X1, X2) over the grid
P = logistic_function(X1, X2)

# Simulate random points within the grid for the training set
np.random.seed(42)  # For reproducibility
X1_train = np.random.uniform(-3, 3, 500)
X2_train = np.random.uniform(-3, 3, 500)

# Calculate the probability for each training sample point
P_train = logistic_function(X1_train, X2_train)

# Generate Y values based on the probabilities for the training set
Y_train = np.random.binomial(1, P_train)

# Plot the heatmap
plt.figure(figsize=(8, 5))
heatmap = plt.contourf(X1, X2, P, levels=50, cmap='viridis')
plt.colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

# Plot the decision boundary where P(Y = 1 | X1, X2) = 0.5
decision_boundary = plt.contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
plt.clabel(decision_boundary, fmt = '%.1f', colors = 'red')

# Plot the simulated training points
plt.scatter(X1_train[Y_train == 1], X2_train[Y_train == 1], color='yellow', label='Y = 1 (Train)', edgecolor='black')
plt.scatter(X1_train[Y_train == 0], X2_train[Y_train == 0], color='blue', label='Y = 0 (Train)', edgecolor='black')

# Labels and title
plt.xlabel('$X1$')
plt.ylabel('$X2$')
plt.show()
```

::: {.fragment}
Unfortunately, "real data" looks more like this.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
למשל מה קורה אם אין מישור מפריד, עם או בלי סלאק, אבל כן יש יריעה מסוימת שמפרידה. במילים אחרות אם ההפרדה היא לא ליניארית.

ראינו דוגמה כזאת ממש כשדיברנו על הטריידאוף בין ביאס ווריאנס בקלסיפיקציה, שם תיארנו לנו שיש איזו פונקצית הסתברות להיות בקלאס 1 לעומת הקלאס מינוס 1, והנחנו שהיא משתנה בצורה חלקה כזאת במרחב של X. מה סאפורט וקטור קלסיפייר יכול לעשות במקרה כזה? מעט מאוד.

ולצערי דאטא אמיתי הרבה פעמים נראה ככה, עם תופעות לא-ליניאריות. אז אנחנו צריכים למצוא דרך להכליל את המתודה שלנו לעולם לא ליניארי.
:::
:::

---

### Adding polynomial terms?

::: {.incremental}
- Similar to polynomial regression we could add quadratic terms:
$$\max_{\beta^1, \beta^2, \beta_0, \epsilon}M \quad s.t. \\
\|\beta^1\|^2 + \|\beta^2\|^2 = 1, \quad y_i (\beta_0 + x_i^T\beta^1 + \left(x^2_i\right)^T\beta^2) \ge M(1-\epsilon_i), \quad \epsilon_i \ge 0, \quad \sum_{i = 1}^n \epsilon_i \le C$$

- What about cubic terms? What about interactions?

- Like in regression we could *expand* $x_i$ to any feature mapping $h(x_i): \mathbb{R}^p \to \mathbb{R}^q$ and continue as is

- But this becomes really high-dimensional, really fast.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשדיברנו על רגרסיה והמודל הליניארי הצענו אפשרות קלה ממש, וזה להוסיף גורמים פולינימיאליים.

כאן למשל, אני רושם את הבעיה שוב, רק שהפעם יש לי וקטור מקדמים בטא1 ועוד וקטור מקדמים בטא2 ששייך לכל הגורמים של X בריבוע. הנורמה של כל הוקטור צריכה להיות 1, וחוץ מזה אין שינוי. בעצם הגדלתי את המרחב של המישור שלי מp ל2p. אבל התוצאה תהיה כמו שנראה תיכף כלל החלטה לא-ליניארי במישור המקורי של האיקסים.

ומה עם גורמים בשלישית, ומה עם אינטראקציות מסדר ראשון, מסדר שני וכולי?

כמו ברגרסיה אפשר להרחיב את X לאיזשהו מיפוי h(X) ולהמשיך כרגיל.

הבעיה היא למשל חישובית. בדרך כלל אם רוצים להתחשב למשל בכל האינטראקציות q נעשה גדול מאוד מהר מאוד, אולי אפילו יותר מכמות התצפיות. באופן מפתיע יש טריק שאפשר להפעיל שהוא אגב לא ייחודי למודל סאפורט וקטור, והוא מאפשר לנו לקבל את המודל בלי צורך לפרט ולחשב את כל הh(X) הארוך הזה.
:::
:::

---

### The kernel trick (I)

::: {.incremental}
- An amazing insight, after a lot of algebra, our solution can be written as:
$$f(x_0) = \text{sign}\left[\beta_0 + x_0^T\beta\right] = \text{sign}\left[\beta_0 + \sum_{i = 1}^n\alpha_i\langle x_i, x_0 \rangle\right]$$
- where:
  - $\langle x_i, x_0 \rangle$ is the inner product $x_i^Tx_0 = \sum_{j = 1}^p x_{ij}x_{0j}$
  - $\alpha_i, \dots, \alpha_n$ are $n$ parameters for $n$ observations,
    - but $\alpha_i > 0$ only for support vectors, otherwise $\alpha_i = 0$
- So for any $h(x_i)$, the solution can be written in terms of inner products only:
$$f(x_0) = \text{sign}\left[\beta_0 + h(x_0)^T\beta\right] = \text{sign}\left[\beta_0 + \sum_{i = 1}^n\alpha_i\langle h(x_i), h(x_0) \rangle\right]$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הטריק שלנו נקרא הקרנל טריק. והוא שהופך סאפורט וקטור קלסיפייר לסאפורט וקטור מאשין. באופן כללי יש עולם שלם של שיטות קרנל שלא נלמד, מי שרוצה יכול לקחת קורסים מתקדמים יותר או לקרוא על זה עוד בספרים, זה מאוד שימושי.

חוקרים הבחינו שבאופן מדהים אפשר לנסח את המודל שלנו לא בשפת המקדמים, בטא1 עד בטאp, אלא בשפת התצפיות, עם וקטור פרמטרים בגודל n שנסמן כאלפא.

ניקח תצפית חדשה X0. האלפות שלנו לא מכפילות את התצפית עצמה אלא את המכפלה הפנימית של התצפית עם תצפית Xi. כלומר אפשר לכתוב את המודל כך (להדגים).

וכאמור יש לנו פרמטר אלפא-איי לכל תצפית i, והנה עובדה מרתקת: אפשר להראות שבהכרח אלפא-איי גדול מאפס רק עבור אותן תצפיות שהן סאפורט וקטורז, שמגדירות את המרג'ין. אחרת, הוא אפס. כלומר בלי קשר לטריק הקרנל שתיכף נראה, המודל של סאפורט וקטור כולל צורה דואלית ברמת התצפיות שמראה ממש איך הוא תלוי רק בקבוצה מסוימת שלהן. כשבאה תצפית חדשה, מספיק לחשב את המכפלה הפנימית שלה עם כל אחד מהסאפורט וקטורז בלבד, להכפיל פי איזו משקולת אלפא-איי ולסכום. החיזוי של תצפית חדשה יקבע רק באמצעות היחס שלה לסאפורט וקטורז, התרומה של כל תצפית אחרת היא אפס.

בחזרה לקרנל: אם ככה, זה אומר שעבור כל הרחבה של X ממימד q שהוא אולי גדול הרבה יותר, עדיין אפשר לרשום את הפתרון כקומבינציה של מכפלות פנימיות, פשוט  המכפלות הפנימיות הן במרחב החדש הזה של h(X).
:::
:::

---

### The kernel trick (II)

::: {.incremental}
- Now suppose $x_i = (x_{i1}, x_{i2})$ and $h: \mathbb{R}^2 \to \mathbb{R}^6$ is:
$$h(x_i) = (1, \sqrt{2}x_{i1}, \sqrt{2}x_{i2}, x^2_{i1}, x^2_{i2}, \sqrt{2}x_{i1}x_{i2})$$
- Amazingly, the inner product with any point $x_0$ is quite compact:
:::
::: {.fragment}
$\langle h(x_i), h(x_0) \rangle = \begin{pmatrix}1 \\ \sqrt{2}x_{i1} \\ \sqrt{2}x_{i2} \\ x^2_{i1} \\ x^2_{i2} \\ \sqrt{2}x_{i1}x_{i2}\end{pmatrix}^T\begin{pmatrix}1 & \sqrt{2}x_{01} & \sqrt{2}x_{02} & x^2_{01} & x^2_{02} & \sqrt{2}x_{01}x_{02}\end{pmatrix} = (1 + x_i^Tx_0)^2$
:::

::: {.fragment}
::: {.callout-note}
Why is that helpful?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בואו ניתן דוגמא: נניח שאני נמצא בעולם עם שני משתנים, דו-מימדי, וההרחבה שלי פולינומית ואכן כוללת חותך, את המשתנים עצמם, את המשתנים בריבוע, ואינטראקציה בין שני המשתנים. כלומר בסך הכל עברתי למרחב ממימד 6. נכון שאני מוסיף גורמים של שורש 2 אבל זה לא משנה דבר ותיכף נבין למה אני עושה את זה.

איך נראית המכפלה הפנימית בין ההרחבה h של תצפית חדשה x0 לתצפית קיימת במדגם הלמידה x1? האמת שדי פשוט:

אם תבצעו את המכפלה הפנימית ותסדרו את האיברים, תגלו שקיבלתם ביטוי קטן ופשוט בשפה המקורית של האיקסים: 1 ועוד המכפלה הפנימית של וקטור המשתנים המקורי X0 כפול Xi, כל זה בריבוע.

במה זה עוזר לנו? זה עוזר לנו כי המכפלה הפנימית היא כל מה שאנחנו צריכים, וגילינו שאנחנו יכולים לעשות אותה במימד המקורי של X, המימד הנמוך יותר שבו צריך להכפיל שני וקטורים באורך 2, לא באורך 6.
:::
:::

---

### The kernel trick (III)

::: {.incremental}
- Let this be a [kernel]{style="color:red;"} function:
$$\langle h(x_i), h(x_0) \rangle = (1 + x_i^Tx_0)^2 = K(x_i, x_0)$$
- Back to our solution:
$$f(x_0) = \text{sign}\left[\beta_0 + \sum_{i = 1}^n\alpha_i\langle h(x_i), h(x_0) \rangle\right] = \text{sign}\left[\beta_0 + \sum_{i = 1}^n\alpha_i K(x_i, x_0)\right]$$
- The **kernel trick**:
  - Forget about specifying $h(x_i): \mathbb{R}^p \to \mathbb{R}^q$!
  - Focus on specifying kernel functions $K(x_i, x_j): \mathbb{R}^p \times \mathbb{R}^p \to \mathbb{R}$
  - That is, flexible similarity functions between feature vectors
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז בואו נקרא לפונקציה הזאת קרנל, נסמן אותה כפונקציה K של שני וקטורים או שתי תצפיות במרחב המקורי של X.

ואם נחזור למודל שלנו של המישור שמפריד בין הקלאסים במרחב של h אנחנו רואים שאפשר להציב את הקרנל הזה במקום המכפלה הפנימית של הhים ולקבל ביטוי פשוט יותר.

אז מה הטריק?

הטריק הוא שמאוד יכול להיות שאני לא צריך לחשב בכלל את h, ויותר מזה אני לא צריך לפרט אותה!

במקום זה אני מפרט פונקצית קרנל כלשהי בין התצפיות, שעבור כל שתי תצפיות בRp אומרת לי כמה הן דומות בעצם.

זה הרבה יותר קל לחשוב ככה, ובאמת יש כל מיני פונקצית קירבה בין תצפיות שמסתתרת מאחוריהן פיצ'ר מאפ h מאוד מאוד מעניינת, ממימד אולי מאוד גבוה, שמאפשרת מודל מאוד מאוד גמיש בלי לחשב אותה.
:::
:::

---

### Common kernels

| kernel          | $K(x_i, x_j)$                             | comment                                           |
|-----------------|-------------------------------------------|---------------------------------------------------|
| Linear          | $\langle x_i, x_j \rangle$                | SVC!                                              |
| Polynomial      | $(1 + \langle x_i, x_j \rangle)^d$        | For $p = 2, d = 2$ we got $h(x)$                  |
| Gaussian/SE/RBF | $\exp\left(-\gamma\|x_i - x_j\|^2\right)$ | infinite feature mapping space!                   |
| Sigmoid         | $\tanh(\kappa_1\langle x_i, x_j \rangle + \kappa_2)$ |                                        |

<br>

::: {.incremental}
- Each of these *implicitly* uses some mapping $h(x)$, but always: $K(x_i, x_j) = \langle h(x_i), h(x_j) \rangle$

- On entire training set need to compute: $K_{n \times n}$ (kernel matrix) where $K_{ij} = K(x_i, x_j) = \langle h(x_i), h(x_j) \rangle$
:::

::: {.fragment}
::: {.callout-note}
Can any $K(x_i, x_j): \mathbb{R}^p \times \mathbb{R}^p \to \mathbb{R}$ function use as a kernel?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הנה כמה פונקציות כאלה או קרנלים כמו שנהוג לקרוא להם:

לדוגמא הקרנל הליניארי -- מידת הדמיון בין שתי תצפיות היא המכפלה הפנימית ביניהן. זה כמובן מחזיר אותנו למודל המקורי, לסאפורט וקטור קלסיפייר. ולמה שהמכפלה הפנימית בין זוג וקטורים תהיה מידה לקרבה ביניהם? כי זה בדיוק קוסינוס הזווית בין שני הוקטורים, עד כדי קבוע, למי שזוכרים.

יש את הקרנל הפולינומיאלי שכרגע ראינו דוגמה שלו, באופן כללי אפשר להעלות את המכפלה הפנימית של האיקסים בחזקת d וזה מבטיח שלנו פיצ'ר מאפ h שכוללת את כל החזקות והאינטראקציות עד מקדם d כולל.

קרנל מאוד פופולרי ושימושי הוא הקרנל הגאוזייני או סקוורד אקספוננשיאל, או RBF. כאן תשימו לב שככל שהתצפיות דומות המרחק ביניהן קטן ומקבלים אקספוננט בחזקת אפס במקסימום, 1. למה זה קרנל מאוד שימושי? כי הוא מאפשר גבולות החלטה מעוגלים, שמאחוריהם עומדת פיצ'ר מאפ h שהיא ממימד אינסופי! שלא צריך לפרט או לחשב! לא נוכיח את זה כאן כמובן.

מכל מקום מובטח לכם שמאחורי כל אחד מהקרנלים האלה עומדת מכפלה פנימית של פיצ'ר מאפס שמחושבות רק באופן אימפליסיטי.

והרבה פעמים אנחנו לא נדבר על פונקצית קרנל אלא על מטריצת קרנל או מטריצת גרם שמחושבת על כל מדגם הלמידה בגודל n על n. כל איבר ij במטריצה הוא הוא הקרנל בין תצפיות i וj.

שאלה מתבקשת היא כמובן, מה אני יכול להשתמש בכל פונקצית דמיון שבא לי? והתשובה היא כמובן שלא. הרי כל איבר צריך להיות מכפלה של איזושהי פיצ'ר מאפ h. נגיד שקריטריון מספיק לקבוע שזה אכן כך הוא שK היא מטריצה סימטרית חיובית למחצה, ונשאיר לכם אולי בתרגיל לברר למה זה נכון.
:::
:::

---

### Support vector machines (SVM)

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC

# Define a more complex logistic function for P(Y = 1 | X1, X2) to create a wavy decision boundary
def logistic_function(x1, x2):
    return 1 / (1 + np.exp(- (np.sin(x1) + np.cos(x2) + 0.5 * x1 * x2 - 1)))

# Create a grid of X1 and X2 values
x1 = np.linspace(-3, 3, 300)
x2 = np.linspace(-3, 3, 300)
X1, X2 = np.meshgrid(x1, x2)

# Compute P(Y = 1 | X1, X2) over the grid
P = logistic_function(X1, X2)

# Simulate random points within the grid for the training set
np.random.seed(42)  # For reproducibility
X1_train = np.random.uniform(-3, 3, 500)
X2_train = np.random.uniform(-3, 3, 500)
X = np.vstack((X1_train, X2_train)).T

# Calculate the probability for each training sample point
P_train = logistic_function(X1_train, X2_train)

# Generate Y values based on the probabilities for the training set
y = 2 * np.random.binomial(1, P_train) - 1

clf_rbf = SVC(kernel='rbf', C=100)
clf_rbf.fit(X, y)

# Plot the heatmap
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))
heatmap = ax1.contourf(X1, X2, P, levels=50, cmap='viridis')
# plt.colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

# Plot the decision boundary where P(Y = 1 | X1, X2) = 0.5
decision_boundary = ax1.contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
ax1.clabel(decision_boundary, fmt = '%.1f', colors = 'red')

# Plot the simulated training points
ax1.scatter(X1_train[y == 1], X2_train[y == 1], color='yellow', label='Y = 1', edgecolor='black')
ax1.scatter(X1_train[y == -1], X2_train[y == -1], color='blue', label='Y = -1', edgecolor='black')

grid_points = np.c_[X1.ravel(), X2.ravel()]

# Evaluate the decision function across the grid
decision_values = clf_rbf.decision_function(grid_points).reshape(X1.shape)

# Plot the decision function (decision boundary and margins)
# ax1.contour(X1, X2, decision_values, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')
ax1.contour(X1, X2, decision_values, levels=[0], linestyles=['-'], colors='k')

# Labels and title
ax1.set_title('RBF kernel, gamma = 1')
ax1.set_xlabel('$X1$')
ax1.set_ylabel('$X2$')

clf_poly = SVC(kernel='poly', degree=6, C=100)
clf_poly.fit(X, y)

# Plot the heatmap
heatmap = ax2.contourf(X1, X2, P, levels=50, cmap='viridis')
# plt.colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

# Plot the decision boundary where P(Y = 1 | X1, X2) = 0.5
decision_boundary = ax2.contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
ax2.clabel(decision_boundary, fmt = '%.1f', colors = 'red')

# Plot the simulated training points
ax2.scatter(X1_train[y == 1], X2_train[y == 1], color='yellow', label='Y = 1', edgecolor='black')
ax2.scatter(X1_train[y == -1], X2_train[y == -1], color='blue', label='Y = -1', edgecolor='black')

# Evaluate the decision function across the grid
decision_values = clf_poly.decision_function(grid_points).reshape(X1.shape)

# Plot the decision function (decision boundary and margins)
# ax2.contour(X1, X2, decision_values, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')
ax2.contour(X1, X2, decision_values, levels=[0], linestyles=['-'], colors='k')

# Labels and title
ax2.set_title('Polynomial kernel, degree = 6')
ax2.set_xlabel('$X1$')
ax2.set_ylabel('$X2$')
plt.show()
```

::: {.fragment}
::: {.callout-note}
Notice this might add a few hyperparameters, including the kernel itself!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז השילוב הזה של סאפורט וקטור קלאסיפיירז והטריק של קרנל הוא שמביא אותנו לסאפורט וקטור משינז.

הנה הביצועים של המודל החזק הזה על הדאטא הקשה לקליספיקציה שהטריד אותנו. אפשר לראות שגם קרנל RBF וגם קרנל פולינמיאלי נגיד מדרגה 6, מצליחים במשהו לקלוט את גבול ההחלטה העקום והלא רציף הזה. הקרנל הRBF קצת יותר.

ורק נזכיר שמודל כל כך חזק לא בא בלי מחיר לשלם. מחיר אחד למשל אתם רואים כאן, לכל קרנל נוספים פרמטרים שצריך לבחור, כנראה עם קרוס ולידיישן. בקרנל הRBF צריך לבחור את הרזולוציה גאמא, בקרנל הפולינומיאלי צריך לבחור את הדרגה d. ויותר מזה צריך לבחור את הקרנל עצמו. בדרך כלל הבחירה היא בין כמה קרנלים נפוצים יותר, אבל אתם כמה קרנלים פותחו עם השנים? עשרות.
:::
:::

---

### Example: SAHeart data

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

saheart = pd.read_table("../datasets/SAheart.data", header = 0, sep=',', index_col=0)

saheart_X=pd.get_dummies(saheart.iloc[:, :9]).iloc[:, :9]
saheart_y=saheart.iloc[:, 9]

Xtr, Xte, Ytr, Yte = train_test_split(saheart_X, saheart_y, test_size=0.2, random_state=42)

# Standardize the data for SVMs and Logistic Regression
scaler = StandardScaler()
Xtr = scaler.fit_transform(Xtr)
Xte = scaler.transform(Xte)

# Initialize the models
log_reg = LogisticRegression()
svc_linear = SVC(kernel='linear', probability=True)
svc_poly = SVC(kernel='poly', degree=2, probability=True)
svc_rbf = SVC(kernel='rbf', probability=True)

# Train the models on the training data
log_reg.fit(Xtr, Ytr)
svc_linear.fit(Xtr, Ytr)
svc_poly.fit(Xtr, Ytr)
svc_rbf.fit(Xtr, Ytr)

# Get predicted probabilities for the test set
log_reg_probs = log_reg.predict_proba(Xte)[:, 1]
svc_linear_probs = svc_linear.predict_proba(Xte)[:, 1]
svc_poly_probs = svc_poly.predict_proba(Xte)[:, 1]
svc_rbf_probs = svc_rbf.predict_proba(Xte)[:, 1]

# Compute ROC curves and AUCs
fpr_log, tpr_log, _ = roc_curve(Yte, log_reg_probs)
fpr_svc_linear, tpr_svc_linear, _ = roc_curve(Yte, svc_linear_probs)
fpr_svc_poly, tpr_svc_poly, _ = roc_curve(Yte, svc_poly_probs)
fpr_svc_rbf, tpr_svc_rbf, _ = roc_curve(Yte, svc_rbf_probs)

auc_log = auc(fpr_log, tpr_log)
auc_svc_linear = auc(fpr_svc_linear, tpr_svc_linear)
auc_svc_poly = auc(fpr_svc_poly, tpr_svc_poly)
auc_svc_rbf = auc(fpr_svc_rbf, tpr_svc_rbf)

# Plot ROC curves
plt.figure(figsize=(5, 5))
plt.plot(fpr_log, tpr_log, label=f'Logistic Regression (AUC = {auc_log:.2f})')
plt.plot(fpr_svc_linear, tpr_svc_linear, label=f'SVM (Linear, AUC = {auc_svc_linear:.2f})')
plt.plot(fpr_svc_poly, tpr_svc_poly, label=f'SVM (Poly d=2, AUC = {auc_svc_poly:.2f})')
plt.plot(fpr_svc_rbf, tpr_svc_rbf, label=f'SVM (RBF, AUC = {auc_svc_rbf:.2f})')

# Plot settings
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line (random classifier)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Test ROC Curves for SVMs and Logistic Regression')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()
```

::: {.fragment}
::: {.callout-note}
Notice to compute ROC SVM needs to output a probability or score.

This is out of scope, but you could think of the distance of an observation to the hyperplane as a type of confidence.
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מחיר נוסף שאנחנו משלמים עם מודל כל כך חזק הוא הסיכוי לאוברפיטינג. אפשר לראות את זה כאן כשאנחנו מפעילים SVM על נתונים אמיתיים, הנתונים שהצגנו על מדגם של פציינטים שחלקם קיבלו התקף לב וחלקם לא, ואנחנו רוצים לחזות על מדגם טסט, על פציינטים חדשים. כאן אני משווה על מדגם הטסט, על חולים שהמודל לא ראה, בין רגרסיה לוגיסטית לבין SVM עם קרנל ליניארי, קרנל פולינומיאלי מדרגה 2 וקרנל RBF. ואפשר לראות מהAUC גם שהקרנלים הכי פשוטים מגיעים לתוצאות הכי טובות, וגם שרגרסיה לוגיסטית מגיע בשורה התחתונה לביצועים דומים מאוד לSVM עם קרנל ליניארי שהוא בעצם SVC כמו שצפינו.

חדי העין בטח שואלים את עצמם רגע, איך אני בכלל מצייר עקומת ROC למודל SVM, הוא הרי מחזיר חיזויים של 1 או מינוס 1, ואנחנו צריכים איזו הסתברות או לפחות איזשהו סקור כדי לצייר ROC, עם קטאופים שונים. אז זה כבר קצת מחוץ לסקופ שלנו, אבל יש דרך לחלץ סקור ואפילו הסתברות ממודל SVM, לכל הפחות ודאי אפשר להתייחס למרחק של כל תצפיות מהמישור שנוצר כאיזושהי מטריקה של קונפידנס. ככל שתצפית רחוקה יותר מהמישור, כך המודל בטוח לגביה שהיא מקלאס אחד ולא מאחר.

ומה עם יותר מקלאס אחד? ומה עם רגרסיה? נגיד גם על זה כמה מילים בחלק האחרון של היחידה.
:::
:::

---

## SVM extensions {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניגע כעת קצת בהרחבות של SVM ליותר משני קלאסים ולרגרסיה. ההרחבה של SVM ליותר משני קלאסים היא יחסית קלה ומתאימה לא רק לSVM אלא לכל מודל שיודע לעשות הפרדה לשני קלאסים בלבד. ההרחבה של SCM לרגרסיה היא מעט יותר מתוחכמת ולא נוכל להיכנס לכל הפרטים והדקויות, אבל בודאי תבינו כיצד היא מתבצעת.
:::
:::

---

### SVM for $K$ classes

- One-versus-one (OVO):
  - Run all $K \choose 2$ pairwise models: class $k$ vs. class $k'$
  - Assign $x_0$ to class $k$ to which it was most frequently assigned to
- One-versus-rest (OVR):
  - Run all $K$ models: class $k$ (+1) vs. remaining $K - 1$ classes (-1)
  - Assign $x_0$ to class $k$ in which it gets the highest distance (confidence) from hyperplane $\beta_{0k} + x_0^T\hat{\beta_{k}}$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איך מרחיבים SVM ליותר משני קלאסים -- איך שמרחיבים כל מודל בינארי ליותר משני קלאסים.

דרך אחת היא וואן-ורסוז-וואן או OVO. נריץ SVM להפרדה בין כל זוג קלאסים K וK טאג. סך הכל נריץ SVM K מעל 2 פעמים. וכשמגיעה תצפית חדשה נעבור אותה בכל המודלים ונסווג אותה אל הקלאס אליו היא שויכה הכי הרבה פעמים, לעומת הקלאסים האחרים.

הדרך השניה היא וואן-ורסוז-רסט או OVR.
נחלק את הדאטא לקלאס הk מול כל שאר התצפיות ונריץ SVM. נעשה את זה לכל קלאס וקלאס, בסך הכל נריץ SVM K פעמים.
וכשמגיעה תצפית חדשה, נעביר אותה בכל המודלים, ונסווג אותה אל הקלאס שבמודל שלו היא הראתה הכי הרבה קונפידנס שהיא שייכת אליו, כלומר כמו שראינו הכוונה היא למודל עם המישור שממנו התצפית הכי רחוקה.
:::
:::

---

### Support vector regression (SVR)


```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR

def plot_svr_intuition(epsilon=0.1):
    # Generate synthetic data
    np.random.seed(42)
    X = np.linspace(0, 10, 50).reshape(-1, 1)
    y = 2 * X.ravel() + np.random.randn(50) * 2.0  # Linear trend with noise

    # Train SVR with linear kernel
    svr = SVR(kernel='linear', epsilon=epsilon)
    svr.fit(X, y)

    # Get predictions
    y_pred = svr.predict(X)
    
    # Plot scatter points (data points)
    plt.figure(figsize=(8, 5))
    plt.scatter(X, y, color='blue', label='Data points', marker='x', s=20)

    # Plot regression line (the hyperplane)
    plt.plot(X, y_pred, 'k--', label='Regression line (hyperplane)')

    # Plot the epsilon-tube margins
    plt.plot(X, y_pred + epsilon, 'k:', label=f'Upper margin (+ε={epsilon})')
    plt.plot(X, y_pred - epsilon, 'k:', label=f'Lower margin (-ε={epsilon})')

    # Highlight the support vectors (points outside the epsilon-tube)
    support_vectors = svr.support_
    plt.scatter(X[support_vectors], y[support_vectors], facecolors='none', edgecolors='black', s=30, label='Support vectors')

    # Plot settings
    plt.xlabel('X')
    plt.ylabel('y')
    plt.grid(True)
    plt.show()

# Example usage
plot_svr_intuition(epsilon=2)
```

::: {.fragment}
::: {.callout-note}
What would be the support vectors for SVR?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ומה עם רגרסיה? איך נבטא רעיון דומה של מישור שמבטא באמצעות איזשהו מרג'ין ממנו את הקשר הכי טוב בין משתנה מסביר X לY רציף?

נתחיל בזה שמישור הוא בדיוק מה שאנחנו מחפשים תמיד ברגרסיה, למשל עם משתנה אחד מדובר בקו. ונגדיר שאנחנו רוצים מישור עם מרג'ין M סביבו (להדגים), שכל התצפיות נכנסות בו, בלוח הזה. זה מאוד הגיוני, כי ככל שהמרג'ין קטן יותר זה אומר שהפיזור סביב המישור קטן יותר וזו התאמה טובה יותר. ככל שהוא גדול יותר כך הפיזור גדול ואנחנו פחות בטוחים במודל, בדומה לרגרסיה.

עכשיו זה כמובן קריטריון מחמיר מאוד ומושפע בקלות מתצפיות קיצוניות, ולכן גם כאן אנחנו עושים רילקסציה ומאפשרים משתני סלאק לתצפיות מסוימות, שיוצאות מהמרג'ין. אפשר להגדיר בדיוק כמו מקודם שיש לנו איזשהו תקציב C לגודל הסלאק או מספר התצפיות שאנחנו מוכנים שיצאו מהמרג'ין ולהמשיך בצורה דומה מאוד לSVM.

עכשיו נאמר והשגנו את זה כמו בתרשים שלפנינו -- אילו נקודות הן הסאפורט וקטורז, שרק בהן המודל הסופי בעצם תלוי? כאן אלה דווקא התצפיות מחוץ למרג'ין, שאם הן יזוזו המישור והמרג'ין יכולים מאוד להשתנות. זה קצת שונה מSVM אבל אם תחשבו על זה זה ממש אותו עקרון -- על תצפיות שבתוך המרג'ין אנחנו לא משלמים כלום, העיקר שהן רחוקות עד כדי M מהמישור. אנחנו משלמים הרבה על התצפיות שמחוץ למרג'ין, לכן הן הסאפורט וקטורז.
:::
:::

---

### SVR Criterion

::: {.incremental}
- From maximum margin classifier:
$\max_{\beta, \beta_0}M \quad s.t. \space \|\beta\| = 1 \text{ and } \quad y_i (\beta_0 + x_i^T\beta) \ge M \quad \forall i$

- To "maximum margin regressor" (not really a thing):
$\max_{\beta, \beta_0}M \quad s.t. \space \|\beta\| = 1 \text{ and } \quad |y_i - (\beta_0 + x_i^T\beta)| \le M \quad \forall i$

- From SVC:
$\max_{\beta, \beta_0, \epsilon}M \quad s.t. \space \|\beta\| = 1, \quad y_i (\beta_0 + x_i^T\beta) \ge M(1-\epsilon_i), \quad \epsilon_i \ge 0, \quad \sum_{i = 1}^n \epsilon_i \le C$

- To SVR:
$\max_{\beta, \beta_0, \epsilon}M \quad s.t. \space \|\beta\| = 1, \quad |y_i - (\beta_0 + x_i^T\beta)| \le M + \epsilon_i, \quad \epsilon_i \ge 0, \quad \sum_{i = 1}^n \epsilon_i \le C$

- Though this is more common:
$$\min_{\beta, \beta_0, \epsilon}\frac{1}{2}\|\beta\|^2 + C \sum_{i = 1}^n \epsilon_i \quad s.t. \space |y_i - (\beta_0 + x_i^T\beta)| \le M + \epsilon_i, \quad \epsilon_i \ge 0$$

- And of course SVR can be kernelized as well
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
פורמלית אפשר בשקף אחד להראות את כל המסע שלנו בדרך לSVM רק ברגרסיה, המסע לSVR.

בקלסיפיקציה היה לנו את הקריטריון המחמיר של מקסימום מרג'ין קלסיפייר.

ברגרסיה אפשר לחשוב על קריטריון מקביל, להביא למקסימום את המרג'ין, כך שוקטור המקדמים הוא וקטור יחידה, ונדאג שכל תצפית רחוקה מהמישור, כלומר מהחיזוי שלה עד כדי המרג'ין M. שימו לב להבדל כאן בין קלסיפיקציה לרגרסיה.

אחר-כך הרשינו קצת סלאק והגענו לקריטריון של סאפורט וקטור קלסיפייר.

ואפשר לחשוב על קריטריון מקביל ברגרסיה: נרצה את המקסימום מרג'ין כך שכל תצפית רחוקה לא יותר מM מהחיזוי שלה, ונוסיף משתני סלאק גדולים מאפס, לחלק מהתצפיות אנחנו מרשים להיות רחוקות מעבר למרג'ין. ועל משתני הסלאק שולט פרמטר של תקציב C.

בפועל, כמו בקלסיפיקציה תראו יותר בספרות קריטריון שנראה כך: מינימום על הנורמה של המקדמים בריבוע, כדי לעשות את זה בעיה קמורה, ועוד פרמטר פנאלטי על סכום משתני הסלאק. גם בעית אופטימיזציה שיש לה פתרונות סטנדרטיים. כאן, רק נשים לב שלא נפטרנו מפרמטר הM שמייצג את המרג'ין, נהוג לפרט גם אותו למשל במימוש של sklearn.

וגם למודל של SVR ניתן לעשות קרנליזציה ולהתחשב בקרנלים שונים.

נעצור כאן, כמו שאמרנו יש עוד הרבה יותר מה להגיד על SVR, ואפשר להראות שגם הוא וגם SVM עושים מינימום לפונקציות לוס מעניינות מאוד, מי שמעוניין בכך יכול לקרוא על כך עוד בספרים.

עד כאן SVM, אחד המודלים האלגנטיים ביותר בלמידת מכונה שיודע גם לטפל בדאטא ממימד גדול מאוד ולכן גם היה אופנתי מאוד ככל שהנתונים נעשו גדולים ועד שהגיעו רשתות הנוירונים.
:::
:::
