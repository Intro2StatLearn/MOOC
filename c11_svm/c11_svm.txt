=== 1. Maximum margin classifier ===

ביחידה הזאת נלמד על אלגוריתם נוסף שנוסח במקור עבור בעיות של קלאסיפיקציה בינארית: הספורט וקטור משינז, או SVM בקיצור.

כמו אדאבוסט גם SVM הגיע מקהילת מדעי המחשב לבעיה ספציפית, והורחב לאחר מכן לבעיות כלליות יותר כמו רגרסיה, וכך נלמד אותו. אולם בניגוד לאדאבוסט בSVM אנחנו מגיעים בסופו של דבר לבעית אופטימיזציה שקצת קשה להסביר בדיוק כיצד פותרים אותה בלי כלים מאופטימיזציה קמורה או חקר ביצועים, לכן לא ניכנס לדרך הפתרון המדויקת. נטען עם זאת שהגענו לבעיה פשוטה יחסית באופטימיזציה שניתנת לפתרון באמצעות תוכנה סטנדרטית.

כמו תמיד, אנחנו מתחילים עם הנחה, ולאט לאט מפירים אותה עד שנגיע לפרוצדורה הסופית.

:::

וההנחה שאנחנו מתחילים היא הנחה גדולה. אנחנו מתחילים כאמור עם Y בינארי שיכול להיות 1 או מינוס 1, ועם מדגם למידה בגודל n.

וההנחה היא שהנתונים שלנו במרחב הX הם לינארלי ספרבל, כלומר ניתן למצוא קו או מישור או מישור-על שמפריד בין התצפיות שהן 1 לבין התצפיות שהן מינוס 1. זאת הנחה כמובן לא סבירה אלא אם המימד של X ממש גבוה. אנחנו עושים אותה במודע ולאחר מכן נאפשר בכל זאת חפיפה בין הקלאסים ואולי אפילו גבול לא לינארי.

:::

אם אנחנו מדברים על מישור-על או הייפרפליין שמפריד, בואו ניזכר מה ההגדרה של מישור.

ההגדרה של מישור היא באמצעות הנקודות שמקיימות את המשוואה הליניארית שיש לנו כאן: חותך בטא-אפס ועוד מקדמים כפול משתנים, כל זה שווה לאפס. ניתן לכתוב זאת גם באמצעות המכפלה הפנימית של וקטור המקדמים ללא החותך בטא בוקטור הקואורדינטות X. כלומר מישור הוא מעין לוח של כל הנקודות שמקיימות את המשוואה. כאן למשל מצויר מישור בדו-מימד שהוא בעצם קו, של כל הנקודות שמקיימות -4 + 2x1 + 3x2 = 0.

ניזכר גם שוקטור שמאונך למישור, שיוצר איתו זווית של 90 מעלות והנורמה שלו 1, נקרא נורמל למישור. ואפשר לראות שמעצם ההגדרה של וקטור המקדמים בטא, בטא הוא וקטור נורמל למישור, הוא אורתוגונלי אליו, יוצר איתו זוית של 90 מעלות, או יותר נכון בטא-כוכב שעבר נרמול. אני אומר מעצם ההגדרה כי בטא כפול כל נקודה 1 לחותך, איקס1 איקס2, ייתן אפס, זאת ההגדרה של אורתגונליות. זה יהיה חשוב לנו תיכף. כלומר אפשר להגדיר מישור באמצעות וקטור בטא, ועוד חותך. מישור בטא זה מישור שאנכי לוקטור בטא, כשהחותך בטא אפס אומר לנו כמה להזיז אותו בכיוון בטא.

מכל מקום, אם אפשר למצוא מישור כזה שמפריד בין תצפיות 1 ומינוס 1, מה יהיה כלל החלטה טבעי לחיזוי תצפית חדשה? אם התצפית מעל למישור, כלומר בטא-אפס ועוד X בטא גדול מאפס, אז נחזה 1. ואם התצפית מתחת למישור, כלומר המכפלה קטנה מאפס, נחזה מינוס 1. לשם אנחנו חותרים, זה אכן יהיה כלל ההחלטה שלנו עוד רגע.

:::

במילים אחרות, כרגע גם מצאנו הגדרה של מהו מישור מפריד, separating. מישור מפריד הוא מישור שעבור מדגם הלמידה שלנו, בכל נקודה (להדגים), אם היא 1 אז היא מעל המישור, ואם היא מינוס 1 אז היא מתחת למישור. ואת זה אנחנו יכולים לרשום בצורה קומפקטית ככה: המכפלה של Y בביטוי המישור חייבת להיות גדולה מאפס לכל התצפיות. כי אם Y הוא 1 הביטוי חיובי אנחנו מעל למישור, פלוס כפול פלוס. ואם Y הוא מינוס 1 הביטוי שלילי אנחנו מתחת למישור, מינוס כפול מינוס.

וכאמור אם נמצא מישור כזה, או אומדים לוקטור בטא ולחותך, זה בסוף יהיה כלל ההחלטה שלנו לכל תצפית חדשה X0: הסיין של הבדיקה הזאת האם הנקודה מעל או מתחת למישור.

אז לפני שנמצא את המישור, צריך להכיר בבעיה של מה שהגדרנו: אם יש מישור כזה, הוא יחיד? לא, (להדגים) יש הרבה מישורים כאלה, למעשה יש אינסוף. ואז נשאל -- איזה מישור הכי טוב למטרה שלנו.

:::

בצורה אינטואיטיבית, נגיד, שהיינו רוצים את המישור, שהוא הכי רחוק מהתצפיות של מדגם הלמידה. כלומר לכל כיוון נרצה את המישור שהוא כמה שיותר באמצע, סביר להניח שזה כלל ההחלטה שיעבור הכללה הכי טובה לתצפיות שהמודל לא ראה.

דרך אחרת להגיד אותו דבר, היא המישור שעושה מקסימום לרווח בין שני הקלאסים, למה שאנחנו קוראים מרג'ין. ולכן לקלסיפייר הנאיבי שנפתח קוראים מקסימום מרג'ין קלסיפייר.

כאן למשל יש שני מישורים שמביאים לשני מרג'ינים, שניהם מפרידים לנו בין הקלאסים. המרג'ין השמאלי אינטואיטיבית נראה טוב יותר כי הוא פשוט עבה יותר, למעשה הוא המרג'ין המקסימלי שאפשר להגיע אליו עם מדגם הלמידה הזה.

נשים לב עוד שלכל נקודה יש מרחק אנכי אל המישור. המרג'ין מוגדר על ידי הנקודות עם המרחק המינימלי למישור. ואנחנו רוצים לעשות מקסימום למרחק הזה, כלומר זאת בעית מקסימום למינימום מרחק.

:::

אז איך אנחנו מנסחים את הקריטריון הזה בצורה שגם נוכל להשיג אותו?

הבנו שאנחנו רוצים את המקסימום של "מינימום מרחק", הבנו שהפרמטרים שלנו הם מה שמגדיר את המישור עצמו, וקטור המקדמים של משוואת המישור, או הנורמל למישור ועוד חותך. והבנו שיש לנו אילוץ שנובע מההנחה שלנו, זה חייב להיות מישור מפריד, מה שמתבטא באילוץ שרשום כאן לכל תצפית i. כל שנשאר לנו זה לפרמל מה זה מינימום מרחק.

אז אם אתם יודעים מלימודי המתמטיקה שלכם, יש לנו נוסחה של מרחק של כל נקודה למישור שמוגדר על-ידי וקטור המקדמים בטא: זה בעצם המכפלה הפנימית בין הנקודה לוקטור המקדמים, ועוד חותך, בערך מוחלט, חלקי הנורמה של וקטור המקדמים.

אפשר להוכיח את זה אבל יש כאן משהו מאוד אינטואיטיבי: הנה נקודה X ואני רוצה לבדוק מה המרחק האנכי שלה למישור נתון. אפשר לראות שהמרחק הזה הוא אורך של וקטור שהוא מקביל לוקטור בטא הנורמל. ולכן האורך פרופורציונלי לאורך וקטור בטא הנורמל. כדי להשיג את המקטע הזה אנחנו מטילים את הנקודה X על בטא, והטלה פירושה המכפלה הפנימית בין שני הוקטורים, שמופיעה בביטוי שלנו.

בפרט, אם נדרוש שוקטור בטא יהיה בעל אורך 1, וקטור יחידה, אז המרחק של כל נקודה למישור הוא המכפלה של הקואורדינטות שלה בוקטור המקדמים של המישור ועוד חותך (להדגים). אם זה שווה לאפס הנקודה מקיימת את משוואת המישור, היא על המישור, אם זה גדול מאפס היא מעליו, ואם זה קטן מאפס היא מתחתיו ואנחנו לוקחים את הערך המוחלט.

:::

אז נסמן את המרחק הזה, את המרג'ין בM.

דרך לסכם את כל מה שדרשנו עד עכשיו היא לעשות מקסימום על M, ככה שוקטור המקדמים שלנו יהיה וקטור יחידה, יהיה באמת נורמל למישור. ועבור כל תצפית i במדגם הלמידה, נחשב את המרחק שלה מהמישור עם הביטוי שמצאנו, ונדרוש שהמרחק הזה יהיה לפחות M. כלומר M הוא המרחק המינימלי. אבל מה זה מרחק, זה הביטוי שרשום בסוגריים בערך מוחלט. אם הנקודה מעל המישור הY שלה הוא 1, אז מכפילים את המרחק פי 1, אין בעיה והביטוי נשאר כפי שהוא, ואם היא מתחת למישור הY שלה הוא מינוס 1 ולכן ההכפלה בו מביאה אותנו גם כן למרחק חיובי.

או במילים אחרות: כל הנקודות חייבות להיות במרחק לפחות M מהמישור שמוגדר על-ידי הנורמל בטא והחותך בטא-אפס, ואנחנו מחפשים בטא ובטא-אפס שיביאו למקסימום את המרחק הזה, את המרג'ין.

נשים לב שאפשר היה עקרונית לקבע את הנורמה של בטא על כל ערך, ואם נחליט שהנורמה של בטא לא חייבת להיות 1 אלא 1 חלקי M, אפשר להיפטר גם מהפרמטר M בבעית האופטימיזציה שלנו. כעת כדי להגדיל את M כמה שיותר צריך להקטין את הנורמה של וקטור בטא כמה שיותר. וככה אנחנו גם נפטרים מM וגם הופכים את הבעיה שלנו לבעית מינימום.

זאת הנקודה שבה הבטחתי שאנחנו עוצרים ולא נכנסים לדיון על אופטימיזציה, רק מצביעים על כך שאפשר לעשות מינימום לנורמה הריבועית (להדגים) ויש לנו בעית אופטימיזציה קמורה יחסית פשוטה: הפונקציה שלנו קמורה ואנחנו עושים מינימום, כלומר מובטח לנו שנמצא מינימום תחת ההנחה שלנו של ספרביליות, והאילוצים הם אי-שוויונות ליניאריים בבטא, אפשר לפתור את זה בכל אלגוריתם או תוכנה לאופטימיזציה סטנדרטית.

:::

אם נסתכל שוב על המוטיבציה שלנו -- אנחנו רוצים למצוא את המישור שממקסם את המרג'ין בין שני הקלאסים - רמזנו לזה כבר קודם אבל כעת אנחנו אומרים במפורש, ניתן לראות שהמרג'ין הזה ברגע שנמצא, הוא מושפע רק מקבוצה קטנה של תצפיות. אם הן יזוזו, גם המישור והמרג'ין יושפעו. אבל אם כל אחת מהתצפיות האחרות יזוזו? בתנאי שיישארו בצד הנכון של המישור ולא ייכנסו פנימה? לא תהיה לזה השפעה על המישור והמרג'ין. לכן הנקודות שבעצם מגדירות את המרג'ין הסופי נקראות נקודות או וקטורים של תמיכה support vectors. ומכאן השם של הבעיה הסופית שנלמד ששומרת על העקרון הזה, סאפורט וקטור מאשינז. למעשה, ניתן להגדיר את כל הבעיה בדרך של מציאת הנקודות שיהיו הסאפורט וקטורז למרג'ין המקסימלי.

כמה הערות לסיום:

קודם כל, חשוב להדגיש שאמנם החיזוי הסופי מושפע רק מהסאפורט וקטורז, ועוד נראה ביטוי מתמטי לזה. אבל זה לא שזרקנו את שאר המדגם, השתמשנו בו כדי להגיע אליהם, כמו שאמרתי אפשר לנסח את כל הבעיה כחיפוש על הנקודות, לא מציאת נורמל למישור.

דבר שני, עם כל מה שלמדתם אתם כבר יכולים לתאר לכם מה יכולים להיות היתרונות והחסרונות בקלסיפייר כזה, שמבוסס רק על סט קטן של תצפיות: למשל הוא מושפע פחות מתצפיות חריגות. קראנו לזה מודל חסין או רובסטי מפני תצפיות חריגות. מצד שני אנחנו תיכף נראה שאמנם המודל לא מושפע מתצפיות שרחוקות מהמרג'ין, אבל הוא מאוד מושפע מתצפיות שקרובות למרג'ין. נזיז תצפית אחת מהסאפורט וקטורז בכיוון קצת לא צפוי והמודל יכול להשתנות בצורה גסה.

שאלה חשובה שיש עליה הרבה מחקר היא מה בעצם ההבדל מרגרסיה לוגיסטית? הרי גם רגרסיה לוגיסטית אם הדאטא הוא ספרבילי אמורה למצוא מישור מפריד דומה מאוד -- וזה אכן מה שקורה, אם תריצו רגרסיה לוגיסטית על המדגם שכאן תקבלו משוואה שמאוד מזכירה את משוואת המישור שהמקסימום מרג'ין קלסיפייר מוצא. ולרגרסיה לוגיסטית יש גם פירוש הסתברותי, ופונקצית הפסד הסתברותית.

שתי השאלות שנדון בהן כעת הן המתבקשות ביותר: אחת, מה עם הדאטא הוא לא ספרבילי? הרי אין מישור למצוא. והשנייה היא האם יש דרך להכליל את הרעיון היצירתי הזה מקלסיפיקציה בינארית לכמה קלאסים ולרגרסיה עם Y רציף.

:::

=== 1. Support Vector Classifier ===

רילקסציה ראשונה לקריטריון שלנו -- אנחנו רוצים שזה יעבוד גם במקרה שיש קצת חפיפה. כלומר לא נניח יותר שהדאטא הוא ספרבילי. זה יביא אותנו סוף סוף למודל שנקרא סאפורט וקטור קלסיפייר.

:::

כבר הזכרנו ונדגים את זה, שהבעיה היא לא רק שיש חפיפה ואין אפשרות למצוא מישור מפריד, כמו בדאטא בתרשים בצד ימין. הבעיה היא גם אם כן אפשר למצוא מישור מפריד אבל המרג'ין שנוצר הוא כל כך צר שזה נראה מאולץ. או במילים אחרות נוצר לנו מצב של אוברפיטינג, ואולי נרצה להתחשב קצת פחות בכל נקודה ונקודה. אני יוצר מצב כזה בדאטא בתרשים האמצעי, על-ידי הוספה של נקודה כחולה אחת ויחידה בצורה כזו. המודל משתנה בצורה דרסטית! איך קראנו לזה? מודל לא יציב, עם שונות מאוד גבוהה.

:::

אז אנחנו נרשה לחלק מהתצפיות להיות גם בתוך המרג'ין. 

או אפילו: לתצפיות מסוימות להיות בצד הלא נכון של המישור!

ואנחנו נעשה את זה באמצעות פרמטר C שיהיה תקציב לחריגות כאלה, מאוד מזכיר את מה שעשינו ברגרסיה עם רגולריזציה כמו רידג' ולאסו.

אנחנו עושים את זה עם אוסף של n פמרטרים לn תצפיות, שאנחנו קוראים להם משתני סלאק, מלשון to cut some slack. נסמן אותם בוקטור אפסילון.

וכעת אנחנו עושים מקסימום על M המרחק המינימלי של התצפיות אל המישור, ככה שוקטור המקדמים בטא הוא בנורמה אחת כלומר הוא נורמל, והמרחק גדול או שווה לא לM אלא לM כפול קצת פחות מ1, כי הוא מוכפל פי 1 מינוס אפסילון i לכל תצפית. אנחנו כמובן גם חייבים לוודא שהאפסילונים הם חיוביים, וכאמור נחסום את הסכום שלהם באמצעות איזשהו פרמטר תקציב C שעכשיו אנחנו חייבים לספק, ותיכף נרחיב על זה. נשים לב גם שהמקסימום על המרג'ין הוא עכשיו גם על עוד וקטור של משתני סלאק אפסילון, כלומר נצטרך להגיע להחלטה כזאת לכל תצפית ותצפית.

ולמרות הכל, בשורה התחתונה המודל שנוצר בסאפורט וקטור קלאסיפייר, הוא בעל צורה זהה למודל של מקסימום מרג'ין קלסיפייר: אנחנו בסופו של דבר חוזים 1 אם תצפית מעל המישור ומינוס 1 אם היא מתחת.

:::

אז בואו נדבר קצת על הסלאק וריאבלז האלה:

בתרשים השמאלי, אין בהם צורך. כל התצפיות במרחק M לפחות, ולכן כל האפסילונים שווים אפס.

בתרשים האמצעי אנחנו מאפשרים שתי הפרות: יש שתי נקודות שנמצאות בתוך המרג'ין, אחת כחולה ואחת אדומה, כלומר יש שני אפסילונים חיוביים, והשאר אפס. יש לנו כאן דוגמא לדאטא שהוא ספרבילי אבל אם לא נאפשר קצת סלאק, נקבל את המרג'ין הצר והמאולץ שקיבלנו קודם, וכעת מתאפשר מרג'ין הרבה יותר יציב.

בתרשים הימני שהוא כבר המקרה הלא-ספרבילי, יש לנו כמה משתני סלאק שהם גדולים מאפס. כאן יש לנו אפילו נקודות שהן בצד הלא נכון של המישור (להדגים). בשביל הנקודות האלה האפסילונים שנמצאו הם גדולים מ1, הם יוצאים מעבר לחצי המרג'ין M.

אז נסכם, האפסילונים הם בסופו של דבר הכמות שבה התצפיות "מפירות" את המרג'ין:

אם אפסילון שווה אפס, התצפית בצד הנכון של המרג'ין, אין צורך בסלאק.

אם אפסילון גדול מאפס, היא בצד הלא נכון של המרג'ין. היא או נכנסת בתוכו או אפילו מחוצה לו, בצד הלא נכון של המישור.

אם אפסילון גדול מ1 אז בודאות התצפית בצד הלא נכון של המישור, אולי בתוך המרג'ין, אולי אפילו מחוצה לו.

והדרך שלנו לשלוט בכמה סלאק אנחנו מאפשרים היא דרך פרמטר C, אז נדבר עליו גם כן.

:::

אם C הוא אפס, יש לנו אפס סבלנות להפרה של המרג'ין, כל האפסילונים יהיו אפס, בעצם אנחנו חוזרים למקסימום מרג'ין קלסיפייר.

אם C גדול מאפס אנחנו כן מאפשרים סלאק, ויותר מזה C הוא חסם על מספר התצפיות שאנחנו מוכנים שיעברו את המישור, כי כל תצפית כזאת תקבל אפסילון יותר מ1.

כאן יש לנו את אותו סט של נתונים שחוזר על עצמו עם C שונים. אפשר לראות כשC התקציב נורא קטן אין כמעט הקצאה להפרה של המרג'ין, מתקבל מרג'ין צר מאוד שמתאים את עצמו מאוד לדאטא, עם מעט סאפורט וקטורז.
מנגד, כשC גדול מאוד, מקבלים מרג'ין עבה, שפחות נצמד לדאטא, ויש למודל הרבה מאוד סאפורט וקטורז שמשפיעים עליו.

אז איך נבחר איזה C לתת למודל? פשוט: נתייחס לC כאל היפרפרמטר, כאל איזה כפתור שאפשר לסובב וצריך לבדוק כמה לסובב אותו באמצעות שיטות שלמדנו כמו קרוס ולידיישן. וברור שכאן אנחנו רואים קשר מיידי לביאס-וריאנס טריידאוף. מומלץ לעצור רגע את ההקלטה ולחשוב לבד איך C משפיע על הטריידאוף.

עצרתם? אוקי, אז כמו שאפשר לראות כאן ככל שC גדול יותר, המרג'ין גדול ויציב כי הוא מתחשב בהרבה סאפורט וקטורז, במילים אחרות השונות נמוכה. אבל, הביאס יכול לגדול כי המישור שמצאנו כבר לא מתאים את עצמו לקו המאוד ספציפי שמקבלים בין הקלאסים. מצד שני כשC קטן המרג'ין מאוד צר, כל שינוי במעט סאפורט וקטורז שיש לו יכול מאוד לערער אותו, כלומר שונות גבוהה. אבל הביאס קטן כי אנחנו מתקרבים לקו הספציפי שמתאים עצמו הכי לנתונים.

:::

נסיים את הדיון בסאפורט וקטור קלסיפייר בזה שנזכיר שבדרך כלל אנחנו רואים צורה מעט שונה של הקריטריון.

קודם כל גם כאן ניתן להיפטר מM עצמו, חצי העובי של המרג'ין, אם פשוט קובעים שהנורמה של וקטור המקדמים בטא היא 1 חלקי M. העניין הוא שבקריטריון בלי M קצת קשה להבין את התפקיד של הסלאק וריאבלז, האפסילונים.

ולמעשה, הרבה פעמים תפגשו בכלל את הקריטריון של SVC בצורה כזאת: מינימום על הנורמה הריבועית של וקטור המקדמים בטא, כדי להפוך את הבעיה לקמורה, וC לא מופיע כפרמטר תקציב אלא כפרמטר עונש על סכום האפסילונים.

זה חשוב להבין את זה כי ככה גם המימוש בsklearn, ככל שC גדול אנחנו מענישים יותר את הסלאק וריאבלז ונראה מרג'ינים צרים יותר, וככל שהוא קטן אנחנו מענישים פחות את הסכום שלהם, נראה הרבה הפרות של המרג'ינים שיהיו עבים יותר.

עד כאן על סאפורט וקטור קלסיפיירז. ביחידה הבאה נוותר גם על דרישה הזאת שמישור מפריד בין הקלאסים, אלא כל יריעה אחרת, ונגיע לפרוצדורה הסופית שנקראת סאפורט וקטור מאשינז.

:::

4. Support Vector machines

אז מה הופך סאפורט וקטור קלסיפיירז לסאפורט וקטור מאשינז?

:::

למשל מה קורה אם אין מישור מפריד, עם או בלי סלאק, אבל כן יש יריעה מסוימת שמפרידה. במילים אחרות אם ההפרדה היא לא ליניארית.

ראינו דוגמה כזאת ממש כשדיברנו על הטריידאוף בין ביאס ווריאנס בקלסיפיקציה, שם תיארנו לנו שיש איזו פונקצית הסתברות להיות בקלאס 1 לעומת הקלאס מינוס 1, והנחנו שהיא משתנה בצורה חלקה כזאת במרחב של X. מה סאפורט וקטור קלסיפייר יכול לעשות במקרה כזה? מעט מאוד.

ולצערי דאטא אמיתי הרבה פעמים נראה ככה, עם תופעות לא-ליניאריות. אז אנחנו צריכים למצוא דרך להכליל את המתודה שלנו לעולם לא ליניארי.

:::

כשדיברנו על רגרסיה והמודל הליניארי הצענו אפשרות קלה ממש, וזה להוסיף גורמים פולינימיאליים.

כאן למשל, אני רושם את הבעיה שוב, רק שהפעם יש לי וקטור מקדמים בטא1 ועוד וקטור מקדמים בטא2 ששייך לכל הגורמים של X בריבוע. הנורמה של כל הוקטור צריכה להיות 1, וחוץ מזה אין שינוי. בעצם הגדלתי את המרחב של המישור שלי מp ל2p. אבל התוצאה תהיה כמו שנראה תיכף כלל החלטה לא-ליניארי במישור המקורי של האיקסים.

ומה עם גורמים בשלישית, ומה עם אינטראקציות, מסדר שני, שלישי וכולי?

כמו ברגרסיה אפשר להרחיב את X לאיזשהו מיפוי h(X) ולהמשיך כרגיל.

הבעיה היא למשל חישובית. בדרך כלל אם רוצים להתחשב למשל בכל האינטראקציות q נעשה גדול מאוד מהר מאוד, אולי אפילו יותר מכמות התצפיות. באופן מפתיע יש טריק שאפשר להפעיל שהוא אגב לא ייחודי למודל סאפורט וקטור, והוא מאפשר לנו לקבל את המודל בלי צורך לפרט ולחשב את כל הh(X) הארוך הזה.

:::

הטריק שלנו נקרא הקרנל טריק. והוא שהופך סאפורט וקטור קלסיפייר לסאפורט וקטור מאשין. באופן כללי יש עולם שלם של שיטות קרנל שלא נלמד, מי שרוצה יכול לקחת קורסים מתקדמים יותר או לקרוא על זה עוד בספרים, זה מאוד שימושי.

חוקרים הבחינו שבאופן מדהים אפשר לנסח את המודל שלנו לא בשפת המקדמים, בטא1 עד בטאp, אלא בשפת התצפיות, עם וקטור פרמטרים בגודל n שנסמן כאלפא.

ניקח תצפית חדשה X0. האלפות שלנו לא מכפילות את התצפית עצמה אלא את המכפלה הפנימית של התצפית עם תצפית Xi. כלומר אפשר לכתוב את המודל כך (להדגים).

וכאמור יש לנו פרמטר אלפא-איי לכל תצפית i, והנה עובדה מרתקת: אפשר להראות שבהכרח אלפא-איי גדול מאפס רק עבור אותן תצפיות שהן סאפורט וקטורז, שמגדירות את המרג'ין. אחרת, הוא אפס. כלומר בלי קשר לטריק הקרנל שתיכף נראה, המודל של סאפורט וקטור כולל צורה דואלית ברמת התצפיות שמראה ממש איך הוא תלוי רק בקבוצה מסוימת שלהן. כשבאה תצפית חדשה, מספיק לחשב את המכפלה הפנימית שלה עם כל אחד מהסאפורט וקטורז בלבד, להכפיל פי איזו משקולת אלפא-איי ולסכום. החיזוי של תצפית חדשה יקבע רק באמצעות היחס שלה לסאפורט וקטורז, התרומה של כל תצפית אחרת היא אפס.

בחזרה לקרנל: אם ככה, זה אומר שעבור כל הרחבה של X ממימד q שהוא אולי גדול הרבה יותר, עדיין אפשר לרשום את הפתרון כקומבינציה של מכפלות פנימיות, פשוט  המכפלות הפנימיות הן במרחב החדש הזה של h(X).

:::

בואו ניתן דוגמא: נניח שאני נמצא בעולם עם שני משתנים, דו-מימדי, וההרחבה שלי פולינומית ואכן כוללת חותך, את המשתנים עצמם, את המשתנים בריבוע, ואינטראקציה בין שני המשתנים. כלומר בסך הכל עברתי למרחב ממימד 6. נכון שאני מוסיף גורמים של שורש 2 אבל זה לא משנה דבר ותיכף נבין למה אני עושה את זה.

איך נראית המכפלה הפנימית בין ההרחבה h של תצפית חדשה x0 לתצפית קיימת במדגם הלמידה x1? האמת שדי פשוט:

אם תבצעו את המכפלה הפנימית ותסדרו את האיברים, תגלו שקיבלתם ביטוי קטן ופשוט בשפה המקורית של האיקסים: 1 ועוד המכפלה הפנימית של וקטור המשתנים המקורי Xi כפול X0, כל זה בריבוע.

במה זה עוזר לנו? זה עוזר לנו כי המכפלה הפנימית היא כל מה שאנחנו צריכים, וגילינו שאנחנו יכולים לעשות אותה במימד המקורי של X, המימד הנמוך יותר שבו צריך להכפיל שני וקטורים באורך 2, לא באורך 6.

:::

אז בואו נקרא לפונקציה הזאת קרנל, נסמן אותה כפונקציה K של שני וקטורים או שתי תצפיות במרחב המקורי של X.

ואם נחזור למודל שלנו של המישור שמפריד בין הקלאסים במרחב של h אנחנו רואים שאפשר להציב את הקרנל הזה במקום המכפלה הפנימית של הhים ולקבל ביטוי פשוט יותר.

אז מה הטריק?

הטריק הוא שמאוד יכול להיות שאני לא צריך לחשב בכלל את h, ויותר מזה אני לא צריך לפרט אותה!

במקום זה אני מפרט פונקצית קרנל כלשהי בין התצפיות, שעבור כל שתי תצפיות בRp אומרת לי כמה הן דומות בעצם.

זה הרבה יותר קל לחשוב ככה, ובאמת יש כל מיני פונקצית קירבה בין תצפיות שמסתתרת מאחוריהן פיצ'ר מאפ h מאוד מאוד מעניינת, ממימד אולי מאוד גבוה, שמאפשרת מודל מאוד מאוד גמיש בלי לחשב אותה.

:::

הנה כמה פונקציות כאלה או קרנלים כמו שנהוג לקרוא להם:

לדוגמא הקרנל הליניארי -- מידת הדמיון בין שתי תצפיות היא המכפלה הפנימית ביניהן. זה כמובן מחזיר אותנו למודל המקורי, לסאפורט וקטור קלסיפייר. ולמה שהמכפלה הפנימית בין זוג וקטורים תהיה מידה לקרבה ביניהם? כי זה בדיוק קוסינוס הזווית בין שני הוקטורים, עד כדי קבוע, למי שזוכרים.

יש את הקרנל הפולינומיאלי שכרגע ראינו דוגמה שלו, באופן כללי אפשר להעלות את המכפלה הפנימית של האיקסים בחזקת d וזה מבטיח שיש לנו פיצ'ר מאפ h שכוללת את כל החזקות והאינטראקציות עד מקדם d כולל.

קרנל מאוד פופולרי ושימושי הוא הקרנל הגאוזייני או סקוורד אקספוננשיאל, או RBF. כאן תשימו לב שככל שהתצפיות דומות המרחק ביניהן קטן ומקבלים אקספוננט בחזקת אפס במקסימום, 1. למה זה קרנל מאוד שימושי? כי הוא מאפשר גבולות החלטה מעוגלים, שמאחוריהם עומדת פיצ'ר מאפ h שהיא ממימד אינסופי! שלא צריך לפרט או לחשב! לא נוכיח את זה כאן כמובן.

מכל מקום מובטח לכם שמאחורי כל אחד מהקרנלים האלה עומדת מכפלה פנימית של פיצ'ר מאפס שמחושבות רק באופן אימפליסיטי.

והרבה פעמים אנחנו לא נדבר על פונקצית קרנל אלא על מטריצת קרנל או מטריצת גרם שמחושבת על כל מדגם הלמידה בגודל n על n. כל איבר ij במטריצה הוא הוא הקרנל בין תצפיות i וj.

שאלה מתבקשת היא כמובן, מה אני יכול להשתמש בכל פונקצית דמיון שבא לי? והתשובה היא כמובן שלא. הרי כל איבר צריך להיות מכפלה של איזושהי פיצ'ר מאפ h. נגיד שקריטריון מספיק לקבוע שזה אכן כך הוא שK היא מטריצה סימטרית חיובית למחצה, ונשאיר לכם אולי בתרגיל לברר למה זה נכון.

:::

אז השילוב הזה של סאפורט וקטור קלאסיפיירז והטריק של קרנל הוא שמביא אותנו לסאפורט וקטור משינז.

הנה הביצועים של המודל החזק הזה על הדאטא הקשה לקליספיקציה שהטריד אותנו. אפשר לראות שגם קרנל RBF וגם קרנל פולינמיאלי נגיד מדרגה 6, מצליחים במשהו לקלוט את גבול ההחלטה העקום והלא רציף הזה. הקרנל הRBF קצת יותר.

ורק נזכיר שמודל כל כך חזק לא בא בלי מחיר לשלם. מחיר אחד למשל אתם רואים כאן, לכל קרנל נוספים פרמטרים שצריך לבחור, כנראה עם קרוס ולידיישן. בקרנל הRBF צריך לבחור את הרזולוציה גאמא, בקרנל הפולינומיאלי צריך לבחור את הדרגה d. ויותר מזה צריך לבחור את הקרנל עצמו. בדרך כלל הבחירה היא בין כמה קרנלים נפוצים יותר, אבל אתם יודעים כמה קרנלים פותחו עם השנים? עשרות.

:::

מחיר נוסף שאנחנו משלמים עם מודל כל כך חזק הוא הסיכוי לאוברפיטינג. אפשר לראות את זה כאן כשאנחנו מפעילים SVM על נתונים אמיתיים, הנתונים שהצגנו על מדגם של פציינטים שחלקם קיבלו התקף לב וחלקם לא, ואנחנו רוצים לחזות על מדגם טסט, על פציינטים חדשים. כאן אני משווה על מדגם הטסט, על חולים שהמודל לא ראה, בין רגרסיה לוגיסטית לבין SVM עם קרנל ליניארי, קרנל פולינומיאלי מדרגה 2 וקרנל RBF. ואפשר לראות מהAUC גם שהקרנלים הכי פשוטים מגיעים לתוצאות הכי טובות, וגם שרגרסיה לוגיסטית מגיעה בשורה התחתונה לביצועים דומים מאוד לSVM עם קרנל ליניארי שהוא בעצם SVC כמו שצפינו.

חדי העין בטח שואלים את עצמם רגע, איך אני בכלל מצייר עקומת ROC למודל SVM, הוא הרי מחזיר חיזויים של 1 או מינוס 1, ואנחנו צריכים איזו הסתברות או לפחות איזשהו סקור כדי לצייר ROC, עם קטאופים שונים. אז זה כבר קצת מחוץ לסקופ שלנו, אבל יש דרך לחלץ סקור ואפילו הסתברות ממודל SVM, לכל הפחות ודאי אפשר להתייחס למרחק של כל תצפיות מהמישור שנוצר כאיזושהי מטריקה של קונפידנס. ככל שתצפית רחוקה יותר מהמישור, כך המודל בטוח לגביה שהיא מקלאס אחד ולא מאחר.

ומה עם יותר מקלאס אחד? ומה עם רגרסיה? נגיד גם על זה כמה מילים בחלק האחרון של היחידה.

:::

=== 4. הרחבות לSVM ===

ניגע כעת קצת בהרחבות של SVM ליותר משני קלאסים ולרגרסיה. ההרחבה של SVM ליותר משני קלאסים היא יחסית קלה ומתאימה לא רק לSVM אלא לכל מודל שיודע לעשות הפרדה לשני קלאסים בלבד. ההרחבה של SVM לרגרסיה היא מעט יותר מתוחכמת ולא נוכל להיכנס לכל הפרטים והדקויות, אבל בודאי תבינו כיצד היא מתבצעת.

:::

אז איך מרחיבים SVM ליותר משני קלאסים -- איך שמרחיבים כל מודל בינארי ליותר משני קלאסים.

דרך אחת היא וואן-ורסוז-וואן או OVO. נריץ SVM להפרדה בין כל זוג קלאסים K וK טאג. סך הכל נריץ SVM K מעל 2 פעמים. וכשמגיעה תצפית חדשה נעביר אותה בכל המודלים ונסווג אותה אל הקלאס אליו היא שויכה הכי הרבה פעמים, לעומת הקלאסים האחרים.

הדרך השניה היא וואן-ורסוז-רסט או OVR.
נחלק את הדאטא לקלאס הk מול כל שאר התצפיות ונריץ SVM. נעשה את זה לכל קלאס וקלאס, בסך הכל נריץ SVM K פעמים.
וכשמגיעה תצפית חדשה, נעביר אותה בכל המודלים, ונסווג אותה אל הקלאס שבמודל שלו היא הראתה הכי הרבה קונפידנס שהיא שייכת אליו, כלומר כמו שראינו הכוונה היא למודל עם המישור שממנו התצפית הכי רחוקה.

:::

ומה עם רגרסיה? איך נבטא רעיון דומה של מישור שמבטא באמצעות איזשהו מרג'ין ממנו את הקשר הכי טוב בין משתנה מסביר X לY רציף?

נתחיל בזה שמישור הוא בדיוק מה שאנחנו מחפשים תמיד ברגרסיה, למשל עם משתנה אחד מדובר בקו. ונגדיר שאנחנו רוצים מישור עם מרג'ין M סביבו (להדגים), שכל התצפיות נכנסות בו, בלוח הזה. זה מאוד הגיוני, כי ככל שהמרג'ין קטן יותר זה אומר שהפיזור סביב המישור קטן יותר וזו התאמה טובה יותר. ככל שהוא גדול יותר כך הפיזור גדול ואנחנו פחות בטוחים במודל, בדומה לרגרסיה.

עכשיו זה כמובן קריטריון מחמיר מאוד ומושפע בקלות מתצפיות קיצוניות, ולכן גם כאן אנחנו עושים רילקסציה ומאפשרים משתני סלאק לתצפיות מסוימות, שיוצאות מהמרג'ין. אפשר להגדיר בדיוק כמו מקודם שיש לנו איזשהו תקציב C לגודל הסלאק או מספר התצפיות שאנחנו מוכנים שיצאו מהמרג'ין ולהמשיך בצורה דומה מאוד לSVM.

עכשיו נאמר והשגנו את זה כמו בתרשים שלפנינו -- אילו נקודות הן הסאפורט וקטורז, שרק בהן המודל הסופי בעצם תלוי? כאן אלה דווקא התצפיות מחוץ למרג'ין, שאם הן יזוזו המישור והמרג'ין יכולים מאוד להשתנות. זה קצת שונה מSVM אבל אם תחשבו על זה זה ממש אותו עקרון -- על תצפיות שבתוך המרג'ין אנחנו לא משלמים כלום, העיקר שהן רחוקות עד כדי M מהמישור. אנחנו משלמים הרבה על התצפיות שמחוץ למרג'ין, לכן הן הסאפורט וקטורז.

:::

פורמלית אפשר בשקף אחד להראות את כל המסע שלנו בדרך לSVM רק ברגרסיה, המסע לSVR.

בקלסיפיקציה היה לנו את הקריטריון המחמיר של מקסימום מרג'ין קלסיפייר.

ברגרסיה אפשר לחשוב על קריטריון מקביל, להביא למקסימום את המרג'ין, כך שוקטור המקדמים הוא וקטור יחידה, ונדאג שכל תצפית רחוקה מהמישור, כלומר מהחיזוי שלה עד כדי המרג'ין M. שימו לב להבדל כאן בין קלסיפיקציה לרגרסיה.

אחר-כך הרשינו קצת סלאק והגענו לקריטריון של סאפורט וקטור קלסיפייר.

ואפשר לחשוב על קריטריון מקביל ברגרסיה: נרצה את המקסימום מרג'ין כך שכל תצפית רחוקה לא יותר מM מהחיזוי שלה, ונוסיף משתני סלאק גדולים מאפס, לחלק מהתצפיות אנחנו מרשים להיות רחוקות מעבר למרג'ין. ועל משתני הסלאק שולט פרמטר של תקציב C.

בפועל, כמו בקלסיפיקציה תראו יותר בספרות קריטריון שנראה כך: מינימום על הנורמה של המקדמים בריבוע, כדי לעשות את זה בעיה קמורה, ועוד פרמטר פנאלטי על סכום משתני הסלאק. גם בעית אופטימיזציה שיש לה פתרונות סטנדרטיים. כאן, רק נשים לב שלא נפטרנו מפרמטר הM שמייצג את המרג'ין, נהוג לפרט גם אותו למשל במימוש של sklearn.

וגם למודל של SVR ניתן לעשות קרנליזציה ולהתחשב בקרנלים שונים.

נעצור כאן, כמו שאמרנו יש עוד הרבה יותר מה להגיד על SVR, ואפשר להראות שגם הוא וגם SVM עושים מינימום לפונקציות לוס מעניינות מאוד, מי שמעוניין בכך יכול לקרוא על כך עוד בספרים.

עד כאן SVM, אחד המודלים האלגנטיים ביותר בלמידת מכונה שיודע גם לטפל בדאטא ממימד גדול מאוד ולכן גם היה אופנתי מאוד ככל שהנתונים נעשו גדולים ועד שהגיעו רשתות הנוירונים.

:::
