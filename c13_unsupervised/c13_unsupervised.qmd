---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Unsupervised Learning"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Unsupervised Learning - Class 13

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Intro. to Unsupervised Learning {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
את השיעור האחרון אנחנו בוחרים להקדיש ללמידה בלתי מפוקחת, כשהמטרה שלנו היא לא בהכרח לחזות איזשהו משתנה מוסבר Y, אלא לתאר את ההתפלגות של הנתונים עצמם. נדבר בקצרה על אלגוריתם Kmeans כדי לעשות קלאסטרינג, ואז נחזור לאלגוריתם PCA כדי לבצע הורדת מימד ונתאר אותו בפירוט יותר, וגם שיטה דומה שמבצעת הורדת מימד בצורה לא ליניארית - טיסני. אבל לפני זה נדבר באופן כללי מה זה למידה בלתי מפוקחת או unsupervised.
:::
:::

---

### From Supervised to Unsupervised

- Recall: each observation is made of a vector $x \in \mathcal{X}$ (for example $x \in \mathbb{R}^p$) and a scalar $y$

- Our goal is to build a model of the relationship between $x$ and $y$:
$$y \approx f(x)$$

- IID assumption: each pair $(x_i, y_i)$ is drawn indepednently from some distribution $P_{x,y}$

- A modeling approach takes $(X, y)$ as input and outputs a *prediction model* $\hat{f}(x)$

- In prediction: we get a new value $x_0$ and predict $\hat{y}_0 = \hat{f}(x_0)$. 

- How good is our prediction? We typically define a loss function $L(y,\hat{y})$ and the quality of the model is $\mathbb{E}_{x_0,y_0}(L(y_0, \hat{y}_0))$

::: {.fragment}
What if there is no $y$?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בלמידה מסוג supervised, יש לנו וקטור X של p משתנים, וסקלאר Y. המטרה היא למדל את Y כפונקציה f של X. אנחנו מניחים שזוגות התצפיות X, Y שלנו מגיעים בלתי תלויים מאיזושהי התפלגות משותפת Pxy, ובונים מודל לחיזוי באמצעות נתוני מדגם הלמידה X, Y, מודל שנקרא f_hat. כשתגיע תצפית חדשה לחיזוי X0 נפעיל עליה את המודל הנלמד f_hat וזה יהיה החיזוי שלנו עבורה. ואיך אנחנו מכמתים את הביצועים של המודל שלנו? באידאל באמצעות איזושהי פונקצית הפסד L בין תצפיות Y האמיתיות והחזויות, כשאנחנו לוקחים תוחלת על תצפיות שהמודל לא ראה. בפועל אנחנו לא יודעים את ההתפלגות של התצפיות שהמודל לא ראה ואנחנו לוקחים את הממוצע האמפירי על מדגם הטסט.

נשאלת השאלה, מה אם אין Y, המשתנה התלוי לחיזוי?
:::
:::

---

### Unsupervised Learning

- Now: each observation is made of a vector $x \in \mathcal{X}$ (for example $x \in \mathbb{R}^p$)

- IID assumption: each observation $x_i$ is drawn indepednently from some distribution $P_{x}$

- Our goal is to *learn* distrubution $P_{x}$ (or properties of it)

- "without a supervisor"

::: {.incremental}
- Example: [Clustering]{style="color:red;"} = Finding modes of $P_{x}$ with high density
    - If we do find them, maybe $P_{x}$ can be represented by a mixture of simpler densities?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בלמידה לא מפוקחת יש לנו רק וקטור של משתנים X ממימד p. אנחנו עדיין מניחים שהתצפיות מגיעות בלתי תלויות מאיזו התפלגות לא-ידועה Px, והמטרה שלנו היא לא לאמוד איזושהי פונקציה או קשר אלא ממש את ההתפלגות הזאת, או תכונות שלה.

לדוגמא, ניתוח אשכולות או קלאסטרינג -- היינו רוצים ללמוד איזורים בהתפלגות עם צפיפות גבוהה, או השכיחים של P_x. אם נמצא שP_x מתחלקת בבירור לכמה איזורים כאלה למשל, אולי ניתן לייצג אותה בעזרתם, ואז זה יפשט אותה, במקום להיות למשל פונקציה מורכבת בהרבה מימדים נוכל לחלק אותה לצירוף של כמה פונקציות פשוטות יותר.

יש המון שיטות לבצע קלאסטרינג, נראה עכשיו נציג בולט אחד שלהן, שנקרא Kmeans clustering.
:::
:::

---

## K-means Clustering {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נדבר כעת על אלגוריתם KMeans, אולי המוכר ביותר בתחום של קלאסטרינג, וגם יעיל יחסית ועובד טוב עם נתונים ממימד גבוה.
:::
:::

---

### How to evaluate a partition?

- Assume $K$ clusters are given

- $C(i) = k$ is some function assigning cluster $k \in \{1, \dots, K\}$ to observation $i \in \{1, \dots, n\}$

- $d(x_i, x_j)$ is a distance metric for pair $i, j$, e.g. Euclidean

- We wish to minimize the extent to which observations assigned to the same cluster tend to be close to one another

::: {.incremental}
- The "within cluster" scatter/loss:
$$W(C) = \frac{1}{2}\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) = k} d(x_i, x_j)$$

- equivalent to maximizing $B(C) = \frac{1}{2}\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) \neq k} d(x_i, x_j)$

- Can we go over all possible $C(i)$ to find the global minimum?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נניח שיש לנו חלוקה נתונה. paritition. איך אנחנו עושים איבליואציה של חלוקה כזאת על הנתונים שלנו.

נניח שאנחנו יודעים כבר את מספר הקלאסטרים בדאטא ומסמנים אותו בK, ונניח שיש לנו כבר כלל חלוקה C שמתאים לכל תצפית i בנתונים את הקלאסטר k שמתאים לה, מ1 עד K גדול.

נניח כעת שצפיפות של נתונים או עד כמה זוג תצפיות קרובות אחת לשניה נמדוד באמצעות איזושהי מטריקת מרחק d, לדוגמא נתחיל עם מרחק אוקלידי.

ואנחנו רוצים לבטא באמצעות איזושהי מטריקה את האינטואיציה שלנו שתצפיות באותו קלאסטר צריכות להיות צפופות ורחוקות מתצפיות בקלאסטרים אחרים.

הרבה אלגוריתמים מסתכלים על המטריקה הבאה, הפיזור within cluster שנסמן בW(C), והוא סכום על כל זוגות התצפיות ששייכות לקלאסטר k של המרחקים ביניהן, וסכום על כל הקלאסטרים, מוכפל פי חצי כי אנחנו סופרים כל זוג ככה פעמיים.

מאחר שהפיזור בין כל זוגות התצפיות בלי קשר לחלוקה לקלאסטרים נשאר זהה, אפשר להראות שלעשות מינימום לקריטריון שלנו אקוויולנטי ללעשות מקסימום לכמות המשלימה של between clusters סקאטר: סכום המרחקים בין כל זוגות התצפיות שנמצאות בקלאסטרים שונים. הרי סכום הכמות הזאת והכמות שלנו W(C) מסתכם בפיזור כללי נאמר T(C).

אז יש לנו מטריקה לעשות לה מינימום. אבל אפילו עם K נתון, נאמר שאנחנו רוצים לחלק את הדאטא ל4 קלאסטרים. האם אנחנו יכולים לעבור על כל החלוקות C(i) האפשריות כדי להגיע למינימום גלובלי? ברור שלא. יש לזה נוסחה שלא מופיעה כאן, אפשר לחשב למשל שלעבור על כל האפשרויות של חלוקת 20 תצפיות ל5 קלאסטרים אנחנו כבר מדברים על כמעט 750 מיליארד אפשרויות!
:::
:::

---

### Road to K-means

- Euclidean distance: $d(x_i, x_j) = \sum_{m=1}^p (x_{im} - x_{jm})^2 = ||x_i - x_j||^2$

- Can show that:
$W(C) = \frac{1}{2}\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) = k} ||x_i - x_j||^2 = \sum_{k = 1}^K n_k \sum_{C(i) = k} ||x_i - \bar{x}_k||^2$

- $\bar{x}_k \in \mathbb{R}^p$ being the mean in cluster $k$, and $n_k$ number of observations in cluster $k$

::: {.incremental}
- But for any set of observations $S$, which $m$ would minimize $\sum_{i \in S} ||x_i - m||^2$?

- Thus, the final goal of K-means:
$$\min\limits_{C, m_1, \dots, m_K} \sum_{k = 1}^K n_k \sum_{C(i) = k} ||x_i - m_k||^2$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז Kmeans קודם כל מצמצם אותנו למרחק אוקלידי בלבד.

תחת מרחק אוקלידי, מסתבר שאפשר לרשום את הקריטריון שלנו בצורה פשוטה יותר: בכל קלאסטר סכום המרחקים מהתצפיות אל ממוצע הקלאסטר, להכפיל במספר התצפיות בקלאסטר n_k, ולסכום על כל הקלאסטרים.

אבל אנחנו יודעים כבר שאם נסתכל על קריטריון דומה, ונשאל מה הנקודה שמביאים למינימום את סכום המרחקים המרובעים ממנה -- נגיע לממוצע המדגם.

לכן נהוג לרשום את הקריטריון של Kmeans בצורה כוללת יותר: למצוא את החלוקה ואת הנקודות של קלאסטרים שיביאו למינימום את סכום המרחקים המרובעים בתוך כל קלאסטר, על פני כל הקלאסטרים. צורת הרישום הזאת מסייעת לנסח את האלגוריתם של Kmeans.
:::
:::

---

### K-means

0. Start with initial guess for $m_1, \dots, m_K$

1. Assign each observation to the closest cluster mean. That is:
$$C(i) = \arg\min\limits_{k = 1,\dots, K} ||x_i - m_k||^2$$

2. Update means $m_1, \dots, m_K$. That is the centroids:
$$m_k = \frac{\sum_{C(i) = k}x_i}{n_k}$$

3. Repeat 1 and 2 until $C(i)$ doesn't change

::: {.incremental}
- Convergence is guaranteed (steps 1 and 2 can only reduce $W(C)$)

- Global optimum is NOT guaranteed

- Can try many different initial starting points
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז מהו האלגוריתם של Kmeans?

K נתון, ואנחנו מתחילים עם איזשהו ניחוש התחלתי עבור הממוצעים m1 עד mk.

כעת החלוקה C(i) של כל תצפית תהיה לפי הקלאסטר שהממוצע שלו הוא הקרוב אליה ביותר.

לאחר החלוקה נעדכן את הממוצעים, בכל קלאסטר ניקח את הממוצע של התצפיות ששייכות אליו.

ונחזור על צעדים 1 ו2 עד שהחלוקה לא משתנה או עד איזשהו קריטריון התכנסות, למשל אפשר לחשב את הלוס שלנו W(C) ולראות שהוא לא משתנה יותר מדי.

מה הבעיה הראשונה באלגוריתם? אמנם התכנסות מובטחת, הצעדים שלנו יכולים רק להפחית את W(C), או לא לשנות אותו. אבל אין הבטחה למינימום גלובלי ואנחנו מאוד תלויים בבחירה הראשונית של הממוצעים שיכולה להיות אקראית.

נהוג לכן בהרבה מימושים לבצע מספר פעמים Kmeans כל פעם מנקודת התחלה אקראית אחרת ולבחור את הפתרון עם הלוס המינימלי.
:::
:::

---

### K-means Demo: Initial Guess

```{python}
#| echo: false

# inspired by https://jakevdp.github.io/PythonDataScienceHandbook/06.00-figure-code.html

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
from sklearn.metrics import pairwise_distances_argmin

X, y_true = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.60, random_state=0)

rng = np.random.RandomState(42)
centers = [0, 4] + rng.randn(4, 2)

def draw_points(ax, c, factor=1):
    ax.scatter(X[:, 0], X[:, 1], c=c, cmap='viridis',
               s=50 * factor, alpha=0.3)
    
def draw_centers(ax, centers, factor=1, alpha=1.0):
    ax.scatter(centers[:, 0], centers[:, 1],
               c=np.arange(4), cmap='viridis', s=200 * factor,
               alpha=alpha)
    ax.scatter(centers[:, 0], centers[:, 1],
               c='black', s=50 * factor, alpha=alpha)

def make_ax(fig, gs):
    ax = fig.add_subplot(gs)
    ax.xaxis.set_major_formatter(plt.NullFormatter())
    ax.yaxis.set_major_formatter(plt.NullFormatter())
    return ax
```

```{python}
#| echo: false

fig = plt.figure(figsize=(4.55, 5))
ax = fig.add_subplot()
draw_points(ax, 'gray', factor=2)
draw_centers(ax, centers, factor=2)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בדוגמא שלפנינו ברור שיש 4 קלאסטרים, ואנחנו מתחילים עם 4 נקודות אקראיות כממוצעים, מאוד לא מתאימות לחלוקה.
:::
:::

---

### K-means Demo: Iteration 1

```{python}
#| echo: false

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))

# E-step
y_pred = pairwise_distances_argmin(X, centers)
draw_points(axes[0], y_pred)
draw_centers(axes[0], centers)

# M-step
new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])
draw_points(axes[1], y_pred)
draw_centers(axes[1], centers, alpha=0.3)
draw_centers(axes[1], new_centers)
for i in range(4):
    axes[1].annotate('', new_centers[i], centers[i],
                    arrowprops=dict(arrowstyle='->', linewidth=1))
    

# Finish iteration
centers = new_centers
_ = axes[0].text(0.95, 0.95, "Assign", transform=axes[0].transAxes, ha='right', va='top', size=14)
_ = axes[1].text(0.95, 0.95, "Update", transform=axes[1].transAxes, ha='right', va='top', size=14)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בצעד 1 אנחנו מחלקים כל תצפית לקלאסטר עם הממוצע הכי קרוב אליה במרחק אוקלידי, כאן זה אומר לצבוע אותן ב4 צבעים שונים.

בצעד 2 אנחנו מעדכנים את הממוצעים, וכבר ניתן לראות איך כל ממוצע מייצג כבר איזור הרבה יותר צפוף באופן טבעי.
:::
:::

---

### K-means Demo: Iteration 2

```{python}
#| echo: false

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))

# E-step
y_pred = pairwise_distances_argmin(X, centers)
draw_points(axes[0], y_pred)
draw_centers(axes[0], centers)

# M-step
new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])
draw_points(axes[1], y_pred)
draw_centers(axes[1], centers, alpha=0.3)
draw_centers(axes[1], new_centers)
for i in range(4):
    axes[1].annotate('', new_centers[i], centers[i],
                    arrowprops=dict(arrowstyle='->', linewidth=1))
    

# Finish iteration
centers = new_centers
_ = axes[0].text(0.95, 0.95, "Assign", transform=axes[0].transAxes, ha='right', va='top', size=14)
_ = axes[1].text(0.95, 0.95, "Update", transform=axes[1].transAxes, ha='right', va='top', size=14)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ושוב אנחנו מחלקים את התצפיות לפי הקרבה שלהן לממוצע החדש.

ושוב אנחנו מעדכנים את הממוצעים. כאן הם כבר זזים ממש מעט.
:::
:::

---

### K-means Demo: Iteration 3

```{python}
#| echo: false

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))

# E-step
y_pred = pairwise_distances_argmin(X, centers)
draw_points(axes[0], y_pred)
draw_centers(axes[0], centers)

# M-step
new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])
draw_points(axes[1], y_pred)
draw_centers(axes[1], centers, alpha=0.3)
draw_centers(axes[1], new_centers)
for i in range(4):
    axes[1].annotate('', new_centers[i], centers[i],
                    arrowprops=dict(arrowstyle='->', linewidth=1))
    

# Finish iteration
centers = new_centers
_ = axes[0].text(0.95, 0.95, "Assign", transform=axes[0].transAxes, ha='right', va='top', size=14)
_ = axes[1].text(0.95, 0.95, "Update", transform=axes[1].transAxes, ha='right', va='top', size=14)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
באיטרציה השלישית כבר בקושי אפשר לראות הבדל, הקלאסטרים כבר ברורים מאוד.
:::
:::

---

### Some issues with K-means

::: {.incremental}
- Limited to Euclidean distance

- Need to always specify $K$!

- How to choose $K$?

- Prefers separable spherical clusters (Gaussian)
  - Bad with unequal densities, unequal cluster sizes

- No concept of outliers
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כבר תוך כדי הדיון אפשר היה להבחין בכמה בעיות של Kmeans, למשל שאין גרנטי למינימום גלובלי על המטריקה W(c).

מגבלה אחרת היא שכל הפיתוח שלנו התבסס על מרחק אוקלידי, אין אפשרות לעשות פלאג-אין בKmeans לפונקצית מרחק אחרת, שאולי היא מתאימה יותר, למשל אם המשתנים שלנו הם קטגוריאליים.

אבל אולי מגבלה בולטת יותר היא העובדה שצריך לספק לאלגוריתם את K, לדעת מראש כמה קלאסטרים יש בדאטא.

ואם ככה זה אומר שצריך לדעת איך לבחור את K, כשהבעיה היא שברור שככל שK גדול המטריקה של סכום הריבועים within-cluster יורדת. עד המצב הקיצוני שבו K = n ואז כל תצפית היא קלאסטר בפני עצמו והמטריקה תהיה אפס. אז כאן יש כל מיני סטטיסטים ויש את השיטה הישנה והטובה של לצייר את W(c) כנגד ערכי K שונים, פשוט לבצע את האלגוריתם נאמר על 5 פולדים ועבור ערכי K שונים, ולראות אם עבור ערך K מסוים כבר אפקטיבית אין ירידה, לראות אם יש איזשהו מרפק בגרף. אם יש כזה אפשר לבחור בK בו יש מרפק.

הבעיה האחרת של Kmeans היא שהוא מאוד טוב בחלוקות יפות לקלאסטרים מעוגלים, יפים, שווי גודל, כאילו כל אחד מהמשתנים בX הגיע מהתפלגות נורמלית. הוא מבצע הרבה פחות טוב על קלאסטרים עם גדלים שונים, עם צפיפות שונה.

עוד דבר בKmeans שחייב להיות ברור לנו זה שאין לו מושג בילט-אין של אאוטליירז. Kmeans יחזיר תשובה לכל תצפית, גם אם ברור לנו בעין שהיא לא שייכת לאף אחד מהקלאסטרים. כאן לKmeans יש הרבה מתחרים מצוינים שלהם דווקא יש קונספט של אאוטלייר, כמו DBSCAN, שכן יכול להחזיר תשובה כזאת לתצפית - לא מצאתי אף קלאסטר.
:::
:::

---

## Dimensionality Reduction {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ביתר השיעור נדבר על הורדת מימד. נתחיל בPCA, שהיא השיטה המוכרת ביותר וליניארית, ונעבור אחר כך לטיסני, שהיא שיטה לא ליניארית. PCA היא שיטה כל כך מוכרת שאפילו בקורס שלנו נתקלנו בה כשדיברנו עליה בהקשר של למידה כן מפוקחת. שם רצינו להוריד מימד לנתונים לפני ביצוע רגרסיה ליניארית על הנתונים במימד הקטן יותר, וקראנו לזה PCR. נראה עכשיו איך מגיעים לפתרון של PCA, אבל לפני זה נזכיר שוב בשביל מה להוריד מימד.
:::
:::

---

### Dimensionality Reduction 

::: {.incremental}
- We have $n$ observations in $p$ dimensions: $X_{n \times p}$
- Why would we want to reduce the data dimensionality to $q \ll p$ dimensions?
  - EDA:
    - Visualize the data (2-d or 3-d visualizations)
    - Identify important dimensions which summarize the data well
  - Speed-up/Improve/Enable machine-learning algorithms (PCR)
  - Clustering after dimensionality reduction
  - Generative modeling - see later

- Naive way: select $q$ out of the original $p$ dimensions (best subset)

- Less Naive way: Look for interesting "projections":
  - linear/non-linear combinations of features
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
יש לנו מטריצת נתונים עם n שורות ו-p עמודות. p בדרך כלל גדול מאוד נאמר לפחות 10 משתנים, ולפעמים אלפי ואפילו מיליון משתנים בדאטה מודרני כמו למשל בגנטיקה.

אנחנו רוצים להוריד מימד מp לq, כשq הרבה יותר קטן.

מטרה ראשונה היא אקספלורטורי דאטא אנליסיס, או EDA. החל מויזואליציה של הנתונים, שלא אפשרית כשp הוא מאוד גדול, ועד התבוננות על פתרון PCA עצמו לראות אם יש צירופים של המשתנים שממצים את הדאטא היטב. פסיכולוגים למשל מבצעים הרבה פעמים PCA על שאלוני אישיות כדי לזהות קונסטרקטים או תמות בתשובות של אנשים.

סיבה אחרת להורדת מימד היא כדי לבצע אלגוריתמים של למידה על הדאטא ממימד נמוך. זה יכול להיות בתקווה לשפר את הביצועים של המודל, כמו שראינו בPCR. זה יכול להיות כי במימד הנוכחי האלגוריתם לא יכול בכלל לפעול.

סיבה אחרת שקצת קשה אולי לראות כרגע, היא שימוש במודל שהרצנו להורדת מימד להעלאה בחזרה של המימד, כדי ליצור דאטא חדש. נגענו בזה כשדיברנו על מודלים גנרטיביים, שמתעניינים בהתפלגות של Y בהינתן הנתונים X. נתעניין בגישה הזאת גם כאן בסוף היחידה.

אז איך להוריד מימד? עשינו את זה כבר. בגישה הנאיבית יותר פשוט בחרנו או ידנית או על פי קריטריון מסוים תת-קבוצה, למשל ברגרסיה של משתנים "חשובים".

שיטה פחות נאיבית היא להשאיר את כל המשתנים, אבל לאחד אותם על ידי כמה פונקציות לכדי כמה אאוטפוטים בודדים חשובים. אני מדבר על הטלה או פרוג'קשן של הדאטא למימד נמוך יותר, והטלה כזאת יכולה להיות ליניארית או לא-ליניארית. נתחיל בהטלה ליניארית.
:::
:::

---

### A non-standard motivation (I)

::: {.incremental}
- We are given:
  - An [encoder]{style="color:red;"} $g(X) = Xw$, where $w \in \mathbb{R}^{p \times 1}$ is a vector with $\|w\|=1$
  - A [decoder]{style="color:red;"} $f(u) = uw^T$, where $u \in \mathbb{R}^{n \times 1}$
- The reconstructed matrix is therefore: $\hat{X} = f(g(X)) = (Xw)w^T = Xww^T$
- Goal: find $w$ that minimizes the [reconstruction error]{style="color:red;"} $\|X - \hat{X}\|^2_F$
  - $\|A\|^2_F = \sum_{i = 1}^m \sum_{j = 1}^n = a_{ij}^2$ is the squared Frobenius norm, the sum of squared elements of any real matrix $A$
  - Also: $\|A\|^2_F = \text{Tr}(AA^T)$
  - $w = \arg\min_{w: \|w\|=1}\text{Tr}((X - Xww^T)(X - Xww^T)^T)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל בגישה קצת לא סטנדרטית לבעיה של הורדת מימד. נניח שיש לנו מקודד לנתונים שלנו, פונקציה g שמורידה את הנתונים שלנו ממימד p למימד 1. נניח אפילו שהפונקציה הזאת ליניארית, כלומר היא פשוט לוקחת את הדאטא שלנו עם פי פיצ'רים ומכפילה אותו בוקטור עם נורמה 1 שנקרא לו w. למקודד הזה אנחנו קוראים אנקודר.

לעומתו יש לנו את הדקודר, שהוא פונקציה f, שמחזירה את הנתונים שלנו שהם עכשיו במימד 1, בחזרה למימד p. ובואו נדרוש שהיא עושה את זה בעזרת אותו וקטור w, הפעם נכפיל פי הטרנספוז של w.

את התוצאה של להפעיל את האנקודר ואז את הדקודר, שהיא שוב מטריצה עם n שורות וp עמודות, אפשר לכנות כאקס-האט. היא בעצם שווה ללהכפיל את X פי w ואז פי w טרנספוז. עכשיו מה זאת הורדת מימד איכותית? קריטריון סביר הוא שהמטריצה המשוחזרת אקס-האט תהיה לא יותר מדי רחוקה מאקס המקורית. אם אתם רוצים אפשר ממש להתייחס לזה כבעיה של הצפנה. אם הצלחנו למצוא וקטור w שאנחנו יכולים לייצג באמצעותו את הנתונים שלנו ממימד הרבה יותר נמוך, ומתי שאנחנו רוצים לחלץ את הנתונים המקוריים באמצעות הכפלה חזרה פי w טרנספוז - זאת הצפנה מעולה.

דרך להגדיר מה זה שהמטריצה אקס-האט לא תהיה רחוקה מאקס, היא באמצעות שגיאת השחזור, ריקונסטרקן ארור, שמוגדרת באמצעות נורמת פרוביניוס.

נורמת פרוביניוס על כל מטריצה ממשית A היא פשוט סכום האלמנטים של A בריבוע. כלומר במקרה שלנו זה יהיה סכום השאריות הריבועיות בין כל אלמנט מאקס לאלמנט אקס-האט.
ומסתבר, שאפשר גם לחשב את הנורמה עם הטרייס של המטריצה כפול עצמה בטרנספוז.

כלומר קריטריון סופי שלנו הוא בעצם למצוא וקטור w עם נורמה 1 שתביא למינימום את הטרייס, של המרחק בין איקס לאיקס-האט, הביטוי שרשום כאן.
:::
:::

---

### A non-standard motivation (II)

$$w = \arg\min_{w: \|w\|=1}\text{Tr}((X - Xww^T)(X - Xww^T)^T)$$

::: {.fragment}
$$\text{Tr}((X - Xww^T)(X - Xww^T)^T)$$
:::
::: {.fragment}
$$= \text{Tr}((X - Xww^T)(X^T - ww^TX^T))$$
:::
::: {.fragment}
$$= \text{Tr}(XX^T - Xww^TX^T - Xww^TX^T + Xww^Tww^TX^T)$$
:::
::: {.fragment}
$$= \text{Tr}(XX^T - Xww^TX^T - Xww^TX^T + Xww^TX^T)$$
:::
::: {.fragment}
$$= \text{Tr}(XX^T) - \text{Tr}(Xww^TX^T)$$
:::
::: {.fragment}
<center>
But: $\text{Tr}(Xww^TX^T) = \text{Tr}(w^TX^TXw) = w^TX^TXw$
</center>
:::

::: {.fragment}
$$\Rightarrow w = \arg\max_{w: \|w\|=1}w^TX^TXw$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נפתח את הביטוי הזה.

נפעיל את הטרנפוז.

נפתח את הסוגריים.

עכשיו האילוץ שהנורמה של w צריכה להיות 1, בעצם אומר שw'w חייב להיות 1, ואנחנו מקבלים את הביטוי הנוכחי.

נכנס איברים דומים, ונפריד את הטרייס לטרייס על XX' וטרייס על Xww'X'.

עכשיו שימו לב, הביטוי עם איקס בלבד תלוי רק בנתונים שלנו, ולא בw שאנחנו מחפשים. הוא איזשהו קבוע, סקלר.

הביטוי השני מעניין הרבה יותר: קודם כל לטרייס יש תכונה מעגלית, Tr(ABC) = Tr(CAB) אם המכפלה אפשרית. זה אומר שאני יכול לכפול את הביטוי שבפנים מצד שמאל כפול X טרנספוז וכפול w טרנספוז. עכשיו מה יש לנו בביטוי הזה? נרשום את המימדים.

יש לנו כאן סקלר! מספר. וטרייס של מספר הוא המספר עצמו: w'X'Xw. המספר הזה מופיע עם מינוס לפניו, זאת אומרת שלמספר עצמו אנחנו רוצים לעשות מקסימום.

ואם נסכם הכל, אנחנו רוצים למצוא וקטור w עם נורמה 1 לעשות עליו הטלה, שמביא למקסימום את הביטוי שמופיע לפנינו. זה בדיוק הקריטריון למצוא את וקטור ההטלה הראשון של PCA!
:::
:::

---

## PCA Dimensionality Reduction {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
איך מגיעים לPCA בדרך כלל?
:::
:::

---

### PCA: the "standard" motivation

::: {.incremental}
- Goal: Find the $q$ direction(s) with the most dispersion
- Center $X$'s columns
- First direction: $w_1 = \arg\max_{w_1:\|w_1\| =1} \|Xw_1\|^2 = \arg\max_{w_1:\|w_1\| =1}w_1^TX^TXw_1$
- Second direction: $w_2 = \arg\max_{\|w_2\| =1, w_2^Tw_1 = 0}w_2^TX^TXw_2$
- Can keep going looking for new directions
- Assuming $p < n$, up to $p$ principal directions can be found this way, stack them into a $p \times q$ "loadings" matrix $W$
- Data with reduced dimensionality: $T_{n \times q} = X_{n \times p}W_{p \times q}$ taking only the first $q$ principal directions
:::

::: {.fragment}
But PCA solution also minimizes the reconstruction error of a linear encoder/decoder system!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בדרך כלל מדברים על מציאת q כיוונים להטיל עליהם את הדאטא, כך שנשמר את מירב הפיזור הטבעי של הדאטא.

אנחנו מחסרים מכל עמודה של X את הממוצע שלה, אם הפיזור שונה מאוד כדאי גם לחלק בסטיית התקן.

אנחנו רוצים למצוא את וקטור ההטלה w הראשון שימקסם את הפיזור של ההטלה, כלומר את הנורמה הריבועית של X כפול w, שזה בדיוק הקריטריון שלנו.

כדי למצוא את הכיוון הבא, נדרוש וקטור w2 שממקסם את פיזור ההטלה, כך שהוא אורתוגונלי לw1.

וכן הלאה וכן הלאה.

בהנחה שיש לנו מטריצת דאטא אופיינית שבה מספר הפיצ'רים p עדיין קטן ממספר התצפיות n, אפשר למצוא עד p כיוונים כאלה. המטרה היא כמובן למצוא הרבה פחות מp, כל אחד מסביר עוד קצת מהשונות, וביחד השאיפה שיסבירו כמה שקרוב ל100 אחוז מהשונות. ברגע שהשגנו q כיוונים כאלה ניתן לצרף אותם למטריצת W שנקראת גם מטריצת לודינגז.

והדאטא הסופי שלנו ממימד נמוך נסמן כT. יש בT עכשיו n שורות על q עמודות, ו-q אמור להיות קטן. על הדאטא הזה כמו שלמדנו אפשר לבצע רגרסיה, אפשר לעשות ויזואליזציה. אפשר גם להסתכל על הW עצמו ולראות את המשקולת לכל פיצ'ר בכל אחת מהעמודות ולראות אולי יש כאן קונסטרקט מעניין.

מה שיפה הוא שלפני רגע הראינו שW יכול להיות תוצר של קריטריון לכאורה לא קשור בכלל, מתוך התייחסות להורדת מימד כמערכת של קידוד ופענוח, אנקודר ודיקודר.
:::
:::

---

### Eigendecomposition: Reminder

::: {.fragment}
A non-zero vector $\mathbf{v}$ is an eigenvector of a square $p \times p$ matrix $\mathbf{A}$ if it satisfies:
$$\mathbf{A}\mathbf{v} = \lambda\mathbf{v},$$
for some scalar $\lambda$.
:::

::: {.incremental}
- Then $\lambda$ is called the eigenvalue corresponding to $\mathbf{v}$.

- Geometrically speaking, the eigenvectors of $\mathbf{A}$ are the vectors that $\mathbf{A}$ merely elongates or shrinks, and the amount that they elongate/shrink by is the eigenvalue

- An eigendecomposition of $\mathbf{A}$ is then: $\mathbf{A} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^{-1}$

- where $\mathbf{V}$ is the square $p \times p$ matrix whose $j$-th column is the eigenvector $\mathbf{v}_j$ of $\mathbf{A}$, and $\mathbf{\Lambda}$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, $\mathbf{\Lambda}_{jj} = \lambda_j$

- If $\mathbf{A}$ is real and symmetric, $\mathbf{V}$ is orthogonal, $\mathbf{A} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^T$ and $\lambda_j$ are real scalars

- If $\mathbf{A}$ is also positive semidefinite (PSD), then $\lambda_j \ge 0$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז יש לנו בעיית מינימום, לא חשוב איך הגענו אליה. איך פותרים אותה? ניזכר בבעיית הערכים העצמיים בקצרה. נרצה למצוא וקטור עצמי, eigenvector, למטריצה A ריבועית, מסדר p על p.

כאשר המטריצה A כופלת וקטור כזה, הדבר שקול בעצם פשוט להכפלה של הוקטור הזה באיזשהו סקלר למדא. ולמדא הוא הערך העצמי.

מבחינה גיאומטרית, נראה שכל מה שעשתה המטריצה A לוקטור v, זה פשוט לכווץ או להאריך אותו. ומסתבר שלמציאת וקטור כזה יש שימושים רבים.

פירוק ערכים עצמיים של A הוא מכפלה של המטריצות V, למדא, V בהופכית.

כשV היא מטריצה ריבועית p על p, שהעמודות שלה הם הוקטורים העצמיים, ולמדא היא מטריצה אלכסונית שעל האלכסון של נמצאים הלמדות, הערכים העצמיים.

אם A היא מטריצה ממשית וסימטרית כמו שתיכף יהיה במקרה שלנו, V היא גם אורתוגונלית, וההופכי שלה הוא הטרנספוז של אז אפשר לרשום את הפירוק כך. יתרה מזאת, הערכים העצמיים שלה הם ממשיים.

ואם המטריצה היא חיובית, פוזיטיב-סמידפיניט, כמו שתיכף יהיה במקרה שלנו - הערכים העצמיים הם אפילו אי-שליליים.
:::
:::

---

### Calculating Principal Components

::: {.incremental}
- Look again at the PCA problem: $w_1 = \arg\max_{w_1:\|w_1\| =1} \|Xw_1\|^2$

- Using Lagrange multiplier $\lambda_1$: $\max_{w_1}{w_1^TX^TXw_1} + \lambda_1(1 - w_1^Tw_1)$

- Take derivative with respect to $w_1$, compare to 0:
$$2X^TXw_1 - 2\lambda_1w_1 = 0 \\
X^TXw_1 = \lambda_1w_1$$

- So $w_1$ must be an eigenvector of the square, real, PSD $X^TX$ matrix, and $\lambda_1$ its eigenvalue!

- Which eigenvalue and eigenvector?

- So we're looking for the set of $W_{p \times q}$ eigenvectors $\mathbf{V}_q$ of $X^TX$ with their corresponding eigenvalues $\lambda_1, \dots, \lambda_q$ ordered from largest to smallest
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז מה הקשר לבעיה שלנו? נכתוב אותה שוב.

אפשר לכתוב אותה כבעית אופטימיזציה, אם נשתמש בכופלי לגראנז'. אנחנו רוצים למקסם את הכמות v'X'Xv, עם אילוץ על v'v.

אם נגזור את הכמות הזאת לפי הרכיבים בv, נקבל את הביטוי שלפנינו, נשווה אותו לאפס ונגיע למסקנה שאנחנו מחפשים וקטור v שיקיים את המשוואה הזאת. זאת בדיוק משוואה שמגדירה וקטור וערך עצמי של המטריצה X'X, מטריצת הקווריאנס של מדגם הנתונים!

לכן v1 חייב להיות וקטור עצמי של מטריצת הקווריאנס, ולמדא1 הערך העצמי שלה. ומאחר שכל מטריצת קווריאנס היא ממשית, סימטרית וחיובית, למדא גם חייב להיות אי-שלילי.

איזה וקטור עצמי וערך עצמי ניקח? אם נכפול את הביטוי כאן ב v טרנספוז מצד שמאל נראה שהפיזור עצמו שווה לערך העצמי, ואנחנו רוצים פיזור כמה שיותר גדול, לכן ניקח את הוקטור העצמי שמתאים לערך העצמי הגדול ביותר. זכרו שהם אי-שליליים!

במילים אחרות הפתרון של PCA, הW הזה שאנחנו מחפשים הוא בדיוק הסט של וקטורים עצמיים של X'X, ששייכים לערכים העצמיים הכי גדולים.

נזכיר רק שלמרות התוצאה היפה הזאת, זה לא איך שהמחשב פותר את בעיית הPCA, מחשב בדרך כלל יפתור את בעיית הPCA דרך פירוק הSVD, שאפשר לעשות גם למטריצות גדולות מאוד, בקירוב. על כך תוכלו ללמוד בקורסים אחרים, אנחנו נסתפק בינתיים בזה.
:::
:::

---

## t-SNE Dimensionality Reduction {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### PCA Limitations

- Linear mapping (encoder/decoder)
- Squared reconstruction error: "punishes" more large differences in $\|X - \hat{X}\|^2_F$
- Focus on preserving **global** structure
- No probabilistic meaning?

::: {.fragment}
Enter **t-Distributed Stochastic Neighbor Embedding** (t-SNE):

- Non-linear mapping
- Focus on preserving **local** structure through pairwise similarites:
  - close observations in high dimension should likely be close in low dimension
  - distant observations in high dimension should likely be distant in low dimension
- Specifically designed for visualization (2-D, 3-D)
- Probabilistic meaning
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
יש הרבה ביקורות על PCA שהביאו חוקרים לפתח שיטות אחרות. ראינו שPCA הוא בהגדרה שיטה ליניארית. אולי אנקודר ודיקודר גמישים יותר ולא-ליניאריים יכולים להביא לקריטריון טוב יותר?
ואם כבר מדברים על קריטריון, כמו ברגרסיה ראינו שהקריטריון של PCA מביא למינימום שגיאה ריבועית. שגיאה ריבועית זו בחירה אחת, אולי בחירות אחרות יביאו לתוצאות טובות יותר, הרי ידוע למשל ששגיאה ריבועית מענישה יותר שגיאות גדולות, כלומר גם אם אנחנו עובדים עם נתונים שעברו סטנדרטיזציה, PCA מאוד מושפע מתצפיות חריגות.
ביקורת נוספת היא שבכלל כל ההסתכלות של PCA היא גלובלית, בצורה שקצת מזכירה רגרסיה. אולי אם נאפשר פונקציה אחרת לכל שורה של הנתונים, או התיחסות לוקאלית נקבל תוצאות טובות יותר?
והביקורת האחרונה היא לא נכונה. במשך הרבה שנים היה נהוג לומר שPCA הוא רק בעיית אופטימיזציה אלגברית, כמו שראינו שאפשר לראות ברגרסיה ליניארית. ואין לה איזשהו פרוש סטטיסטי, בניגוד לרגרסיה ליניארית. כלומר אין כאן מודל עם הנחה של התפלגות שפתרון PCA הוא המודל הטוב ביותר שמתאים לנתונים שלנו. נראה תיכף שזה התגלה כלא נכון.

מכל מקום, כל הבעיות האלה הביאו חוקרים לנסח אלגוריתם קצת שונה להורדת מימד, ספציפית למטרה של ויזאליזציה של נתונים: הt-distributed stochastic neighbor embedding, או הטיסני בקיצור.
אנחנו נראה שהמיפוי או האמבדינג למימד נמוך יותר הוא כבר לא ליניארי, שהפוקוס הוא לא על איזשהו אנקודר/דיקודר גלובליים להורדת מימד, אלא משהו הרבה יותר לוקאלי, המטרה של השיטה היא לדאוג שכל זוג תצפיות שדומה זו לזו במימד המקורי הגבוה, יהיה גם קרוב במימד הנמוך או בהסתברות גבוהה, ולהיפך: זוג רחוק יהיה רחוק בממד הנמוך.

בנוסף לזה טיסני יוסיף לנו משמעות הסתברותית בצורה מיידית מזו של PCA.
:::
:::

---

### t-SNE: How to define close/distant? (I)

::: {.incremental}
- In high dimension ($p$) with a Gaussian kernel:
  - Let $\mathbf{x}_1, \dots, \mathbf{x}_n$ be the data rows (each $\mathbf{x}_i \in \mathbb{R}^p$)
  - $p_{j|i} = \frac{\exp(-\|\mathbf{x}_i - \mathbf{x}_j\|^2 / 2\sigma^2_i)}{\sum_{k\neq i}\exp(-\|\mathbf{x}_i - \mathbf{x}_k\|^2 / 2\sigma^2_i)}$
  - Set $p_{i|i} = 0$
  - Notice that $p_{j|i} \in [0, 1]$ and $\sum_j p_{j|i} = 1$
  - "Symmetrize": $p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$ (makes sense if $p_i = \frac{1}{n} \space \forall i$)
  - This $n \times n$ table is computed once
  - How to get $\sigma_i$ not shown here, but:
    - for observation $i$ in a dense area, want to be specific $\Rightarrow$ need small $\sigma_i$
    - for observation $i$ in a sparse area, need large $\sigma_i$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### t-SNE: How to define close/distant? (II)

::: {.incremental}
- In low dimension ($q = 2, 3$) with a $t(1)$-distribution kernel:
  - Define $\mathbf{y}_1, \dots, \mathbf{y}_n$ the low-dimensional mappings (each $\mathbf{y} \in \mathbb{R}^q$)
  - If $Z \sim t(1)$, then: $f(z) = \frac{1}{\pi(1+z^2)}$ (also called Cauchy)
  - Here, no need to go through conditional probs
  - $q_{ij} = \frac{(1 + \|\mathbf{y}_i - \mathbf{y}_j\|^2)^{-1}}{\sum_{k}\sum_{k\neq l}(1 + \|\mathbf{y}_k - \mathbf{y}_l\|^2)^{-1}}$, and set $q_{ii} = 0 \space \forall i$
  - Why $t(1)$? See the "crowding problem".
:::

::: {.fragment}

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, t

# Define the x range for plotting
x = np.linspace(-5, 5, 1000)

# Get the probability density function (PDF) values for N(0, 1) and t-distribution with 1 degree of freedom
normal_pdf = norm.pdf(x, 0, 1)
t_pdf = t.pdf(x, 1)

# Plot both distributions
plt.figure(figsize=(4, 3))
plt.plot(x, normal_pdf, label="N(0, 1)", color='black', linewidth=2)
plt.plot(x, t_pdf, label="t(1)", color='gray', linestyle='--', linewidth=2)

# Customize the plot
plt.ylabel("f(y)")
plt.legend()
plt.show()
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### t-SNE: How to compare $p$ and $q$ distributions?

::: {.incremental}
- The Kullback-Leibler (KL) divergence is a distance metric from distribution $p$ to $q$:
$$KL(p||q) = \sum_{i}\sum_{j}p_{ij} \log\frac{p_{ij}}{q_{ij}}$$
:::

::: {.fragment}
```{python}
#| echo: false
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt

# Define two simple discrete distributions P and Q
P = np.array([0.1, 0.2, 0.3, 0.25, 0.15])
Q = np.array([0.2, 0.1, 0.25, 0.25, 0.2])

# Define labels for the x-axis
x = np.arange(len(P))

# Plot the histograms
plt.figure(figsize=(3.5, 2.5))
plt.bar(x - 0.2, P, width=0.4, label="P", color='blue')
plt.bar(x + 0.2, Q, width=0.4, label="Q", color='red')

# Customize the plot
plt.ylabel("Probability P/Q")
plt.xticks(x, ['A', 'B', 'C', 'D', 'E'])  # Custom labels for categories
plt.legend()
plt.grid(True, axis='y')
plt.show()
```
:::

::: {.fragment}
- At each iteration of t-SNE we walk a step down the gradient of $KL(p||q)$ with respect to every $\mathbf{y}_i$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### t-SNE: at high level

1. Prepare $p_{ij}$ table with Gaussian kernel and $\sigma_1, \dots, \sigma_n$
2. Sample initial low-dimensional mappings $Y^{(0)} = \mathbf{y}^{(0)}_1, \dots, \mathbf{y}^{(0)}_n$
3. For $t = 1$ to $T$ do:
  
    i. Compute $q_{ij}$ with $t(1)$ kernel
  
    ii. Gradient step: $\mathbf{y}_i^{(t)} = \mathbf{y}_i^{(t - 1)} - \alpha \cdot \frac{\partial KL}{\partial \mathbf{y}_i}$

::: {.incremental}
- $\frac{\partial KL}{\partial \mathbf{y}_i} = 4 \sum_j (p_{ij} - q_{ij})(\mathbf{y}_i - \mathbf{y}_j)(1+ \|\mathbf{y}_i - \mathbf{y}_j\|^2)^{-1}$
- Modifications exist for very large datasets, e.g. consider only local neighborhood for $i$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Example: MNIST dataset

![](images/mnist_examples.png){fig-align="center" height=400}

7000 X 10 digits ($n = 70000$), hand-written, in 28 X 28 pixels ($p=784$)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### PCA vs. t-SNE

![](images/pca_vs_tsne_mnist.png)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## PCA as a Generative Model {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Probabilistic PCA (I)

- It is not true that there is nothing "probabilistic" about PCA!

::: {.incremental}
- Suppose every data row $\mathbf{x}_i$ was generated by:
$$\mathbf{x}_i = W\mathbf{u}_i + \mathbf{\mu} + \mathbf{\varepsilon}_i$$
  - $\mathbf{u}_i \in \mathbb{R}^q$ is a latent vector from $\mathcal{N}(\mathbf{0}, I_q)$
  - $W$ is a $p \times q$ loadings matrix
  - $\mathbf{\mu} \in \mathbb{R}^p$ is a mean vector for $p$ features
  - $\mathbf{\varepsilon}_i \in \mathbb{R}^p$ is random noise from $\mathcal{N}(\mathbf{0}, \sigma^2I_p)$
:::
::: {.fragment}
$\Rightarrow \mathbf{x}_i \sim \mathcal{N}(\mathbf{\mu}, \Sigma)$, where: $\Sigma = WW^T + \sigma^2I_p$
:::
::: {.fragment}
It's likelihood: $f(\mathbf{x}_i) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf{x}_i - \mu)^T\Sigma^{-1}(\mathbf{x}_i-\mu)\right)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Probabilistic PCA (II)

::: {.incremental}
- After some algebra, the log-likelihood of our entire data $X = \mathbf{x}_1, \dots, \mathbf{x}_n$:
$$\ell(\mathbf{\mu}, W, \sigma^2|X) = -\frac{n}{2}\left[p\ln(2\pi) + \ln(\Sigma) + \text{Tr}(\Sigma^{-1}S)\right]$$
  - where $S = \frac{1}{n}\sum_i (\mathbf{x}_i - \mu)(\mathbf{x}_i - \mu)^T$ is the covariance matrix of $X$
- The maximum likelihood estimate (MLE) for $W$:
$$W_{MLE} = \mathbf{V}_q(\mathbf{\Lambda}_q - \sigma^2I_q)^{1/2}\mathbf{R}$$
  - where the columns of $\mathbf{V}_q$ are eigenvectors of $S$, with corresponding $q$ largest eigenvalues in the diagonal matrix $\mathbf{\Lambda}_q$, and $\mathbf{R}$ is a $q\times q$ arbitrary rotation matrix
  - In the limit $\sigma^2 \to 0$ this solution is equivalent to PCA!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The many faces of PCA

Why is this so important?

::: {.incremental}
- PCA as an eigenvalue problem to maximize dispersion
- PCA as an SVD problem
- PCA as an encoder/decoder problem to minimize reconstruction error
- PCA as a generative model to maximize likelihood
:::

::: {.incremental}
- Unsupervised learning can be used to generate new data: ![](images/eigenface.png){fig-align="center" height=200}
- PCA is the ancestor of many generative models
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
