=== 1. הקדמה ללמידה בלתי מפוקחת ===

את השיעור האחרון אנחנו בוחרים להקדיש ללמידה בלתי מפוקחת, כשהמטרה שלנו היא לא בהכרח לחזות איזשהו משתנה מוסבר Y, אלא לתאר את ההתפלגות של הנתונים עצמם. נדבר בקצרה על אלגוריתם Kmeans כדי לעשות קלאסטרינג, ואז נחזור לאלגוריתם PCA כדי לבצע הורדת מימד ונתאר אותו בפירוט יותר, וגם שיטה דומה שמבצעת הורדת מימד בצורה לא ליניארית - טיסני. אבל לפני זה נדבר באופן כללי מה זה למידה בלתי מפוקחת או unsupervised.

:::

בלמידה מסוג supervised, יש לנו וקטור X של p משתנים, וסקלאר Y. המטרה היא למדל את Y כפונקציה f של X. אנחנו מניחים שזוגות התצפיות X, Y שלנו מגיעים בלתי תלויים מאיזושהי התפלגות משותפת Pxy, ובונים מודל לחיזוי באמצעות נתוני מדגם הלמידה X, Y, מודל שנקרא f_hat. כשתגיע תצפית חדשה לחיזוי X0 נפעיל עליה את המודל הנלמד f_hat וזה יהיה החיזוי שלנו עבורה. ואיך אנחנו מכמתים את הביצועים של המודל שלנו? באידאל באמצעות איזושהי פונקצית הפסד L בין תצפיות Y האמיתיות והחזויות, כשאנחנו לוקחים תוחלת על תצפיות שהמודל לא ראה. בפועל אנחנו לא יודעים את ההתפלגות של התצפיות שהמודל לא ראה ואנחנו לוקחים את הממוצע האמפירי על מדגם הטסט.

נשאלת השאלה, מה אם אין Y, המשתנה התלוי לחיזוי?

:::

בלמידה לא מפוקחת יש לנו רק וקטור של משתנים X ממימד p. אנחנו עדיין מניחים שהתצפיות מגיעות בלתי תלויות מאיזו התפלגות לא-ידועה Px, והמטרה שלנו היא לא לאמוד איזושהי פונקציה או קשר אלא ממש את ההתפלגות הזאת, או תכונות שלה.

לדוגמא, ניתוח אשכולות או קלאסטרינג -- היינו רוצים ללמוד איזורים בהתפלגות עם צפיפות גבוהה, או השכיחים של P_x. אם נמצא שP_x מתחלקת בבירור לכמה איזורים כאלה למשל, אולי ניתן לייצג אותה בעזרתם, ואז זה יפשט אותה, במקום להיות למשל פונקציה מורכבת בהרבה מימדים נוכל לחלק אותה לצירוף של כמה פונקציות פשוטות יותר.

יש המון שיטות לבצע קלאסטרינג, נראה עכשיו נציג בולט אחד שלהן, שנקרא Kmeans clustering.

:::

=== 2. קלאסטרינג Kmeans ===

נדבר כעת על אלגוריתם KMeans, אולי המוכר ביותר בתחום של קלאסטרינג, וגם יעיל יחסית ועובד טוב עם נתונים ממימד גבוה.

:::

נניח שיש לנו חלוקה נתונה. paritition. איך אנחנו עושים איבליואציה של חלוקה כזאת על הנתונים שלנו.

נניח שאנחנו יודעים כבר את מספר הקלאסטרים בדאטא ומסמנים אותו בK, ונניח שיש לנו כבר כלל חלוקה C שמתאים לכל תצפית i בנתונים את הקלאסטר k שמתאים לה, מ1 עד K גדול.

נניח כעת שצפיפות של נתונים או עד כמה זוג תצפיות קרובות אחת לשניה נמדוד באמצעות איזושהי מטריקת מרחק d, לדוגמא נתחיל עם מרחק אוקלידי.

ואנחנו רוצים לבטא באמצעות איזושהי מטריקה את האינטואיציה שלנו שתצפיות באותו קלאסטר צריכות להיות צפופות ורחוקות מתצפיות בקלאסטרים אחרים.

הרבה אלגוריתמים מסתכלים על המטריקה הבאה, הפיזור within cluster שנסמן בW(C), והוא סכום על כל זוגות התצפיות ששייכות לקלאסטר k של המרחקים ביניהן, וסכום על כל הקלאסטרים, מוכפל פי חצי כי אנחנו סופרים כל זוג ככה פעמיים.

מאחר שהפיזור בין כל זוגות התצפיות בלי קשר לחלוקה לקלאסטרים נשאר זהה, אפשר להראות שלעשות מינימום לקריטריון שלנו אקוויולנטי ללעשות מקסימום לכמות המשלימה של between clusters סקאטר: סכום המרחקים בין כל זוגות התצפיות שנמצאות בקלאסטרים שונים. הרי סכום הכמות הזאת והכמות שלנו W(C) מסתכם בפיזור כללי נאמר T(C).

אז יש לנו מטריקה לעשות לה מינימום. אבל אפילו עם K נתון, נאמר שאנחנו רוצים לחלק את הדאטא ל4 קלאסטרים. האם אנחנו יכולים לעבור על כל החלוקות C(i) האפשריות כדי להגיע למינימום גלובלי? ברור שלא. יש לזה נוסחה שלא מופיעה כאן, אפשר לחשב למשל שלעבור על כל האפשרויות של חלוקת 20 תצפיות ל5 קלאסטרים אנחנו כבר מדברים על כמעט 750 מיליארד אפשרויות!

:::

אז Kmeans קודם כל מצמצם אותנו למרחק אוקלידי בלבד.

תחת מרחק אוקלידי, מסתבר שאפשר לרשום את הקריטריון שלנו בצורה פשוטה יותר: בכל קלאסטר סכום המרחקים מהתצפיות אל ממוצע הקלאסטר, להכפיל במספר התצפיות בקלאסטר n_k, ולסכום על כל הקלאסטרים.

אבל אנחנו יודעים כבר שאם נסתכל על קריטריון דומה, ונשאל מה הנקודה שמביאים למינימום את סכום המרחקים המרובעים ממנה -- נגיע לממוצע המדגם.

לכן נהוג לרשום את הקריטריון של Kmeans בצורה כוללת יותר: למצוא את החלוקה ואת הנקודות של קלאסטרים שיביאו למינימום את סכום המרחקים המרובעים בתוך כל קלאסטר, על פני כל הקלאסטרים. צורת הרישום הזאת מסייעת לנסח את האלגוריתם של Kmeans.

:::

אז מהו האלגוריתם של Kmeans?

K נתון, ואנחנו מתחילים עם איזשהו ניחוש התחלתי עבור הממוצעים m1 עד mk.

כעת החלוקה C(i) של כל תצפית תהיה לפי הקלאסטר שהממוצע שלו הוא הקרוב אליה ביותר.

לאחר החלוקה נעדכן את הממוצעים, בכל קלאסטר ניקח את הממוצע של התצפיות ששייכות אליו.

ונחזור על צעדים 1 ו2 עד שהחלוקה לא משתנה או עד איזשהו קריטריון התכנסות, למשל אפשר לחשב את הלוס שלנו W(C) ולראות שהוא לא משתנה יותר מדי.

מה הבעיה הראשונה באלגוריתם? אמנם התכנסות מובטחת, הצעדים שלנו יכולים רק להפחית את W(C), או לא לשנות אותו. אבל אין הבטחה למינימום גלובלי ואנחנו מאוד תלויים בבחירה הראשונית של הממוצעים שיכולה להיות אקראית.

נהוג לכן בהרבה מימושים לבצע מספר פעמים Kmeans כל פעם מנקודת התחלה אקראית אחרת ולבחור את הפתרון עם הלוס המינימלי.

:::

בדוגמא שלפנינו ברור שיש 4 קלאסטרים, ואנחנו מתחילים עם 4 נקודות אקראיות כממוצעים, מאוד לא מתאימות לחלוקה.

:::

בצעד 1 אנחנו מחלקים כל תצפית לקלאסטר עם הממוצע הכי קרוב אליה במרחק אוקלידי, כאן זה אומר לצבוע אותן ב4 צבעים שונים.

בצעד 2 אנחנו מעדכנים את הממוצעים, וכבר ניתן לראות איך כל ממוצע מייצג כבר איזור הרבה יותר צפוף באופן טבעי.

:::

ושוב אנחנו מחלקים את התצפיות לפי הקרבה שלהן לממוצע החדש.

ושוב אנחנו מעדכנים את הממוצעים. כאן הם כבר זזים ממש מעט.

:::

באיטרציה השלישית כבר בקושי אפשר לראות הבדל, הקלאסטרים כבר ברורים מאוד.

:::

כבר תוך כדי הדיון אפשר היה להבחין בכמה בעיות של Kmeans, למשל שאין גרנטי למינימום גלובלי על המטריקה W(c).

מגבלה אחרת היא שכל הפיתוח שלנו התבסס על מרחק אוקלידי, אין אפשרות לעשות פלאג-אין בKmeans לפונקצית מרחק אחרת, שאולי היא מתאימה יותר, למשל אם המשתנים שלנו הם קטגוריאליים.

אבל אולי מגבלה בולטת יותר היא העובדה שצריך לספק לאלגוריתם את K, לדעת מראש כמה קלאסטרים יש בדאטא.

ואם ככה זה אומר שצריך לדעת איך לבחור את K, כשהבעיה היא שברור שככל שK גדול המטריקה של סכום הריבועים within-cluster יורדת. עד המצב הקיצוני שבו K = n ואז כל תצפית היא קלאסטר בפני עצמו והמטריקה תהיה אפס. אז כאן יש כל מיני סטטיסטים ויש את השיטה הישנה והטובה של לצייר את W(c) כנגד ערכי K שונים, פשוט לבצע את האלגוריתם נאמר על 5 פולדים ועבור ערכי K שונים, ולראות אם עבור ערך K מסוים כבר אפקטיבית אין ירידה, לראות אם יש איזשהו מרפק בגרף. אם יש כזה אפשר לבחור בK בו יש מרפק.

הבעיה האחרת של Kmeans היא שהוא מאוד טוב בחלוקות יפות לקלאסטרים מעוגלים, יפים, שווי גודל, כאילו כל אחד מהמשתנים בX הגיע מהתפלגות נורמלית. הוא מבצע הרבה פחות טוב על קלאסטרים עם גדלים שונים, עם צפיפות שונה.

עוד דבר בKmeans שחייב להיות ברור לנו זה שאין לו מושג בילט-אין של אאוטליירז. Kmeans יחזיר תשובה לכל תצפית, גם אם ברור לנו בעין שהיא לא שייכת לאף אחד מהקלאסטרים. כאן לKmeans יש הרבה מתחרים מצוינים שלהם דווקא יש קונספט של אאוטלייר, כמו DBSCAN, שכן יכול להחזיר תשובה כזאת לתצפית - לא מצאתי אף קלאסטר.

:::

=== 3. הורדת מימד ===

ביתר השיעור נדבר על הורדת מימד. נתחיל בPCA, שהיא השיטה המוכרת ביותר וליניארית, ונעבור אחר כך לטיסני, שהיא שיטה לא ליניארית. PCA היא שיטה כל כך מוכרת שאפילו בקורס שלנו נתקלנו בה כשדיברנו עליה בהקשר של למידה כן מפוקחת. שם רצינו להוריד מימד לנתונים לפני ביצוע רגרסיה ליניארית על הנתונים במימד הקטן יותר, וקראנו לזה PCR. נראה עכשיו איך מגיעים לפתרון של PCA, אבל לפני זה נזכיר שוב בשביל מה להוריד מימד.

:::

יש לנו מטריצת נתונים עם n שורות ו-p עמודות. p בדרך כלל גדול מאוד נאמר לפחות 10 משתנים, ולפעמים אלפי ואפילו מיליון משתנים בדאטה מודרני כמו למשל בגנטיקה.

אנחנו רוצים להוריד מימד מp לq, כשq הרבה יותר קטן.

מטרה ראשונה היא אקספלורטורי דאטא אנליסיס, או EDA. החל מויזואליציה של הנתונים, שלא אפשרית כשp הוא מאוד גדול, ועד התבוננות על פתרון PCA עצמו לראות אם יש צירופים של המשתנים שממצים את הדאטא היטב. פסיכולוגים למשל מבצעים הרבה פעמים PCA על שאלוני אישיות כדי לזהות קונסטרקטים או תמות בתשובות של אנשים.

סיבה אחרת להורדת מימד היא כדי לבצע אלגוריתמים של למידה על הדאטא ממימד נמוך, או קלאסטרינג. זה יכול להיות בתקווה לשפר את הביצועים של המודל, כמו שראינו בPCR. זה יכול להיות כי במימד הנוכחי האלגוריתם לא יכול בכלל לפעול.

סיבה אחרת שקצת קשה אולי לראות כרגע, היא שימוש במודל שהרצנו להורדת מימד להעלאה בחזרה של המימד, כדי ליצור דאטא חדש. נגענו בזה כשדיברנו על מודלים גנרטיביים, שמתעניינים בהתפלגות של הנתונים X בהינתן הקלאס Y. נתעניין בגישה הזאת גם כאן בסוף היחידה.

אז איך להוריד מימד? עשינו את זה כבר. בגישה הנאיבית יותר פשוט בחרנו או ידנית או על פי קריטריון מסוים תת-קבוצה, למשל ברגרסיה של משתנים "חשובים".

שיטה פחות נאיבית היא להשאיר את כל המשתנים, אבל לאחד אותם על ידי כמה פונקציות לכדי כמה אאוטפוטים בודדים חשובים. אני מדבר על הטלה או פרוג'קשן של הדאטא למימד נמוך יותר, והטלה כזאת יכולה להיות ליניארית או לא-ליניארית. נתחיל בהטלה ליניארית.

:::

נתחיל בגישה קצת לא סטנדרטית לבעיה של הורדת מימד. נניח שיש לנו מקודד לנתונים שלנו, פונקציה g שמורידה את הנתונים שלנו ממימד p למימד 1. נניח אפילו שהפונקציה הזאת ליניארית, כלומר היא פשוט לוקחת את הדאטא שלנו עם פי פיצ'רים ומכפילה אותו בוקטור עם נורמה 1 שנקרא לו w. למקודד הזה אנחנו קוראים אנקודר.

לעומתו יש לנו את הדקודר, שהוא פונקציה f, שמחזירה את הנתונים שלנו שהם עכשיו במימד 1, בחזרה למימד p. ובואו נדרוש שהיא עושה את זה בעזרת אותו וקטור w, הפעם נכפיל פי הטרנספוז של w.

את התוצאה של להפעיל את האנקודר ואז את הדקודר, שהיא שוב מטריצה עם n שורות וp עמודות, אפשר לכנות כאקס-האט. היא בעצם שווה ללהכפיל את X פי w ואז פי w טרנספוז. עכשיו מה זאת הורדת מימד איכותית? קריטריון סביר הוא שהמטריצה המשוחזרת אקס-האט תהיה לא יותר מדי רחוקה מאקס המקורית. אם אתם רוצים אפשר ממש להתייחס לזה כבעיה של הצפנה. אם הצלחנו למצוא וקטור w שאנחנו יכולים לייצג באמצעותו את הנתונים שלנו ממימד הרבה יותר נמוך, ומתי שאנחנו רוצים לחלץ את הנתונים המקוריים באמצעות הכפלה חזרה פי w טרנספוז - זאת הצפנה מעולה.

דרך להגדיר מה זה שהמטריצה אקס-האט לא תהיה רחוקה מאקס, היא באמצעות שגיאת השחזור, ריקונסטרקשן ארור, שמוגדרת באמצעות נורמת פרוביניוס.

נורמת פרוביניוס על כל מטריצה ממשית A היא פשוט סכום האלמנטים של A בריבוע. כלומר במקרה שלנו זה יהיה סכום השאריות הריבועיות בין כל אלמנט מאקס לאלמנט אקס-האט.
ומסתבר, שאפשר גם לחשב את הנורמה עם הטרייס של המטריצה כפול עצמה בטרנספוז.

כלומר קריטריון סופי שלנו הוא בעצם למצוא וקטור w עם נורמה 1 שיביא למינימום את הטרייס, של המרחק בין איקס לאיקס-האט, הביטוי שרשום כאן.

:::

ניקח את הביטוי שהגענו אליו, נעתיק אותו רגע הצידה ונפתח אותו.

נפעיל את הטרנפוז.

נפתח את הסוגריים.

עכשיו האילוץ שהנורמה של w צריכה להיות 1, בעצם אומר שw'w חייב להיות 1, ואנחנו מקבלים את הביטוי הנוכחי.

נכנס איברים דומים, ונפריד את הטרייס לטרייס על XX' וטרייס על Xww'X'.

עכשיו שימו לב, הביטוי עם איקס בלבד תלוי רק בנתונים שלנו, ולא בw שאנחנו מחפשים. הוא איזשהו קבוע, סקלר.

הביטוי השני מעניין הרבה יותר: קודם כל לטרייס יש תכונה מעגלית, Tr(ABC) = Tr(CAB) אם המכפלה אפשרית. זה אומר שאני יכול לכפול את הביטוי שבפנים מצד שמאל כפול X טרנספוז וכפול w טרנספוז. עכשיו מה יש לנו בביטוי הזה? נרשום את המימדים.

יש לנו כאן סקלר! מספר. וטרייס של מספר הוא המספר עצמו: w'X'Xw. המספר הזה מופיע עם מינוס לפניו, זאת אומרת שלמספר עצמו אנחנו רוצים לעשות מקסימום.

ואם נסכם הכל, אנחנו רוצים למצוא וקטור w עם נורמה 1 לעשות עליו הטלה, שמביא למקסימום את הביטוי שמופיע לפנינו. זה בדיוק הקריטריון למצוא את וקטור ההטלה הראשון של PCA!

:::

=== 4. הורדת מימד באמצעות PCA ===

איך מגיעים לPCA בדרך כלל?

:::

בדרך כלל מדברים על מציאת q כיוונים להטיל עליהם את הדאטא, כך שנשמר את מירב הפיזור הטבעי של הדאטא.

אנחנו מחסרים מכל עמודה של X את הממוצע שלה, אם הפיזור שונה מאוד כדאי גם לחלק בסטיית התקן.

אנחנו רוצים למצוא את וקטור ההטלה w הראשון שימקסם את הפיזור של ההטלה, כלומר את הנורמה הריבועית של X כפול w, שזה בדיוק הקריטריון שלנו.

כדי למצוא את הכיוון הבא, נדרוש וקטור w2 שממקסם את פיזור ההטלה, כך שהוא אורתוגונלי לw1.

וכן הלאה וכן הלאה.

בהנחה שיש לנו מטריצת דאטא אופיינית שבה מספר הפיצ'רים p עדיין קטן ממספר התצפיות n, אפשר למצוא עד p כיוונים כאלה. המטרה היא כמובן למצוא הרבה פחות מp, כל אחד מסביר עוד קצת מהשונות, וביחד השאיפה שיסבירו כמה שקרוב ל100 אחוז מהשונות. ברגע שהשגנו q כיוונים כאלה ניתן לצרף אותם למטריצת W שנקראת גם מטריצת לודינגז.

והדאטא הסופי שלנו ממימד נמוך נסמן כT. יש בT עכשיו n שורות על q עמודות, ו-q אמור להיות קטן. על הדאטא הזה כמו שלמדנו אפשר לבצע רגרסיה, אפשר לעשות ויזואליזציה. אפשר גם להסתכל על הW עצמו ולראות את המשקולת לכל פיצ'ר בכל אחת מהעמודות ולראות אולי יש כאן קונסטרקט מעניין.

מה שיפה הוא שלפני רגע הראינו שW יכול להיות תוצר של קריטריון לכאורה לא קשור בכלל, מתוך התייחסות להורדת מימד כמערכת של קידוד ופענוח, אנקודר ודיקודר.

:::

אז יש לנו בעיית מינימום, לא חשוב איך הגענו אליה. איך פותרים אותה? ניזכר בבעיית הערכים העצמיים בקצרה. נרצה למצוא וקטור עצמי, eigenvector, למטריצה A ריבועית, מסדר p על p.

כאשר המטריצה A כופלת וקטור כזה, הדבר שקול בעצם פשוט להכפלה של הוקטור הזה באיזשהו סקלר למדא. ולמדא הוא הערך העצמי.

מבחינה גיאומטרית, נראה שכל מה שעשתה המטריצה A לוקטור v, זה פשוט לכווץ או להאריך אותו. ומסתבר שלמציאת וקטור כזה יש שימושים רבים.

פירוק ערכים עצמיים של A הוא מכפלה של המטריצות V, למדא, V בהופכית.

כשV היא מטריצה ריבועית p על p, שהעמודות שלה הם הוקטורים העצמיים, ולמדא היא מטריצה אלכסונית שעל האלכסון של נמצאים הלמדות, הערכים העצמיים.

אם A היא מטריצה ממשית וסימטרית כמו שתיכף יהיה במקרה שלנו, V היא גם אורתוגונלית, וההופכי שלה הוא הטרנספוז שלה אז אפשר לרשום את הפירוק כך. יתרה מזאת, הערכים העצמיים שלה הם ממשיים.

ואם המטריצה היא חיובית למחצה, פוזיטיב-סמידפיניט, כמו שתיכף יהיה במקרה שלנו - הערכים העצמיים הם אפילו אי-שליליים.

:::

אז מה הקשר לבעיה שלנו? נכתוב אותה שוב.

אפשר לכתוב אותה כבעית אופטימיזציה, אם נשתמש בכופלי לגראנז'. אנחנו רוצים למקסם את הכמות w'X'Xw, עם אילוץ על w'w.

אם נגזור את הכמות הזאת לפי הרכיבים בw, נקבל את הביטוי שלפנינו, נשווה אותו לאפס ונגיע למסקנה שאנחנו מחפשים וקטור w שיקיים את המשוואה הזאת. זאת בדיוק משוואה שמגדירה וקטור וערך עצמי של המטריצה X'X, מטריצת הקווריאנס של מדגם הנתונים (עד כדי קבוע)!

לכן w1 חייב להיות וקטור עצמי של מטריצת הקווריאנס, ולמדא1 הערך העצמי שלה. ומאחר שכל מטריצת קווריאנס היא ממשית, סימטרית וחיובית, למדא גם חייב להיות אי-שלילי.

איזה וקטור עצמי וערך עצמי ניקח? אם נכפול את הביטוי כאן ב w טרנספוז מצד שמאל נראה שהפיזור עצמו שווה לערך העצמי, ואנחנו רוצים פיזור כמה שיותר גדול, לכן ניקח את הוקטור העצמי שמתאים לערך העצמי הגדול ביותר. זכרו שהם אי-שליליים!

במילים אחרות הפתרון של PCA, הW הזה שאנחנו מחפשים הוא בדיוק הסט של וקטורים עצמיים של X'X, ששייכים לערכים העצמיים הכי גדולים, ממוינים מגדול לקטן.

נזכיר רק שלמרות התוצאה היפה הזאת, זה לא איך שהמחשב פותר את בעיית הPCA, מחשב בדרך כלל יפתור את בעיית הPCA דרך פירוק הSVD, שאפשר לעשות גם למטריצות גדולות מאוד, בקירוב. על כך תוכלו ללמוד בקורסים אחרים, אנחנו נסתפק בינתיים בזה.

:::

=== 5. הורדת מימד באמצעות tSNE ===

יש הרבה ביקורות על PCA שהביאו חוקרים לפתח שיטות אחרות. ראינו שPCA הוא בהגדרה שיטה ליניארית. אולי אנקודר ודיקודר גמישים יותר ולא-ליניאריים יכולים להביא לקריטריון טוב יותר?
ואם כבר מדברים על קריטריון, כמו ברגרסיה ראינו שהקריטריון של PCA מביא למינימום שגיאה ריבועית. שגיאה ריבועית זו בחירה אחת, אולי בחירות אחרות יביאו לתוצאות טובות יותר, הרי ידוע למשל ששגיאה ריבועית מענישה יותר שגיאות גדולות, כלומר גם אם אנחנו עובדים עם נתונים שעברו סטנדרטיזציה, PCA מאוד מושפע מתצפיות חריגות.
ביקורת נוספת היא שבכלל כל ההסתכלות של PCA היא גלובלית, בצורה שקצת מזכירה רגרסיה. אולי אם נאפשר פונקציה אחרת לכל שורה של הנתונים, או התיחסות לוקאלית נקבל תוצאות טובות יותר?
והביקורת האחרונה היא לא נכונה. במשך הרבה שנים היה נהוג לומר שPCA הוא רק בעיית אופטימיזציה אלגברית, כמו שראינו שאפשר לראות ברגרסיה ליניארית. ואין לה איזשהו פרוש סטטיסטי, בניגוד לרגרסיה ליניארית. כלומר אין כאן מודל עם הנחה של התפלגות שפתרון PCA הוא המודל הטוב ביותר שמתאים לנתונים שלנו. נראה תיכף שזה התגלה כלא נכון.

:::

מכל מקום, כל הבעיות האלה הביאו חוקרים לנסח אלגוריתם קצת שונה להורדת מימד, ספציפית למטרה של ויזאליזציה של נתונים: הt-distributed stochastic neighbor embedding, או הטיסני בקיצור.
אנחנו נראה שהמיפוי או האמבדינג ממימד גבוה X למימד נמוך יותר Y, הוא כבר לא ליניארי, שהפוקוס הוא לא על איזשהו אנקודר/דיקודר גלובליים להורדת מימד, אלא משהו הרבה יותר לוקאלי, המטרה של השיטה היא לדאוג שכל זוג תצפיות שדומה זו לזו במימד המקורי הגבוה X, יהיה גם קרוב במימד הנמוך Y, או בהסתברות גבוהה, ולהיפך: זוג רחוק יהיה רחוק בממד הנמוך.

בנוסף לזה טיסני יוסיף לנו משמעות הסתברותית בצורה מיידית מזו של PCA. לכל זוג תצפיות יש הסתברות להיות "בשכונה" אחת של השניה במימד הגבוה X, ובממד הנמוך Y. אם הן קרובות בממד המקורי, ההסתברות הזאת גדולה, ואם הן רחוקות היא קטנה, ונרצה לראות התפלגות דומה בממד הנמוך Y. כלומר המרחק ביניהן בממד המקורי הגבוה X יכתיב את ההסתברות שלהן להיות שכנות בממד הנמוך Y.

:::

אז יש הרבה פרטים קטנים באלגוריתם טיסני, לא נעבור על כולם רק על החשובים ביותר. השאלה הראשונה שצריך לשאול את עצמנו היא אם קיבלנו שתי תצפיות, בממד המקורי הגבוה, איך נמדל באמצעותן את ההסתברות שיהיו שכנות גם בממד הנמוך.

במימד הגבוה אנחנו משתמשים בטריק שכבר ראינו והוא הקרנל הגאוזייני, או הנורמלי.

כל תצפית או שורה באיקס היא וקטור באורך פי.

ואנחנו לא מגדירים ישר את הסתברות החיתוך, ששתי תצפיות יהיו שכנות זו לזו, אלא את ההסתברות המותנית, שאם ראינו את תצפית i מה הסיכוי לקבל את תצפית j באותה שכונה. כאן אפשר לראות בXi את תוחלת ההתפלגות הנורמלית, ואנחנו שואלים מה הצפיפות של לקבל את Xj בהתפלגות שזאת התוחלת שלה. טכנית זה המרחק האוקלידי ביניהם, חלקי שני סיגמא-i, כפול מינוס 1 באקספוננט. ואת כל זה אנחנו מחלקים בסכום על כל התצפיות השונו מ-i. 

נשים לב שאם המרחק מאוד מאוד קטן הכמות הזאת תשאף ל1, ואם הוא מאוד מאוד גדול היא תשאף לאפס. סכום ההסתברויות יוצא גם 1 כמו שצריך.

בהגדרה ההסתברות של i בהינתן i היא אפס, כי אין טעם לאפשר בגרף כזה קשת מi לעצמו.

וכדי לקבל את ההסתברות הסופית שi ו-j יהיו שכנים אנחנו עושים מה שהכותבים קראו לו סימטריזציה, שזה פשוט הסכום של ההסתברויות המותנות מחולק פי 2n. המוטיבציה לזה היא שאם אפריורית הסיכוי לכל תצפית i הוא 1 חלקי n, אז לפי ההגדרה של הסתברות מותנית כל אחת מההסתברויות המותנות בסכום היא הסתברות החיתוך חלקי הסתברות המאורע המתנה או כפול n, מה שנותן לנו פסוק אמת.

ואת היחס הזה אנחנו מחשבים פעם אחת לכל זוג תצפיות, כלומר אנחנו צריכים לאחסן טבלה של n על n הסתברויות. לפני שנמשיך שתי שאלות חשובות:

קודם כל למה לעבוד ככה דרך ההסתברות המותנית ולא פשוט להגדיר ישר את ההסתברות המשותפת? אז באמת בגרסאות קודמות של האלגוריתם זה בדיוק מה שהיה נהוג לעשות, אבל החוקרים שמו לב שזה בעייתי מאוד עם תצפיות שהן אאוטליירים, תצפיות שרחוקות מכל התצפיות האחרות. אם הביטוי כאן הוא ההסתברות המשותפת pij ולא המותנית, ותצפית i למשל היא אאוטלייר, כל ההסתברויות pij, לכל j, יהיו קטנות מאוד, כי אנחנו תמיד נחלק בסכום של אלמנטים נורא גדולים. וזה אומר שלתצפית i תהיה השפעה קטנה מאוד על האלגוריתם של טיסני שתיכף נציג. המיקום שלה בריצות חוזרות יכול להיות בכל מקום במימד הנמוך. בצורה הזאת אנחנו דואגים שלא כל ההסתברויות של התצפית החריגה i יהיו כל כך קטנות.

שאלה אחרת היא מאיפה אנחנו משיגים את הסיגמא-i. אז לא נגדיר את זה כאן כדי לקצר, אבל נשים לב שהסיגמא-i היא בעצם המדד לפיזור לתצפית i בקרנל הגאוזייני. אנחנו מחשבים את סיגמא-i בצורה כזאת שאם תצפית i היא בסביבה נורא צפופה, ויש המון תצפיות קרובות אליה, נרצה להתמקד בסביבה קטנה שלה, להגדיר שרק התצפיות הקרובות ביותר אליה יהיו באמת קרובות במימד הנמוך, והאחרות לא בהכרח, אז נרצה סיגמא-i קטן. אם תצפית i היא באיזור דליל, או ספרסי, נרצה להגדיל את סיגמא-i, להסתכל על סביבה גדולה יותר כדי לכלול בכל זאת את השכנים הקרובים.

מכל מקום זה מצריך מאיתנו להגדיר רק עוד פרמטר נוסף שנקרא פרפלקסיטי, שאפשר לראות בו כמספר השכנים עליהם נרצה להתמקד, בדרך כלל מספר בין 10 ל50, וככה אנחנו מחשבים את הסיגמא-i לכל תצפית, אתם יכולים לראות את הפרטים המדויקים במאמר המקורי.

:::

במימד הנמוך q, שהוא לצרכי ויזואליזציה אמרנו יהיה בדרך כלל 2 או 3, יש לנו תצפיות שנקרא להן y, כל אחד מהוואים הוא וקטור באורך q. וכאן נשתמש דווקא בקרנל של התפלגות טי עם דרגת חופש אחת. תיכף נדבר על למה.

אז מה זה התפלגות טי עם דרגת חופש אחת, אם משתנה Z למשל מתפלג טי עם דרגת חופש אחת אז זו פונקצית הצפיפות שלו: אחת חלקי פאי כפול 1 ועוד זי בריבוע. זאת התפלגות פעמונית סימטרית, נקראת גם התפלגות קושי.

כאן אין צורך לעבור דרך ההתפלגויות המותנות, כי אנחנו מראש עושים אמבדינג או מיפוי למרחב דו-מימדי או תלת מימדי, לא סביר שתהיה במרחב המאוד קטן שלנו תצפית שהיא נורא רחוקה מהשאר. אז אנחנו ממדלים ישירות את ההסתברות ששתי תצפיות שכנות זו של זו, כאמור עם הקרנל טי, אתם רואים שהמרחק האוקלידי עובר ריבוע, פלוס 1 והכל בהופכי, כשהגורם פאי מצטמצם עם המכנה. שימו לב שכדי שתהיה לנו הסתברות חיתוך כאן אנחנו מחלקים בסכום המרחקים על פני כל המטריצת מרחקים, חוץ מהאלכסון, כשעל האלכסון נשים כמו מקודם הסתברויות אפס.

את הטבלה הזאת נחשב בכל איטרציה של האלגוריתם, לא רק פעם אחת, כי המיקומים של הנקודות שלנו כל פעם ישתנו. נרצה שהיא תהיה דומה כמה שיותר לטבלה של pij. אבל לפני זה נשאל למה לא להשתמש גם כאן בקרנל גאוזייני? אז שוב בגרסאות קודמות יותר של האלגוריתם זה בדיוק מה שעשו החוקרים, והם שמו לב שזה מייצר בעייה מוכרת בתחום של הורדת מימד, בעיית ההתקהלות או קראודינג: כשאנחנו עושים אמבדינג של תצפיות ממימד גבוה למימד נמוך יש להן נטייה להצטופף או להתקהל. השימוש בהתפלגות טי שהיא התפלגות רחבה יותר, פחות מרוכזת סביב האפס ונותנת זנבות שמנים יותר, מאפשר לרווח את הנקודות בדו-מימד או בתלת-מימד, וככה כמו שתיכף נראה מתקבלים קלאסטרים באופן קל יותר, ולא גוש אחד גדול של תצפיות.

:::

השאלה האחרונה שנרצה לענות עליה היא מה זה אומר שההתפלגויות P וQ הן דומות? איזה מין מדד יש לנו להשוואה בין שתי התפלגויות, ואיך ייראה הקריטריון הסופי שלנו?

אז מדד מפורסם מאוד שאולי ראיתם בכל מיני מקומות אחרים הוא הקולבק-לייבליר דיוורג'נס, או הKL דיוורג'נס בקיצור. יש כאן פונקצית מרחק בין שתי התפלגויות בדידות במקרה הזה, אנחנו פשוט עוברים תא תא בין שתי המטריצות וסוכמים את הביטוי שרשום כאן.

לדוגמא בהתפלגות חד-מימדית עם כמה קטגוריות או כמה תאים, נעבור קטגוריה קטגוריה ונחשב את הכמות שרשומה כאן. אם ההתפלגות האדומה היתה זהה לכחולה, היינו מקבלים פשוט אפס, וככל שהן שונות נקבל מספר גדול יותר. נשים לב שזה לא סימטרי.

והKL יהיה הקריטריון שלנו לעשות עליו מינימום ביחס למיקומי וואי שלנו במימד הנמוך. איך אנחנו עושים מינימום? לדוגמא עם גרדיאנט דיסנט. בכל שלב של האלגוריתם טיסני נלך צעד קטן במורד הגרדיאנט של הפונקציה הזאת KL ביחס למיקומים, עד שהם ישתנו באופן כזה שההתפלגות Q תהיה כמה שיותר דומה להתפלגות P במימד המקורי.

:::

והנה האלגוריתם טיסני בהיי-לבל, מסכם את כל מה שאמרנו:

נחשב את טבלת ההסתברויות המשותפות בין כל זוג תצפיות כפי שלמדנו, נשים לב שבשביל זה אנחנו צריכים לחשב לכל תצפית את הסיגמא-i.

נדגום מיקומים רנדומליים במימד הנמוך בתור וקטורי וואי, אפשר גם להתחיל עם פתרון PCA.

וכעת בכל שלב של האלגוריתם נחשב את התפלגות Q עם הקרנל טי לפי הנוסחה שראינו, ונלך צעד אלפא קטן במורד הגרדיאנט של הKL דיוורג'נס. נמשיך ככה טי צעדים או עד התכנסות. שימו לב שאנחנו עוד פעם מתייחסים לתצפיות, לכל הוקטורים כפרמטרים שאנחנו רוצים למצוא, מזכיר קצת את הגישה של גרדיאנט בוסטינג.

כל מה שנשאר לעשות זה לחשב את הגרדיאנט, וזה לא חישוב קשה במיוחד ולא ביטוי קשה במיוחד, הנה הוא לפנינו.

האלגוריתם טיסני רץ מהר מאוד כל עוד n לא גדול מדי, נגיד עד 10000, מעבר לזה יכולה להיות בעיה אפילו רק באחסון הטבלאות האלה של P וQ. אבל כפי שאתם מתארים לעצמכם יש מודיפיקציות למסדי נתונים גדולים מאוד. למשל, בכל שלב של חישוב p_ij וq_ij, לא בטוח שצריך לחשב או אפילו לאחסן את היחס הזה בין כל זוג תצפיות אלא רק באיזושהי שכונה של תצפית i שאותה אפשר למצוא למשל בעזרת הילוך מקרי על הגרף התיאורטי הזה של תצפיות.

:::

בואו נראה דוגמה שממחישה יפה את השיפור שטיסני משיג הרבה פעמים לעומת PCA בהורדת מימד ממימד גבוה מאוד, למימד 2, בשביל ויזואליזציה.

הדאטא שנשתמש בו מפורסם מאוד, הוא כמעט קלישאה בתחום של למידה. הוא נקרא MNIST, ויש בו 70 אלף תמונות קטנות של 28 על 28 פיקסלים, של ספרות שנכתבו בכתב יד. כלומר שבעת אלפים דוגמאות לכל ספרה. אז איקס היא מטריצה בגודל 70 אלף שורות על 784 עמודות.

:::

טיסני לוקח יותר זמן להריץ על הנתונים האלה לעומת PCA אבל התוצאה איכותית הרבה יותר. כל ספרה צבועה בצבע אחר ומעניין לראות אם אחרי הורדת מימד הספרות מתחלקות לקלאסטרים ברורים. בצד שמאל אתם יכולים לראות מקרה ברור של קראודינג, גם אם יש הפרדה מסוימת בין הספרות, זה נראה כמו גוש אחד גדול של תצפיות. בצד ימין ההפרדה ברורה הרבה יותר, ואם למשל היינו מבצעים קלאסטרינג עם Kmeans על האמבדינג הזה סביר שהיינו מגיעים למודל הרבה יותר טוב.

:::

=== 6. PCA כמודל גנרטיבי ===

הגענו לחלק האחרון של השיעור על למידה unsupervised בדגש על PCA, שאני חושב שסוגר את הנושא יפה.

:::

מסתבר שגם לPCA יש משמעות הסתברותית, ממש כמו שלרגרסיה יש משמעות הסתברותית. וכשמבינים את זה אפשר לראות בPCA כמודל גנרטיבי, מודל כמו שלמדנו בשיעור שעבר, שמאפשר לג'נרט עוד דאטא, וככה שני הנושאים נפגשים.

מה אם כל שורת נתונים שלנו, נוצרה על ידי מודל שכזה: איזושהי תוחלת, מספר לכל פיצ'ר, ועוד מטריצה W כפול איזשהו וקטור מקרי, ועוד רעש מקרי אפסילון. מאוד מזכיר את המודל ברגרסיה.

אז שוב u_i הוא וקטור מקרי ממימד נמוך q מתוך התפלגות נורמלית, עם תוחלת אפס ומטריצת שונות שהיא מטריצת היחידה. כלומר מטריצת אפסים עם אחת על האלכסון.

W היא מטריצה ממשית בגודל p על q. מיו הוא וקטור באורך פי. ואפסילון-איי הוא וקטור מקרי באורך p שמייצג רעש אקראי מהתפלגות נורמלית עם איזשהו פרמטר סיגמה בריבוע שנצטרך לאמוד.

לפי החוקים של התפלגות נורמלית, אפשר לרשום את ההתפלגות השולית של שורה מהנתונים שלנו, של איקס-איי עצמו: הוא מתפלג נורמלית עם תוחלת שהיא אותו וקטור מיו שראינו, ומטריצת שונות שנסמן כסיגמה. סיגמה היא WW טרנספוז ועוד סיגמה בריבוע כפול מטריצה I.

וזה אומר שניתן גם לרשום את הנראות או הצפיפות של כל שורת נתונים שלנו: לפי הנוסחה של התפלגות רב-נורמלית שראינו בשיעור הקודם. כשאני אומר ששורת נתונים שלנו מתפלגת נורמלית עם תוחלת מיו ומטריצת שונות סיגמה, זאת בדיוק פונקצית הצפיפות שלה.

:::

הנראות של כל הדאטא היא מכפלת הצפיפויות של כל השורות. זה אומר שלוג הנראות היא סכום הלוגים של כל הצפיפויות, ואחרי קצת אלגברה אפשר להגיע לביטוי שלפנינו.

כאן המטריצה S היא מטריצת הקווריאנס של הנתונים, אם מתייחסים אליהם כמדגם מתוך ההתפלגות ועושים סכום על המכפלה של שורה פחות התוחלת שלה. אם הביטוי הזה מוכר לכם זה לא במקרה, הרי אם התוחלת היא אפס, המטריצה הזאת היא X'X שאנחנו עושים בה שימוש נרחב בPCA.

בכל מקרה, כמו בכל אמידת נראות מקסימלית האומד שלנו לפרמטרים שאנחנו לא יודעים הוא זה שמביא למקסימום את הנראות. וכאן יש לנו נוסחה סגורה לכל אחד מהפרמטרים. הפרמטר הכי מעניין הוא המטריצה W, שזה הביטוי לאומד נראות מקסימלית שלה.

V היא מטריצה בגודל p על q, ובה יש את q הוקטורים העצמיים הראשונים של מטריצת הקווריאנס S. למדא-קיו היא מטריצה אלכסונית q על q שעל האלכסון שלה נמצאים q הערכים העצמיים הגדולים ביותר של מטריצה S. וR היא מטריצת סיבוב בגודל q על q. מטריצת סיבוב היא מטריצה שכשאר מכפילים בה איזשהו וקטור היא לא משנה את הגודל שלו אלא רק את הכיוון, כלומר כל פתרון שקיבלנו אפשר לסובב איך שרוצים, למשל להשאיר אותו כפי שהוא כי הגדרנו שR היא מטריצת היחידה I.

אז מה זה אומר? שאם סיגמה בריבוע קטן מספיק, הפתרון שקיבלנו הוא דומה לפתרון של PCA, עדי כדי סיבוב! ולכל הגישה הזאת קוראים פרובבליסטיק PCA, או PCA הסתברותי. כלומר שנים אחרי שניסחו את הבעיה והפתרון של PCA, הצלחנו להראות שיש מודל הסתברותי גם מאחורי PCA, ושהוא למעשה מודל גנרטיבי. כעת אם תדגמו וקטור u_* ממימד נמוך, תעבירו אותו דרך המודל הגנרטיבי ליצירת דאטא ממימד גבוה, תוכלו ליצור דאטא חדש. ויש אינסוף וקטורים u_* שכאלה, כלומר ניתן לייצר אינסוף תצפיות במימד הגבוה, המקורי.

:::

למה אני כל כך נרגש, למה זה כל כך חשוב.

כי יש לנו פרוצדורה שמתקבלת בכל כך הרבה דרכים, שמקשרת לנו בין כל כך הרבה נושאים.

ראינו שאפשר לקבל את PCA מתוך רצון למקסם את פיזור ההטלה, עם בעיית ערכים עצמיים.

הזכרנו גם שאפשר לפתח את זה כבעיית פירוק SVD.

הוכחנו שאפשר לקבל PCA מהסתכלות על הבעיה כבעיה של דחיסה של נתונים בכלל, עם אנקודר ודיקודר ליניאריים -- דבר שיעזור לכם להבין רשתות נוירונים של אוטואנקודרים כשתלמדו עליהן.

ולבסוף ראינו שPCA הוא גם מודל גנרטיבי, שאנחנו מניחים שיש איזשהו מנגנון שיצר כל שורה, שמעורבים בו משתנים מקריים. הפתרון מתקבל דרך אמידת נראות מקסימלית.

וכאן אנחנו גם מגיעים לאיזשהו צומת בין השיעור הקודם על מודלים גנרטיביים, שמניחים שיש מנגנון שיוצר את הדאטא, לבין למידה בלתי מפוקחת. באופן טבעי אם הגענו למודל טוב, אנחנו יכולים במימד הנמוך להגריל משתני U, להעביר אותם למימד הגבוה ולקבל דאטא חדש. איפה הדבר יכול מאוד לעזור? ביצירה של תמונות חדשות. ככה בדיוק נוצרו האייגנפייסז שדיברנו עליהם בשיעור הקודם. פשוט מגרילים וקטור יו במימד הנמוך וכופלים אותו פי W טרנספוז. ואפשר לייצר אינסוף פרצופים כאלה.

במילים אחרות אפשר לראות בPCA כאב קדמון של המודלים הגנרטיביים שאנחנו רואים כיום שמחוללים טקסט שאף פעם לא נכתב או תמונות שאף פעם לא צולמו. והמפתח לתמונות מג'ונרטות איכותיות הוא למידה לא מפוקחת איכותית.

עד כאן הטעימה שלנו בנושא החשוב הזה שאפשר לבלות עליו סמסטר שלם. למידה לא מפוקחת נעשית מעניינת אפילו יותר כשמוסיפים רשתות נוירונים, על כך תצטרכו ללמוד כבר בקורס אחר.

:::
