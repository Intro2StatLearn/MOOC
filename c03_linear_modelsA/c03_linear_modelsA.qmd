---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Linear Models - Part A"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Linear Models - Part A - Class 3

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Simple linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשיעור זה נעסוק ברגרסיה ליניארית, אחד המודלים הותיקים והנחקרים ביותר בספרות. חשוב להדגיש כי רגרסיה ליניארית זה נושא שאפשר להקדיש לו גם סמסטר שלם ואפילו שניים ושלושה, נושא שנכתבו עליו אינספור ספרים עבי כרס, ולא נוכל להתעמק בכל ניואנס. נתמקד בדברים החשובים לנו בקורס.
:::
:::

---

### Why learn linear regression?

::: {.incremental}
- Linear regression is still a highly useful modeling tool
- Super-fast to train and predict, super-simple to implement
- It can be used in many non-linear cases by using transformed variables / transformed goals
- Its probabilistic aspect is fully understood, so inference questions can be answered exactly (under the regression assumptions)
- Many newer methods can be seen as a generalization for linear regression
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז למה אנחנו בעצם לומדים רגרסיה ליניארית, זה לא קצת מיושן? התשובה המפתיעה היא שלא. בניגוד למה שנדמה לנו לפעמים מהתקשורת, רגרסיה ליניארית עדיין חיה ובועטת ובשימוש בתעשייה.

קודם כל מדובר במודל סגור, אינטואיטיבי ומהיר מאוד, גם לאימון ולא פחות מזה לחיזוי, בסופו של דבר מדובר במכפלה של שני וקטורים כמו שנראה, שקל לממש בכל שפת תכנות.

רגרסיה ליניארית ניתנת להכללה גם להרבה מצבים לא-ליניאריים בעליל, באמצעות טרנספורמציות מתאימות על המשתנים המסבירים או המשתנה התלוי.

ההיבט ההסתברותי שלה ידוע ומובן, כך שניתן לבצע בקלות הסקה סטטיסטית על המקדמים ולקבל תשובות על הרבה שאלות תחת הנחות המודל.

ואולי הכי חשוב, רגרסיה ליניארית משמשת כבייסליין להרבה שיטות מודרניות אחרות שנראה, שהן הכללה של רגרסיה ליניארית. הרבה חוקרים שוכחים את זה, ותוקפים את הנתונים שלהם באמצעות כל מיני מודלים שונים ומשונים בלי לבדוק כמה כבר הביצוע שלהם משפר את הבייסליין הפשוט הזה
:::
:::

---

### Simple linear regression

::: {.incremental}
- Assume: $y \approx \beta_0 + \beta_1x$
- This is the *unobserved* [population regression]{style="color:red;"} line
- We look for the values of $\beta_0, \beta_1$ through a sample $\{(x_1, y_1), \dots, (x_n, y_n)\}$
- In order to estimate the parameters we need to define a measure of error
- By far the most common measure is:
:::

::: {.fragment}
$RSS = \sum_{i = 1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2$ or its scaled version: $MSE = \frac{RSS}{n}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל ברגרסיה ליניארית פשוטה, עבור משתנה תלוי יחיד Y, ומשתנה מסביר יחיד X. אנחנו מניחים שY משתנה בצורה ליניארית עם X, כלומר קו ישר.

לקו הזה קוראים קו הרגרסיה של האוכלוסיה, והוא לא נצפה. אנחנו לוקחים מדגם בגודל n של זוגות תצפיות של X וY, ומחפשים את הבטא-אפס ואת הבטא-אחת, החותך והשיפוע של הקו הנסתר הזה.

אנחנו קובעים קריטריון שאומר מתי הבטאות טובות ומתי לא, איזו מידה של טעות, והקריטריון הנפוץ ביותר הוא הRSS: הרזידואל סאם אוף סקוורז, כלומר לקחת את השאריות של Y פחות הY הנחזה עם הבטאות הנאמדות, להעלות בריבוע ולסכום. או, סכום המרחקים הריבועיים של התצפיות מקו הרגרסיה הנאמד.

אפשר גם לחלק את הRSS פי n ולקבל את המין סקוורד ארור או הMSE, אפשר גם לקחת שורש ולקבל את הרוט מין סקוורד ארור, או הRMSE.
:::
:::

---

### From line to RSS surface

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from mpl_toolkits.mplot3d import Axes3D

# Set the random seed for reproducibility
np.random.seed(42)

# Number of samples
n = 100

# Generate X values
X = np.random.rand(n, 1)

# Generate noise
e = np.random.randn(n, 1)

# True population line parameters
beta_0_true = 2
beta_1_true = 3

# Generate Y values
Y = beta_0_true + beta_1_true * X + e

# Fit the least squares regression line
model = LinearRegression().fit(X, Y)
Y_pred = model.predict(X)

# Create a meshgrid for beta_0 and beta_1
beta_0_range = np.linspace(beta_0_true - 2, beta_0_true + 2, 100)
beta_1_range = np.linspace(beta_1_true - 2, beta_1_true + 2, 100)
beta_0, beta_1 = np.meshgrid(beta_0_range, beta_1_range)

# Calculate the RSS for each combination of beta_0 and beta_1
RSS = np.zeros(beta_0.shape)
for i in range(beta_0.shape[0]):
    for j in range(beta_0.shape[1]):
        Y_pred_mesh = beta_0[i, j] + beta_1[i, j] * X
        RSS[i, j] = np.sum((Y - Y_pred_mesh) ** 2)

# Prepare for plotting
fig = plt.figure(figsize=(5 * 3, 3.5))

ax1 = fig.add_subplot(131)
# Plot the sample points
ax1.scatter(X, Y, color='blue', label='Sample points')
ax1.set_xlabel(r'$x$')
ax1.set_ylabel(r'$y$')
# Plot the least squares regression line
ax1.plot(X, Y_pred, color='red', linestyle='--', label='Least squares line', linewidth=2)
# Plot vertical lines from each sample point to the fitted line
for i in range(n):
    ax1.plot([X[i], X[i]], [Y[i], Y_pred[i]], color='gray', linestyle=':', linewidth=1)
ax1.set_title(r'Residuals for specific $\hat{\beta}_0, \hat{\beta}_1$')

# Contour plot on the left
ax2 = fig.add_subplot(132)
contour = ax2.contour(beta_0, beta_1, RSS, levels=50, cmap='viridis')
ax2.set_xlabel(r'$\beta_0$')
ax2.set_ylabel(r'$\beta_1$')
ax2.set_title('Contour plot of RSS')
fig.colorbar(contour, ax=ax2, label='RSS')

# Add a red cross at the true beta values
ax2.plot(beta_0_true, beta_1_true, 'rx', markersize=12, markeredgewidth=3)
# Add dashed lines from the cross to the true beta_0 and beta_1 axes
ax2.plot([beta_0_true, beta_0_true], [beta_1_range.min(), beta_1_true], 'r--')
ax2.plot([beta_0_range.min(), beta_0_true], [beta_1_true, beta_1_true], 'r--')

# 3D plot on the right
ax3 = fig.add_subplot(133, projection='3d')
ax3.plot_surface(beta_0, beta_1, RSS, cmap='viridis', edgecolor='none')
ax3.set_xlabel(r'$\beta_0$')
ax3.set_ylabel(r'$\beta_1$')
ax3.set_zlabel('RSS')
ax3.set_title('3D plot of RSS surface')

# Add a red cross at the true beta values
ax3.plot([beta_0_true], [beta_1_true], [np.min(RSS)], 'rx', markersize=12, markeredgewidth=3, zorder=10)

# Add dashed lines from the cross to the true beta_0 and beta_1 axes
ax3.plot([beta_0_true, beta_0_true], [0, beta_1_true], [np.min(RSS), np.min(RSS)], 'r--', zorder=10)
ax3.plot([4, beta_0_true], [beta_1_true, beta_1_true], [np.min(RSS), np.min(RSS)], 'r--', zorder=10)

# plt.tight_layout()
plt.show()
```

$$RSS = \sum_{i = 1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז כאמור ככה נראות התצפיות שלנו, כל נקודה היא תצפית עם קואורדינטה X וקואורדינטה Y, נראה שיש יחס ליניארי עולה בין X לY ולכן המודל הליניארי מתאים.

אנחנו מחשבים לכל תצפית את השארית שלה מקו רגרסיה אדום שאנחנו משיגים באמצעות חיפוש על הבטאות, ומחשבים את סכום המרחקים הריבועיים. בתרשימים האחרים ניתן לראות איך נראה קריטריון הRSS כפונקציה של הבטאות, באמצעות קונטור-פלוט או באמצעות תרשים תלת-מימדי. בכל מקרה הבטאות שנבחר הן הבטאות שמביאות למינימום את הRSS, וזה מתבטא בקו הרגרסיה האדום.
:::
:::

---

### Simple linear regression

::: {.incremental}
- A simple derivation gives:

::: {.fragment}
$\hat{\beta}_1 = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} = \frac{\widehat{Cov(X,Y)}}{\widehat{Var(X)}},\;\;\;\;\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$
:::
- This is the [least squares]{style="color:red;"} line (OLS), the *best* linear predictor for the population regression line
- Replacing the sample averages by the population means (or having a huge sample) should get us to the true line
- Using $\hat{\beta}_0, \hat{\beta}_1$ we can easily perform prediction
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
במקרה של רגרסיה ליניארית פשוטה, אין צורך באמת "לחפש", הRSS הוא פונקציה פשוטה וקמורה. אם גוזרים אותו לפי בטא-אפס ולפי בטא-אחת ומשווים לאפס, מתקבלים הביטויים הסגורים האלה. בטא-אחת-האט הוא פשוט הקווריאנס במדגם בין X לY מחולק בשונות המדגם של X. ובטא-אפס-האט מתקבל מהצבת בטא-אחת-האט וממוצעי המדגם של X ושל Y בקו הרגרסיה.

זה קו הליסט סקוורז, או הOLS, והוא הקו האומד הטוב ביותר לקו הרגרסיה של האוכלוסיה, תחת ההנחות. הטוב ביותר כלומר מביא למינימום את קריטריון הRSS. כשנוסיף את ההיבט ההסתברותי לרגרסיה ליניארית נוכל להוכיח שהקו הזה הוא גם אומד בלתי-מוטה לקו הרגרסיה של האוכלוסיה, כרגע רק נגיד שאם ניקח מדגם עצום, ואז ממוצעי המדגמים הם כבר לא ממוצעים אלא תוחלות, אנחנו צריכים לקבל את קו הרגרסיה האמיתי, של האוכלוסיה.

מכל מקום, ברגע שמתקבלים האומדים למקדמי הרגרסיה כאמור קל לתת חיזוי לכל תצפית, לפי הנוסחה.
:::
:::

---

### Average of many OLS lines

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Set the random seed for reproducibility
np.random.seed(42)

# Number of samples
n = 100

# Generate X values
X = np.random.rand(n, 1)

# Generate noise
e = np.random.randn(n, 1)

# True population line parameters
beta_0 = 2
beta_1 = 3

# Generate Y values
Y = beta_0 + beta_1 * X + e

# Fit the least squares regression line
model = LinearRegression().fit(X, Y)
Y_pred = model.predict(X)

# Prepare for plotting
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Plot on the left
axs[0].scatter(X, Y, color='blue', label='Sample points')
axs[0].plot(X, beta_0 + beta_1 * X, color='green', label='True population line', linewidth=2)
axs[0].plot(X, Y_pred, color='red', label='Least squares line', linewidth=2)
axs[0].set_title('Single Sample Regression')
axs[0].legend()

# Plot on the right
# axs[1].scatter(X, Y, color='blue', label='Sample points')
axs[1].plot(X, beta_0 + beta_1 * X, color='green', label='True population line', linewidth=2)

# Plot 10 additional estimated regression lines
for _ in range(10):
    X_sample = np.random.rand(n, 1)
    e_sample = np.random.randn(n, 1)
    Y_sample = beta_0 + beta_1 * X_sample + e_sample
    model_sample = LinearRegression().fit(X_sample, Y_sample)
    Y_sample_pred = model_sample.predict(X)
    axs[1].plot(X, Y_sample_pred, color='orange', linestyle='--', alpha=0.6)

# Also plot the estimated line from the original sample
axs[1].plot(X, Y_pred, color='red', label='Original sample estimated line', linewidth=2)
axs[1].set_title('Multiple Sample Regressions')
axs[1].legend()

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בסימולציה שלפנינו אפשר לראות את שני הקווים המדוברים: קו הרגרסיה האמיתי של האוכלוסיה בירוק, על-פי הקו הזה נדגמו 100 נקודות. וקו הריבועים הפחותים באדום, שנאמד אך ורק מתוך הנקודות. אפשר לראות שהם דומים. בתרשים הימני אנחנו חוזרים על התרגיל עוד 10 פעמים, כל פעם דוגמים 100 תצפיות אחרות ואומדים את קו הריבועים הפחותים. אם נחזור על זה הרבה פעמים או אם נדגום המון תצפיות, אנחנו אמורים לקבל בממוצע את קו הרגרסיה האמיתי של האוכלוסיה.
:::
:::

---

## Detour: Maximum likelihood estimation (MLE) {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לפני שנמשיך אנחנו עושים איזה דיטור קצר כדי להבין מושג חשוב מאוד: אמידת נראות מקסימלית. מקסימום לייקליהוד אסטימיישן. נראות, היא הקריטריון המקובל להתאמת מודלים סטטיסטיים. בלי להבין מה זה הקריטריון הזה של נראות ולמה צריך למקסם אותו, די קשה להבין את ההסתכלות ההסתברותית על רגרסיה ליניארית שנראה בהמשך, שמוסיפה לנו כל כך הרבה.
:::
:::

---

### Example: Coin Toss

::: {.incremental}
- Suppose I get a coin, its result is $X_i$, where $P(X_i = H) = p$
- How can I estimate $p$?
- I toss the coin $n = 5$ times: $\{H, H, T, T, H\}$
- What is the probability of getting $3H$ and $2T$?
  - For $p = 0.2 \rightarrow P(3H, 2T) = {5\choose3} \cdot 0.2^3 \cdot 0.8^2 \approx 0.05$
  - For $p = 0.3 \rightarrow P(3H, 2T) = {5\choose3} \cdot 0.3^3 \cdot 0.7^2 \approx 0.13$
- So simple, let's calculate for any $p$ and choose $\hat{p} = \arg\max_p P(3H, 2T)$
:::

::: {.fragment}

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from scipy.special import comb

# Likelihood function
def likelihood(p, k, n):
    return comb(n, k) * (p**k) * ((1 - p)**(n - k))

# Parameters for the coin toss experiment
k = 3  # number of heads
n = 5  # total coin tosses

# Values of p from 0 to 1
p_values = np.linspace(0, 1, 100)

# Calculate likelihood for each value of p
likelihood_values = [likelihood(p, k, n) for p in p_values]

# Plot the likelihood function
plt.figure(figsize=(3.5, 2.5))
plt.plot(p_values, likelihood_values, color='black')

# Add a vertical line at the MLE (p = 0.6)
plt.axvline(x=0.6, color='black', linestyle='dotted')

# Set plot labels and show
plt.xlabel('p')
plt.ylabel('P(3H, 2T)')
plt.show()
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל בדוגמא של הטלת מטבע. אני מקבל מטבע שנסמן את התוצאה של ההטלה שלו בX, ואנחנו מניחים שיש סיכוי קבוע להטלה של ראש או הדז, p.

איך אפשר לאמוד את הפרמטר p? אני בטוח שיש לכולם תשובה אינטואיטיבית לזה, אנחנו נפרמל את זה קצת.

בואו נזרוק את המטבע למשל 5 פעמים. ונניח שאלה התוצאות שמתקבלות, כשכנראה אין חשיבות לסדר: קיבלנו 3 פעמים הדז ופעמיים טיילז.

מה הסיכוי לקבל תוצאה כזאת?

אני לא יודע, כי אני לא יודע את p. אני כן יכול להציב ערכים.

לדוגמא אם p = 0.2, מדובר בהסתברות בינומית רגילה, לבחור 3 מקומות להדז מתוך 5, כפול הסיכוי להדז בחזקת 3, כפול הסיכוי לטיילז בחזקת 2.

ואם p = 0.3, אז יוצא ביטוי אחר. בכל מקרה ברור שאם הסיכוי לקבל את המדגם שקיבלתי תחת ההנחה שp = 0.3 גדול יותר מהסיכוי לקבל את המדגם תחת ההנחה שp = 0.2, נראה שהערך 0.3 סביר יותר.

אז למה לעצור כאן, אפשר לחשב את הסיכוי של המדגם שלנו לכל ערך של p ולבחור באומד p-hat שימקסם את ההסתברות הזאת.

וזה בדיוק מה שאנחנו עושים בגרף הבא, אנחנו משרטטים את הסיכוי לקבל את המדגם שלנו, של שלושה הדז ושני טיילז, כנגד p. וכבר סימנו את הp שמביא את הקריטריון הזה למקסימום. אלה שהשתמשו באינטואיציה שלהם לא יופתעו לראות שהp הטוב ביותר הוא 0.6, שזה 3 הצלחות מתוך 5 ניסיונות. אבל זאת דוגמא ממש פשוטה. נכליל אותה, לקריטריון הזה נקרא הנראות, או הלייקליהוד, והאומד שלנו ייקרא אומד נראות מקסימלית.
:::
:::

---

### Maximum likelihood estimation (MLE) (I)

::: {.incremental}
- First Generalization:
  - $X_i \sim Bernoulli(p)$, $n$ tosses, got $k$ Heads
  - $\text{Likelihood}(p) = L(p|x_1, \dots, x_n) = P(x_1, \dots, x_n) = {n \choose k}p^{k}(1-p)^{n -k}$
  - Goal: $\hat{p} = \arg\max_p{L(p|x_1, \dots, x_n)}$
  - Not surprising: $\hat{p} = \frac{k}{n}$
  - Notice with: $\text{Log-Likelihood}(p) = \ell(p|x_1, \dots, x_n) = \log P(x_1, \dots, x_n) \approx k\log p + (n -k)\log(1-p)$
    - we get the same $\hat{p} = \frac{k}{p}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז הכללה ראשונה היא לסיפור כללי יותר. תוצאת ההטלה מתפלגת ברנולי עם איזשהו סיכוי p, אנחנו רוצים לאמוד אותו. אנחנו לוקחים מדגם של n הטלות ומקבלים בשורה התחתונה k פעמים הדז.

ההסתברות של המדגם שלנו היא ההסתברות של תוצאה בינומית שרשומה כאן, ונגדיר שהיא פונקציה של p שנקרא לה נראות, לייקליהוד. מסמנים בדרך כלל עם L גדולה כפונקציה של p בהינתן המדגם שהתקבל. זה בעצם ההסתברות לקבל את המדגם.

המטרה היא לעשות מקסימיזציה לפונקציה הזאת, והאומד שלנו p-hat יהיה הערך שיביא אותה למקסימום, לכן זה נקרא אמידת נראות מקסימלית.

במקרה שלנו, אם נגזור את הביטוי, נשווה לאפס, ונחפש נקודות קיצון, נקבל ביטוי סגור לנקודת המקסימום: k חלקי n. שזה בעצם ממוצע המדגם אם היינו מסמנים את הדז כ1 וטיילז כ0, בדוגמא שלנו זה יצא 3/5 ואמרנו שזה לא מפתיע.

ונשים לב, שאם היינו בוחרים לעשות לוג על הקריטריון, ולעבוד עם לוג הנראות, שמסומן בדרך כלל עם l קטנה, היינו מגיעים לביטוי פשוט יותר, ועדיין מקבלים אותו אומד נראות מקסימלית. הסיבה היא שלוג היא פונקציה מונוטונית עולה של הנראות.
:::
:::

---

### Maximum likelihood estimation (MLE) (II)

::: {.incremental}
- Second generalization, when $X_1, \dots, X_n \sim P_\theta$ i.i.d
  - $\text{Likelihood}(\theta) = L(\theta|x) = P_\theta(x_1, \dots, x_n) = \prod_{i = 1}^n P_\theta(X_i = x_i)$
  - Goal: $\hat{\theta} = \arg\max_\theta{L(\theta|x)}$
  - $\hat{\theta}$ is the MLE, not always a closed solution
  - Usually easier: $\text{Log-Likelihood}(\theta) = \ell(\theta|x) = \log P_\theta(x_1, \dots, x_n) = \sum_{i = 1}^n \log P_\theta(X_i = x_i)$
- For a continuous $X \sim f_\theta(x)$:
  - $L(\theta|x) = \prod_{i = 1}^n f_\theta(x_i)$
  - $\ell(\theta|x) = \sum_{i = 1}^n \log f_\theta(x_i)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
באופן כללי אפילו יותר אפשר לדבר על מדגם של משתנה מקרי X שמתפלג כל התפלגות שתלויה בפרמטר אחד או יותר שנקרא להם תטא. ואנחנו בדרך כלל עובדים עם מדגם iid כלומר תצפיות שמתפלגות בצורה זהה ובלתי תלויה אחת בשניה.

הנראות היא פונקציה של תטא שמקובל לסמן בL גדולה של תטא בהינתן הנתונים או תוצאת המדגם. ואם הוא iid זה פשוט מכפלת ההסתברויות.

והמטרה היא למצוא אנ"מ, אומד נראות מקסימלית שיביא למקסימום את הנראות.

במקרה של התפלגות ברנולי יש פתרון סגור, אבל הגישה הזאת נכונה גם למקרים הרבה יותר מסובכים, ואין תמיד פתרון סגור ואז משתמשים בשיטות נומריות.

ובאופן כללי, הרבה פעמים עוברים דרך לוג הנראות כי הוא מביא לביטוי פשוט יותר, מכפלת ההסתברויות הופכת להיות סכום הלוג של ההסתברויות.

ואם המשתנה X הוא רציף, פשוט נעבוד דרך הצפיפויות:

הנראות תהיה מכפלת הצפיפויות, ולוג הנראות יהיה סכום הלוג של הצפיפויות.

זהו לגבי הדיטור שלנו. חשוב להזכיר אם אנחנו כבר מבקרים כאן, אמידת נראות מקסימלית היא נושא כבד בסטטיסטיקה, אפשר להראות שלאומד נראות מקסימלית יש כל מיני תכונות מאוד מושכות, אפשר להשוות בינו לבין אומדים אחרים -- אם זה מעניין אתכם בהחלט מומלץ להרחיב את הדעת עם קורסים אחרים. ועכשיו בחזרה לרגרסיה ליניארית.
:::
:::

---

## Probabilistic view of linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Probabilistic justification for OLS

::: {.incremental}
- Assume the data's true model is:
  - $y_i = \beta_0 + \beta_1x_i + \varepsilon_i$
  - $\varepsilon_i$ are i.i.d, specifically: $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$
  - $(x_i, \varepsilon_i)$ are independent
- *Now* we can write the log likelihood and maximize it
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עד כה לא הנחנו אף הנחה הסתברותית. לא הזכרנו משתנים מקריים, בטח לא התפלגות נורמלית. זה חשוב להבין שפתרון הרגרסיה שראינו הוא פתרון עם הצדקה מתמטית בלי שום הצדקה סטטיסטית. ומסתבר שאם נרצה הצדקה סטטיסטית כזאת נגיע לפתרון זהה, אבל נרוויח עוד כמה דברים בדרך.

אז נוסיף למודל שלנו משתנה מקרי אפסילון, משתנה רעש. כלומר אנחנו מניחים שיש פונקציה ליניארית קבועה של Y כנגד X והסיבה שהתצפיות לא יושבות בדיוק על הקו הזה היא שעבור כל אחת נוסף איזשהו רעש טבעי שמזיז אותה קצת למעלה או למטה מהקו. בנוסף נניח שהאפסילונים הם בלתי תלויים ומתפלגים זהה או IID. נזכיר שוב שניתן להכליל גם למצב שתוספות הרעש הזה אינן בלתי-תלויות, אלא תלויות למשל בX. אבל נתחיל במודל הפשוט הזה, ונניח עוד שהרעש מתפלג נורמלית עם תוחלת אפס ושונות סיגמא בריבוע, פרמטר לא ידוע בדרך כלל שנצטרך לאמוד.

אז כאמור המשתנה המסביר X והרעש אפסילון בלתי תלויים, וכעת יש לנו מודל הסתברותי מדויק וברור, והקריטריון הטבעי לאמוד את הפרמטרים שלו הוא למקסם את הנראות, הלייקליהוד, או לוג הנראות, כי יש לו צורה פשוטה יותר בדרך כלל.
:::
:::

---

### Simple linear regression maximum likelihood

::: {.fragment}
Since the errors are normal and the model is linear we get:

$y_i = \beta_0 + \beta_1x_i + \varepsilon_i  \Rightarrow y_i \sim \mathcal{N}(\beta_0 + \beta_1x_i, \sigma^2) \Rightarrow$
:::

::: {.fragment}
$$L(\beta, \sigma^2 | x, y) = \prod_{i = 1}^n f(y_i) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(y_i - \beta_0 - \beta_1x_i)^2}{2\sigma^2}\right]$$
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_i)^2\right]$$
:::

::: {.fragment}
Calculating the log-likelihood we get:

$\ell(\beta, \sigma^2 | x, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}$[$\sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_i)^2$]{style="color:red;"}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נשים לב ש X הוא לא משתנה מקרי תחת מודל הרגרסיה, רק אפסילון הרעש משתנה מקרי. יוצא מזה שY הוא טרנספורמציה ליניארית של משתנה מקרי, ולכן הוא בעצמו משתנה מקרי שמתפלג נורמלית, עם תוחלת שהיא קו הרגרסיה האמיתי, ושונות סיגמה בריבוע.

מהי פונקצית הנראות? פונקצית הנראות היא פונקציה של הפרמטרים שאנחנו צריכים לאמוד, בהינתן מדגם הנתונים. ההגדרה שלה עבור משתנה רציף היא הצפיפות המשותפת של המדגם, ואם התצפיות בלתי תלויות מדובר במכפלת הצפיפויות. מכפלת הצפיפויות היא מכפלת הצפיפויות הנורמליות של Y, ואפשר לפשט אם זוכרים שמכפלת האקספוננטים היא אקספוננט בחזקת הסכום. את הביטוי הזה נרצה להביא למקסימום כפונקציה של הבטאות וסיגמא בריבוע.

אבל נוח יותר לקחת לוג ולהביא למקסימום את לוג הנראות. וכשלוקחים לוג, מתקבל איזשהו קבוע שלא חשוב לנו כי הוא לא פונקציה של הפרמטרים, ביטוי שתלוי רק בסיגמא בריבוע, אבל הבטאות משחקות תפקיד רק בביטוי השלישי, שמזכיר באופן חשוד את הRSS. המסקנה היא שלגבי הבטאות, להביא למינימום את הRSS או להביא למקסימום את לוג הנראות -- זה בדיוק אותו דבר! כלומר באמצעות ההנחות הסטטיסטיות שהוספנו, אנחנו מקבלים בדיוק את אותם אומדים בטא-האט, ועד לנקודה זו לא השתנה שום דבר.

מה כן השתנה? השינוי העיקרי הוא שהאומדים שלנו, בטא-אפס-האט ובטא-אחת-האט הם כעת משתנים מקריים, שניתן לבצע עליהם הסקה סטטיסטית. נראה את זה תיכף, אבל קודם כל נדבר על רגרסיה ליניארית מרובה, מה קורה כשיש P משתנים מסבירים?
:::
:::

---

## Multiple linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### From simple to multiple regression

<br></br>

$y_i = \beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip} + \varepsilon_i \Rightarrow y_i = x_i^T\beta + \varepsilon_i$, $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$

::: {.fragment}
Or even more concisely:

$y = \begin{pmatrix}y_{1} \\ \vdots \\ y_{n}\end{pmatrix}$, $X = \begin{pmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p} \\
1 & x_{31} & x_{32} & \cdots & x_{3p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{np}
\end{pmatrix}$, $\beta = \begin{pmatrix}\beta_0 \\ \beta_{1} \\ \vdots \\ \beta_{p}\end{pmatrix}$, $\varepsilon = \begin{pmatrix}\varepsilon_{1} \\ \vdots \\ \varepsilon_{n}\end{pmatrix}$
:::

::: {.fragment}
$$\Rightarrow y = X\beta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(\textbf{0}, \sigma^2\mathbf{I}_n)$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז כעת יש לנו P משתנים מסבירים, ועוד חותך. זה אומר P פלוס אחת פרמטרים, לא כולל את סיגמא בריבוע מהמודל ההסתברותי. דרך נוחה יותר לרשום את זה היא אם x_i, ובטא הם וקטורי עמודות, ואנחנו מוסיפים 1 לx_i כדי להכפיל אותו בחותך. אז y_i שווה למכפלה הפנימית בין וקטור הX לוקטור הבטא, ועוד משתנה רעש אפסילון, שכמו מקודם מתפלג נורמלית עם תוחלת אפס ושונות סיגמא בריבוע.

אבל אם אנחנו כבר נעזרים בוקטורים, נהוג לרשום את Y עצמו גם כן כוקטור עמודה באורך n, אותו דבר לגבי וקטור הבטא ווקטור אפסילון. וX גדול הוא מטריצת הנתונים שלנו, כל שורה היא אותו וקטור x_i עם התוספת 1 לחותך, כלומר קיבלנו מטריצה עם n שורות ל-n תצפיות, וP פלוס 1 עמודות, P עמודות למשתני X ועוד עמודת אחדות שהתווספה מצד שמאל בדרך כלל.

אז אפשר לרשום את המודל בכתיב וקטורי שעושה את המימוש למהיר הרבה יותר, Y הוא המטריצה X מוכפלת בוקטור בטא, ועוד תוספת וקטור רעש אפסילון. כעת נשים לב שההתפלגות של אפסילון היא התפלגות של וקטור. אם אתם לא זוכרים מה זה התפלגות של וקטור משתנים מקריים לא נורא, אנחנו נחזור לזה כשנצטרך את זה ממש. בינתיים שימו-לב, שהתוחלת של הוקטור משתנים מקריים הזה היא וקטור, וקטור האפסים, והשונות שלו היא מטריצה. מטריצה בגודל n על n, שעל האלכסון שלה נמצאות השונויות של כל אלמנט ואלמנט של הוקטור - במקרה שלנו, כולן סיגמא בריבוע. ומחוץ לאלכסון נמצאים הקווריאנסים בין כל שני אלמנטים בוקטור, במקרה שלנו ההנחה שכל זוג טעויות הן בלתי תלויות, כלומר הקווריאנס הוא אפס. ובקיצור אפשר לרשום את המטריצה כסיגמא בריבוע, הסקלר, כפול מטריצת היחידה בגודל n על n.
:::
:::

---

### From line to (hyper)plane
```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from matplotlib.patches import Patch
from matplotlib.lines import Line2D

df = pd.read_csv('../datasets/ISLP_income_data.csv')
X1 = df['Education']
X2 = df['Seniority']
X = np.column_stack((X1, X2))
Y = df['Income']

# Plotting
fig, ax = plt.subplots(1, 1, figsize=(14, 7), subplot_kw={'projection': '3d'})

# Create a grid of values for X1 and X2
X1_grid, X2_grid = np.meshgrid(np.linspace(min(X1), max(X1), 200),
                               np.linspace(min(X2), max(X2), 200))

# Flatten the grid to pass into the model
X_grid = np.column_stack((X1_grid.ravel(), X2_grid.ravel()))

model = LinearRegression()

model.fit(X, Y)

# Predict Y values
Y_pred = model.predict(X_grid).reshape(X1_grid.shape)

# Plot the fitted plane
ax.scatter(X1, X2, Y, color='red', label='Data')
ax.plot_surface(X1_grid, X2_grid, Y_pred, color='blue', alpha=0.5)

# Add vertical lines from data points to the surface
Y_surface = model.predict(X).reshape(Y.shape)  # Predicted Y values for the actual data points
for x1, x2, y, y_surface in zip(X1, X2, Y, Y_surface):
    ax.plot([x1, x1], [x2, x2], [y, y_surface], color='black')

# Labels
ax.set_xlabel('Education')
ax.set_ylabel('Seniority')
ax.set_zlabel('Income')
# ax.set_title(title)

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז לפני לוג הנראות, מה אומר המודל הליניארי כשיש יותר ממשתנה אחד? המודל שלנו הוא לא קו, אלא מישור, או היפר-מישור כשמדובר במימד גבוה. אם לדוגמא אנחנו ממדלים הכנסה כפונקציה של השכלה וותק במודל הליניארי, אנחנו מניחים שהכנסה היא מישור על המרחב הנפרש בידי השכלה וותק.
:::
:::

---

### Multiple linear regression ML

$y_i = x_i^T\beta + \varepsilon_i  \Rightarrow y \sim \mathcal{N}(x_i^T\beta, \sigma^2) \Rightarrow$

::: {.fragment}
$$L(\beta, \sigma^2 | X, y) = \prod_{i = 1}^n f(y_i) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(y_i - x_i^T\beta)^2}{2\sigma^2}\right]$$
:::

::: {.fragment}
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - x_i^T\beta)^2\right]$$
:::

::: {.fragment}
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}(y - X\beta)^T(y - X\beta)\right]$$
:::

::: {.fragment}
Calculating the log-likelihood we get:

$\ell(\beta, \sigma^2 | X, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}$[$(y - X\beta)^T(y - X\beta)$]{style="color:red;"}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איך מחשבים את לוג הנראות עם P משתנים? אין הרבה שינוי.

הנראות היא עדיין מכפלת הצפיפויות, כך שמקבלים אותו ביטוי. ההמשך כרגיל, מכפלת האקספוננטים היא אקספוננט של הסכום, אבל מהו הסכום הזה? יש כאן הפרש של שני וקטורים באורך n, גם הוא וקטור באורך n. ואנחנו רוצים את סכום האיברים שלו בריבוע. הדבר הזה שווה בעצם למכפלה פנימית של הוקטור עם עצמו. וכך אנחנו מגיעים לY פחות Xbeta טרנספוז כפול Y פחות Xbeta.

כשאנחנו לוקחים לוג, אנחנו שוב רואים את הRSS שלנו במינוס רק שעכשיו הוא רשום בצורה וקטורית. בעצם לא השתנה הרבה, גם רגרסיה מרובת משתנים יכולנו לפתח בצורה מתמטית, כי הנה להביא למינימום את הRSS גם במקרה הכללי זה כמו להביא למקסימום את לוג-הנראות, כשזה נוגע למקדמי הרגרסיה.

אז בואו נעשה את זה פעם אחת למקרה כללי.
:::
:::

---

### Linear regression MLE

$\ell(\beta, \sigma^2 | X, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}(y - X\beta)^T(y - X\beta)$

::: {.fragment}
$= \mathcal{C} - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y^Ty -2\beta^TX^Ty + \beta^TX^TX\beta)$
:::

<br></br>

::: {.fragment}
$\frac{\partial l}{\partial \beta} = -\frac{1}{2\sigma^2}(2X^Ty - 2X^TX\beta)$
:::

::: {.fragment}
$\frac{1}{\sigma^2}(X^Ty - X^TX\beta) = 0$
:::
::: {.fragment}
$X^TX\beta = X^Ty$
:::
::: {.fragment}
[$\hat{\beta} = (X^TX)^{-1}X^Ty$]{style="color:red;"}
:::

<br></br>

::: {.fragment}
Similarly:

$\frac{\partial l}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}(y - X\beta)^T(y - X\beta)$ $\Rightarrow$ [$\hat{\sigma}^2 = \frac{1}{n}(y - X\beta)^T(y - X\beta) = \frac{RSS}{n}$]{style="color:red;"}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הביטוי הראשון הוא כאמור קבוע שאינו תלוי בפרמטרים של הבעיה, בטא וסיגמא בריבוע. את הביטוי השני נעתיק כפי שהוא, ואת הביטוי השלישי נפתח. אם תפתחו את המכפלה הפנימית של הביטוי לפנינו תקבלו משהו שמזכיר את נוסחאות הכפל המקוצר, יש לנו את המכפלה הפנימית של Y בעצמו, פחות פעמיים המכפלה הפנימית של הוקטור Xbeta בY, ועוד המכפלה הפנימית של הוקטור Xbeta בעצמו.

אנחנו רוצים לגזור את לוג הנראות לפי בטא קודם כל, להשוות לאפס, ולחלץ את האומד לבטא שהוא נקודת מקסימום. נשים לב שמה שרשום כאן הוא וקטור של נגזרות חלקיות. אפשר היה לגזור את הביטוי לכל בטא בנפרד, אבל כדי לקצר תהליכים ניעזר קצת בנגזרות פשוטות של וקטורים. את הגורם הכופל נעתיק כפי שהוא. את הגורם שתלוי רק בY נשמיט. כעת הגורם האמצעי הוא כמו לקחת את כל אחת מהבטאות ולכפול אותה פי ביטוי מסוים. הנגזרת תהיה הביטוי עצמו. ולכן ניתן לרשום זאת כוקטור של כל הביטויים האלה, הוא הוקטור שמוכפל פי בטא, X טרנספוז Y.

באופן דומה נגזור את הביטוי השלישי, אם תפתחו אותו תראו שכל אחת מהבטאות מועלית בריבוע ומוכפלת פי ביטוי מסוים. לכן נגזרת תהיה 2 כפול בטא כפול הביטוי. בכתיב קצר נקבל 2 כפול המטריצה X טרנספוז X כפול וקטור בטא.

הגורמים של 2 מצטמצמים, ואת הנגזרת המתקבלת אנחנו רוצים להשוות לאפס. מעבירים את הביטוי עם בטא לאגף אחד ואת הביטוי השני לאגף אחר. כלומר אם נכפיל משמאל את המשוואה במטריצה ההופכית של X טרנספוז X נוכל לבודד את בטא, וזה אומד הריבועים הפחותים המפורסם לכן הוא צבוע באדום. נזכיר רק שכדי לקבוע שזאת נקודת מקסימום של לוג הנראות צריך לחשב גם את מטריצת הנגזרות השניות ולהראות שהיא שלילית. וכבר עכשיו אפשר לראות שהאומד שקיבלנו הוא צירוף ליניארי של תצפיות Y! ולמי ששואל את עצמו, אז כאן, עבור וקטור בטא של משתנה אחד, כלומר וקטור של בטא-אפס, בטא-אחת, אנחנו צריכים לקבל בחזרה את הנוסחאות הפשוטות של בטא-אפס-האט, ובטא-אחת-האט.

באופן דומה אפשר לגזור את לוג הנראות לפי סיגמא בריבוע ולקבל את אומד הנראות המקסימלית לסיגמא בריבוע, שיוצא באופן אינטואיטיבי שונות המדגם של השאריות, הRSS סכום הריבועים של שאריות התצפיות מהאמידה שלהן, מחולק פי n.

יש לנו את האומדים ברגרסיה ליניארית, וכמו שאמרנו הם משתנים מקריים - אנחנו יכולים לחשב עכשיו את ההתפלגות שלהם.
:::
:::

---

## Distribution of OLS estimators {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה עכשיו מה אנחנו מרוויחים מהמודל ההסתברותי - נחשב את התפלגות האומדים שקיבלנו.
:::
:::

---

### Distribution of the OLS solution

::: {.incremental}
- What we know: 
$$(a)\; E(y) = X\beta,\;\;\;\; (b)\; Cov(y) = \sigma^2 I_n ,\;\;\;\;(c)\; \hat{\beta} = (X^TX)^{-1} X^T y$$

- Mean: 
$$E(\hat{\beta}) \stackrel{(c)}{=} (X^TX)^{-1} X^T E(y) \stackrel{(a)}{=} (X^TX)^{-1} X^TX\beta = \beta.$$

- Covariance matrix: 
$$Cov(\hat{\beta}) \stackrel{(c)}{=} (X^TX)^{-1} X^T Cov(y) X (X^TX)^{-1} \stackrel{(b)}{=} \sigma^2 (X^TX)^{-1} (X^T X) (X^TX)^{-1} = \sigma^2 (X^TX)^{-1}.$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אנחנו רוצים למצוא את ההתפלגות של האומד בטא האט. בטא-האט הוא צירוף לינארי של משתנים נורמליים ולכן גם הוא, מתפלג נורמלית, עם איזשהו וקטור תוחלות ומטריצת שונויות (להדגים). השאלה היחידה היא מהו וקטור התוחלות ומטריצת השונויות האלה. 

מה ידוע לנו עד כה? התוחלת המותנית של Y היא X בטא. הYים בלתי תלויים, מטריצת השונויות שלהם היא אלכסונית עם סיגמא בריבוע על האלכסון, ניתן לסמן זאת כך. והאומד לבטא-האט נראה כך.

נחשב את התוחלת של בטא-האט: האיקסים הם קבועים, ומליניאריות התוחלת הם יוצאים החוצה ונשארת רק התוחלת של Y, שהיא כידוע X בטא, וכך אנחנו מגיעים לעובדה שהתוחלת של בטא-האט היא בטא עצמו, בטא-האט נקרא אומד חסר הטיה לבטא.

ומטריצת השונות או הקווריאנס של וקטור בטא-האט: כשמחשבים שונות של סקלאר כפול משתנה הסקלאר יוצא בריבוע. כשמחשבים מטריצת שונות של מטריצת קבועים כפול הוקטור שלנו, היא יוצאת בהכפלה משמאל ומימין. אבל מטריצת השונות של Y היא כאמור אלכסונית, וכל מה שנשאר זה הכפלה של הביטוי הזה בסיגמא בריבוע. דברים מצטמצמים ומגיעים לביטוי סופי, סיגמא בריבוע כפול המטריצה ההופכית של X'X.

נסכם: בטא-האט מתפלג נורמלית עם תוחלת בטא האמיתית, ושונות סיגמא בריבוע כפול מטריצת X'X בהופכי.
:::
:::

---

### Variance of OLS estimators

::: {.incremental}
- Again: $Cov(\hat{\beta}) = \sigma^2(X^TX)^{-1}$

- For simple regression and scalar $\hat{\beta}_0, \hat{\beta}_1$, this amounts to the (squared) [Standard Errors]{style="color:red;"}:
$$SE(\beta_0)^2 = \sigma^2\left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i = 1}^n(x_i - \bar{x})^2}\right]; \quad SE(\beta_1)^2 = \frac{\sigma^2}{\sum_{i = 1}^n(x_i - \bar{x})^2}$$

- where, $\sigma^2$ would be replaced by its MLE or unbiased estimator the (squared) [Residual Standard Error]{style="color:red;"} (RSE): $RSE^2 = \frac{RSS}{n - p - 1}$
:::

::: {.fragment}
::: {.callout-note}
Question: what happens to the SE when $x$ is more "spread out"?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אפשר להתעכב קצת על הביטוי לשונות של מקדמי הרגרסיה. קודם כל עבור רגרסיה ליניארית פשוטה, עם משתנה אחד, ניתן לראות שעל האלכסון של מטריצת השונויות שקיבלנו יש ביטויים יחסית פשוטים. כשמוציאים מהם שורש מקבלים את סטיות התקן של האומדים, שמכנים טעויות התקן, סטנדרט ארורז או SE בקיצור.

בכל מקרה, בין אם מסתכלים על הביטוי הכללי למטריצת השונויות של המקדמים או על הביטויים לטעות התקן של החותך ושל השיפוע ברגרסיה ליניארית פשוטה, אנחנו רואים שיש שם פרמטר לא ידוע סיגמא בריבוע שנהוג להחליף באומד שלו. מקובל אבל להשתמש לא באומד הנראות המקסימלית שכרגע קיבלנו, אלא באומד מעט שונה, הRSS לא מחולק פי n אלא פי n פחות מספר הפרמטרים שאמדנו. באופן כללי אמדנו P ועוד 1 פרמטרים אז נחלק פי N פחות P פחות 1. לדוגמא, ברגרסיה ליניארית פשוטה אמדנו 2 פרמטרים, אז נקבל במכנה n פחות 2. כשמוציאים שורש מקבלים את מה שמכונה הRSE, רזידואל סטנדרט ארור.

הביטויים שקיבלנו הם האומדים לשונויות של האומדים של המקדמים. הם קובעים כמה אנחנו יכולים לסמוך על האומדנים שקיבלנו. אם הם גדולים מאוד למשל, אז האומד שלנו רועש וקשה לסמוך על המספר הזה שהתקבל. ומעניין להסתכל עליהם רגע ולשאול מה משפיע עליהם. דבר אחד ברור, שהם מושפעים מאוד מסיגמא בריבוע, השונות הטבעית של הרעש. עוד הגיוני לראות שהם מושפעים מגודל המדגם. ודבר נוסף יפה תנסו לראות לבד: מה קורה לטעות התקן, כשX הוא יחסית מפוזר? ככל שהX שקיבלנו הוא בעל פיזור גדול יותר, הוא מגוון, טעויות התקן של האומדים קטנות יותר, אנחנו יכולים יותר לסמוך עליהן.
:::
:::

---

### SE vs. $x$ spread

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Function to create sample data and perform linear regression
def generate_data(n, spread_x=False, seed=1):
    np.random.seed(seed)
    bound = 1
    if spread_x:
      bound = bound * 5
    X = np.random.uniform(-bound, bound, n)
    noise = np.random.normal(0, 1, n)
    Y = 2 * X + 3 + noise  # Y = 2X + 3 + noise
    return X, Y

def plot_regression(ax, X, Y, x_min, x_max, label, spread_x=False):
    ax.scatter(X, Y, label='Sample Data')
    
    # Perform linear regression
    model = LinearRegression()
    X_reshaped = X.reshape(-1, 1)
    model.fit(X_reshaped, Y)
    X_full_range = np.linspace(x_min, x_max, 100).reshape(-1, 1)
    Y_pred = model.predict(X_full_range)
    ax.plot(X_full_range, Y_pred, color='red', label='Least Squares Line')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    
    # Calculate standard error of the slope
    X_mean = np.mean(X)
    se = np.sqrt(np.sum((Y - model.predict(X_reshaped)) ** 2) / (len(Y) - 2)) / np.sqrt(np.sum((X - X_mean) ** 2))
    ax.annotate(f'SE(slope): {se:.2f}', xy=(0.05, 0.7), xycoords='axes fraction', fontsize=12, 
                bbox=dict(facecolor='white', alpha=0.6))
    
    # Plot a few other least squares lines from different samples
    for _ in range(5):
        new_X, new_Y = generate_data(len(X), spread_x, seed=np.random.randint(1000))
        new_X_reshaped = new_X.reshape(-1, 1)
        model.fit(new_X_reshaped, new_Y)
        new_Y_pred = model.predict(X_full_range)
        ax.plot(X_full_range, new_Y_pred, color='gray', linestyle='--', alpha=0.5)
    
    ax.set_title(label)

# Create figure and axes
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Generate data for both cases
X1, Y1 = generate_data(50)
X2, Y2 = generate_data(50, spread_x=True)

# Define the same X/Y axis limits for both plots
x_min, x_max = min(np.min(X1), np.min(X2)), max(np.max(X1), np.max(X2))
y_min, y_max = min(np.min(Y1), np.min(Y2)), max(np.max(Y1), np.max(Y2))

# Plot for the first case (normal spread)
plot_regression(axs[0], X1, Y1, x_min, x_max, 'Small spread of X')
axs[0].set_xlim(x_min, x_max)
axs[0].set_ylim(y_min, y_max)

# Plot for the second case (more spread out X)
plot_regression(axs[1], X2, Y2, x_min, x_max, 'Large spread of X', True)
axs[1].set_xlim(x_min, x_max)
axs[1].set_ylim(y_min, y_max)

plt.tight_layout()
plt.show()
```

::: {.fragment}
Closely related to a $x$ points' [leverage]{style="color:red;"}.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אפשר להדגים את זה בקלות. בשני הגרפים שלפנינו התאמנו קו רגרסיה ל50 תצפיות שמקיימות את המודל הליניארי. אבל בגרף השמאלי הפיזור של משתנה הX שלהן קטן יחסית, הם נלקחו מאיזור מצומצם. ובגרף הימני הפיזור של משתנה הX שלהם גדול, הם נלקחו מאיזור רחב.

אז טכנית אפשר לחשב את טעות התקן לפי הנוסחאות שראינו ולראות שבגרף השמאלי היא פי 5 מבגרף הימני. אפשר אבל גם לחזור על התרגיל כמה פעמים ולסרטט כל פעם את קו הרגרסיה המתקבל ולראות למשל לגבי השיפוע איך הוא יכול להשתנות כשהX כל כך מרוכז, לעומת המצב כשהX מפוזר היטב.

ברמה הפיסיקלית אפשר להקצין את זה עוד יותר ולשאול מה קורה אם הרגרסיה היתה נקבעת רק משתי נקודות. אם הן נורא נורא קרובות אפשר לדמיין מן נדנדה כזאת לא בטוחה. והתופעה הזאת קשורה מאוד להגדרה של מושג שנקרא לברג' או משען של נקודות שנראה בהמשך. ככל שהוא גדול כך לנקודות יש יותר השפעה, משקל באיך ייראה קו הרגרסיה.
:::
:::

---

### Gauss-Markov Theorem

Under the assumptions of the previous slides: the linear regression $\hat{\beta}$ is the [best linear unbiased estimator]{style="color:red;"} (BLUE), i.e.: the unbiased linear estimator with the smallest variance.

<br></br>

::: {.fragment}
In other words, for any linear **unbiased** $\tilde{\beta} = Cy$:
$$Var(\tilde{\beta}) \succeq Var(\hat{\beta})$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לסיום החלק הזה נזכיר תוצאה מעניינת שנקראת משפט גאוס-מרקוב: תחת ההנחות של המודל הסטטיסטי, אומד הריבועים הפחותים הוא אומד בלו: בסט ליניאר אנבייסד אסטימטור. כלומר מתוך כל האומדים הליניארים חסרי הטיה, הוא האומד בעל השונות המינימלית. 

שימו-לב, לא מכל האומדים האפשריים, אלא אם תיתנו לי איזשהו אומד בטא-טילדא שהוא ליניארי והכוונה ליניארי בY כמו האומד שלנו, והוא חסר-הטייה, השונות שלו בהכרח גדולה או שווה לשונות של בטא-האט שמצאנו. כאן אני כותב את זה בשפה של מטריצות.

השאלה המתבקשת היא אם יש אומד שהוא לא בהכרח חסר-הטייה שיכולה להיות לו שונות קטנה יותר, והתשובה המפתיעה כפי שנראה כשנדבר על רגרסיית רידג' היא: כן!
:::
:::

---

## Hypothesis testing {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
חוץ מזה שהשונות של האומדים שלנו מעניינת בפני עצמה, אמרנו שאחת המטרות שלנו בלמידה סטטיסטית היא הסקה סטטיסטית. יש לנו הרבה שאלות כחוקרים על המודל הזה. האם משתנה מסוים משפיע או לא על Y מעבר לספק סטטיסטי. ואם אנחנו משתמשים במודל הזה לחיזוי תצפיות שלא ראינו, למשל בתרחיש של מדגם למידה ומדגם טסט שדיברנו עליו -- האם יכול להיות שיש תת-קבוצה של משתנים ששווה בכלל להוציא מהמודל, היא לגמרי מזיקה לטיב החיזוי?
:::
:::

---

### Hypothesis testing: simple

$$H0\text{: feature }x\text{ does not affect } y$$
$$H1\text{: feature }x\text{ does affect } y$$

::: {.fragment}
Translates to:
$$H0: \beta_1 = 0$$
$$H1: \beta_1 \neq 0$$
:::

::: {.fragment}
$t_{obs} = \frac{\hat{\beta}_1 - 0}{\hat{SE}(\hat{\beta}_1)} \sim T_{n - 2}$
:::

::: {.fragment}
$\Rightarrow \text{P-value} = P(T_{n - 2} > |t_{obs}|)$

$\Rightarrow CI_{0.95}(\beta_1) = \hat{\beta}_1 \pm T_{n - 2, 0.975} \cdot \hat{SE}(\hat{\beta}_1) \approx \hat{\beta}_1 \pm 2 \cdot \hat{SE}(\hat{\beta}_1)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל ברגרסיה פשוטה עם משתנה אחד, שם השאלה הטבעית היא מה השורה התחתונה המשתנה קשור לY, משפיע עליו, או לא. כמו תמיד, יש כאן אלמנט של ספק תמיד, והשאלה היא האם השתכנענו מעל לספק סביר שהמשתנה משפיע על Y. אנחנו תמיד לוקחים בחשבון את האפשרות שאנחנו טועים.

זה מתרגם לבדיקת השערות פשוטה על מקדם השיפוע. אם הוא אפס אין למשתנה X קשר עם Y, ורק אם הוא שונה מאפס קו הרגרסיה יהיה עם טרנד חיובי או שלילי ונסיק שיש השפעה לX, מעל לספק סביר.

להדגים: אז יש כאן משתנה מקרי שמתפלג תחת השערת האפס נורמלית עם תוחלת אפס ושונות SE בריבוע. אם אנחנו מחסרים את התוחלת ומחלקים בטעות התקן שאיננה ידועה אז אנחנו מחליפים אותה באומד, הסטטיסטי הזה מתפלג טי עם n פחות 2 דרגות חופש.

מדובר בהשערה דו-צדדית, אז אפשר לחשב עבורה פי-ווליו דו-צדדי. אם t אובזרווד הוא חיובי אז אנחנו רוצים את הסיכוי להיות גדול כמוהו או יותר ועוד הסיכוי להיות קטן כמו מינוס t אוברזווד או יותר. באופן כללי נרשום את זה כך.

אפשר לא להסתפק גם בתשובה בינארית לשאלה אלא ממש לתת רווח סמך לשיפוע, לדוגמא רווח סמך ברמת ביטחון שיכלול את השיפוע האמיתי בסיכוי 95 אחוז, והוא בערך האומדן שקיבלנו פלוס מינוס פעמיים טעות התקן הנאמדת.
:::
:::

---

### Hypothesis testing: multiple

$$H0\text{: features }X\text{ do not affect } y$$
$$H1\text{: features }X\text{ do affect } y$$

::: {.fragment}
Translates to:
$$H0: \beta_1 = \dots = \beta_p = 0$$
$$H1: \beta_j \neq 0 \text{ for at least one }j$$
:::

::: {.fragment}
$f_{obs} = \frac{(TSS - RSS)/p}{RSS / (n - p - 1)} \sim F_{p, n - p - 1}$
:::

::: {.fragment}
$\Rightarrow \text{P-value} = P(F_{p, n - p - 1} > f_{obs})$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשיש לנו P משתנים, אנחנו בדרך נשאל קודם כל את השאלה האם כלל המשתנים בX משפיעים על Y או לא, ותיכף נדבר על למה.

זה מתרגם להשערת האפס שאומרת שכל הבטאות שוות אפס, מול ההשערה האלטרנטיבית שלפחות אחת מהן לא שווה לאפס.

את ההשערה הזאת נכון לבדוק עם סטטיסטי שמתפלג התפלגות F. התפלגות F היא התפלגות שבה הערך המינימלי הוא אפס, היא לא סימטרית ויש לה שני פרמטרים, דרגות החופש של המונה, ודרגות החופש של המכנה. במכנה יש לנו את האומד לסיגמא בריבוע שכבר ראינו הRSS מחולק בn פחות p פחות 1. ובמונה יש לנו את הטוטאל סאם אוף סקוורז פחות הRSS חלקי p. הטוטאל סאם אוף סקוורס הוא בעצם המונה של השונות של Y (להדגים), כלומר הרעש של Y הטבעי תחת מודל שבו יש רק חותך, שהחיזוי שלו הוא הכי פשוט, הממוצע של Y.

מכל מקום עדיין מדובר בבדיקת השערת רגילה ואפשר לחשב P-value תחת התפלגות F, זה הסיכוי לקבל סטטיסטי גדול כמו שקיבלנו או יותר. רק אם הP-value  הזה קטן נהוג להמשיך לבדוק את ההשפעה של כל אחד מהמשתנים, ותיכף נגיד למה.
:::
:::

---

### Example with `statsmodels`

```{python}
#| echo: false

import numpy as np
import pandas as pd
import statsmodels.api as sm

df = pd.read_csv('../datasets/ISLP_income_data.csv')
X_df = df[['Education', 'Seniority']]
Y = df['Income']

X_df1 = sm.add_constant(X_df)

model = sm.OLS(Y, X_df1)
res = model.fit()
print(res.summary())
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
דיברנו המון תיאוריה אז בואו נראה פלט רגרסיה טיפוסי, כאן עם ספריה סטטיסטית של פייתון בשם סטאטסמודלז.

במקרה הזה מידלנו הכנסה, כנגד השכלה וותק, על פני 30 נבדקים. זה אומר שהn הוא 30, p הוא 2, וזה רשום. מצד ימין אפשר לראות את ערך הסטטיסטי F והוא גדול מאוד כמו שמי שיש לו ניסיון עם התפלגות F יודע, ובהתאמה הP-value של כל הרגרסיה הוא אפסי, כלומר יש בהחלט קשר בין השכלה וותק לבין הכנסה. אפשר גם לראות את לוג-הנראות, ואת אומדני המקדמים שיצאו: החותך יצא מינוס 50, שזה לא נשמע הגיוני כל-כך, הכנסה של מינוס 50 אלף דולר בשנה. אבל נזכור שהחותך מתקבל מתי שהאיקסים עצמם הם אפסים, כלומר עבור אפס השכלה ואפס ותק, ולא בטוח שיש לנו רצון לחזות בכלל עבור ערכים כאלה, כך שיכול להיות שהמודל עדיין שימושי. המקדמים האחרים יצאו כמעט 6 להשכלה, ו0.17 לותק. זה אומר שנצפה לעלייה של כששת אלפים דולר לשנה על כל שנה של השכלה ועלייה של כ170 דולר על כל שנת ותק.

לא נדבר על כל מספר בפלט הזה, אבל בכל זאת שני חלקים נרצה להסביר: אחד זה מאיפה מגיעים מבחני הT על כל אחד מהמקדמים, הרי לא הזכרנו את האפשרות הזאת עבור רגרסיה מרובה, הזכרנו רק את מבחן הF. ושתיים זה להזכיר את ערך הR בריבוע והזהירות שצריך לנקוט כאשר משתמשים בו.
:::
:::

---

### Choosing between nested models

::: {.incremental}
- Checking whether a subset of $q$ features given the other $p - q$ features affect $y$:
$$H0: \beta_{p - q + 1} = \dots = \beta_p = 0$$
$$H1: \beta_j \neq 0 \text{ for at least one } j \text{ out of } q \text{ features}$$

- Run an additional regression *without* these $q$ features, reach $RSS_0$, then:
:::

::: {.fragment}
$f_{obs} = \frac{(RSS_0 - RSS)/q}{RSS / (n - p - 1)} \sim F_{q, n - p - 1}$
:::

::: {.fragment}
- When $q = 1$, $f_{obs} = t^2_{obs}$, where $t_{obs} \sim T_{n - p - 1}$ (testing a single feature given others)
:::

::: {.fragment}
::: {.callout-note}
Question: why $F$-test? Why not test for each of the $p$ features with this $t_{obs}$ test?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז בדרך להכיר את המבחנים למקדמים בודדים רצוי להתחיל מהאפשרות לעשות בדיקת השערות ברגרסיה מרובה לתת-קבוצה Q של משתנים. למשל לשאול האם לQ המשתנים האחרונים שנכנסו לרגרסיה יש השפעה על Y, בהינתן שP-Q המשתנים האחרים כבר נמצאים במודל. זה מה שנקרא לבדוק האם אפשר להישאר במודל מקונן, נסטד, של P-Q משתנים.

הדרך לבצע את זה היא להריץ עוד רגרסיה על המודל הקטן יותר עם P-Q משתנים, ולקבל סכום ריבועים של שאריות שנסמן בRSS0. שימו-לב שככל שנוסיף משתנים הRSS יכול רק להיות קטן יותר. כלומר הRSS במודל המקורי הגדול הוא בטוח קטן יותר. אם הוא קטן מספיק לעומת RSS0, שחושב על מודל ללא הQ משתנים שאנחנו בוחנים, אז נגיד שלפחות אחד מהם חשוב. מכמתים את זה שוב עם סטטיסטי שמתפלג F עם דרגות חופש מעט שונות. כדי שהוא יהיה גדול צריך שהמודל הרווי, עם כל הP משתנים יביא לRSS קטן מספיק.

ואם Q = 1, כלומר אנחנו בודקים האם להכניס או לא משתנה ספציפי מעבר לשאר המשתנים שנמצאים במודל, מסתבר שהערך שנקבל הוא בעצם ערך הטי בריבוע שהיינו מקבלים, עבור מבחן טי פשוט אם להכניס את המשתנה הזה לרגרסיה או לא, בהינתן כל המשתנים האחרים. שימו-לב זה חשוב, ברגרסיה עם משתנה אחד אין משתנים אחרים ולכן השאלה הזאת כלל לא עולה. ההשערה הזאת בודקת האם מעבר למה שתורמים המשתנים האחרים ברגרסיה המשתנה הבודד הנידון מוסיף איזשהו אפקט על Y. מכל מקום אלה הם ערכי הטי והP-values ורווחי הסמך שראינו בפלט הרגרסיה לכל משתנה בנפרד.

ועכשיו אנחנו רוצים לחזור אחורה ולשאול למה צריך מבחני F. למשל לגבי ההשערה הכוללת של האם לפחות אחד מהמשתנים המסבירים משפיע על Y, אם יש לנו דרך לבצע בדיקת השערות עבור משתנה אחד, אפשר למשל לבדוק השערות לכל אחד מP המשתנים, ואם לפחות אחד מהם יגיע למובהקות סטטיסטית, לקבוע שכן, יש קשר בין המשתנים בX לבין Y.

זה לא רעיון טוב, מסתבר, במיוחד במודל עם הרבה משתנים. סיבה אחת היא שיכול להיות שהמשתנים כן מתואמים ביניהם, ורק כשלוקחים את כולם או קבוצה מהם אפשר לראות השפעה מצרפית על המשתנה התלוי, השפעה שלא נראה אותה אם נבדוק אותם אחד אחד. סיבה אחרת היא בעיה מפורסמת שנקראת בעיית ההשוואות המרובות. כל אחד מהמשתנים ייבדק מול טעות מסוג ראשון של נאמר 5 אחוז, אבל הסיכוי שלפחות אחד מהם נראה מובהק סטטיסטית כשהוא לא, הוא כבר גדול הרבה יותר. הוא 1 פחות 95 אחוז בחזקת P. עבור שני משתנים זה כבר כמעט 10 אחוז, עבור 100 משתנים זה כבר סיכוי של 100 אחוז לראות משתנה מובהק סטטיסטית למרות שאין אף משתנה בעל קשר לY. לכן אנחנו צריכים קודם בדיקת השערות עם מבחן F, עם טעות מסוג ראשון של 5 אחוז שתקבע האם בכלל להסתכל במבחני הT עבור המקדמים השונים.
:::
:::

---

## Goodness of Fit and Feature Selection {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
המודל הליניארי, אף על פי שבשיעור הבא נדון איך להגמיש אותו, הוא עדיין מודל נוקשה. ובתור מודל נוקשה, בכל פעם שמריצים רגרסיה ליניארית מקובל גם לבדוק את ההנחות שלנו ואת טיב המודל. באופן טבעי נוצרה ספרות שלמה סביב מטריקות ושיטות מדידה לצורך בדיקת טיב המודל, אנחנו רק נזכיר זאת בקצרה ונשתמש בזה כדי לבצע בחירת משתנים או פיצ'ר סלקשן, בצורה פשוטה.
:::
:::

---

### Model Fit: plotting

When $p$ is low - draw!

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression

# Generate the dataset
np.random.seed(42)
n_samples = 100
X = np.random.normal(size=(n_samples, 2))  # Random values in 2D
X[:, 0] *= 3  # Scale the first feature
X[:, 1] *= 10  # Scale the second feature
noise = np.random.normal(size=(n_samples,)) * 0.1 # Noise to add to the output

# Non-linear function with an upward trend
y = np.sin(X[:, 0]) + 0.2*X[:, 1] + noise

# Fit a linear regression model
reg = LinearRegression()
reg.fit(X, y)
y_pred = reg.predict(X)

# Plot the data and the fitted plane
fig, axes = plt.subplots(1, 2, figsize=(12, 5), subplot_kw={'projection': '3d'})

# Scatter plot of the original data
axes[0].scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')
axes[1].scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')

# Create a grid for plotting the plane
x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
y_grid = reg.intercept_ + reg.coef_[0] * x1_grid + reg.coef_[1] * x2_grid

# Plot the linear regression plane
axes[0].plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')
axes[1].plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')

# Draw vertical lines from the plane to the data points
for i in range(n_samples):
    axes[0].plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')
    axes[1].plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')

# Adjust the viewing angle
axes[0].view_init(elev=30, azim=-60)
axes[1].view_init(elev=30, azim=90)

# Set labels and title
axes[0].set_xlabel('X1')
axes[1].set_xlabel('X1')
axes[0].set_ylabel('X2')
axes[1].set_ylabel('X2')
axes[0].set_zlabel('y')
axes[1].set_zlabel('y')
# ax.set_title('Non-linear Data with Linear Regression Plane')

plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז קודם כל כשמספר המשתנים נמוך, פשוט ציירו את הנתונים. כאן למשל אני מצייר את Y כנגד שני משתנים מסבירים עם תרשים תלת-מימדי. אני מוסיף גם את המישור שיתאים מודל ליניארי. במבט חטוף הוא נראה סביר, ונראה שאין השפעה בכלל למשתנה X1, אבל אם אני מסובב את הצירים קצת, אני רואה שיש ועוד איך השפעה למשתנה X1, אני רואה שזה מין גל סינוס בX1! אז המודל כפי שהוא כרגע לא מתאים, תיכף ניתן פתרונות.
:::
:::

---

### Model fit: residuals plots

```{python}
#| echo: false

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].scatter(X[:, 0], y_pred - y)
axes[0].axhline(y=0, color='r', linestyle='--')
axes[0].set_xlabel('X1')
axes[0].set_ylabel('Residuals')
axes[1].scatter(X[:, 1], y_pred - y)
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('X2')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
גם כשמספר המשתנים גבוה ומריצים רגרסיה, אפשר לבצע דיאגנוסטיקה, למשל עם תרשימי שאריות. כאן ציר הX יהיה המשתנה עצמו, כל פעם משתנה אחר, ועל ציר הY השאריות החזויות של המודל Y פחות Y_hat, או איזושהי גרסה מתוקננת שלהן.

כאן אנחנו מצפים לראות ענן חסר צורה סביב ציר האפס, אבל בתרשים השמאלי עבור משתנה X1 זה בבירור לא קורה והמודל שמניח רעש קבוע, שלא תלוי בX, בבירור לא מתקיים, וצריך לשנות את המודל. אנליסטים מנוסים בהרצת רגרסיות אכן עושים זאת בצורה איטרטיבית, מניחים מודל, מתאימים אותו בהרצת רגרסיה, בודקים את ההנחות ומשנים בהתאם לצורך. יש עוד הרבה תרשימים מהסוג הזה וזאת אומנות שלמה, אבל נעצור כאן.
:::
:::

---

### Model Fit: $R$ squared

Measure of "explained variance":

$R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{\sum_{i = 1}^n (y_i - x_i^T\hat{\beta})^2}{\sum_{i = 1}^n (y_i - \bar{y})^2} = 1 - \frac{(y - X\hat{\beta})^T(y - X\hat{\beta})}{(y - X\hat{\beta^*_0})^T(y - X\hat{\beta^*_0})}$

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Generate sample data
np.random.seed(42)
X = np.linspace(0, 10, 15)
y = 2 * X + 1 + np.random.randn(15) * 2  # Linear relation with noise

# Perform linear regression
coeffs = np.polyfit(X, y, 1)
y_hat = np.polyval(coeffs, X)

# Choose a sample point far from the mean
x_i = 7
y_i = 2 * x_i + 1 + 3  # Simulated data point
y_hat_i = np.polyval(coeffs, x_i)

# Mean line
y_mean = np.mean(y)

# Create the plot
fig, ax = plt.subplots(figsize=(7, 4))

# Plot data points
ax.scatter(X, y, color='red', label='Data points')
ax.scatter(x_i, y_i, color='red')  # Highlighted data point

# Plot regression line
ax.plot(X, y_hat, color='blue', label='Regression line')

# Plot mean line
ax.axhline(y=y_mean, color='blue', linestyle='--', label='Mean line ($\overline{y}$)')

# Plot fitted point
ax.scatter(x_i, y_hat_i, color='blue', zorder=5)  # Fitted point

# Draw vertical lines from the points to the lines
ax.vlines(x_i, y_i, y_mean, color='black', linestyle='--')
ax.vlines(x_i, y_i, y_hat_i, color='black', linestyle='--')

# Setting labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.legend()

plt.show()
```

::: {.fragment}
::: {.callout-note}
Careful: $R^2$ is monotone increasing in $p$ for the training data!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מדד אחר לטיב המודל שגם ראינו בפלט הרגרסיה הוא האר בריבוע, R סקוורד.

אנחנו מסתכלים על כל תצפית ושואלים כמה המודל שלנו מצליח לחזות אותה היטב. יחסית למה? יחסית לממוצע. אז הבייסליין שלנו זה סכום השאריות הריבועיות מהממוצע, הTSS, והיינו רוצים שהחיזוי שלנו יקלוט כמה שיותר מהטעות הזאת, כלומר יסביר כמה שיותר מהשונות הטבעית של Y. כלומר נרצה שהשאריות שלנו יהיו כמה שיותר קרובות לאפס. נחסר מהTSS את הRSS ונרצה שהביטוי יהיה כמה שיותר גבוה יחסית לTSS, אז נחלק אותו בTSS וזהו הR בריבוע. אפשר לכתוב אותו כ1 פחות סכום השאריות המרובעות שהתקבלו מהמודל שלנו יחסית לסכום השאריות המרובעות ממודל עם פרמטר אחד בטא-אפס-כוכב וכל שאר הבטאות אפסים, כי אז ברור שבטא אפס כוכב יהיה ממוצע Y.

וכאן מגיעה אזהרה חשובה: מה זאת אומרת "נרצה שהRSS יהיה קטן ככל שאפשר"? הרי אם נוסיף עוד ועוד משתנים למודל הRSS יכול רק להיות קטן יותר. במילים אחרות הR בריבוע יכול רק לעלות, הוא פונקציה מונוטונית עולה במספר המשתנים. כבר נתקלנו בתופעה הזאת! בסיטואציה שבה אנחנו נעזרים במודל לחיזוי, ויש לנו את המדגם שעליו הוא מאומן ומדגם טסט שהוא לא ראה, התופעה הזאת של הוספת עוד ועוד משתנים לשיפור החיזוי על הטריין נקראת אוברפיטינג.

אבל זה לא אומר שמדד הR בריבוע הוא חסר-תועלת כפי שיצא שמו לרעה, פשוט צריך לדעת להשתמש בו נכון. זה סביר להשוות בין שני מודלים על-פי הR בריבוע אם מדובר למשל באותו מספר משתנים. זה סביר להסתכל על התוספת בR בריבוע עקב הוספת משתנה אחד ולשאול את עצמנו אם זה "שווה" את זה. זה לא סביר להסתכל על R בריבוע כאיזה אורקל ובגללו להוסיף עוד ועוד משתנים.
:::
:::

---

### Intro. to feature selection

How can we choose a subset of the variables?

::: {.fragment}
We will devote an entire class for that.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מה שמביא אותנו לדיון על פיצ'ר סלקשן, בחירת משתנים: איך נזהה את תת הקבוצה הטובה ביותר? היא לא קטנה כל כך שהמודל פשטני מדי עם שאריות גדולות ועם R בריבוע קטן. והיא לא גדולה מדי, וסתם מנפחת את R בריבוע.

השאלה הזאת היא חלק מנושא רחב יותר שנעסוק בו בהרחבה בהמשך. עד כאן לגבי רגרסיה ליניארית. בשיעור הבא נראה איך להגמיש את המודל הליניארי, ולהתאים אותו גם למצב שY הוא משתנה קטגוריאלי.
:::
:::
