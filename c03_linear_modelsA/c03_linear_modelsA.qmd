---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Linear Models - Part A"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Linear Models - Part A - Class 3

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Simple linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשיעור זה נעסוק ברגרסיה ליניארית, אחד המודלים הותיקים והנחקרים ביותר בספרות. חשוב להדגיש כי רגרסיה ליניארית זה נושא שאפשר להקדיש לו גם סמסטר שלם ואפילו שניים ושלושה, נושא שנכתבו עליו אינספור ספרים עבי קרס, ולא נוכל להתעמק בכל ניואנס. נתמקד בדברים החשובים לנו בקורס.
:::
:::

---

### Why learn linear regression?

::: {.incremental}
- Linear regression is still a highly useful modeling tool
- Super-fast to train and predict, super-simple to implement
- It can be used in many non-linear cases by using transformed variables / transformed goals
- Its probabilistic aspect is fully understood, so inference questions can be answered exactly (under the regression assumptions)
- Many newer methods can be seen as a generalization for linear regression
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז למה אנחנו בעצם לומדים רגרסיה ליניארית, זה לא קצת מיושן? התשובה המפתיעה היא שלא. בניגוד למה שנדמה לנו לפעמים מהתקשורת, רגרסיה ליניארית עדיין חיה ובועטת ובשימוש בתעשייה.

קודם כל מדובר במודל סגור, אינטואיטיבי ומהיר מאוד, גם לאימון ולא פחות מזה לחיזוי, בסופו של דבר מדובר במכפלה של שני וקטורים כמו שנראה, שקל לממש בכל שפת תכנות.

רגרסיה ליניארית ניתנת להכללה גם להרבה מצבים לא-ליניאריים בעליל, באמצעות טרנספורמציות מתאימות על המשתנים המסבירים או המשתנה התלוי.

ההיבט ההסתברותי שלה ידוע ומובן, כך שניתן לבצע בקלות הסקה סטטיסטית על המקדמים ולקבל תשובות על הרבה שאלות תחת הנחות המודל.

ואולי הכי חשוב, רגרסיה ליניארית משמשת כבייסליין להרבה שיטות מודרניות אחרות שנראה, שהן הכללה של רגרסיה ליניארית. הרבה חוקרים שוכחים את זה, ותוקפים את הנתונים שלהם באמצעות כל מיני מודלים שונים ומשונים בלי לבדוק כמה כבר הביצוע שלהם משפר את הבייסליין הפשוט הזה
:::
:::

---

### Simple linear regression

::: {.incremental}
- Assume: $y \approx \beta_0 + \beta_1x$
- This is the *unobserved* [population regression]{style="color:red;"} line
- We look for the values of $\beta_0, \beta_1$ through a sample $\{(x_1, y_1), \dots, (x_n, y_n)\}$
- In order to estimate the parameters we need to define a measure of error
- By far the most common measure is:
:::

::: {.fragment}
$RSS = \sum_{i = 1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2$ or its scaled version: $MSE = \frac{RSS}{n}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל ברגרסיה ליניארית פשוטה, עבור משתנה תלוי יחיד Y, ומשתנה מסביר יחיד X. אנחנו מניחים שY משתנה בצורה ליניארית עם X, כלומר קו ישר.

לקו הזה קוראים קו הרגרסיה של האוכלוסיה, והוא לא נצפה. אנחנו לוקחים מדגם בגודל n של זוגות תצפיות של X וY, ומחפשים את הבטא-אפס ואת הבטא-אחת, החותך והשיפוע של הקו הנסתר הזה.

אנחנו קובעים קריטריון שאומר מתי הבטאות טובות ומתי לא, איזו מידה של טעות, והקריטריון הנפוץ ביותר הוא הRSS: הרזידואל סאם אוף סקוורז, כלומר לקחת את השאריות של Y פחות הY הנחזה עם הבטאות הנאמדות, להעלות בריבוע ולסכום. או, סכום המרחקים הריבועיים של התצפיות מקו הרגרסיה הנאמד.

אפשר גם לחלק את הRSS פי n ולקבל את המין סקוורד ארור או הMSE, אפשר גם לקחת שורש ולקבל את הרוט מין סקוורד ארור, או הRMSE.
:::
:::

---

### From line to RSS surface

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from mpl_toolkits.mplot3d import Axes3D

# Set the random seed for reproducibility
np.random.seed(42)

# Number of samples
n = 100

# Generate X values
X = np.random.rand(n, 1)

# Generate noise
e = np.random.randn(n, 1)

# True population line parameters
beta_0_true = 2
beta_1_true = 3

# Generate Y values
Y = beta_0_true + beta_1_true * X + e

# Fit the least squares regression line
model = LinearRegression().fit(X, Y)
Y_pred = model.predict(X)

# Create a meshgrid for beta_0 and beta_1
beta_0_range = np.linspace(beta_0_true - 2, beta_0_true + 2, 100)
beta_1_range = np.linspace(beta_1_true - 2, beta_1_true + 2, 100)
beta_0, beta_1 = np.meshgrid(beta_0_range, beta_1_range)

# Calculate the RSS for each combination of beta_0 and beta_1
RSS = np.zeros(beta_0.shape)
for i in range(beta_0.shape[0]):
    for j in range(beta_0.shape[1]):
        Y_pred_mesh = beta_0[i, j] + beta_1[i, j] * X
        RSS[i, j] = np.sum((Y - Y_pred_mesh) ** 2)

# Prepare for plotting
fig = plt.figure(figsize=(5 * 3, 3.5))

ax1 = fig.add_subplot(131)
# Plot the sample points
ax1.scatter(X, Y, color='blue', label='Sample points')
ax1.set_xlabel(r'$x$')
ax1.set_ylabel(r'$y$')
# Plot the least squares regression line
ax1.plot(X, Y_pred, color='red', linestyle='--', label='Least squares line', linewidth=2)
# Plot vertical lines from each sample point to the fitted line
for i in range(n):
    ax1.plot([X[i], X[i]], [Y[i], Y_pred[i]], color='gray', linestyle=':', linewidth=1)
ax1.set_title(r'Residuals for specific $\hat{\beta}_0, \hat{\beta}_1$')

# Contour plot on the left
ax2 = fig.add_subplot(132)
contour = ax2.contour(beta_0, beta_1, RSS, levels=50, cmap='viridis')
ax2.set_xlabel(r'$\beta_0$')
ax2.set_ylabel(r'$\beta_1$')
ax2.set_title('Contour plot of RSS')
fig.colorbar(contour, ax=ax2, label='RSS')

# Add a red cross at the true beta values
ax2.plot(beta_0_true, beta_1_true, 'rx', markersize=12, markeredgewidth=3)
# Add dashed lines from the cross to the true beta_0 and beta_1 axes
ax2.plot([beta_0_true, beta_0_true], [beta_1_range.min(), beta_1_true], 'r--')
ax2.plot([beta_0_range.min(), beta_0_true], [beta_1_true, beta_1_true], 'r--')

# 3D plot on the right
ax3 = fig.add_subplot(133, projection='3d')
ax3.plot_surface(beta_0, beta_1, RSS, cmap='viridis', edgecolor='none')
ax3.set_xlabel(r'$\beta_0$')
ax3.set_ylabel(r'$\beta_1$')
ax3.set_zlabel('RSS')
ax3.set_title('3D plot of RSS surface')

# Add a red cross at the true beta values
ax3.plot([beta_0_true], [beta_1_true], [np.min(RSS)], 'rx', markersize=12, markeredgewidth=3, zorder=10)

# Add dashed lines from the cross to the true beta_0 and beta_1 axes
ax3.plot([beta_0_true, beta_0_true], [0, beta_1_true], [np.min(RSS), np.min(RSS)], 'r--', zorder=10)
ax3.plot([4, beta_0_true], [beta_1_true, beta_1_true], [np.min(RSS), np.min(RSS)], 'r--', zorder=10)

# plt.tight_layout()
plt.show()
```

$$RSS = \sum_{i = 1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז כאמור ככה נראות התצפיות שלנו, כל נקודה היא תצפית עם קואורדינטה X וקואורדינטה Y, נראה שיש יחס ליניארי עולה בין X לY ולכן המודל הליניארי מתאים.

אנחנו מחשבים לכל תצפית את השארית של מקו רגרסיה אדום שאנחנו משיגים באמצעות חיפוש על הבטאות, ומחשבים את סכום המרחקים הריבועיים. בתרשימים האחרים ניתן לראות איך נראה קריטריון הRSS כפונקציה של הבטאות, באמצעות קונטור-פלוט או באמצעות תרשים תלת-מימדי. בכל מקרה הבטאות שנבחר הן הבטאות שמביאות למינימום את הRSS, וזה מתבטא בקו הרגרסיה האדום.
:::
:::

---

### Simple linear regression

::: {.incremental}
- A simple derivation gives:

::: {.fragment}
$\hat{\beta}_1 = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} = \frac{\widehat{Cov(X,Y)}}{\widehat{Var(X)}},\;\;\;\;\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$
:::
- This is the [least squares]{style="color:red;"} line (OLS), the *best* linear predictor for the population regression line
- Replacing the sample averages by the population means (or having a huge sample) should get us to the true line
- Using $\hat{\beta}_0, \hat{\beta}_1$ we can easily perform prediction
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
במקרה של רגרסיה ליניארית פשוטה, אין צורך באמת "לחפש", הRSS הוא פונקציה פשוטה וקמורה. אם גוזרים אותו לפי בטא-אפס ולפי בטא-אחת מתקבלים הביטויים הסגורים האלה. בטא-אחת-האט הוא פשוט הקווריאנס במדגם בין X לY מחולק בשונות המדגם של X. ובטא-אפס-האט מתקבל מהצבת בטא-אחת-האט וממוצעי המדגם של X ושל Y בקו הרגרסיה.

זה קו הליסט סקוורז, או הOLS, והוא הקו האומד הטוב ביותר לקו הרגרסיה של האוכלוסיה, תחת ההנחות. הטוב יותר כלומר מביא למינימום את קריטריון הRSS. כשנוסיף את ההיבט ההסתברותי לרגרסיה ליניארית נוכל להוכיח שהקו הזה הוא גם אומד בלתי-מוטה לקו הרגרסיה של האוכלוסיה, כרגע רק נגיד שאם ניקח מדגם עצום, ואז ממוצעי המדגמים הם כבר לא ממוצעים אלא תוחלות, אנחנו צריכים לקבל את קו הרגרסיה האמיתי, של האוכלוסיה.

מכל מקום, ברגע שמתקבלים האומדים למקדמי הרגרסיה כאמור קל לתת חיזוי לכל תצפית, לפי הנוסחה.
:::
:::

---

### Average of many OLS lines

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Set the random seed for reproducibility
np.random.seed(42)

# Number of samples
n = 100

# Generate X values
X = np.random.rand(n, 1)

# Generate noise
e = np.random.randn(n, 1)

# True population line parameters
beta_0 = 2
beta_1 = 3

# Generate Y values
Y = beta_0 + beta_1 * X + e

# Fit the least squares regression line
model = LinearRegression().fit(X, Y)
Y_pred = model.predict(X)

# Prepare for plotting
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Plot on the left
axs[0].scatter(X, Y, color='blue', label='Sample points')
axs[0].plot(X, beta_0 + beta_1 * X, color='green', label='True population line', linewidth=2)
axs[0].plot(X, Y_pred, color='red', label='Least squares line', linewidth=2)
axs[0].set_title('Single Sample Regression')
axs[0].legend()

# Plot on the right
# axs[1].scatter(X, Y, color='blue', label='Sample points')
axs[1].plot(X, beta_0 + beta_1 * X, color='green', label='True population line', linewidth=2)

# Plot 10 additional estimated regression lines
for _ in range(10):
    X_sample = np.random.rand(n, 1)
    e_sample = np.random.randn(n, 1)
    Y_sample = beta_0 + beta_1 * X_sample + e_sample
    model_sample = LinearRegression().fit(X_sample, Y_sample)
    Y_sample_pred = model_sample.predict(X)
    axs[1].plot(X, Y_sample_pred, color='orange', linestyle='--', alpha=0.6)

# Also plot the estimated line from the original sample
axs[1].plot(X, Y_pred, color='red', label='Original sample estimated line', linewidth=2)
axs[1].set_title('Multiple Sample Regressions')
axs[1].legend()

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בסימולציה שלפנינו אפשר לראות את שני הקווים המדוברים: קו הרגרסיה האמיתי של האוכלוסיה בירוק, על-פי הקו הזה נדגמו 100 נקודות. וקו הריבועים הפחותים באדום, שנאמד אך ורק מתוך הנקודות. אפשר לראות שהם דומים. בתרשים הימני אנחנו חוזרים על התרגיל עוד 10 פעמים, כל פעם דוגמים 100 תצפיות אחרות ואומדים את קו הריבועים הפחותים. אם נחזור על זה הרבה פעמים או אם נדגום המון תצפיות, אנחנו אמורים לקבל בממוצע את קו הרגרסיה האמיתי של האוכלוסיה.
:::
:::

---

## Probabilistic view of linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Probabilistic justification for OLS

::: {.incremental}
- Assume the data's true model is:
  - $y_i = \beta_0 + \beta_1x_i + \varepsilon_i$
  - $\varepsilon_i$ are i.i.d, specifically: $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$
  - $(x_i, \varepsilon_i)$ are independent
- *Now* we can write the log likelihood and maximize it
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עד כה לא הנחנו אף הנחה הסתברותית. לא הזכרנו משתנים מקריים, בטח לא התפלגות נורמלית. זה חשוב להבין שפתרון הרגרסיה שראינו הוא פתרון עם הצדקה מתמטית בלי שום הצדקה סטטיסטית. ומסתבר שעם נרצה הצדקה סטטיסטית כזאת נגיע לפתרון זהה, אבל נרוויח עוד כמה דברים בדרך.

אז נוסיף למודל שלנו משתנה מקרי אפסילון, משתנה רעש. כלומר אנחנו מניחים שיש פונקציה ליניארית קבועה של Y כנגד X והסיבר שהתצפיות לא יושבות בדיוק על הקו הזה היא שעבור כל אחת נוסף איזשהו רעש טבעי שמזיז אותה קצת למעלה או למטה מהקו. בנוסף נניח שהאפסילונים הם בלתי תלויים ומתפלגים זהה או IID. נזכיר שוב שניתן להכליל גם למצב שתוספות הרעש הזה אינן בלתי-תלויות, אלא תלויות למשל בX. אבל נתחיל במודל הפשוט הזה, ונניח עוד שהרעש מתפלג נורמלית עם תוחלת אפס ושונות סיגמא בריבוע, פרמטר לא ידוע בדרך כלל שנצטרך לאמוד.

אז כאמור המשתנה המסביר X והרעש אפסילון בלתי תלויים, וכעת יש לנו מודל הסתברותי מדויק וברור, והקריטריון הטבעי לאמוד את הפרמטרים שלו הוא למקסם את הנראות, הלייקליהוד, או לוג הנראות, כי יש לו צורה פשוטה יותר בדרך כלל.
:::
:::

---

### Simple linear regression maximum likelihood

::: {.fragment}
Since the errors are normal and the model is linear we get:

$y_i = \beta_0 + \beta_1x_i + \varepsilon_i  \to y_i \sim \mathcal{N}(\beta_0 + \beta_1x_i, \sigma^2) \to$
:::

::: {.fragment}
$$L(\beta, \sigma^2 | x, y) = \prod_{i = 1}^n f(y_i) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(y_i - \beta_0 - \beta_1x_i)^2}{2\sigma^2}\right]$$
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_i)^2\right]$$
:::

::: {.fragment}
Calculating the log-likelihood we get:

$\ell(\beta, \sigma^2 | x, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}$[$\sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_i)^2$]{style="color:red;"}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז כאמור X הוא לא משתנה מקרי תחת מודל הרגרסיה, רק אפסילון הרעש משתנה מקרי. יוצא מזה שY הוא טרנספורמציה ליניארית של משתנה מקרי, ולכן הוא בעצמו משתנה מקרי שמתפלג נורמלית, עם תוחלת שהיא קו הרגרסיה האמיתי, ושנות סיגמה בריבוע.

מהי פונקצית הנראות? פונקצית הנראות היא פונקציה של הפרמטרים שאנחנו צריכים לאמוד, בהינתן מדגם הנתונים. ההגדרה שלה עבור משתנה רציף היא הצפיפות המשותפת של המדגם, ואם התצפיות בלתי תלויות מדובר במכפלת הצפיפויות. מכפלת הצפיפויות היא מכפלת הצפיפויות הנורמליות של Y, ואפשר לפשט אם זוכרים שמכפלת האקספוננטים היא אקספוננט בחזקת הסכום. את הביטוי הזה נרצה להביא למקסימום כפונקציה של הבטאות וסיגמא בריבוע.

אבל נוח יותר לקחת לוג ולהביא למקסימום את לוג הנראות. וכשלוקחים לוג, מתקבל איזשהו קבוע שלא חשוב לנו כי הוא לא פונקציה של הפרמטרים, ביטוי שתלוי רק בסיגמא בריבוע, אבל הבטאות משחקות תפקיד רק בביטוי השלישי, שמזכיר באופן חשוד את הRSS. המסקנה היא שלגבי הבטאות, להביא למינימום את הRSS או להביא למקסימום את לוג הנראות -- זה בדיוק אותו דבר! כלומר באמצעות ההנחות הסטטיסטיות שהוספנו, אנחנו מקבלים בדיוק את אותם אומדים בטא-האט, ועד לנקודה זו לא השתנה שום דבר.

מה כן השתנה? השינוי העיקרי הוא שהאומדים שלנו, בטא-אפס-האט ובטא-אחת-האט הם כעת משתנים מקריים, שניתן לבצע עליהם הסקה סטטיסטית. נראה את זה תיכף, אבל קודם כל נדבר על רגרסיה ליניארית מרובה, מה קורה כשיש P משתנים מסבירים?
:::
:::

---

## Multiple linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### From simple to multiple regression

<br></br>

$y_i = \beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip} + \varepsilon_i \to y_i = x_i'\beta + \varepsilon_i$, $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$

::: {.fragment}
Or even more concisely:

$y = \begin{pmatrix}y_{1} \\ \vdots \\ y_{n}\end{pmatrix}$, $X = \begin{pmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p} \\
1 & x_{31} & x_{32} & \cdots & x_{3p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{np}
\end{pmatrix}$, $\beta = \begin{pmatrix}\beta_0 \\ \beta_{1} \\ \vdots \\ \beta_{p}\end{pmatrix}$, $\varepsilon = \begin{pmatrix}\varepsilon_{1} \\ \vdots \\ \varepsilon_{n}\end{pmatrix}$
:::

::: {.fragment}
$$\to y = X\beta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(\textbf{0}, \sigma^2I_n)$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז כעת יש לנו P משתנים מסבירים, ועוד חותך. זה אומר P פלוס אחת פרמטרים, לא כולל את סיגמא בריבוע מהמודל ההסתברותי. דרך נוחה יותר לרשום את זה אם x_i, ובטא הם וקטורי עמודות, ואנחנו מוסיפים 1 לx_i כדי להכפיל אותו בחותך, היא שy_i שווה למכפלה הפנימית בין וקטור הX לוקטור הבטא, ועוד משתנה רעש אפסילון, שכמו מקודם מתפלג נורמלית עם תוחלת אפס ושונות סיגמא בריבוע.

אבל אם אנחנו כבר נעזרים בוקטורים, נהוג לרשום את Y עצמו גם כן כוקטור עמודה באורך n, אותו דבר לגבי וקטור הבטא ווקטור אפסילון. וX גדול הוא מטריצת הנתונים שלנו, כל שורה היא אותו וקטור x_i עם התוספת 1 לחותך, כלומר קיבלנו מטריצה עם n שורות ל-n תצפיות, וP פלוס 1 עמודות, P עמודות למשתני X ועוד עמודת אחדות שהתווספה מצד שמאל בדרך כלל.

אז אפשר לרשום את המודל בכתיב וקטורי שעושה את המימוש למהיר הרבה יותר, Y הוא המטריצה X מוכפלת בוקטור בטא, ועוד תוספת וקטור רעש אפסילון. כעת נשים לב שההתפלגות של אפסילון היא התפלגות של וקטור, התוחלת שלו היא וקטור, וקטור האפסים, והשונות שלו היא מטריצה. מטריצה בגודל n על n, שעל האלכסון שלה נמצאות השונויות של כל אלמנט ואלמנט של הוקטור - במקרה שלנו, כולן סיגמא בריבוע. ומחוץ לאלכסון נמצאים הקווריאנסים בין כל שני אלמנטים, במקרה שלנו ההנחה שכל זוג טעויות הן בלתי תלויות, כלומר הקווריאנס הוא אפס. ובקיצור אפשר לרשום את המטריצה כסיגמא בריבוע, הסקלר, כפול מטריצת היחידה בגודל n על n.
:::
:::

---

### From line to (hyper)plane
```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from matplotlib.patches import Patch
from matplotlib.lines import Line2D

df = pd.read_csv('../datasets/ISLP_income_data.csv')
X1 = df['Education']
X2 = df['Seniority']
X = np.column_stack((X1, X2))
Y = df['Income']

# Plotting
fig, ax = plt.subplots(1, 1, figsize=(14, 7), subplot_kw={'projection': '3d'})

# Create a grid of values for X1 and X2
X1_grid, X2_grid = np.meshgrid(np.linspace(min(X1), max(X1), 200),
                               np.linspace(min(X2), max(X2), 200))

# Flatten the grid to pass into the model
X_grid = np.column_stack((X1_grid.ravel(), X2_grid.ravel()))

model = LinearRegression()

model.fit(X, Y)

# Predict Y values
Y_pred = model.predict(X_grid).reshape(X1_grid.shape)

# Plot the fitted plane
ax.scatter(X1, X2, Y, color='red', label='Data')
ax.plot_surface(X1_grid, X2_grid, Y_pred, color='blue', alpha=0.5)

# Add vertical lines from data points to the surface
Y_surface = model.predict(X).reshape(Y.shape)  # Predicted Y values for the actual data points
for x1, x2, y, y_surface in zip(X1, X2, Y, Y_surface):
    ax.plot([x1, x1], [x2, x2], [y, y_surface], color='black')

# Labels
ax.set_xlabel('Education')
ax.set_ylabel('Seniority')
ax.set_zlabel('Income')
# ax.set_title(title)

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז לפני לוג הנראות, מה אומר המודל הליניארי כשיש יותר ממשתנה אחד? המודל שלנו הוא לא קו, אלא מישור, או היפר-מישור כשמדובר במימד גבוה. אם לדוגמא אנחנו ממדלים הכנסה כפונקציה של השכלה וותק במודל הליניארי, אנחנו מניחים שהכנסה היא מישור על המרחב הנפרש בידי השכלה וותק.
:::
:::

---

### Multiple linear regression ML

$y_i = x_i'\beta + \varepsilon_i  \to y \sim \mathcal{N}(x_i'\beta, \sigma^2) \to$

::: {.fragment}
$$L(\beta, \sigma^2 | X, y) = \prod_{i = 1}^n f(y_i) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(y_i - x_i'\beta)^2}{2\sigma^2}\right]$$
:::

::: {.fragment}
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - x_i'\beta)^2\right]$$
:::

::: {.fragment}
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta)\right]$$
:::

::: {.fragment}
Calculating the log-likelihood we get:

$\ell(\beta, \sigma^2 | X, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}$[$(y - X\beta)'(y - X\beta)$]{style="color:red;"}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איך מחשבים את לוג הנראות עם P משתנים? אין הרבה שינוי.

הנראות היא עדיין מכפלת הצפיפויות, כך שמקבלים אותו ביטוי. ההמשך כרגיל, מכפלת האקספוננטים היא אקספוננט של הסכום, אבל מהו הסכום הזה? יש כאן הפרש של שני וקטורים באורך n, גם הוא וקטור באורך n. ואנחנו רוצים את סכום האיברים שלו בריבוע. הדבר הזה שווה בעצם למכפלה פנימית של הוקטור עם עצמו. וכך אנחנו מגיעים לY פחות Xbeta טרנספוז כפול Y פחות Xbeta.

כשאנחנו לוקחים לוג, אנחנו שוב רואים את הRSS שלנו במינוס רק שעכשיו הוא רשום בצורה וקטורית. בעצם לא השתנה הרבה, גם רגרסיה מרובת משתנים יכולנו לפתח בצורה מתמטית, כי הנה להביא למינימום את הRSS גם במקרה הכללי זה כמו להביא למקסימום את לוג-הנראות, כשזה נוגע למקדמי הרגרסיה.

אז בואו נעשה את זה פעם אחת למקרה כללי.
:::
:::

---

### Linear regression MLE

$\ell(\beta, \sigma^2 | X, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta)$

::: {.fragment}
$= \mathcal{C} - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y'y -2\beta'X'y + \beta'X'X\beta)$
:::

<br></br>

::: {.fragment}
$\frac{\partial l}{\partial \beta} = -\frac{1}{2\sigma^2}(2X'y - 2X'X\beta)$
:::

::: {.fragment}
$\frac{1}{\sigma^2}(X'y - X'X\beta) = 0$
:::
::: {.fragment}
$X'X\beta = X'y$
:::
::: {.fragment}
[$\hat{\beta} = (X'X)^{-1}X'y$]{style="color:red;"}
:::

<br></br>

::: {.fragment}
Similarly:

$\frac{\partial l}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}(y - X\beta)'(y - X\beta)$ $\to$ [$\hat{\sigma}^2 = \frac{1}{n}(y - X\beta)'(y - X\beta) = \frac{RSS}{n}$]{style="color:red;"}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הביטוי הראשון הוא כאמור קבוע שאינו תלוי בפרמטרים של הבעיה, בטא וסיגמא בריבוע. את הביטוי השני נעתיק כפי שהוא, ואת הביטוי השלישי נפתח. אם תפתחו את המכפלה הפנימית של לפנינו תקבלו משהו שמזכיר את נוסחאות הכפל המקוצר, יש לנו את המכפלה הפנימית של Y בעצמו, פחות פעמיים המכפלה הפנימית של הוקטור Xbeta בY, ועוד המכפלה הפנימית של הוקטור Xbeta בעצמו.

אנחנו רוצים לגזור את לוג הנראות לפי בטא קודם כל, להשוות לאפס, ולחלץ את האומד לבטא שהוא נקודת מינימום. נשים לב שמה שרשום כאן הוא וקטור של נגזרות חלקיות. אפשר היה לגזור את הביטוי לכל בטא בנפרד, אבל כדי לקצר תהליכים ניעזר קצת בנגזרות פשוטות של וקטורים. את הגורם הכופל נעתיק כפי שהוא. את הגורם שתלוי רק בY נשמיט. כעת הגורם האמצעי הוא כמו לקחת את כל אחת מהבטאות ולכפול אותה פי ביטוי מסוים. הנגזרת תהיה הביטוי עצמו. ולכן ניתן לרשום זאת כוקטור של כל הביטויים האלה, הוא הוקטור שמוכפל פי בטא, X טרנספוז Y.

באופן דומה נגזור את הביטוי השלישי, אם תפתחו אותו תראו שכל אחת מהבטאות מועלית בריבוע ומוכפלת פי ביטוי מסוים. לכן נגזרת תהיה 2 כפול בטא כפול הביטוי. בכתיב קצר נקבל 2 כפול המטריצה X טרנספוז X כפול וקטור בטא.

הגורמים של 2 מצטמצמים, ואת הנגזרת המתקבלית אנחנו רוצים להשוות לאפס. מעבירים את הביטוי עם בטא לאגף אחד ואת הביטוי השני לאגף אחר. כלומר אם נכפיל משמאל את המשווה במטריצה ההופכית של X טרנספוז X נוכל לבודד את בטא, וזה אומד הריבועים הפחותים המפורסם לכן הוא צבוע באדום. נזכיר רק שכדי לקבוע שזאת נקודת מקסימום של לוג הנראות צריך לחשב גם את מטריצת הנגזרות השניות ולהראות שהיא שלילית. וכבר עכשיו אפשר לראות שהאומד שקיבלנו הוא צירוף ליניארי של תצפיות Y! ולמי ששואל את עצמו, אז כאן, עבור וקטור בטא של משתנה אחד, כלומר וקטור של בטא-אפס, בטא-אחת, אנחנו צריכים לקבל בחזרה את הנוסחאות הפשוטות של בטא-אפס-האט, ובטא-אחת-האט.

באופן דומה אפשר לגזור את לוג הנראות לפי סיגמא בריבוע ולקבל את אומד הנראות המקסימלית לסיגמא בריבוע, שיוצא באופן אינטואיטיבי שונות המדגם של השאריות, הRSS סכום הריבועים הפחותים או של התצפיות מהאמידה שלהן, מחולק פי n.

יש לנו את האומדים ברגרסיה ליניארית, וכמו שאמרנו הם משתנים מקריים - אנחנו יכולים לחשב עכשיו את ההתפלגות שלהם.
:::
:::

---

## Distribution of OLS estimators {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה עכשיו מה אנחנו מרוויחים מהמודל ההסתברותי - נחשב את התפלגות האומדים שקיבלנו.
:::
:::

---

### Distribution of the OLS solution

::: {.incremental}
- What we know: 
$$(a)\; E(y) = X\beta,\;\;\;\; (b)\; Cov(y) = \sigma^2 I_n ,\;\;\;\;(c)\; \hat{\beta} = (X'X)^{-1} X' y$$

- Mean: 
$$E(\hat{\beta}) \stackrel{(c)}{=} (X'X)^{-1} X' E(y) \stackrel{(a)}{=} (X'X)^{-1} X' X\beta = \beta.$$

- Covariance matrix: 
$$Cov(\hat{\beta}) \stackrel{(c)}{=} (X'X)^{-1} X' Cov(y) X (X'X)^{-1} \stackrel{(b)}{=} \sigma^2 (X'X)^{-1} (X' X) (X'X)^{-1} = \sigma^2 (X'X)^{-1}.$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אנחנו רוצים למצוא את ההתפלגות של האומד בטא האט. בטא-האט הוא צירוף לינארי של משתנים נורמליים ולכן גם הוא, מתפלג נורמלית, עם איזשהו וקטור תוחלות ומטריצת שונויות (להדגים). השאלה היחידה היא מהו וקטור התוחלות ומטריצת השונויות האלה. 

מה ידוע לנו עד כה? התוחלת המותנית של Y היא X בטא. הYים בלתי תלויים, מטריצת השונויות שלהם היא אלכסונית עם סיגמא בריבוע על האלכסון, ניתן לסמן זאת כך. והאומד לבטא-האט נראה כך.

נחשב את התוחלת של בטא-האט: האיקסים הם קבועים, ומליניאריות התוחלת הם יוצאים החוצה ונשארת רק התוחלת של Y, שהיא כידוע X בטא, וכך אנחנו מגיעים לעובדה שהתוחלת של בטא-האט היא בטא עצמו, בטא-האט נקרא אומד חסר הטיה לבטא.

ומטריצת השונות או הקווריאנס של וקטור בטא-האט: כשמחשבים שונות של סקלאר כפול משתנה הסקלאר יוצא בריבוע. כשמחשבים מטריצת שונות של מטריצת קבועים כפול הוקטור שלנו, היא יוצאת בהכפלה משמאל ומימין. אבל מטריצת השונות של Y היא כאמור אלכסונית, וכל מה שנשאר זה הכפלה של הביטוי הזה בסיגמא בריבוע. דברים מצטמצמים ומגיעים לביטוי סופי, סיגמא בריבוע כפול המטריצה ההופכית של X'X.

נסכם: בטא-האט מתפלג נורמלית עם תוחלת בטא האמיתית, ושונות סיגמא בריבוע כפול מטריצת X'X.
:::
:::

---

### Variance of OLS estimators

::: {.incremental}
- Again: $Cov(\hat{\beta}) = \sigma^2(X'X)^{-1}$

- For simple regression and scalar $\hat{\beta}_0, \hat{\beta}_1$, this amounts to the (squared) [Standard Errors]{style="color:red;"}:
$$SE(\beta_0)^2 = \sigma^2\left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i = 1}^n(x_i - \bar{x})^2}\right]; \quad SE(\beta_1)^2 = \frac{\sigma^2}{\sum_{i = 1}^n(x_i - \bar{x})^2}$$

- where, $\sigma^2$ would be replaced be its MLE or unbiased estimator the (squared) [Residual Standard Error]{style="color:red;"} (RSE): $RSE^2 = \frac{RSS}{n - p - 1}$
:::

::: {.fragment}
::: {.callout-note}
Question: what happens to the SE when $x$ is more "spread out"?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אפשר להתעכב קצת על הביטוי לשונות של מקדמי הרגרסיה. קודם כל עבור רגרסיה ליניארית פשוטה, עם משתנה אחד, ניתן לראות שעל האלכסון של מטריצת השונויות שקיבלנו יש ביטויים יחסית פשוטים. כשמוציאים מהם שורש מקבלים את סטיות התקן של האומדים, שמכנים טעויות התקן, סטנדרט ארורז או SE בקיצור.

בכל מקרה, בין אם מסתכלים על הביטוי הכללי למטריצת השונויות של המקדמים או על הביטויים לטעות התקן של החותך ושל השיפוע ברגרסיה ליניארית פשוטה, אנחנו רואים שיש שם פרמטר לא ידוע סיגמא בריבוע שנהוג להחליף באומד שלו. מקובל אבל להשתמש לא באומד הנראות המקסימלית שכרגע קיבלנו, אלא באומד מעט שונה, הRSS לא מחולק פי n אלא פי n פחות מספר הפרמטרים שאמדנו. באופן כללי אמדנו P ועוד 1 פרמטרים, ועבור רגרסיה ליניארית פשוטה אמדנו 2 פרמטרים, אז נקבל במכנה n פחות 2. כשמוציאים שורש מקבלים את מה שמכונה הRSE, רזידואל סטנדרט ארור.

הביטויים שקיבלנו הם האומדים לשונויות של האומדים של המקדמים. הם קובעים כמה אנחנו יכולים לסמוך על האומדנים שקיבלנו. אם הם גדולים מאוד למשל, אז האומד שלנו רועש וקשה לסמוך על המספר הזה שיתקבל. ומעניין להסתכל עליהם רגע ולשאול מה משפיע עליהם. דבר אחד ברור, שהם מושפעים מאוד מסיגמא בריבוע, השונות הטבעית של הרעש. עוד הגיוני לראות שהם מושפעים מגודל המדגם. ודבר נוסף יפה לראות, זה שהם מאוד מושפעים מהפיזור של X. ככל שהX שקיבלנו הוא בעל פיזור גדול יותר, הוא מגוון, טעויות התקן של האומדים קטנות יותר, אנחנו יכולים יותר לסמוך עליהן.
:::
:::

---

### SE vs. $x$ spread

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Function to create sample data and perform linear regression
def generate_data(n, spread_x=False, seed=1):
    np.random.seed(seed)
    bound = 1
    if spread_x:
      bound = bound * 5
    X = np.random.uniform(-bound, bound, n)
    noise = np.random.normal(0, 1, n)
    Y = 2 * X + 3 + noise  # Y = 2X + 3 + noise
    return X, Y

def plot_regression(ax, X, Y, x_min, x_max, label, spread_x=False):
    ax.scatter(X, Y, label='Sample Data')
    
    # Perform linear regression
    model = LinearRegression()
    X_reshaped = X.reshape(-1, 1)
    model.fit(X_reshaped, Y)
    X_full_range = np.linspace(x_min, x_max, 100).reshape(-1, 1)
    Y_pred = model.predict(X_full_range)
    ax.plot(X_full_range, Y_pred, color='red', label='Least Squares Line')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    
    # Calculate standard error of the slope
    X_mean = np.mean(X)
    se = np.sqrt(np.sum((Y - model.predict(X_reshaped)) ** 2) / (len(Y) - 2)) / np.sqrt(np.sum((X - X_mean) ** 2))
    ax.annotate(f'SE(slope): {se:.2f}', xy=(0.05, 0.7), xycoords='axes fraction', fontsize=12, 
                bbox=dict(facecolor='white', alpha=0.6))
    
    # Plot a few other least squares lines from different samples
    for _ in range(5):
        new_X, new_Y = generate_data(len(X), spread_x, seed=np.random.randint(1000))
        new_X_reshaped = new_X.reshape(-1, 1)
        model.fit(new_X_reshaped, new_Y)
        new_Y_pred = model.predict(X_full_range)
        ax.plot(X_full_range, new_Y_pred, color='gray', linestyle='--', alpha=0.5)
    
    ax.set_title(label)

# Create figure and axes
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Generate data for both cases
X1, Y1 = generate_data(50)
X2, Y2 = generate_data(50, spread_x=True)

# Define the same X/Y axis limits for both plots
x_min, x_max = min(np.min(X1), np.min(X2)), max(np.max(X1), np.max(X2))
y_min, y_max = min(np.min(Y1), np.min(Y2)), max(np.max(Y1), np.max(Y2))

# Plot for the first case (normal spread)
plot_regression(axs[0], X1, Y1, x_min, x_max, 'Small spread of X')
axs[0].set_xlim(x_min, x_max)
axs[0].set_ylim(y_min, y_max)

# Plot for the second case (more spread out X)
plot_regression(axs[1], X2, Y2, x_min, x_max, 'Large spread of X', True)
axs[1].set_xlim(x_min, x_max)
axs[1].set_ylim(y_min, y_max)

plt.tight_layout()
plt.show()
```

::: {.fragment}
Closely related to a $x$ points' [leverage]{style="color:red;"}.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אפשר להדגים את זה בקלות. בשני הגרפים שלפנינו התאמנו קו רגרסיה ל50 תצפיות שמקיימות את המודל הליניארי. אול בגרף השמאלי הפיזור של משתנה הX שלהם קטן יחסית, הם נלקחו מאיזור מצומצם. ובגרף הימני הפיזור של משתנה הX שלהם גדול, הם נלקחו מאיזור רחב.

אז טכנית אפשר לחשב את טעות התקן לפי הנוסחאות שראינו ולראות שבגרף השמאלי היא פי 5 מבגרף הימני. אפשר אבל גם לחזור על התרגיל כמה פעמים ולסרטט כל פעם את קו הרגרסיה המתקבל ולראות למשל לגבי השיפוע איך הוא יכול להשתנות כשהX כל כך מפוזר, לעומת המצב כשהX מפוזר היטב.

ברמה הפיסיקלית אפשר להקצין את זה עוד יותר ולשאול מה קורה אם הרגרסיה היתה נקבעת רק משתי נקודות. אם הן נורא נורא קרובות אפשר לדמיין מן נדנדה כזאת לא בטוחה. והתופעה הזאת קשורה מאוד להגדרה של מושג שנקרא לברג' או משען של נקודות שנראה בהמשך. ככל שהוא גדול כך לנקודות יש יותר השפעה, משקל באיך ייראה קו הרגרסיה.
:::
:::

---

### Gauss-Markov Theorem

Under the assumptions of the previous slides: the linear regression $\hat{\beta}$ is the [best linear unbiased estimator]{style="color:red;"} (BLUE), i.e.: the unbiased linear estimator with the smallest variance.

<br></br>

::: {.fragment}
In other words, for any linear **unbiased** $\tilde{\beta} = Cy$:
$$Var(\tilde{\beta}) \succeq Var(\hat{\beta})$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לסיום החלק הזה נזכיר תוצאה מעניינית שנקראת משפט גרוס-מרקוב: תחת ההנחות של המודל הסטטיסטי, אומד הריבועים הפחותים הוא אומד בלו: בסט ליניאר אנבייסד אסטימטור. כלומר מתוך כל האומדים הליניארים חסרי הטיה, הוא האומד בעל השונות המינימלית. 

שימו-לב, לא מכל האומדים האפשריים, אלא אם תיתנו לי איזשהו אומד בטא-טילדא שהוא ליניארי והכוונה ליניארי בY כמו האומד שלנו, והוא חסר-הטייה, השונות שלו בהכרח גדולה או שווה לשונות של בטא-האט שמצאנו. כאן אני כותב את זה בשפה של מטריצות.

השאלה המתבקשת היא אם יש אומד שהוא לא בהכרח חסר-הטייה שיכולה להיות לו שונות קטנה יותר, והתשובה המפתיעה כפי שנראה כשנדבר על רגרסיית רידג' היא: כן!
:::
:::

---

## Hypothesis testing {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
חוץ מזה שהשונות של האומדים שלנו מעניינת בפני עצמה, אמרנו שאחת המטרות שלנו בלמידה סטטיסטית היא הסקה סטטיסטית. יש לנו הרבה שאלות כחוקרים על המודל הזה. האם משתנה מסוים משפיע או לא על Y מעבר לספק סטטיסטי. ואם אנחנו משתמשים במודל הזה לחיזוי תצפיות שלא ראינו, למשל בתרחיש של מדגם למידה ומדגם טסט שדיברנו עליו -- האם יכול להיות שיש תת-קבוצה של משתנים ששווה בכלל להוציא מהמודל, היא לגמרי מזיקה לטיב החיזוי?
:::
:::

---

### Hypothesis testing: simple

$$H0\text{: feature }x\text{ does not affect } y$$
$$H1\text{: feature }x\text{ does affect } y$$

::: {.fragment}
Translates to:
$$H0: \beta_1 = 0$$
$$H1: \beta_1 \neq 0$$
:::

::: {.fragment}
$t_{obs} = \frac{\hat{\beta}_1 - 0}{\hat{SE}(\hat{\beta}_1)} \sim T_{n - 2}$
:::

::: {.fragment}
$\to \text{P-value} = P(T_{n - 2} > |t_{obs}|)$

$\to CI_{0.95}(\beta_1) = \hat{\beta}_1 \pm T_{n - 2, 0.975} \cdot \hat{SE}(\hat{\beta}_1) \approx \hat{\beta}_1 \pm 2 \cdot \hat{SE}(\hat{\beta}_1)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל ברגרסיה פשוטה עם משתנה אחד, שם השאלה הטבעית היא מה השורה התחתונה המשתנה קשור לY, משפע עליו, או לא.

זה מתרגם לבדיקת השערות פשוטה על מקדם השיפוע. אם הוא אפס אין למשתנה X קשר עם Y, ורק אם הוא שונה מאפס קו הרגרסיה יהיה עם טרנד חיובי או שלילי ונסיק שיש השפעה לX.

להדגים: אז יש כאן משתנה מקרי שמתפלג תחת השערת האפס נורמלית עם תוחלת אפס ושונות SE בריבוע. אם אנחנו מחסרים את התוחלת ומחלקים בטעות התקן שאיננה ידועה אז אנחנו מחליפים אותו באומד, הסטטיסטי הזה מתפלג טי עם n פחות 2 דרגות חופש.

מדובר בהשערה דו-צדדית, אז אפשר לחשב עבורה פי-ווליו דו-צדדי. אם t אובזרווד הוא חיובי אז אנחנו רוצים את הסיכוי להיות גדול כמוהו או יותר ועוד הסיכוי להיות קטן כמו מינוס t אוברזווד או יותר. באופן כללי נרשום את זה כך.

אפשר לא להסתפק גם בתשובה בינארית לשאלה אלא ממש לתת רווח סמך לשיפוע, לדוגמא רווח סמך ברמת ביטחון שיכלול את השיפוע האמיתי בסיכוי 95 אחוז, והוא בערך האומדן שקיבלנו פלוס מינוס פעמיים טעות התקן הנאמדת.
:::
:::

---

### Hypothesis testing: multiple

$$H0\text{: features }X\text{ do not affect } y$$
$$H1\text{: features }X\text{ do affect } y$$

::: {.fragment}
Translates to:
$$H0: \beta_1 = \dots = \beta_p = 0$$
$$H1: \beta_j \neq 0 \text{ for at least one }j$$
:::

::: {.fragment}
$f_{obs} = \frac{(TSS - RSS)/p}{RSS / (n - p - 1)} \sim F_{p, n - p - 1}$
:::

::: {.fragment}
$\to \text{P-value} = P(F_{p, n - p - 1} > f_{obs})$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשיש לנו P משתנים, אנחנו בדרך נשאל קודם כל את השאלה האם כלל המשתנים בX משפיעים על Y או לא, ותיכף נדבר על למה.

זה מתרגם להשערת האפס שאומרת שכל הבטאות שוות אפס, מול ההשערה האלטרנטיבית שלפחות אחת מהן לא שווה לאפס.

את ההשערה הזאת נכון לבדוק עם סטטיסטי שמתפלג התפלגות F. התפלגות F היא התפלגות שבה הערך המינימלי הוא אפס, היא לא סימטרית ויש לה שני פרמטרים, דרגות החופש של המונה, ודרגות החופש של המכנה. במכנה יש לנו את האומד לסיגמא בריבוע שכבר ראינו הRSS מחולק בn פחות p פחות 1. ובמונה יש לנו את הטוטאל סאם אוף סקוורז פחות הRSS חלקי p. הטוטאל סאם אוף סקוורס הוא בעצם המונה של השונות של Y (להדגים), כלומר הרעש של Y הטבעי תחת מודל שבו יש רק חותך, שהחיזוי שלו הוא הכי פשוט, הממוצע של Y.

מכל מקום עדיין מדובר בבדיקת השערת רגילה ואפשר לחשב P-value תחת התפלגות F, זה הסיכוי לקבל סטטיסטי גדול כמו שקיבלנו או יותר. רק אם הP-value  הזה קטן נהוג להמשיך לבדוק את ההשפעה של כל אחד מהמשתנים, ותיכף נגיד למה.
:::
:::

---

### Example with `statsmodels`

```{python}
#| echo: false

import numpy as np
import pandas as pd
import statsmodels.api as sm

df = pd.read_csv('../datasets/ISLP_income_data.csv')
X_df = df[['Education', 'Seniority']]
Y = df['Income']

X_df1 = sm.add_constant(X_df)

model = sm.OLS(Y, X_df1)
res = model.fit()
print(res.summary())
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
דיברנו המון תיאוריה אז בואו נראה פלט רגרסיה טיפוסי, כאן עם ספריה סטטיסטית של פייתון בשם סטאטסמודלז.

במקרה הזה מידלנו הכנסה, כנגד השכלה וותק, על פני 30 נבדקים. זה אומר שהn הוא 30, p הוא 2, וזה רשום. מצד ימין אפשר לראות את ערך הסטטיסטי F והוא גדול מאוד כמו שמי שיש לו ניסיון עם התפלגות F יודע, ובהתאמה הP-value של כל הרגרסיה הוא אפסי, כלומר יש בהחלט קשר בין השכלה וותק לבין הכנסה. אפשר גם לראות את לוג-הנראות, ואת אומדני המקדמים שיצאו: החותך יצא מינוס 50, שזה לא נשמע הגיוני כל-כך, הכנסה של מינוס 50 אלף דולר בשנה. אבל נזכור שהחותך מתקבל מתי שהאיקסים עצמם הם אפסים, כלומר עבור אפס השכלה ואפס ותק, ולא בטוח שיש לנו רצון לחזות בכלל עבור ערכים כאלה, כך שיכול להיות שהמודל עדיין שימושי. המקדמים האחרים יצאו כמעט 6 להשכלה, ו0.17 לותק. זה אומר שנצפה לעלייה של כששת אלפים דולר לשנה על כל שנה של השכלה ועלייה של כ170 דולר על כל שנת ותק.

לא נדבר על כל מספר בפלט הזה, אבל בכל זאת שני חלקים נרצה להסביר: אחד זה מאיפה מגיעים מבחני הT על כל אחד מהמקדמים, הרי לא הזכרנו את האפשרות הזאת עבור רגרסיה מרובה, הזכרנו רק את מבחן הF. ושתיים זה להזכיר את ערך הR בריבוע והזהירות שצריך לנקוט כאשר משתמשים בו.
:::
:::

---

### Choosing between nested models

::: {.incremental}
- Checking whether a subset of $q$ features given the other $p - q$ features affect $y$:
$$H0: \beta_{p - q + 1} = \dots = \beta_p = 0$$
$$H1: \beta_j \neq 0 \text{ for at least one } j \text{ out of } q \text{ features}$$

- Run an additional regression *without* these $q$ features, reach $RSS_0$, then:
:::

::: {.fragment}
$f_{obs} = \frac{(RSS_0 - RSS)/q}{RSS / (n - p - 1)} \sim F_{q, n - p - 1}$
:::

::: {.fragment}
- When $q = 1$, $f_{obs} = t^2_{obs}$, where $t_{obs} \sim T_{n - p - 1}$ (testing a single feature given others)
:::

::: {.fragment}
::: {.callout-note}
Question: why $F$-test? Why not test for each of the $p$ features with this $t_{obs}$ test?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז בדרך להכיר את המבחנים למקדמים בודדים רצוי להתחיל מהאפשרות לעשות בדיקת השערות ברגרסיה מרובה לתת-קבוצה Q של משתנים. למשל לשאול האם לQ המשתנים האחרונים שנכנסו לרגרסיה יש השפעה על Y, בהינתן שP-Q המשתנים האחרים כבר נמצאים במודל. זה מה שנקרא לבדוק האם אפשר להישאר במודל מקונן, נסטד, של P-Q משתנים.

הדרך לבצע את זה היא להריץ עוד רגרסיה על המודל הקטן יותר עם P-Q משתנים, ולקבל סכום ריבועים של שאריות שנסמן בRSS0. שימו-לב שככל שנוסיף משתנים הRSS יכול רק להיות קטן יותר. כלומר הRSS במודל המקורי הגדול הוא בטוח קטן יותר. אם הוא קטן מספיק לעומת RSS0, שחושב על מודל ללא הQ משתנים שאנחנו בוחנים, אז נגיד שלפחות אחד מהם חשוב. מכמתים את זה שוב עם סטטיסטי שמתפלג F עם דרגות חופש מעט שונות. כדי שהוא יהיה גדול צריך שהמודל הרווי, עם כל הP משתנים יביא לRSS קטן מספיק.

ואם Q = 1, כלומר אנחנו בודקים האם להכניס או לא משתנה ספציפי מעבר לשאר המשתנים שנמצאים במודל, מסתבר שהערך שנקבל הוא בעצם ערך הטי בריבוע שהיינו מקבלים, עבור מבחן טי פשוט אם להכניס את המשתנה הזה לרגרסיה או לא, בהינתן כל המשתנים האחרים. שימו-לב זה חשוב, ברגרסיה עם משתנה אחד אין משתנים אחרים ולכן השאלה הזאת כלל לא עולה. ההשערה הזאת בודקת האם מעבר למה שתורמים המשתנים האחרים ברגרסיה המשתנה הבודד הנידון מוסיף איזשהו אפקט על Y. מכל מקום אלה הם ערכי הטי והP-values ורווחי הסמך שראינו בפלט הרגרסיה לכל משתנה בנפרד.

ועכשיו אנחנו רוצים לחזור אחורה ולשאול למה צריך מבחני F. למשל לגבי ההשערה הכוללת של האם לפחות אחד מהמשתנים המסבירים משפיע על Y, אם יש לנו דרך לבצע בדיקת השערות עבור משתנה אחד, אפשר למשל לבדוק השערות לכל אחד מP המשתנים, ואם לפחות אחד מהם יגיע למובהקות סטטיסטית, לקבוע שכן, יש קשר בין המשתנים בX לבין Y.

זה לא רעיון טוב, מסתבר, במיוחד במודל עם הרבה משתנים, בגלל בעיה מפורסמת שנקראת בעיית ההשוואות המרובות. כל אחד מהמשתנים ייבדק מול טעות מסוג ראשון של נאמר 5 אחוז, אבל הסיכוי שלפחות אחד מהם נראה מובהק סטטיסטית כשהוא לא, הוא כבר גדול הרבה יותר. הוא 1 פחות 95 אחוז בחזקת P. עבור שני משתנים זה כבר כמעט 10 אחוז, עבור 100 משתנים זה כבר סיכוי של 100 אחוז לראות משתנה מובהק סטטיסטית למרות שאין אף משתנה בעל קשר לY. אז אנחנו צריכים קודם בדיקת השערות עם טעות מסוג ראשון של 5 אחוז שתקבע האם בכלל להסתכל במבחני הT עבור המקדמים השונים.
:::
:::

---

## Goodness of Fit and Feature Selection {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
המודל הליניארי, אף על פי שתיכף נדון איך להגמיש אותו, הוא עדיין מודל נוקשה. ובתור מודל נוקשה, בכל פעם שמריצים רגרסיה ליניארית מקובל גם לבדוק את ההנחות שלנו ואת טיב המודל. באופן טבעי נוצרה ספרות שלמה סביב מטריקות ושיטות מדידה לצורך בדיקת טיב המודל, אנחנו רק נזכיר זאת בקצרה ונשתמש בזה כדי לבצע בחירת משתנים או פיצ'ר סלקשן, בצורה פשוטה.
:::
:::

---

### Model Fit: plotting

When $p$ is low - draw!

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression

# Generate the dataset
np.random.seed(42)
n_samples = 100
X = np.random.normal(size=(n_samples, 2))  # Random values in 2D
X[:, 0] *= 3  # Scale the first feature
X[:, 1] *= 10  # Scale the second feature
noise = np.random.normal(size=(n_samples,)) * 0.1 # Noise to add to the output

# Non-linear function with an upward trend
y = np.sin(X[:, 0]) + 0.2*X[:, 1] + noise

# Fit a linear regression model
reg = LinearRegression()
reg.fit(X, y)
y_pred = reg.predict(X)

# Plot the data and the fitted plane
fig, axes = plt.subplots(1, 2, figsize=(12, 5), subplot_kw={'projection': '3d'})

# Scatter plot of the original data
axes[0].scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')
axes[1].scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')

# Create a grid for plotting the plane
x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
y_grid = reg.intercept_ + reg.coef_[0] * x1_grid + reg.coef_[1] * x2_grid

# Plot the linear regression plane
axes[0].plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')
axes[1].plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')

# Draw vertical lines from the plane to the data points
for i in range(n_samples):
    axes[0].plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')
    axes[1].plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')

# Adjust the viewing angle
axes[0].view_init(elev=30, azim=-60)
axes[1].view_init(elev=30, azim=90)

# Set labels and title
axes[0].set_xlabel('X1')
axes[1].set_xlabel('X1')
axes[0].set_ylabel('X2')
axes[1].set_ylabel('X2')
axes[0].set_zlabel('y')
axes[1].set_zlabel('y')
# ax.set_title('Non-linear Data with Linear Regression Plane')

plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז קודם כל כשמספר המשתנים נמוך, פשוט ציירו את הנתונים. כאן למשל אני מצייר את Y כנגד שני משתנים מסבירים עם תרשים תלת-מימדי. אני מוסיף גם את המישור שיתאים מודל ליניארי. במבט חטוף הוא נראה סביר, ונראה שאין השפעה בכלל למשתנה X1, אבל אם אני מסובב את הצירים קצת, אני רואה שיש ועוד איך השפעה למשתנה X1, אבל זה גל סינוס! אז המודל כפי שהוא כרגע לא מתאים, תיכף ניתן פתרונות.
:::
:::

---

### Model fit: residuals plots

```{python}
#| echo: false

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].scatter(X[:, 0], y_pred - y)
axes[0].axhline(y=0, color='r', linestyle='--')
axes[0].set_xlabel('X1')
axes[0].set_ylabel('Residuals')
axes[1].scatter(X[:, 1], y_pred - y)
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('X2')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
גם כשמספר המשתנים גבוה ומריצים רגרסיה, אפשר לבצע דיאגנוסטיקה, למשל עם תרשימי שאריות. כאן ציר הX יהיה המשתנה עצמו, כל פעם משתנה אחר, ועל ציר הY השאריות החזויות של המודל Y פחות Y_hat, או איזושהי גרסה מתוקננת שלהן.

כאן אנחנו מצפים לראות ענן חסר צורה סביב ציר האפס, אבל בתרשים השמאלי עבור משתנה X1 זה בבירור לא קורה והמודל שמניח רעש קבוע, שלא תלוי בX, בבירור לא מתקיים, וצריך לשנות את המודל. אנליסטים מנוסים בהרצת רגרסיות אכן עושים זאת בצורה איטרטיבית, מניחים מודל, מתאימים אותו בהרצת רגרסיה, בודקים את ההנחות ומשנים בהתאם לצורך. יש עוד הרבה תרשימים מהסוג הזאת וזאת אומנות שלמה אבל נעצור כאן.
:::
:::

---

### Model Fit: $R$ squared

Measure of "explained variance":

$R^2 = 1 - \frac{\sum_{i = 1}^n (y_i - X\hat{\beta})^2}{\sum_{i = 1}^n (y_i - \bar{y})^2} = 1 - \frac{RSS}{TSS} = 1 - \frac{(y - X\hat{\beta})'(y - X\hat{\beta})}{(y - X\hat{\beta^*_0})'(y - X\hat{\beta^*_0})}$

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Generate sample data
np.random.seed(42)
X = np.linspace(0, 10, 15)
y = 2 * X + 1 + np.random.randn(15) * 2  # Linear relation with noise

# Perform linear regression
coeffs = np.polyfit(X, y, 1)
y_hat = np.polyval(coeffs, X)

# Choose a sample point far from the mean
x_i = 7
y_i = 2 * x_i + 1 + 3  # Simulated data point
y_hat_i = np.polyval(coeffs, x_i)

# Mean line
y_mean = np.mean(y)

# Create the plot
fig, ax = plt.subplots(figsize=(7, 4))

# Plot data points
ax.scatter(X, y, color='red', label='Data points')
ax.scatter(x_i, y_i, color='red')  # Highlighted data point

# Plot regression line
ax.plot(X, y_hat, color='blue', label='Regression line')

# Plot mean line
ax.axhline(y=y_mean, color='blue', linestyle='--', label='Mean line ($\overline{y}$)')

# Plot fitted point
ax.scatter(x_i, y_hat_i, color='blue', zorder=5)  # Fitted point

# Draw vertical lines from the points to the lines
ax.vlines(x_i, y_i, y_mean, color='black', linestyle='--')
ax.vlines(x_i, y_i, y_hat_i, color='black', linestyle='--')

# Setting labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.legend()

plt.show()
```

::: {.fragment}
::: {.callout-note}
Careful: $R^2$ is monotone increasing in $p$ for the training data!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מדד אחר לטיב המודל שגם ראינו בפלט הרגרסיה הוא האר בריבוע, R סקוורד.

אנחנו מסתכלים על כל תצפית ושואלים כמה המודל שלנו מצליח לחזות אותה היטב. יחסית למה? יחסית לממוצע. אז הבייסליין שלנו זה סכום השאריות הריבועיות מהממוצע, הTSS, והיינו רוצים שהחיזוי שלנו יקלוט כמה שיותר מהטעות הזאת, כלומר יסביר כמה שיותר מהשונות הטבעית של Y. כלומר נרצה שהשאריות שלנו יהיו כמה שיותר קרובות לאפס. נחסר מהTSS את הRSS ונרצה שהביטוי יהיה כמה שיותר גבוה יחסית לTSS, אז נחלק אותו בTSS וזהו הR בריבוע. אפשר לכתוב אותו כ1 פחות סכום השאריות המרובעות שהתקבלו מהמודל שלנו יחסית לסכום השאריות המרובעות ממודל עם פרמטר אחד בטא-אפס-כוכב וכל שאר הבטאות אפסים, כי אז ברור שבטא אפס כוכב יהיה ממוצע Y.

וכאן מגיעה אזהרה חשובה: מה זאת אומרת "נרצה שהRSS יהיה קטן ככל שאפשר"? הרי אם נוסיף עוד ועוד משתנים למודל הRSS יכול רק להיות קטן יותר. במילים אחרות הR בריבוע יכול רק לעלות, הוא פונקציה מונוטונית עולה במספר המשתנים. כבר נתקלנו בתופעה הזאת! בסיטואציה שבה אנחנו נעזרים במודל לחיזוי, ויש לנו את המדגם שעליו הוא מאומן ומדגם טסט שהוא לא ראה, התופעה הזאת של הוספת עוד ועוד משתנים נקראת אוברפיטינג.

אבל זה לא אומר שמדד הR בריבוע הוא חסר-תועלת כפי שיצא שמו לרעה, פשוט צריך לדעת להשתמש בו נכון. זה סביר להשוות בין שני מודלים על-פי הR בריבוע אם מדובר למשל באותו מספר משתנים. זה סביר להסתכל על התוספת בR בריבוע עקב הוספת משתנה אחד ולשאול את עצמנו אם זה "שווה" את זה. זה לא סביר להסתכל על R בריבוע כאיזה אורקל ובגללו להוסיף עוד ועוד משתנים.
:::
:::

---

### Intro. to feature selection

How can we choose a subset of the variables?

::: {.incremental}
- All subsets: what's wrong with that?
- Forward: what's wrong with that?
- Backward
- Hybrid
- Other more advanced methods
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מה שמביא אותנו לדיון קצר על פיצ'ר סלקשן, בחירת משתנים, נושא שנעסוק בו בהמשך. איך נזהה את תת הקבוצה הטובה ביותר? היא לא קטנה כל כך שהמודל פשטני מדי עם שאריות גדולות ואולי שיטתיות עם R בריבוע גבוה. והיא לא גדולה מדי, וסתם מנפחת את R בריבוע.

האם יש לנו אפשרות לבדוק את כל תתי הקבוצות? לכל בעיה מעשית לא ממש, מדובר בבדיקה בסדר גדול של 2 בחזקת P. כבר עבור 10 משתנים זה אומר להריץ כאלף רגרסיות.

אפשר לבצע רגרסיית פורוורד: להתחיל במודל עם חותך בלבד, וכל פעם להוסיף את המשתנה המשפיע ביותר, למשל זה שמגדיל את הR בריבוע הכי הרבה. אם נגיע למצב שבו אף משתנה לא מביא להגדלה משמעותית של הR בריבוע או שהמקדם שלו לא מובהק מעבר לשאר המשתנים - נעצור. הבעיה היא שזאת שיטה מאוד גרידית. וכפי שנראה בהמשך, מאחר שבכל זאת המשתנים יכולים להיות מתואמים ביניהם במידה מסוימת, בהחלט יכול לקרות מצב שיהיה לנו במודל משתנה שהוא כבר מיותר, לעומת שניים שהגיעו אחריו.

אפשרות אחרת זה רגרסיית באקוורד: להתחיל עם כל המשתנים במודל, וכל פעם להוריד משתנה שהמקדם שלו לא מובהק סטטיסטית, הכי פחות מועיל, למשל שהP-value שלו הכי גדול. נבצע שוב רגרסייה ונבחר במשתנה הבא הכי פחות מועיל. נעצור למשל כשכל המקדמים מובהקים, כלומר כשברור שלכל אחד המשתנים יש השפעה מובהקת מעבר לאחרים. בעיה עם גישה כזאת יכולה להיות למשל במקרה שמספר המשתנים גדול ממספר התצפיות, ולא ניתן להתאים מודל ליניארי סטנדרטי.

הגישה ההיברידית משלבת בין שתי הגישות: נתחיל עם מודל ללא משתנים, נוסיף את המשתנה המועיל ביותר ונמשיך ככה. בכל נקודה נעצור ונשאל אם יש משתנה שאינו מובהק סטטיסטית, כלומר הוא כבר לא מועיל מעבר לאחרים, ואם כן נוריד אותו מהמודל. ככה נמשיך עד שלא תהיה יותר תנועה פנימה והחוצה מהמודל, כל המשתנים בפנים מובהקים, ואף משתנה שנוסיף לא יהיה מובהק. זאת הגישה הטובה ביותר בשלב זה, אבל נשים לב שחישובית היא הרבה יותר קשה.

מלבד הגישות האלה יש עוד שאולי ניתקל בהן בהמשך הקורס.
:::
:::

---

## Extending the linear model {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נדבר כעת על כמה דרכים להגמיש את המודל הליניארי. יש הרבה נושאים כאן, אז נתמקד בהוספת משתנים קטגוריאליים, באינטראקציות וברגרסיה פולינומיאלית.
:::
:::

---

### Categorical features with $k$ levels

::: {.fragment}
When $k = 2$:
:::

::: {.incremental}
- If $x$ indicates whether person "has tattoos" or "no tattoos"
- Define: $x_i =
\begin{cases} 
1 & \text{if person } i \text{ has tattoos,} \\
0 & \text{otherwise.}
\end{cases}$
- Model: $y_i = \beta_0 + \beta_1x_i + \varepsilon_i$
- Meaning: $\beta_0 = E(y|\text{no tattoos}) \quad \beta_1 = E(y|\text{has tattoos}) - E(y|\text{no tattoos})$
:::

::: {.fragment}
::: {.callout-note}
Question: how would you test if "tattoo" has effect on $y$?
:::
:::

::: {.fragment}
::: {.callout-note}
Question: what if $x_i =
\begin{cases} 
1 & \text{if person } i \text{ has tattoos} \\
-1 & \text{otherwise.}
\end{cases}$
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל במשתנים קטגוריאליים בעלי K קטגוריות, כאשר הנפוצים ביותר הם משתנים בינאריים, או משתני אינדיקטור. לדוגמא אנחנו ממדלים זמן שהות של עצורים במעצר ואחת העמודות שלנו כוללת חיווי האם לעצור יש קעקוע או לא.

הדרך להתמודד עם משתנה כזה היא בדרך כלל להגדיר משתנה אינדיקטור, X יקבל את הערך 1 אם לעצור יש קעקוע, ו0 אחרת.

עכשיו אפשר להכניס את X למודל הליניארי בלי בעיה. ומה המשמעות של בטא-אפס ובטא-אחת?

כש-X הוא 1 Y יהיה בטא אפס ועוד בטא אחת ועוד רעש. כשX הוא 0 Y יהיה רק בטא-אפס. כלומר בטא-אפס הוא החיזוי לברירת המחדל, אנשים בלי קעקוע. בטא-אחת יבטא את ההפרש בין זמן המעצר של אנשים עם ובלי קעקוע. באופן מדויק יותר הוא הפרש התוחלת המותנית של Y בהינתן קעקוע לעומת בלי קעקוע.

אז אם אני רוצה לבדוק האם יש השפעה למשתנה קעקוע על זמן המעצר איך אני בוחן הזה? באמצעות בדיקת השערות על בטא-אחת, אם הוא לא שונה במובהק מאפס לא צריך פרמטר נוסף בשביל קעקוע.

אבל זו לא הדרך היחידה לקודד משתנה כזה, למעשה יש הרבה. אחת האפשרויות היא לקודד אותו כ1 לבעלי קעקוע ומינוס 1 לאלה שאין להם קעקוע. במקרה כזה עצרו וחשבו מה המשמעות של בטא-אפס ובטא-אחת. בטא-אפס יהיה החותך כרגיל, זמן המעצר הממוצע על פני כל העצורים, ובטא-אחת יבטא עד כמה נבדלים עצורים עם קעקוע ובלי קעקוע, מעל ומתחת לממוצע הזה. זה לא בדיוק ממוצע, ברור, כי הקבוצות לא חייבות להיות שוות בגודלן, זה מדגיש כמה הקידוד יכול לשנות את פירוש המקדמים.
:::
:::

---

#### Categorical features with $k$ levels: $k > 2$

::: {.incremental}
- If $x$ is a person's profession (carpenter, gardener, or teacher)

::: {.fragment}
::: {.callout-note}
Question: what if we just had $x_{i} =
\begin{cases} 
0 & \text{if person } i \text{ is a teacher} \\
1 & \text{if person } i \text{ is a carpenter} \\
2 & \text{if person } i \text{ is a gardener}
\end{cases}$
:::
:::

- Define $k - 1$ indicators:

::: {.fragment}
$x_{i1} =
\begin{cases} 
1 & \text{if person } i \text{ is a carpenter,} \\
0 & \text{otherwise.}
\end{cases}$, $x_{i2} =
\begin{cases} 
1 & \text{if person } i \text{ is a gardener,} \\
0 & \text{otherwise.}
\end{cases}$
:::

- Why $k - 1$ indicators?
- Model: $y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \varepsilon_i$
- Meaning: $\beta_1 = E(y|\text{carpenter}) - E(y|\text{teacher}); \quad \beta_2 = E(y|\text{gardener}) - E(y|\text{teacher})$
:::

::: {.fragment}
::: {.callout-note}
Question: how would you test if "profession" has effect on $y$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מה קורה כאשר יש יותר מ2 קטגוריות?

לדוגמא משתנה ברגרסיה עם מקצוע של נבדק, ויש לנו 3 רמות: נגר, גנן ומורה.

אז קודם כל נשאלת השאלה למה שלא נמשיך בקו הקודם ונקודד מורה למשל להיות קוד 0, נגר להיות 1 וגנן להיות 2? זה לא לא-נכון, אם אנחנו מאמינים שיש קודם כל איזשהו סדר כזה, שלהיות מורה זה "פחות" מלהיות נגר, ונגר זה "פחות" מלהיות גנן. כאן זה נשמע שרירותי לחלוטין, ולא רק זה, מסתתרת כאן הנחה שמשתנה כזה יכול להימדד בסולם מנה או סולם רווח, שהאינטרוולים בין מורה לנגר, ובין נגר לגנן הם זהים, קבועים, בעלי איזושהי משמעות. וזה כבר ממש לא סביר, אבל ישנם מקרים שאפשר היה לחשוב על קידוד כזה, כשהמשתנה קטגוריאלי אבל עדיין בעל סדר ברור, לדוגמא: חומרת מחלה, או קבוצת גיל.

מכל מקום מה שמקובל לעשות עבור משתנים שאין היררכיה באמת בין הרמות שלהם, נקרא one hot encoding. נגדיר לK הקטגוריות K מינוס 1 משתני אינדיקטור. רמה אחת נשאיר כבייסליין, כאן זה מורה. וכל רמה אחרת תוגדר כמשתנה אינדיקטור שמקבל 1 אם X הוא ברמה הזאת או 0 אחרת.

למה K פחות 1? למה לא להוסיף אינדיקטור כאן ל"מורה"? כי אז תהיה לנו תלות ליניארית מושלמת בין המשתנים שלנו. המשתנה של מורה יהיה בדיוק 1 פחות סכום המשתנים של גנן ונגר. מה שיהווה בעיה מתמטית לפתרון הריבועים הפחותים ועוד נדבר על זה. אנחנו לא באמת צריכים 3 משתנים לתיאור 3 קטגוריות כי הרי איך נדע שנבדק הוא מורה? אם יש לו אפס גם במשתנה נגר וגם במשתנה גנן.

כעת אנחנו מכניסים את הK מינוס 1 משתנים למודל, ושוב נחשוב מה משמעות הבטאות הנאמדות?

עבור מורים נחזה בטא-אפס. עבור נגרים נחזה בטא-אפס ועוד בטא-אחת. עבור גננים נחזה בטא-אפס ועוד בטא-שתיים. כלומר בטא-אפס הוא התוחלת המותנית של מורים, הבייסליין. ובטא-אחת ובטא-שתיים מודדים את ההפרש, התוספת של נגר ממורה וגנן ממורה בהתאמה. 

ושוב נשאל, בסיטואציה כזאת: איך נבדוק את ההשערה האם למשתנה מקצוע יש אפקט על המשתנה התלוי Y? כאן נהיה חייבים להשתמש בהשערה בו-זמנית של שני הפרמטרים בטא-אחת ובטא-שתיים, ולבדוק האם הם שווים אפס או שלפחות אחד לא, כלומר כאן נצטרך לערוך 2 רגרסיות ולהשתמש במבחן F להשוואה בין שני מודלים מקוננים.

ובכל זאת עוד שאלה: מה עם K הוא ממש גדול? הנבדקים שלנו יש להם אלף מקצועות? אפשר לעבוד באותה צורה בדיוק, אבל כאן אני מקווה שאתם חשים בסכנה, שאפשר לכנות פשוט כאוברפיטינג. לעשות OHE על משתנה קטגוריאלי עם כל כך הרבה רמות, סביר להניח שגודל הקבוצה של חלק מהרמות הוא קטן מאוד, והמקדם שנקבל הוא רועש מאוד, הוא אוברפיטד לקבוצה קטנה ולא מייצגת. אכן בעיה, אבל היא לא בסקופ של הקורס שלנו.
:::
:::

---

### Interactions

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression

# Generate the dataset
np.random.seed(42)
n_samples = 100
X = np.random.normal(size=(n_samples, 2))  # Random values in 2D
X[:, 0] *= 3  # Scale the first feature
X[:, 1] *= 3  # Scale the second feature
noise = np.random.normal(size=(n_samples,)) * 0.1 # Noise to add to the output

# Non-linear function with an upward trend
y = 0.5*X[:, 0] + 0.2*X[:, 1] + 0.3*X[:, 0] * X[:, 1] + noise

# Fit a linear regression model
reg = LinearRegression()
reg.fit(X, y)
y_pred = reg.predict(X)

# Plot the data and the fitted plane
fig = plt.figure(figsize=(14, 7))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot of the original data
ax.scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')

# Create a grid for plotting the plane
x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
y_grid = reg.intercept_ + reg.coef_[0] * x1_grid + reg.coef_[1] * x2_grid

# Plot the linear regression plane
ax.plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')

# Draw vertical lines from the plane to the data points
for i in range(n_samples):
    ax.plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

# Set labels and title
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('y')

plt.show()
```

::: {.fragment}
A [multiplicative]{style="color:red;"} effect?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
דרך נוספת להגמיש את המודל הליניארי, היא להתחשב בתופעה שנקראת אינטראקציה בין משתנים. אנחנו רואים כאן איזשהו מישור שהותאם לאיזשהו Y כנגד שני משתנים X1 וX2, וכבר נראה שיש בעיה. אף על פי שהמישור בסך הכל מזהה טרנד נכון, נראה כאילו יכולנו לקבל משטח הרבה יותר טוב. בפינה אחת הוא לא חוזה מספיק גבוה עבור התצפיות, אז היינו רוצים שהמשטח "יעלה" קצת כלפי מעלה, ובפינה אחרת הוא לא חוזה מספיק נמוך עבור התצפיות, אז היינו רוצים שהמשטח "ירד" קצת כלפי מטה.

במילים אחרות נראה שY משתנה בX1 וגם בX2, אבל גם באיזשהו שילוב שלהם, ויש כאן אפקט כפלי, או אפקט מולטיפליקטיבי. שימו לב אני עדיין לא אומר שX1 וX2 תלויים זה בזה, אני פשוט אומר שאי אפשר לנתק את ההשפעה של X2 כשמסתכלים על ההשפעה של X1 על Y ולהיפך. X2 משנה את ההשפעה של X1 על Y, מגביר או מחליש אותה, ולהיפך.
:::
:::

---

### Interactions: residuals plots

```{python}
#| echo: false

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].scatter(X[:, 0], y_pred - y)
axes[0].axhline(y=0, color='r', linestyle='--')
axes[0].set_xlabel('X1')
axes[0].set_ylabel('Residuals')
axes[1].scatter(X[:, 1], y_pred - y)
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('X2')
plt.show()
```

::: {.fragment}
Consider adding an interaction term to the model: $x_1 \cdot x_2$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אחת הדרכים לראות שהמודל האדיטיבי כפי שהוא לא מתאים היא באמצעות תרשימי שאריות. כאן גם עבור X1 וגם עבור X2 מתקבלת תמונה בעייתית מאוד, ככל שהמודל מתרחק ממרכז המשתנים, מאפס, הפיזור הולך וגדל.

אז במקרה כזה, שווה להוסיף למודל אפקט כפלי, וזה נקרא אפקט של אינטראקציה. כשההשפעה של משתנה אחד, מושפעת מההשתנות של משתנה אחר.
:::
:::

---

### Interactions: adding an interaction term

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression

# Generate the dataset
np.random.seed(42)
n_samples = 100
X = np.random.normal(size=(n_samples, 2))  # Random values in 2D
X[:, 0] *= 3  # Scale the first feature
X[:, 1] *= 3  # Scale the second feature
X = np.hstack((X, (X[:, 0] * X[:, 1]).reshape(-1, 1)))  # Add a new interaction feature
noise = np.random.normal(size=(n_samples,)) * 0.1 # Noise to add to the output

# Non-linear function with an upward trend
y = 0.5*X[:, 0] + 0.2*X[:, 1] + 0.3*X[:, 0] * X[:, 1] + noise

# Fit a linear regression model
reg = LinearRegression()
reg.fit(X, y)
y_pred = reg.predict(X)

# Plot the data and the fitted plane
fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot of the original data
ax.scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')

# Create a grid for plotting the plane
x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
y_grid = reg.intercept_ + reg.coef_[0] * x1_grid + reg.coef_[1] * x2_grid + reg.coef_[2] * x1_grid * x2_grid

# Plot the linear regression plane
ax.plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')

# Draw vertical lines from the plane to the data points
for i in range(n_samples):
    ax.plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

# Set labels and title
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('y')

plt.show()
```

$y_i = \beta_0 + \beta_1x_{i1} + \beta_2 x_{i1} + \beta_3 x_{i1} \cdot x_{i2}+ \varepsilon_i$

::: {.fragment}
::: {.callout-note}
Note: in multiplicative models we usually add the features without interactions to the model ("main effects"), even if their P-value is not small.
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשאנחנו מוסיפים את גורם האינטראקציה המשטח שלנו נראה הרבה יותר מתאים לנתונים, וניתן לראות איך למרות שאנחנו עדיין נשארים בפריימוורק של המודל הליניארי, אנחנו כבר לא מתאימים רק מישור, אנחנו כבר משיגים משטח מעניין.

יש כל מיני אתגרים עם אינטראקציות, לא בטוח שכדאי למהר אם ככה להכניס את כל האינטראקציות הזוגיות כמועמדות לרגרסיה. אם יש לכם P משתנים זה אומר עוד P מעל 2 משתנים שנוספו, סדר גודל של P בריבוע.

בעיה אחרת היא מה לעשות אם מקדם האינטראקציה עצמו יוצר מובהק סטטיסטית, אבל המקדמים של האפקטים ה"ראשיים" עצמם לא יוצאים מובהקים. במקרה כזה נהוג בכל זאת להשאיר אותם במודל.

ועוד לא דיברנו על אינטראקציות מסדר גבוה יותר, סדר שלישי ומעלה. נסו לחשוב מה המשמעות של אינטראקציה מסדר שלישי.
:::
:::

---

### Non-linear relations

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression

df = pd.read_csv('../datasets/mtcars.csv')

model = LinearRegression().fit(df[['hp']], df['mpg'])
y_pred = model.predict(df[['hp']])

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.scatterplot(data=df, x='hp', y='mpg', ax=axes[0])
axes[0].plot(df['hp'], y_pred, color='red')
axes[0].set_xlabel('Horsepower')
axes[0].set_ylabel('Miles per Gallon')

sns.scatterplot(x=df['hp'], y=y_pred - df['mpg'], ax=axes[1])
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('Horsepower')
axes[1].set_ylabel('Residuals')

plt.show()
```

::: {.fragment}
Consider adding polynomial terms $x^2_1, x^3_1$, ...
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אינטראקציה היא לא הגורם היחיד שניתן להוסיף על מנת להגמיש את המודל הליניארי. כאן אנחנו רואים נתונים של כ30 מכוניות, ואנחנו מנסים למדל את צריכת הדלק שלהן, מיילים לגלון, כנגד כוח הסוס של המנוע שלהן. מייד ברור שיש יחס יורד, אבל האם הוא ליניארי? הוא נראה יותר עקום. ובאמת תרשים שאריות מראה את זה מייד, נוצרת איזושהי פרסה, שמעידה שהנחות המודל לא מתקיימות.

יש הרבה גורמים שאפשר לנסות להוסיף ועדיין להישאר במודל הליניארי, סינוס, קוסינוס. אבל בואו נתחיל מהוספת גורמים פולינומיים, כמו X בחזקת 2, 3...
:::
:::

---

### Plynomial regression: Beware 1

```{python}
#| echo: false

x_hp = df[['hp']].values
X2 = np.hstack((x_hp, x_hp**2))
X5 = np.hstack((x_hp, x_hp**2, x_hp**3, x_hp**4, x_hp**5))
model1 = LinearRegression().fit(x_hp, df['mpg'])
model2 = LinearRegression().fit(X2, df['mpg'])
model5 = LinearRegression().fit(X5, df['mpg'])

X1_range = np.linspace(df['hp'].min(), df['hp'].max(), 100).reshape(-1, 1)
X2_range = np.hstack((X1_range, X1_range**2))
X5_range = np.hstack((X1_range, X1_range**2, X1_range**3, X1_range**4, X1_range**5))
y_pred1 = model1.predict(X1_range)
y_pred2 = model2.predict(X2_range)
y_pred5 = model5.predict(X5_range)

fig = plt.figure(figsize=(6, 5))
sns.scatterplot(data=df, x='hp', y='mpg')
plt.plot(X1_range, y_pred1, color='red', label = 'Linear')
plt.plot(X1_range, y_pred2, color='orange', label = 'Degree 2')
plt.plot(X1_range, y_pred5, color='green', label = 'Degree 5')
plt.xlabel('Horsepower')
plt.ylabel('Miles per Gallon')

plt.legend()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשאנחנו מוסיפים גורמים פולינומיאליים יש שמכנים זאת רגרסיה פולינומיאלית. אבל כאן צריך לנקוט משנה זהירות. כמו שניתן לראות בתרשים, כשאנחנו עוברים מקו ליניארי שהוא מוגבל לפולינום, אנחנו לאט לאט עושים אוברפיטינג חריג לנתונים. פולינום מדרגה חמישית למשל כבר ממש מנסה לגעת בכל הנקודות האפשריות של הנתונים.
:::
:::

---

### Plynomial regression: Beware 1

Quadratic:

```{python}
#| echo: false

import statsmodels.api as sm

X2 = sm.add_constant(X2)
model = sm.OLS(df['mpg'], X2).fit()
model.summary().tables[1]
```

Cubic:

```{python}
#| echo: false

X3 = np.hstack((x_hp, x_hp**2, x_hp**3))
X3 = sm.add_constant(X3)
model = sm.OLS(df['mpg'], X3).fit()
model.summary().tables[1]
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הדרך כמובן להגביל את עצמנו ברגרסיה פולינומיאלית היא להוסיף גורמים בצורה הדרגתית ולעצור אם הם לא מובהקים ונראה שאין בהם צורך, בהסתכלות על מבחנים סטטיסטיים ותרשימי שאריות. כאן ניתן לראות שפולינום ריבועי מתאים היטב לנתונים של המכוניות, שני הגורמים X וX בריבוע מובהקים. כשאנחנו מוסיפים X בשלישית הוא כבר גורם לא חשוב ברגרסיה והוא ממסך גם את ההשפעה של הגורם הריבועי, אז כאן כדאי לעצור ולהישאר עם המודל הריבועי.
:::
:::

---

### Plynomial regression: Beware 2

```{python}
#| echo: false

X1_range = np.linspace(df['hp'].min() - 100, df['hp'].max() + 100, 1000).reshape(-1, 1)
X2_range = np.hstack((X1_range, X1_range**2))
X5_range = np.hstack((X1_range, X1_range**2, X1_range**3, X1_range**4, X1_range**5))
y_pred1 = model1.predict(X1_range)
y_pred2 = model2.predict(X2_range)
y_pred5 = model5.predict(X5_range)

fig = plt.figure(figsize=(6, 5))
sns.scatterplot(data=df, x='hp', y='mpg')
plt.plot(X1_range, y_pred1, color='red', label = 'Linear')
plt.plot(X1_range, y_pred2, color='orange', label = 'Degree 2')
plt.plot(X1_range, y_pred5, color='green', label = 'Degree 5')
plt.xlabel('Horsepower')
plt.ylabel('Miles per Gallon')

plt.legend()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
סכנה נוספת במעבר מקו ליניארי לפולינום: מה קורה כשמתרחקים, או שחוזים על נקודה חדשה מעבר לסקאלה שראינו?

הבעיה הזאת חמורה גם לקו ליניארי, אבל קו ליניארי הוא צפוי ואינטואיטיבי. כאן אם נקודה חדשה תהיה מאוד רחוקה מנקודות שראינו עדיין נחזה עבורה ערך צפוי. בפולינום הרבה פעמים מקבלים ערך לא הגיוני בכלל. כאן כבר בפולינום ריבועי המשמעות היא שעבור ערכים גבוהים של כוח סוס החיזוי יעלה. ועבור פולינום ממעלה 5 החיזוי של צריכת דלק ירד בצורה חדה מתחת לאפס!

אז ראינו כמה דרכים להגמיש את המודל הליניארי. בכל אחת מהדרכים שדיברנו עליהן אבל יש מחיר לשלם, והמחיר הוא פוטנציאל גבוה יותר לאוברפיטינג, ולמודל מורכב מדי שלא יהיו לו ביצועים טובים על נתונים שהמודל לא ראה.
:::
:::

---

## Other issues with linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כבר הזהרנו שרגרסיה ליניארית הוא אחד המודלים הנחקרים ביותר בספרות, זה אומר שעל כל ניואנס בו נכתבו תלי תלים של מאמרים. לסיכום הנושא נזכיר שתי בעיות שצריך להיות מודעים אליהן ולקרוא עוד קצת על הטיפול בהן: קוליניאריות ותצפיות חריגות.
:::
:::

---

### Why "not significant"?

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from matplotlib.patches import Patch
from matplotlib.lines import Line2D

# Seed for reproducibility
np.random.seed(42)

# Generate synthetic data
n_samples = 100
X1 = np.random.normal(0, 1, n_samples)
X2 = 0.5 * X1 + np.random.normal(0, 0.1, n_samples)  # X2 is correlated with X1
Y = 0.5 * X1 + 1 * X2 + np.random.normal(0, 1, n_samples)

# Create a DataFrame
data = pd.DataFrame({'X1': X1, 'X2': X2, 'Y': Y})
X = data[['X1', 'X2']].values

# Plotting
fig, ax = plt.subplots(1, 1, figsize=(12, 5), subplot_kw={'projection': '3d'})

# Create a grid of values for X1 and X2
X1_grid, X2_grid = np.meshgrid(np.linspace(min(X1), max(X1), 200),
                               np.linspace(min(X2), max(X2), 200))

# Flatten the grid to pass into the model
X_grid = np.column_stack((X1_grid.ravel(), X2_grid.ravel()))

model = LinearRegression()

model.fit(X, Y)

# Predict Y values
Y_pred = model.predict(X_grid).reshape(X1_grid.shape)

# Plot the fitted plane
ax.scatter(X1, X2, Y, color='red', label='Data')
ax.plot_surface(X1_grid, X2_grid, Y_pred, color='blue', alpha=0.5)

# Add vertical lines from data points to the surface
Y_surface = model.predict(X).reshape(Y.shape)  # Predicted Y values for the actual data points
for x1, x2, y, y_surface in zip(X1, X2, Y, Y_surface):
    ax.plot([x1, x1], [x2, x2], [y, y_surface], color='black')

# Labels
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('y')

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

plt.show()
```

```{python}
#| echo: false

import statsmodels.api as sm

# Perform linear regression with both features
X_both = sm.add_constant(data[['X1', 'X2']])
model_both = sm.OLS(data['Y'], X_both).fit()
print(model_both.summary().tables[1])
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כדי להציג את הבעיה הראשונה בואו נסתכל על הנתונים האלה. Y משתנה בבירור כפונקציה של X1 ושל X2. המישור שאנחנו מתאימים ברגרסיה ליניארית אפילו נראה סביר. אבל אם נבצע הסקה סטטיסטית על מקדמי הרגרסיה שקיבלנו, אף אחד לא מובהק סטטיסטית, נראה כאילו היה מתאים פה המישור השוכב של הממוצע של Y. למה זה קורה? אם נביט היטב נראה שבכל זאת יש משהו חריג בנתונים האלה. אם נשרטט את X2 כנגד X1 נראה בעיה: יש ביניהם קורלציה גבוהה מאוד.
:::
:::

---

### Collinearity

::: {.incremental}
- [Collinearity]{style="color:red;"} is when a feature is in the span of other features
- Usually it is not exactly in the span but near the span
- In the case of exact collinearity: the matrix $X'X$ is singular, no unique solution
- In the case of approximate collinearity:
  - If $X_1 \approx X_2$, then: $y = X_1; \quad y = X_2; \quad y = \frac{1}{2}X_1 + \frac{1}{2}X_2;\quad y = 1000X_1 - 999X_2$ are the same models!
  - the solution is not numerically stable
  - $SE(\hat{\beta})$ grows, $t_{obs}$ value decreases, feature is "masked" in inference
- Explore your data with a correlation matrix, many other metrics
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
קוליניאריות היא מצב שבו פיצ'ר מסוים, או יותר, הם בספאן של פיצ'רים אחרים, כלומר צירוף ליניארי של משתנים אחרים. ראינו פוטנציאל לזה כשדיברנו על ייצוג של משתני אינדיקטור למשתנים קטגוריאליים עם OHE. דוגמאות שכיחות יותר זה כשמישהו החליט למשל לתת את הטמפרטורה בצלזיוס וגם בפרנהייט. בשני המקרים מדובר במשתנה מיותר שכדאי היה פשוט להוריד.

אבל זאת דוגמא קיצונית, נגיד שקוליניאריות מתרחשת גם כשפיצ'ר אחד הוא בקירוב צירוף ליניארי של פיצ'רים אחרים, כמו למשל שערכתם מחקר על ילדים והחלטתם להכניס למודל גם את הגיל של הילדים וגם את מידת הנעליים שלהם. ובין שני אלה הרי יש מתאם לא מבוטל. או אולי יש לכם פשוט הרבה פיצ'רים, ומה לעשות במקרה יש שם פיצ'רים עם מתאם גדול ביניהם.

מה יכול להיות בעייתי בזה? שימו-לב, לא הנחנו שהעמודות של X הן בלתי תלויות, אבל אם עמודה אחת היא צירוף ליניארי של האחרות אז הדרגה של X תהיה לא מלאה, והמטריצה X'X הזאת שמופיעה באומדן הריבועים הפחותים תהיה סינגולרית, אין לה הופכית ולכן אין גם פתרון.

במצב הפחות קיצוני אבל המדאיג יותר, שבו יש קוליניאריות בקירוב, לדוגמא כשמשתנה X1 דומה מאוד למשתנה X2 כמו בדוגמא שלנו. המודל Y שווה X1 או המודל Y שווה X2. או המודל Y שווה אלף X1 פחות 999 X2 - הם נורא דומים. בפועל, הופכי של X'X יהיה לא יציב נומרית, הוא יכול להיות פתאום נורא גדול. אם ניקח את ערך הטי לפיצ'ר ספציפי, נראה שאם האיבר על אלכסון הX'X נעשה גדול יותר ויותר, זה אומר שטעות התקן או המכנה של הסטטיסטי נעשה גדול יותר ויותר, ובהתאמה ערך הטי קטן יותר, והאיבר הזה לא ייראה כמובהק סטטיסטי. הוא עבר מיסוך בהסקה סטטיסטית.

אז לא ניכנס לכל פתרון אפשרי לטיפול בקוליניאריות, אבל זו בעיה שצריכים להיות מודעים אליה. לכל הפחות הביטו במטריצת הקורלציה בין המשתנים שלכם, וקראו עוד על מטריקות שאמורות להציף את הבעיה.
:::
:::

---

### High-leverage and Outlier observations

::: {.incremental}
- An observation with high [leverage]{style="color:red;"} has an extreme $x$ value

- For simple linear regression: $h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum (x_{i'} - \bar{x})^2}$

- Observations with large $h_i$ $\to$ large effect on $SE(\hat{\beta})$

- If such an observation is also an [outlier]{style="color:red;"} $\to$ the model changes a lot if removed
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הנושא השני שנצביע עליו הוא תצפיות חריגות.

באופו ספציפי מעניינות אותנו תצפיות עם מנוף או לברג' גבוה, והכוונה באופן כללי לתצפיות עם ערך X חריג.

ברגרסיה עם משתנה אחד יש אינדקס H שנהוג לחשב לכל תצפית כדי להגדיר את הלברג' שלה. אפשר לראות כאן בעצם יחס בין המרחק הריבועי של התצפית מממוצע הX לעומת סך המרחקים הריבועיים של התצפיות X מהממוצע שלהן. ברגרסיה מרובה אגב יש לנו הכללה לזה אבל לא נרשום אותה כאן.

חשבו מה יכולה להיות הבעיה עם תצפיות עם לברג' גבוה. קודם כל מתמטית כפי שראינו ברגרסיה ליניארית פשוטה, יש להן אפקט גדול נורא על טעות התקן של האומד. אם המכנה הזה של הסטטיסטי טי מתנפח בגלל תצפית אחת, האומד לא יכול להימצא מובהק סטטיסטית כי השונות שלו נורא גבוהה. נקודה שכזאת יכולה להשפיע כל כך על קו הרגרסיה, לכן היא נקראת מנוף, לברג'.

אם ערך הY של נקודה כזאת גם כן חריג, זה אומר שיש לנו תצפית חריגה עם מנוף גבוה - היא יכולה להטות את המודל לגמרי. בואו נראה דוגמא.
:::
:::

---

### High-leverage and Outlier observations

```{python}
#| echo: false

model = LinearRegression().fit(df[['hp']], df['mpg'])
y_pred = model.predict(df[['hp']])

SSx = np.sum((x_hp - np.mean(x_hp))**2)
h = 1 / x_hp.shape[0] + (x_hp - np.mean(x_hp))**2 / SSx

highlight_point1 = {'X1': df['hp'].values[-2], 'Y': df['mpg'].values[-2]}
highlight_point2 = {'X1': h[-2], 'Y': (y_pred - df['mpg'].values)[-2]}

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.scatterplot(data=df, x='hp', y='mpg', ax=axes[0])
axes[0].scatter(highlight_point1['X1'], highlight_point1['Y'], color='red', s=100)
axes[0].plot(df['hp'], y_pred, color='red')
axes[0].set_xlabel('Horsepower')
axes[0].set_ylabel('Miles per Gallon')

sns.scatterplot(x=h.flatten(), y=y_pred - df['mpg'], ax=axes[1])
axes[1].scatter(highlight_point2['X1'], highlight_point2['Y'], color='red', s=100)
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('Leverage')
axes[1].set_ylabel('Residuals')

plt.show()
```

::: {.fragment}
Consider removing (careful!).
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
האמת היא שכבר יש לנו דוגמא מוכנה, ברגרסיה של צריכת דלק של מכוניות כפונקציה של כוח סוס. אם תחשבו את אינדקס הH, תראו שהלברג' של התצפית האחרונה מצד ימין עם הכי הרבה כוח סוס, הוא ממש חריג. רואים את זה בדרך כלל בגרף של שאריות מול לברג'. אנחנו רואים תצפית עם לברג' גבוה שיש לה גם ערך Y קטן מאוד, כדאי לחשוד בה, ומעניין לראות איך היה נראה המודל בלעדיה.
:::
:::

---

#### Excluding high-leverage + outlier

```{python}
#| echo: false

df2 = df.drop(x_hp.shape[0] - 2, axis=0)
model = LinearRegression().fit(df2[['hp']], df2['mpg'])
y_pred2 = model.predict(df[['hp']])

x_hp2 = np.delete(x_hp, -2)
SSx = np.sum((x_hp2 - np.mean(x_hp2))**2)
h = 1 / x_hp2.shape[0] + (x_hp2 - np.mean(x_hp2))**2 / SSx

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.scatterplot(data=df2, x='hp', y='mpg', ax=axes[0])
axes[0].plot(df['hp'], y_pred, color='red', label = 'All obs')
axes[0].plot(df['hp'], y_pred2, color='green', label = 'Excluding outlier')
axes[0].set_xlabel('Horsepower')
axes[0].set_ylabel('Miles per Gallon')
axes[0].legend()

sns.scatterplot(x=h.flatten(), y=y_pred2[:-1] - df2['mpg'], ax=axes[1])
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('Leverage')
axes[1].set_ylabel('Residuals')

plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אם אנחנו מסירים את התצפית המודל אכן משתנה בצורה ניכרת לעין למרות שמדובר בתצפית אחת מתוך יותר מ30, וגרף השאריות מול לברג' נראה הרבה יותר טוב. מטריקה חשובה לקרוא בהקשר זה היא הקוקס דיסטנס.

נסיים כאן את הדיון ברגרסיה ליניארית. נזכיר שאפשר לפתח כל נושא כאן לשיעור שלם, ומי שרוצים להעמיק במודל הכל כך מפורסם הזה, יש להם אינספור ספרים בכל מיני רמות לקרוא עליו.
:::
:::
