<!DOCTYPE html>
<html lang="en"><head>
<script src="../libs/clipboard/clipboard.min.js"></script>
<script src="../libs/quarto-html/tabby.min.js"></script>
<script src="../libs/quarto-html/popper.min.js"></script>
<script src="../libs/quarto-html/tippy.umd.min.js"></script>
<link href="../libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.554">

  <title>Feature Selection and Regularization</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="../slides_quarto.css">
  <link href="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  

    <link rel="icon" href="../Intro2SL_logo.jpg" type="image/jpg"> 

    <link rel="shortcut icon" href="../Intro2SL_logo.jpg" type="image/jpg">

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css">

  </head>

<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section id="section" class="slide level2 logo-slide">
<h2></h2>
</section>
<section id="introduction-to-statistical-learning" class="slide level2 title-slide center">
<h2>Introduction to Statistical Learning</h2>
<h3 id="feat.-selection-and-regularization---class-8">Feat. Selection and Regularization - Class 8</h3>
<h3 id="giora-simchoni">Giora Simchoni</h3>
<h4 id="gsimchonigmail.com-and-add-intro2sl-in-subject"><code>gsimchoni@gmail.com</code> and add <code>#intro2sl</code> in subject</h4>
<h3 id="stat.-and-or-department-tau">Stat. and OR Department, TAU</h3>
</section>
<section id="goals-of-selection-and-regularization" class="slide level2 title-slide center">
<h2>Goals of Selection and Regularization</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>השיעור שלפנינו עוסק בבחירת משתנים וברגולריזציה שלהם. אמנם אנחנו מתמקדים עדיין במודל הליניארי וספציפית ברגרסיה ליניארית, אבל העקרונות שנלמד נכונים גם לקלסיפיקציה ולמודלים מורכבים הרבה יותר, והם חיוניים להבנה שלנו בעתיד איך אפשר לשפר כל מודל באמצעות שליטה עליו, ולהימנע מאוברפיטינג.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="why-select-features-why-regularize">Why select features? Why regularize?</h3>
<p>E.g., did we not see the Gauss-Markov Theorem?</p>
<div>
<ul>
<li class="fragment">Improve prediction accuracy
<ul>
<li class="fragment">Recall: the Bias-Variance Tradeoff <span class="math inline">\(\Rightarrow\)</span> allowing some bias might decrease variance!</li>
<li class="fragment">Recall: <span class="math inline">\(op \approx \mathcal{O}\left(\frac{p\sigma^2}{n}\right)\)</span></li>
<li class="fragment">If <span class="math inline">\(p &gt; n\)</span>: <span class="math inline">\(X^TX\)</span> has no inverse, infinite solutions</li>
</ul></li>
<li class="fragment">Improve interpretability
<ul>
<li class="fragment"><p>Discarding features with small “unlikely” coefficients</p>
<p><span class="math inline">\(\Rightarrow\)</span> lowering model complexity</p>
<p><span class="math inline">\(\Rightarrow\)</span> parsimony!</p></li>
<li class="fragment"><p>Often coincides with improving prediction accuracy</p></li>
</ul></li>
<li class="fragment">In general: “don’t believe <em>everything</em> the data says”</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>חלק מהשיטות שנלמד לרגולריזציה משנות את המודל הליניארי, ונשאלת השאלה: למה לגעת בו? למה להחליט שאנחנו שמים משתנים מסוימים במודל ואחרים לא, או שתיכף נראה אנחנו מוסיפים אילוצים על המקדמים – למשל, לא אמרנו שהאומד הליניארי שמצאנו בטא-האט לפי משפט גאוס-מרקוב הוא האומד הליניארי חסר ההטיה בעל השונות הקטנה ביותר?</p>
<p>אז מסתבר שאנחנו מרוויחים הרבה באמצעות רגולריזציה. בראש ובראשונה אנחנו משפרים את שגיאת החיזוי. אם ניזכר ששגיאת החיזוי למשל הריבועית מתפרקת לסכום של טעות אירדוסיבל ועוד ביאס בריבוע ועוד שונות המודל – נראה היום שאפשר לשלם מעט בהעלאת הביאס, ולהרוויח הרבה בהורדת השונות, כך שבשורה התחתונה הטעות הריבועית תקטן.</p>
<p>ניזכר גם שראינו שאופטימיזם כפי שהגדרנו אותו במודל הליניארי, הוא סדר גודל של p מספר המשתנים חלקי n.&nbsp;כלומר באופן כללי זה עושה שכל להשתמש בכמה שפחות מהם, אם רוצים להקטין את האופטימיזם, את הפער בין הטעות שאנחנו רואים על מדגם הלמידה הספציפי שלנו, וטעות החיזוי הכללית על נתונים שלא ראינו.</p>
<p>אבל לא רק שאנחנו עשויים לשפר את שגיאת החיזוי, במצבים מסוימים המודל הליניארי פשוט לא פיזיבילי ואנחנו נראה שבאמצעות שיטות לבחירת משתנים או רגולריזציה פתאום כן אפשר למדל את הנתונים. זה קורה למשל כאשר יש יותר משתנים מתצפיות, למטריצה X’X אין הופכי, יש אינסוף פתרונות ואנחנו באים לתקן את זה.</p>
<p>היבט אחר שבו רגולריזציה מסייעת לנו הוא האינטרפרטביליות של המודל. בזה שאנחנו נפטרים ממקדמים שהם קטנים מדי לעומת הרעש, שלא סביר שהם שונים מאפס או שאין מספיק דאטא לאמוד אותם בצורה מדויקת – המודל שלנו הופך לפחות מסובך ואנחנו משיגים פרסימוניה, המודל קומפקטי וברור יותר למי שרוצה ללמוד ממנו משהו.</p>
<p>נזכיר גם שהרבה פעמים שתי המטרות האלה הולכות יחד. נראה שלהיפטר ממשתנים בעלי השפעה קטנה שקשה לאמוד, הרבה פעמים מקטין גם את שגיאת החיזוי.</p>
<p>באופן כללי אפשר לסכם את השיעור שלנו באמירה השנויה במחלוקת: אל תאמינו לכל מה שהנתונים אומרים. מתוך הכרה שהנתונים שלנו הם מדגם מקרי בגודל סופי, אנחנו שמים על המודל מגבלות או מוסיפים קצת הטיה, ומרוויחים מזה בכל החזיתות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="feature-selection-and-regularization">Feature Selection and Regularization</h3>
<p>We will focus on:</p>
<ul>
<li>Subset selection (best, stepwise, stagewise)</li>
<li>Regularized regression (Ridge, Lasso)</li>
<li>Dimensionality reduction (PCR)</li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אנחנו נתמקד בשלוש משפחות של שיטות לבחירת משתנים ורגולריזציה: בחירת תת-קבוצה של משתנים ברגרסיה, בכל מיני דרכים. רגולריזציה של מקדמי הרגרסיה באמצעות שיטות מפורסמות כמו רידג’ ולאסו. ולבסוף, הורדת מימד לנתונים ממימד גבוה באמצעות PCA, ורק אז ביצוע רגרסיה על הדאטא ממימד נמוך.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="subset-selection" class="slide level2 title-slide center">
<h2>Subset Selection</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נתחיל בבחירה של תת-קבוצה של משתנים. בשיטות אלה סך המשתנים שעומדים לפני הם רק מועמדים להיכנס לרגרסיה, יכול להיות שאבחר בהם ויכול להיות שלא. ראינו בשיעורים על מודל סלקשן מדדים טובים לאמוד את השגיאה ובהם נשתמש. בסופו של דבר יהיה לי סט של משתנים בהם בחרתי, ואני אריץ רגרסיה ליניארית רגילה על כל הדאטה שלי, רק על סט המשתנים הנבחר.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="best-subset-selection">Best subset selection</h3>
<ol type="1">
<li><span class="math inline">\(M_0\)</span> model: predict <span class="math inline">\(\hat{y} = \hat{\beta}_0 = \bar{y}\)</span></li>
<li>For <span class="math inline">\(k = 1, \dots, p\)</span>:
<ol type="i">
<li>Fit all <span class="math inline">\({p \choose k}\)</span> models containing <span class="math inline">\(k\)</span> features</li>
<li>Pick the best <span class="math inline">\(M_k\)</span> with <span class="math inline">\(\min RSS\)</span></li>
</ol></li>
<li>Select the best model from <span class="math inline">\(M_0, \dots, M_p\)</span> with the <span class="math inline">\(C_p\)</span>/<span class="math inline">\(AIC\)</span> criterion or CV</li>
</ol>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נתחיל בגישה התמימה ביותר, שעוברת על כל האפשרויות – בסט סאבסט סלקשן.</p>
<p>מתחילים עם מודל M0 שבו אין משתנים בכלל והחיזוי הוא ממוצע Y.</p>
<p>לאחר מכן בשלב k = 1 אני רוצה לבחון את כל המודלים עם משתנה אחד בלבד, ואני עובר על כל p המודלים האפשריים, כל פעם מכניס משתנה אחר, ובודק איזה משתנה נותן לי את מינימום השגיאה, במקרה שלנו של רגרסיה ליניארית מינימום RSS. למודל הזה אני קורא M1, ואז אני עובר לk = 2, עובר על כל האפשרויות של מודל עם 2 משתנים, בוחר את המודל M2 וכך הלאה עד המודל שמכניס את כל p המשתנים.</p>
<p>כעת אני צריך לבחור בין p + 1 מודלים, ואת זה ראינו בשיעור על מודל סלקשן איך לעשות, אני אבחר במדד שמתחשב במספר המשתנים כי אני יודע שזה מנפח את השגיאה האמיתית על תצפיות שלא ראיתי, מדד כמו הCp של מאלו או AIC, או ביצוע קרוס ולידיישן על המודלים. המודל הסופי יהיה המודל שמביא למינימום את הקריטריון שבחרתי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="best-subset-selection-1">Best subset selection</h3>
<div id="a221fa4f" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-2-output-1.png" width="821" height="449"></p>
</figure>
</div>
</div>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>How many models are run?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בואו נראה דוגמא של הרצת רגרסיה בסט סאבסט על נתוני אשראי. בדוגמה שלנו יש p = 11 משתנים כמו הכנסה, מגבלה על החשבון, האם בעל החשבון הוא סטודנט וכולי. ואנחנו מנסים לחזות את היתרה של בעל החשבון.</p>
<p>על ציר האיקס אנחנו רואים את k בכל שלב של האלגוריתם. בהתחלה k = 0 כלומר אנחנו עובדים רק עם חותך. ואז k = 1 ואנחנו מתאימים 11 מודלים עם משתנה אחד, כל נקודה כאן היא הלוג של הRSS של אחד המודלים האלה. וכך ממשיכים עד המודל האחרון בו מכניסים את כל המשתנים.</p>
<p>עבור כל k אפשר לראות את המודל הנבחר שמשיג את הRSS הנמוך ביותר, הקו האדום מחבר ביניהם. באופן לא מפתיע הקו האדום יכול רק לרדת, את זה ראינו בשיעורים קודמים, הוספת משתנים לרגרסיה יכולה רק לשפר את הRSS.</p>
<p>לכן כשאנחנו באים לבחור באיזה מודל סופי להשתמש אנחנו יכולים לעשות קרוס ולידציה עם המודלים הנבחרים או להשתמש למשל בקריטריון הCp שמוסיף עונש לRSS שגדל ככל שמספר המשתנים עולה. כאן אפשר לראות שהמודל שהביא למינימום את הCp הוא המודל עם 6 משתנים, גם אם קצת קשה לראות את זה מהגרף. נזכיר שזה המודל עם 6 משתנים הטוב ביותר מכל האפשרויות למודלים עם 6 משתנים.</p>
<p>אז כמה מודלים הרצנו כאן? כל המודלים האפשריים, 2 בחזקת 11, שזה קצת יותר מאלפיים מודלים! ברור מייד שהגישה הזאת לא סבירה עבור p גדול הרבה יותר מ10, וגם אז עבור גדלי מדגם לא גדולים מדי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="forward-stepwise-selection">Forward stepwise selection</h3>
<ol type="1">
<li><span class="math inline">\(M_0\)</span> model: predict <span class="math inline">\(\hat{y} = \hat{\beta}_0 = \bar{y}\)</span></li>
<li>For <span class="math inline">\(k = 0, \dots, p - 1\)</span>:
<ol type="i">
<li>Fit all <span class="math inline">\(p - k\)</span> models adding 1 additional feature</li>
<li>Pick the best <span class="math inline">\(M_{k + 1}\)</span> with <span class="math inline">\(\min RSS\)</span></li>
</ol></li>
<li>Select the best model from <span class="math inline">\(M_0, \dots, M_p\)</span> with the <span class="math inline">\(C_p\)</span>/<span class="math inline">\(AIC\)</span> criterion or CV</li>
</ol>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ולכן הומצאו גישות אחרות של פורוורד או בקוורד סלקשן. הגישות האלה הרבה יותר קלות חישובית, אנחנו בשום פנים ואופן לא נריץ את כל האפשרויות למודל, אבל המחיר שמשלמים עבורן הוא לא קטן, כי הן מאוד גרידיות.</p>
<p>באלגוריתם הפורוורד סטפווייז, מתחילים כמו עם בסט סאבסט במודל שבו יש רק חותך והוא ממוצע Y. כעת k = 0 ואנחנו רוצים להוסיף רק משתנה אחד מתוך p המשתנים, מאוד מזכיר מה שעשינו קודם. נבחר את המשתנה שיביא למינימום RSS ונקרא למודל M1. כשk = 1 נכנס ההבדל, אנחנו לא לוקחים בחשבון את כל האפשרויות למודל עם 2 משתנים, אלא רוצים רק להוסיף משתנה אחד למודל שכבר קיים לנו. אז יש לנו p - 1 אפשרויות. ברגע שבחרנו את המשתנה השני אנחנו בוחרים את המשתנה השלישי וכך הלאה, בלי לגעת בסט המשתנים שכבר נבחרו.</p>
<p>בסוף בשלב השלישי כמו בבסט סאבסט אנחנו בוחרים את המודל הטוב ביותר מבין הקבוצה של מודלים שמצאנו בשלב 2, באמצעות קרוס ולידיצה או קריטריון ראוי אחר של מודל סלקשן.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="forward-stepwise-selection-1">Forward stepwise selection</h3>
<div id="3c823ebd" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-3-output-1.png" width="821" height="449"></p>
</figure>
</div>
</div>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>How many models are run?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בדוגמא שלנו זה ייראה כך. מתחילים כמו קודם במודל עם חותך בלבד, ממשיכים אותו דבר עם p אפשרויות להוספת משתנה אחד, ואז יש רק p - 1 אפשרויות להוספת משתנה שני, וכך הלאה עד אפשרות אחרת בלבד להוספת המשתנה האחרון.</p>
<p>כמו קודם הקו האדום הוא שמחבר בין המודלים הטובים ביותר והקו הירוק המקווקו הוא קריטריון הCp למודלים הטובים ביותר, שבוחר גם הפעם, במודל עם 6 משתנים.</p>
<p>אז לפני שנשווה בין הבחירה עצמה של משתנים – נשאל כמה מודלים הרצנו כאן? p ועוד p - 1 ועוד p - 2… מי שיחשב את סכום הסדרה הזאת יגיע ל-1 ועוד p(p + 1)/2, כלומר סדר גודל של p בריבוע.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="stepwise-regression-main-disadvantage">Stepwise regression main disadvantage</h3>
<table>
<colgroup>
<col style="width: 3%">
<col style="width: 49%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>k</th>
<th>Best subset</th>
<th>Forward stepwise</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>{Rating}</td>
<td>{Rating}</td>
</tr>
<tr class="even">
<td>2</td>
<td>{Rating, Income}</td>
<td>{Rating, Income}</td>
</tr>
<tr class="odd">
<td>3</td>
<td>{Rating, Income, Student}</td>
<td>{Rating, Income, Student}</td>
</tr>
<tr class="even">
<td>4</td>
<td>{Income, Student, Limit, Cards}</td>
<td>{Rating, Income, Student, Limit}</td>
</tr>
</tbody>
</table>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>What happens if <span class="math inline">\(p &gt; n\)</span>?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אבל ההקלה בסיבוכיות לא באה בלי מחיר, על הגרידיות של האלגוריתם הסטפווייז. כאן אנחנו ממש משווים בין המשתנים עצמם שהבסט סאבסט חוזה מול הפורוורד סטפווייז. עד מודל עם 3 משתנים אין הבדל ביניהם. במודל עם 4 משתנים הבסט סאבסט לוקח בחשבון את כל האפשרויות ומגיע למסקנה שהמודל הכי טוב צריך לזנוח את משתנה הרייטינג, ולהוסיף שני משתנים אחרים. ואילו עבור מודל הפורוורד סטפוויז זאת לא אופציה בכלל! הוא לעולם לא יגיע לאיזור הזה במרחב החיפוש! וזאת יכולה להיות בעיה רצינית של האלגוריתם הזה.</p>
<p>לפני שנעבור לאלגוריתם האחרון בקבוצה, האם אפשר לטפל באמצעות הפורוורד סטפוויז בבעיות ממימד גבוה בהן p גדול מn, כלומר יש יותר משתנים מתצפיות? בודאי, מתחילים עם חותך ועוצרים עבור n - 1 מודלים, כלומר עד שהרגרסיה הליניארית כבר לא פיזיבילית.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="forward-stagewise-selection">Forward <span style="color:red;">stagewise</span> selection</h3>
<ol start="0" type="1">
<li>Standardize all features, input some <span class="math inline">\(\tau_{thresh} \in (0, 1)\)</span> and <span class="math inline">\(\varepsilon &gt; 0\)</span> step size</li>
<li>Residual <span class="math inline">\(\mathbf{r} = \mathbf{y} - \bar{y}\)</span>, <span class="math inline">\(\beta_1, \dots, \beta_p = 0\)</span></li>
<li>Find the predictor <span class="math inline">\(\mathbf{x}_j\)</span> most correlated with <span class="math inline">\(\mathbf{r}\)</span>, and let <span class="math inline">\(\tau = Corr(\mathbf{r}, \mathbf{x}_j)\)</span></li>
<li>While <span class="math inline">\(|\tau| &gt; \tau_{thresh}\)</span>:
<ol type="i">
<li>Update <span class="math inline">\(\beta_j \leftarrow \beta_j + \delta_j\)</span>, where <span class="math inline">\(\delta_j = \varepsilon \cdot \text{sign}(\tau)\)</span></li>
<li>Update <span class="math inline">\(\mathbf{r} \leftarrow \mathbf{r} - \delta_j\mathbf{x}_j\)</span></li>
<li>Find the predictor <span class="math inline">\(\mathbf{x}_j\)</span> most correlated with <span class="math inline">\(\mathbf{r}\)</span>, and let <span class="math inline">\(\tau = Corr(\mathbf{r}, \mathbf{x}_j)\)</span></li>
</ol></li>
</ol>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Why would we want to “slow-learn”?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>האלגוריתם בסט סאבסט האחרון שנלמד נראה בהתחלה יצור מוזר, ולא רק בגלל השם שלו שמזכיר את השם של האלגוריתם הסטפוויז. קוראים לו פורווד סטייג’וויז, והוא מציע לעשות דבר כזה:</p>
<p>נעשה סטנדרטיזציה למשתנים, ניקח איזשהו ערך סף לקורלציה שתיכף נשתמש בו, והכי חשוב - פרמטר אפסילון שייקרא סטפ-סייז, או קצב למידה.</p>
<p>נגדיר את השארית שבחיסור הממוצע של Y מכל התצפיות כR, ונציב אפס בכל מקדמי הרגרסיה.</p>
<p>כעת נמצא את המקדם הJ שיש לו את המתאם הגבוה ביותר עם השארית, כלומר זה המקדם שהכי נכון היה להכניס לרגרסיה, מעבר למודל הקיים.</p>
<p>כל עוד המתאם הזה גדול מאיזשהו ערך סף שאומר מה זה מתאם גבוה, בערך מוחלט, נעדכן את מקדם הרגרסיה של המשתנה הזה. אבל שימו לב לזה, לא נוסיף פשוט את המתאם עצמו, אלא נלך צעד קטן בכיוון שלו דלתא J, שהיא אפסילון בכיוון המתאם. כלומר נוסיף אפסילון לבטא-ג’יי של המשתנה או נחסיר בהתאם לגודל המתאם.</p>
<p>כעת נעדכן את השארית אחרי שחיסרנו ממנה את מה שמידלנו, את המשתנה הJ כפול הדלתא J. ונחזור למצוא את המשתנה ש<em>עכשיו</em> הכי מתואם עם השארית.</p>
<p>נחזור לבדוק האם המתאם הזה גדול עדיין מספיק לעומת איזשהו ערך סף, וכך הלאה, עד שלא יישאר משתנה שמתואם מספיק עם השארית. המודל הסופי שלנו מגולם במקדמי הרגרסיה שמצאנו.</p>
<p>חשוב מאוד לשים לב! בכל שלב ניתן לבחור שוב משתנה שכבר נבחר! אפילו בשני שלבים רצופים.</p>
<p>אבל למה שנרצה לעשות את זה? יותר מהכל זה גם נראה למידה נורא איטית לא? אם גיליתם שלמשתנה יש קורלציה של 0.9 עם השארית, למה לא להוסיף אותו כמו שהוא לרגרסיה, למה רק אפסילון ממנו?</p>
<p>עוד נחזור לאלגוריתם המעניין הזה, אבל בינתיים נגיד, שמסתבר שלמידה איטית היא דבר טוב מאוד לשיפור שגיאת החיזוי, וזה עולה בקנה אחד עם העיקרון שאיתו פתחנו את השיעור – לפעמים שווה לא ללכת עיוורים אחרי הדאטא, ולקחת בערבון מוגבל את הכיוון שהוא מוביל אותנו אליו. זה בדיוק מה שאנחנו עושים עם קצב למידה אפסילון.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="regularized-regression-ridge" class="slide level2 title-slide center">
<h2>Regularized Regression: Ridge</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>את רוב השיעור נקדיש לשיטות של רגולריזציה, איך אנחנו ממתנים את המודל, מפקחים עליו ומגבילים אותו. השיטה הותיקה והמפורסמת ביותר, היא רגרסיית רידג’.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="ridge-regression">Ridge regression</h3>
<div>
<ul>
<li class="fragment"><p>Instead of reducing the number of parameters <span class="math inline">\(\Rightarrow\)</span> constrain them, penalize their norm</p></li>
<li class="fragment"><p>With <span class="math inline">\(\ell_2\)</span> norm we get the penalized RSS criterion for some regularization/penalty parameter <span class="math inline">\(\lambda &gt; 0\)</span>: <span class="math display">\[PRSS(\lambda) = \sum_{i = 1}^n \left(y_i - \beta_0 + \sum_{j = 1}^p \beta_j x_{ij}\right)^2 + \lambda\sum_{j = 1}^p \beta_j^2 = RSS + \lambda\|\beta^*\|^2_2\]</span></p></li>
<li class="fragment"><p>Standardize the features in <span class="math inline">\(X\)</span>, then:</p>
<ul>
<li class="fragment"><span class="math inline">\(\hat{\beta}_0 = \bar{y}\)</span></li>
<li class="fragment"><span class="math inline">\(\lambda\)</span> punishes features of different scale comparably</li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אנחנו לא עושים משהו שונה מאוד. במקום להפחית את מספר הפרמטרים, אנחנו מגבילים אותם, אנחנו מחפשים אותם במרחב מצומצם יותר. ואת זה אנחנו משיגים על-ידי הוספת עונש, פנאלטי, על הנורמה שלהם.</p>
<p>אם הנורמה היא L2 זה נראה ככה: הקריטריון שלנו הוא פנאלייזד RSS או PRSS, כלומר סכום הריבועים הפחותים הרגיל שאנחנו מכירים, ועוד פרמטר למדא גדול מאפס כפול סכום על הריבוע של מקדמי הרגרסיה. העונש הזה שומר על האומדנים לפרמטרים שלא יהיו גדולים מדי, ותיכף נרחיב על איך בוחרים את למדא.</p>
<p>נשים לב שאת החותך אנחנו לא מענישים כי אין בזה היגיון, לכל Y יש פיזור טבעי ואין סיבה לא להתחיל מהממוצע שלו כמנבא בייסליין. אם נסמן את וקטור המקדמים בטא בלי בטא-אפס כבטא כוכב נקבל שאפשר לרשום את הקריטריון שלנו כRSS ועוד למדא כפול הנורמה בריבוע של וקטור בטא-האט.</p>
<p>שימו-לב שצריך לעשות סטנדרטיזציה של המשתנים, כלומר לחסר מכל עמודה של X את הממוצע ולחלק בסטיית התקן. אם מקפידים על זה אפשר לאמוד את בטא-אפס כממוצע של Y. ואז נקבל גם שלמדא משפיע בצורה דומה על כל המשתנים. אחרת שימוש בפרמטר למדא יהיה בעייתי, כי הוא יכול להיות מושפע מאוד ממשתנה ספציפי עם סקאלה מאוד רחבה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="ridge-solution">Ridge solution</h3>
<p><span class="math display">\[PRSS(\lambda) = \|y - X\beta\|^2_2 + \lambda\|\beta\|^2_2\]</span></p>
<div class="fragment">
<p><span class="math inline">\(\frac{\partial PRSS}{\partial \beta} = -2X^Ty + 2X^TX\beta +2\lambda\beta\)</span></p>
</div>
<div class="fragment">
<p><span class="math inline">\(2X^Ty - 2X^TX\beta - 2\lambda\beta = 0\)</span></p>
</div>
<div class="fragment">
<p><span class="math inline">\((X^TX + \lambda I_p)\beta = X^Ty\)</span></p>
</div>
<div class="fragment">
<p><span class="math inline">\(\hat{\beta}(\lambda) = (X^TX + \lambda I_p)^{-1}X^Ty\)</span></p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נניח שאנחנו יודעים את למדא. איך נראה הפתרון של רידג’?</p>
<p>בואו נכתוב את הPRSS בצורה וקטורית, כאן כבר ויתרנו על הסימון בטא-כוכב, בטא לא כולל את החותך בטא-אפס.</p>
<p>אנחנו עושים בדיוק מה שעשינו ברגרסיה ליניארית, כלומר גוזרים את הPRSS לפי וקטור בטא, משווים לאפס, ומחלצים את בטא.</p>
<p>הפתרון שקיבלנו מזכיר מאוד את הפתרון של רגרסיה ליניארית, ואפשר כמובן לוודא שהוא נקודת מינימום. ההבדל הוא במטריצה שאנחנו הופכים, כאן נוסף לה הפרמטר למדא הקטן על האלכסון, ומכאן השם רידג’, רכס באנגלית.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="ridge-original-justification">Ridge (original) justification</h3>
<p><span class="math display">\[\hat{\beta}(\lambda) = (X^TX + \lambda I_p)^{-1}X^Ty\]</span></p>
<div>
<ul>
<li class="fragment">Numerical stability:
<ul>
<li class="fragment">If <span class="math inline">\(X\)</span>’s columns are highly correlated, <span class="math inline">\(X^TX\)</span> is ill-conditioned and its inverse is unstable, variance of <span class="math inline">\(\hat{\beta}\)</span> is high – but <span class="math inline">\(X^TX + \lambda I_p\)</span> improves on this</li>
</ul></li>
<li class="fragment">Feasibility:
<ul>
<li class="fragment">If <span class="math inline">\(X\)</span>’s columns are linearly dependent <span class="math inline">\(X^TX\)</span> is not invertible but <span class="math inline">\(X^TX + \lambda I_p\)</span> is!</li>
<li class="fragment">If <span class="math inline">\(p &gt; n\)</span>: same!</li>
</ul></li>
<li class="fragment">Guaranteed prediction error reduction for some <span class="math inline">\(\lambda\)</span>:
<ul>
<li class="fragment">Bias increases but variance decreases <em>more</em></li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מה זה נותן לנו, הרידג’ הזה על האלכסון? מסתבר שלא מעט, כאן אנחנו נוגעים בהצדקה המקורית לשיטה הזאת.</p>
<p>קודם כל, למי שיש ניסיון בהפיכה של מטריצות סימטריות כמו X’X, יודע שלהוסיף למדא קטן לאלכסון של המטריצה עוזר ליציבות נומרית.</p>
<p>ספציפית כאן, כפי שראינו הסכנה היא שיש קורלציה בין העמודות של X, אז אנחנו קוראים למטריצה X’X איל קונדישונד, ההופכי שלה לא יציב מה שעלול להגדיל מאוד את שונות המקדמים כמו שדיברנו בשיעור על רגרסיה ליניארית. וההוספה הזאת של קבוע חיובי קטן לאלכסון של המטריצה הזאת עוזרת מאוד להילחם בתופעה הזאת.</p>
<p>חוץ מזה כמו שאמרנו במצב שבו העמודות של X תלויות ליניארית, או במצב שיש יותר משתנים מתצפיות ההופכי של X’X בכלל לא קיים, ורגרסיית רידג’ בכלל מאפשרת לנו פתרון לבעיה.</p>
<p>אבל אולי ההצדקה הכי מעניינת לרידג’ זה גרנטי שיש לנו, בלי הוכחה כרגע, לשיפור טעות החיזוי הריבועית. האומד שלנו בטא-האט שתלוי בלמדא כבר לא חסר הטייה, אנחנו משלמים קצת בהטייה אבל מקבלים הפחתה ניכרת בשונות, ובסך הכל הפחתה בשגיאה הריבועית.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="choosing-the-lambda-hyperparameter">Choosing the <span class="math inline">\(\lambda\)</span> <span style="color:red;">hyperparameter</span></h3>
<div id="81772cda" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-4-output-1.png" width="949" height="468"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נראה את רגרסיית רידג’ בפעולה על הנתונים שלנו ועל הדרך נדגים גם איך בוחרים את הפרמטר למדא.</p>
<p>הפרמטר למדא הוא מה שקרוי היפרפרמטר. זה פרמטר שלא ניתן ללמוד ממדגם הלמידה, אלא צריך לבחור אותו באמצעות בדיקה של אפשרויות שונות, בדרך כלל עם קרוס ולידיישן, ולראות איזו מהן נותנת את שגיאת החיזוי הטובה ביותר על מדגמים שהמודל לא ראה. כאן אנחנו בודקים את זה עם קרוס ולידיישן עם חמישה פולדים ומדווחים את ממוצע הMSE על הטריין ועל הטסט.</p>
<p>אנחנו רואים כאן כמה דברים: שבאופן צפוי השגיאה על הטריין נמוכה יותר מאשר השגיאה על הטסט. שהפרמטר למדא הטוב ביותר שמביא למינימום את השגיאה על הטסט הוא קצת פחות מ10 בחזקת 0 כלומר קצת פחות מ1. והכי חשוב, אנחנו רואים שאם אנחנו משווים לשגיאה של רגרסיה ליניארית רגילה, OLS, שהיא לא תלויה בלמדא בכלל כמובן ומופיעה כאן במקווקו, עבור הלמדא הנכון רידג’ אכן מצליח להגיע לירידה ניכרת בMSE.</p>
<p>מצד ימין, אנחנו מראים איך נראים המקדמים הנאמדים של 11 המשתנים שלנו, בהתאם ללמדא. כשהלמדא הוא אפסי, אין ענישה בכלל וכל אומדני המקדמים מקבלים את הערכים שהם היו מקבלים ברגרסיה רגילה. באופן צפוי, ככל שלמדא גדול יותר, העונש על הנורמה של וקטור בטא גדול יותר והאילוץ חמור יותר ומאפשר פחות ופחות חופש למודל והמקדמים הנאמדים נעשים קטנים יותר, הם מתכווצים לכיוון אפס. לתופעה כזאת של כיווץ אנחנו קוראים שרינקג’, אפשר לראות שהיא עשויה מאוד לסייע להקטין את שגיאת החיזוי ונדבר עליה עוד בהמשך.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="regularized-regression-lasso" class="slide level2 title-slide center">
<h2>Regularized Regression: Lasso</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נדבר עכשיו על שיטה אחרת ותיקה לעשות רגולריזציה למקדמי הרגרסיה. גם השיטה הזאת, כמו רידג’ היא בעלת השלכות ושימושים נרחבים הרבה יותר מרגרסיה ליניארית בלבד, והיא נקראת לאסו.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="lasso-regression">Lasso regression</h3>
<p><span class="math display">\[PRSS(\lambda) = \sum_{i = 1}^n \left(y_i - \beta_0 + \sum_{j = 1}^p \beta_j x_{ij}\right)^2 + \lambda\sum_{j = 1}^p |\beta_j| = RSS + \lambda\|\beta^*\|_1\]</span></p>
<ul>
<li>As with Ridge, standardize the features in <span class="math inline">\(X\)</span>, then:
<ul>
<li><span class="math inline">\(\hat{\beta}_0 = \bar{y}\)</span></li>
<li><span class="math inline">\(\lambda\)</span> punishes features of different scale comparably</li>
</ul></li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ברגרסיית לאסו, אנחנו גם מענישים את הנורמה של מקדמי הרגרסיה כמו ברידג’, אלא שכאן אנחנו מענישים את הנורמה L1, כלומר סכום המקדמים בערך מוחלט.</p>
<p>גם כאן בטא-אפס לא תחת אילוץ ואם אנחנו עושים סטנדרטיזציה של המשתנים אפשר לאמוד אותו כממוצע Y, והעונש למדא ישפיע במידה דומה על כל הפיצ’רים.</p>
<p>לקריטריון שלפנינו אין פתרון סגור. תיכף נגיד כמה מילים על איך פותרים את בעיית הלאסו. אבל קודם כל, למה שנרצה להעניש דווקא את נורמה L1 ובמה זה שונה מרידג’.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-lasso-path">The Lasso Path</h3>
<div id="66756bac" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-5-output-1.png" width="949" height="468"></p>
</figure>
</div>
</div>
</div>
<div>
<ul>
<li class="fragment">Lasso’s selling point: <em>sparsity</em>!</li>
<li class="fragment">Highly useful, especially for <span class="math inline">\(p &gt; n\)</span> datasets</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נעיף מבט על שגיאת החיזוי. אפשר לראות שעבור הנתונים שלנו לאסו לא מאוד הועיל. הלמדא שהביא למינימום את שגיאת החיזוי על מדגמי הטסט בקרוס ולידיישן הוא למדא אפסי, ולכן האומדנים למקדמי הרגרסיה שהתקבלו עבור הלמדא האופטימלי הם כמעט ללא שינוי מרגרסיה ליניארית רגילה.</p>
<p>נראה שגם המקדמים של לאסו עוברים שרינקג’ אבל הדפוס שבו הם מתכווצים הוא מאוד ייחודי. בעוד שברידג’ המקדמים קטנים וקטנים לכיוון אפס, בלאסו בנקודה מסוימת מובטח לנו שהמקדמים מתאפסים, ממש. אפשר לראות כאן שזה קורה אחד אחרי השני.</p>
<p>התופעה הזאת היא ההצדקה המרכזית של שימוש דווקא ברגרסיית לאסו, באנגלית לאסו מביא למודלים ספרסיים עם מעט משתנים, בעברית אנחנו קוראים לזה מודלים דלילים.</p>
<p>כאן אנחנו לא רואים את היתרון של זה, מדובר בסך הכל ב11 משתנים, אבל הדבר יעיל מאוד ברגרסיה לנתונים ממימד גבוה כשP מספר המשתנים גדול מאוד ואולי אפילו גדול ממספר התצפיות. במקרה כזה לא סביר שכל המשתנים שונים באופן מהותי מאפס, ושימוש בלאסו משיג לנו מודל דליל, שהוא לא רק יותר אינטרפרטבילי אלא פעמים רבות מביא לשגיאת חיזוי נמוכה יותר כי הוא מקטין אוברפיטינג.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="lasso-solution">Lasso solution</h3>
<ul>
<li>Quadratic Programming, LARS</li>
<li>But also, surprisingly:</li>
</ul>
<div class="fragment">
<div id="9c71f862" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-6-output-1.png" width="949" height="468"></p>
</figure>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בחלק הבא ננסה לתת תחושה מה עומד מאחורי המודלים הספרסיים שלאסו נותן. לפני זה נגיד כמה מילים על איך פותרים את בעיית לאסו, כי כפי שאמרנו פתרון סגור אין.</p>
<p>כשרק נוסחה הבעיה של לאסו, המחברים הצליחו להציג אותה כבעית אופטימיזציה די סטנדרטית, שמצריכה תכנות ריבועי או קוואדרטיק פרוגרמינג. מאוחר יותר נמצא פתרון יעיל יותר שנקרא least angle regression, או לארס, שלקח השראה מהתופעה הזאת שאפשר להוכיח, שכשלמדא הולך וקטן כל פעם נכנס משתנה אחר לרגרסיה, והמקדם שלו גדל בצורה ליניארית בקירוב, עד שנכנס המשתנה הבא.</p>
<p>אבל מה בעצם אמרנו כאן? איזה עוד אלגוריתם מכניס בזהירות כל פעם משתנה בצעד קטן עד שיימצא משתנה אחר שהכי מתואם עם Y שצריך להכניס לרגרסיה?</p>
<p>אני מדבר כמובן על אלגוריתם הפורוורד סטייג’וויז. אם ניקח אפסילון מספיק קטן, אפשר להראות שאלגוריתם הסטייג’וויז פותר בקירוב את בעיית לאסו. כאן אנחנו מראים את הדמיון המפתיע בין לאסו לפורוורד סטייג’וויז על הנתונים שלנו, באמצעות השוואה של הפרופילים של המקדמים.</p>
<p>נדגיש לסיום שפורוורד סטייג’וויז נחשבת לשיטה עם שגיאת חיזוי איכותית אבל היא מאוד איטית, ככה המחשב לא פותר את בעיית הלאסו אלא באמצעות שיטות אופטימיזציה. זה עדיין מעניין לראות את הקשר בין השיטות השונות ולנסות להבין מה משותף ומה שונה בהן, ועם הקו הזה נמשיך בחלקים הבאים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="on-shrinkage-and-sparsity" class="slide level2 title-slide center">
<h2>On Shrinkage and Sparsity</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>האם אנחנו יכולים להגיד עוד על האופן שבו המקדמים מתכווצים ברידג’ ומתדללים בלאסו? האמת היא שיש הרבה עבודות בתחום, ומעניין לעבור על כמה תובנות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="dual-criteria">Dual criteria</h3>
<p>There is 1-to-1 correspondence between Ridge and Lasso’s previous criteria and:</p>
<div class="fragment">
<ul>
<li>Ridge: <span class="math inline">\(\min_{\beta} RSS \text{  s.t.  } \sum_{j = 1}^p\beta_j^2 \le s\)</span></li>
<li>Lasso: <span class="math inline">\(\min_{\beta} RSS \text{  s.t.  } \sum_{j = 1}^p|\beta_j| \le s\)</span></li>
</ul>
</div>
<div class="fragment">
<p>In this context we can also write for Best Subset regression:</p>
<ul>
<li><span class="math inline">\(\min_{\beta} RSS \text{  s.t.  } \sum_{j = 1}^p\mathbb{I}\left[\beta_j \neq 0\right] \le s\)</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ראשית מעניין לציין שגם את רידג’ וגם את לאסו ניתן לכתוב בצורה דואלית מעט שונה: ברידג’ אנחנו רוצים להביא את הRSS למינימום כפוף לזה שסכום הריבועים של המקדמים או הנורמה L2 בריבוע, קטן או שווה לאיזשהו S.</p>
<p>בלאסו אנחנו רוצים להביא למינימום את הRSS באילוץ על סכום המקדמים בערך מוחלט שיהיה לכל היותר S.</p>
<p>אלה לא ניסוחים שונים כאמור, אפשר להראות שיש מיפוי 1 ל1 בין פרמטר למדא בבעיה המקורית לפרמטר S מתאים בבעיה הדואלית.</p>
<p>אבל הניסוח הזה מראה בצורה ברורה יותר למה יש שרינקג’ ולכן השיטות האלה נקראות גם שיטות שרינקג’. הפרמטר S הוא כמו תקציב שהמקדמים צריכים לעמוד בו. גם מאוד ברור ככה למה כדאי לעשות סטנדרטיזציה על המשתנים, אם לא נעשה, כל ה”תקציב” יכול להתבזבז על משתנה אחד עם סקאלה רחבה.</p>
<p>יותר מזה, צורת הכתיבה הזאת מאפשרת לנו גם לרשום את הקריטריון שבסט סאבסט מנסה להציג בכל אחד מהשלבים שלו: מינימום RSS כפוף לכך שלא יותר מS מקדמים יהיו שונים מאפס. אבל זו בעיה קומבינטורית די קשה לפתרון כמו שראינו. אז דרך אחת זה לנסות להשיג את המינימום הזה עם שיטות פורוורד או בקוורד סטפווייז שראינו. דרך אחרת כמו שאנחנו רואים כאן היא לעשות רילקסציה על הקריטריון הקשה הזה. כלומר אנחנו יכולים לחשוב על רידג’ ולאסו, כאלטרנטיבות לבסט סאבסט, שמציבות קריטריון קל יותר להשגה, כמו שקורה אגב בהרבה בעיות אחרות באופטימיזציה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="seeing-shrinkage-and-sparsity-i">Seeing Shrinkage and Sparsity (I)</h3>
<div id="860c9ab3" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-7-output-1.png" width="948" height="468"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נמשיך לפתח את זה. נאמר ויש לנו רגרסיה עם שני מקדמים בטא אחת ובטא שתיים. מה אומר האילוץ של רידג’? בטא אחת בריבוע ועוד בטא שתיים בריבוע צריך להיות קטן מאיזשהו S. כלומר יש איפשהו בטא-האט שמתאים למינימום RSS, ואנחנו מחפשים נקודה קצת ליד שנמצאת בתוך המעגל הזה, שעומדת בתקציב שלנו. כאן אנחנו רואים את הנקודה הזאת בחיתוך בין הקונטורים של RSS סביב בטא-האט הכי טובה, ברגע שהם נוגעים בתקציב. עכשיו כאן, בשני משתנים התקציב יהיה עיגול, התקציב בתלת-מימד יהיה כדור, בp מימד זה יהיה הייפרספיר.</p>
<p>לעומת זאת איך נראה האילוץ בלאסו? בטא אחת בערך מוחלט ועוד בטא שתיים בערך מוחלט קטן או שווה לאיזשהו תקציב S, זה יוצר לנו צורה של מעוין, עם שפיצים. בתלת מימד זה יהיה מעין יהלום, וככל שנעלה במימד נראה שזאת צורה עם הרבה שפיצים. כלומר כאן יש נטייה לקונטורים של הRSS להיפגש עם שפיצים של מעוין או יהלום. ומה המשמעות של לקבל פתרון באחד השפיצים? בדוגמא הזאת זה אומר שהמקדם בטא אחת מתאפס! מה שלא סביר שיקרה עם האילוץ של רידג’.</p>
<p>כלומר אנחנו ממש יכולים לראות גיאומטרית איך קורה ששרינקג’ של לאסו מביא למודלים דלילים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="seeing-shrinkage-and-sparsity-ii">Seeing Shrinkage and Sparsity (II)</h3>
<p>If <span class="math inline">\(X\)</span> has orthonormal columns (<span class="math inline">\(X^TX = I_p\)</span>) and <span class="math inline">\(\hat{\beta}_j\)</span> is the OLS estimator:</p>
<table>
<colgroup>
<col style="width: 22%">
<col style="width: 77%">
</colgroup>
<thead>
<tr class="header">
<th>Estimator</th>
<th><span class="math inline">\(\tilde{\beta}_j\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Best <span class="math inline">\(M\)</span> subset</td>
<td><span class="math inline">\(\hat{\beta}_j \cdot \mathbb{I}\left[|\hat{\beta}_j| \ge  |\hat{\beta}_{(M)}|\right]\)</span></td>
</tr>
<tr class="even">
<td>Ridge</td>
<td><span class="math inline">\(\hat{\beta}_j / (1 + \lambda)\)</span></td>
</tr>
<tr class="odd">
<td>Lasso</td>
<td><span class="math inline">\(\text{sign}(\hat{\beta}_j)(|\hat{\beta}_j| - \lambda)_+\)</span></td>
</tr>
</tbody>
</table>
<div class="fragment">
<div id="aa6f765a" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-8-output-1.png" width="1428" height="325"></p>
</figure>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ואנחנו יכולים לשחק עם זה בצורה אפילו יותר מתוחכמת.</p>
<p>אם נניח שהעמודות של X הן אורתונורמליות, כלומר X’X שווה למטריצת היחידה, ונסמן את בטא-האט כאומד הOLS הקלאסי. אנחנו ממש יכולים לקבל נוסחה לכל מקדם בטא-J ולראות מתמטית גם את השרינקג’, גם את הדלילות.</p>
<p>נצייר את הנוסחאות שרשומות כאן כדי לראות את זה טוב יותר. בכל הגרפים הקו הכחול הרציף מייצג את האומד בטא של רגרסיה רגילה, בטא-האט, והקו האדום המקווקו מייצג את בטא-טילדא, גודל הבטא החדש אם מפעילים את אחת השיטות לרגולריזציה שדיברנו עליהן.</p>
<p>ברגרסיית בסט סאבסט שבה אנחנו מחפשים M משתנים הכי טובים, ונסמן את המקדם הכי קטן כבטא-האט M, המקדם בטא-J לא נבחר או מתאפס כל עוד הוא קטן מהמקדם הזה. ברגע שיעבור אותו, יופיע כפי שהוא.</p>
<p>ברגרסיית רידג’ הנה השרינקג’, ככל שלמדא יהיה גדול יותר כך נכווץ את בטא-J המקורי יותר, לכיוון קו האפס, אבל למדא צריך להיות ממש גדול כדי ממש לאפס את המקדם הזה. אפשר גם לראות שהשרינקג’ לא אחיד לכל מקדם. אם מקדם גדול אז כיווץ פי 1 ועוד למדא משמעותי יותר מאשר אם המקדם קטן.</p>
<p>לבסוף ברגרסיית לאסו הסימון הזה אומר המקסימום בין הביטוי בסוגריים לבין אפס. כלומר אם בטא J בערך מוחלט לא גדול מלמדא הוא יקבל אפס. ואם הוא גדול אז נפחית ממנו את למדא, בצורה קבועה לא משנה מה הגודל שלו. אנחנו רואים גם את הדלילות וגם את השרינקג’.</p>
<p>נגיד שבעזרת אלגברה קצת יותר מסובכת אפשר להכליל את ההמחשות האלה גם למצבים מורכבים יותר, אבל אנחנו נעצור כאן, ונעבור לאפילו הצדקה נוספת לשימוש ברידג’ ובלאסו.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="bayesian-viewpoint" class="slide level2 title-slide center">
<h2>Bayesian Viewpoint</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נקודת מבט נוספת על רידג’ ולאסו מגיעה מכיוון פחות צפוי, של סטטיסטיקה בייזיאנית. אנחנו נראה שהאומדים של רידג’ ולאסו הם בדיוק מה שמתקבל עבור המודל הליניארי, תחת ציפיות מסוימות עוד לפני שראינו את הנתונים. אבל קודם נעשה הקדמה קצרה לגבי סטטיסטיקה בייזיאנית, נושא שאפשר לבלות רק עליו סמסטר שלם.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="conditional-distribution">Conditional Distribution</h3>
<div>
<ul>
<li class="fragment"><p>Recall Bayes Rule: <span class="math display">\[P(B | A) = \frac{P(A | B) \cdot P(B)}{P(A)} \text{     or     } \text{posterior} = \frac{\text{likelihood}\cdot\text{prior}}{\text{evidence}}\]</span></p></li>
<li class="fragment"><p>For continuous distributions: <span class="math display">\[f_{Y|X}(y|x) = \frac{f_{X|Y}(x|y)f_Y(y)}{f_X(x)} \propto f_{X|Y}(x|y)f_Y(y)\]</span></p></li>
<li class="fragment"><p><span class="math inline">\(f_{Y|X}\)</span> may not always have closed form, but sometimes…</p></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בבסיס של סטטיסטיקה בייזיאנית עומד כלל בייז. ההסתברות למאורע B בהינתן שאני יודע שמאורע A קרה, היא הסתברות החיתוך חלקי הסתברות המאורע המתנה. והסתברות החיתוך ניתן לרשום כהסתברות המאורע B השולית, כפול ההסתברות המותנית ההפוכה של מאורע A בהינתן מאורע B. כלומר כלל בייז מקשר לנו בין ההסתברות האפריורית שהנחנו על מאורע B, לבין ההסתברות הפוסטריורית, בהינתן שאנחנו יודעים שמאורע A קרה. אנחנו נגיד שההסתברות האפוסטריורית שווה למכפלת ההסתברות האפריורית בסבירות או לייקליהוד לראות את מאורע A בהינתן מאורע B, חלקי ההסתברות השולית לראות את מאורע A או האבידנס.</p>
<p>עבור התפלגויות רציפות אין לנו פונקצית הסתברות יש לנו פונקצית צפיפות אבל העיקרון דומה. הצפיפות המותנית של משתנה Y בהינתן שראינו מהו משתנה X, היא הצפיפות השולית של Y כפול הצפיפות המותנית של X בהינתן Y, חלקי הצפיפות השולית של X. הרבה פעמים נרשום שהצפיפות המותנית היא פרופורציונלית למכפלת הצפיפות האפריורית והצפיפות המותנית ההפוכה.</p>
<p>לא תמיד אנחנו מצליחים לזהות את ההתפלגות המותנית כהתפלגות שמוכרת לנו אבל הרבה פעמים כן.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="conditional-gaussian">Conditional Gaussian</h3>
<p>For Gaussian distribution, if <span class="math inline">\(X \sim \mathcal{N}(\mu_X, \sigma^2_X), Y \sim \mathcal{N}(\mu_Y, \sigma^2_Y), \rho = Corr(X, Y)\)</span>: <span class="math display">\[Y | X = x \sim \mathcal{N}\left(\mu_Y + \frac{\sigma_Y}{\sigma_X}\rho(x - \mu_X), (1 - \rho^2)\sigma_Y^2\right)\]</span></p>
<div class="fragment">
<div id="c4bb034a" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-9-output-1.png" width="1430" height="326"></p>
</figure>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>למשל, אם X מתפלג בצורה שולית נורמלית עם תוחלת ושונות משלו, ו-Y מתפלג בצורה שולית נורמלית עם תוחלת ושונות משלו, הקורלציה ביניהם מסומנת ברו ואני דוגם את X, הוא “קרה”, הוא שווה לאיזשהו X קטן. אז כעת אם אני ארשום את הצפיפות המותנית של Y בהינתן X לפי ההגדרה שראינו ואעשה קצת אלגברה, אני אזהה בסופו של דבר שההתפלגות של Y בהינתן X היא התפלגות שמוכרת לי. היא עדיין התפלגות נורמלית, רק שהתוחלת והשונות האפוסטריורים התעדכנו. למשל התוחלת היא התוחלת המקורית של Y ועוד תוספת שתלויה ביחס בין סטיות התקן, בגודל וסימן של רו, ובמרחק של X שהתקבל מהתוחלת שלו. מעניין לציין שהשונות של Y בהינתן X, אלא אם כן רו שווה לאפס, בהכרח קטנה, מה שעושה שכל, כי צפיתי כבר במשתנה שמתואם עם Y, הציפיות שלי מתעדכנות ויש הרבה פחות שונות לערכים שY יכול לקבל.</p>
<p>ניתן לראות את זה בגרף שלפנינו. X וגם Y מתפלגים אפריורית נאמר נורמלית סטנדרטית, עם תוחלת אפס ושונות 1. אם אני יודע שX נדגם והיה שווה ל-1, ובין X לY יש קורלציה חיובית, נאמר חצי, זה אומר שאני מעדכן את הידע שלי על Y, התוחלת שלו זזה ימינה לעבר ערכים גבוהים יותר, במקרה הזה לחצי אם תציבו בנוסחה. והשונות שלו קטנה יותר. ואם הקורלציה היא גבוהה ממש, 0.9, אז בכלל אני מעדכן את ההתפלגות המותנית של Y להיות עם פיזור ממש קטן סביב התוחלת החדשה שמתקרבת ל-1.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="bayesian-statistics">Bayesian Statistics</h3>
<div>
<ul>
<li class="fragment">In Bayesian statistics a parameter <span class="math inline">\(\theta\)</span> isn’t a <em>fixed</em> number but a RV having a <span style="color:red;">prior</span> distribution <span class="math inline">\(P(\theta)\)</span></li>
<li class="fragment">In comes data sample <span class="math inline">\(Y = y_1, \dots, y_n\)</span> with distribution <span class="math inline">\(P(Y | \theta)\)</span> (likelihood)</li>
<li class="fragment">We calculate the <span style="color:red;">posterior</span> distribution given the data <span class="math inline">\(P(\theta|Y) \propto P(Y|\theta)P(\theta)\)</span></li>
<li class="fragment"><span class="math inline">\(\theta\)</span>’s final estimate is the mean or mode of <span class="math inline">\(P(\theta|Y)\)</span></li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Why would we take this approach?</p>
</div>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li>For linear regression and normal prior:
<ul>
<li>Prior: <span class="math inline">\(\beta \sim \mathcal{N}(0, \tau^2I_p)\)</span></li>
<li>Data: <span class="math inline">\(Y | \beta \sim \mathcal{N}(X\beta, \sigma^2I_n)\)</span></li>
<li>Posterior: <span class="math inline">\(\beta|Y \sim \mathcal{N}\left((X^TX + \frac{\sigma^2}{\tau^2}I_p)^{-1}X^Ty, \Sigma\right)\)</span></li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בסטטיסטיקה בייזיאנית באופן כללי, לאו דוקא בבעיות רגרסיה, הפרמטר שאנחנו מנסים לאמוד תטא, הוא כבר לא פרמטר. הוא משתנה מקרי, שיש לו התפלגות אפריורית, פי של תטא. אם הוא רציף כמובן נדבר על צפיפויות, נהוג לסמן פי של תטא באופן כללי.</p>
<p>כעת מגיע משתנה אחר שאנחנו רואים, והוא מדגם מקרי של נתונים Y. למדגם הזה יש הסתברות מותנית או לייקליהוד.</p>
<p>וכעת, אחרי שראינו את הנתונים, אנחנו רוצים לעדכן את הידע שלנו לגבי תטא, את ההתפלגות האפוסטריורית של תטא בהינתן הנתונים, שהיא כאמור פרופורציונלית ללייקליהוד כפול הפריור. בדרך כלל נשתמש בפריור ובלייקליהוד נוחים ככה שנוכל לזהות את ההתפלגות האפוסטריורית בקלות.</p>
<p>ואם אנחנו רוצים לאמוד את תטא, נשתמש בתוחלת של ההתפלגות הזאת, או בשכיח.</p>
<p>עכשיו כדאי לשאול למה לעשות את זה, בפרט שצורת החשיבה הזאת מכריחה אותנו להוסיף עוד הנחות וציפיות, שלא ברור בכלל שיש לנו. והתשובה כמו תמיד בסטטיסטיקה נעוצה בהפחתת השונות של האומד, רמז לזה ראינו בדוגמא בשקף הקודם. אם יש לכם איזשהו ידע קודם על העולם, על הפרמטר הזה שאתם רוצים לאמוד, למשל שהוא תחום בין גבולות מסוימים, כמו מטבע שלא סביר שהסיכוי בו לעץ הוא נמוך יותר מ20 אחוז או גבוה יותר מ80 אחוז – זה ידע שכדאי להשתמש בו כי האמידה הופכת לטובה ומדויקת יותר, מאשר אם באים לבעיה כזאת טאבולה ראסה. יש פחות סיכוי לדאטא לקחת אותנו איתו לכיוונים לא צפויים.</p>
<p>זה יכול להיות פרמטר אחד וזה יכול להיות וקטור של פרמטרים כמו בבעיה שלנו של רגרסיה ליניארית. ברגרסיה ליניארית למשל נוח להשתמש בפריור על המקדמים של התפלגות נורמלית. כלומר הוקטור עצמו בעל התפלגות רב נורמלית עם איזושהי שונות על אלכסון המטריצת קווריאנס שהיא נגיד טאו בריבוע. מראש אני מניח את זה. מגיע הדאטא עם הלייקליהוד המוכר של התפלגות נורמלית עם תוחלת שהיא הצירוף הליניארי ושונות סיגמא בריבוע.</p>
<p>וכעת מסתבר כמו מקודם שיש צורה סגורה ופשוטה להתפלגות האפוסטריורית של מקדמי הרגרסיה אחרי שראינו את הדאטא. הם מתפלגים נורמלית, עם איזושהי מטריצת שונות סיגמא שלא פרטנו כאן, ושימו לב לתוחלת. X’X + המנה של סיגמא בריבוע חלקי טאו בריבוע על האלכסון, בהופכי, X’y.</p>
<p>התוחלת המותנית הזאת מוכרת לנו! זה בדיוק פתרון רידג’, אם נגדיר את למדא להיות היחס בין סיגמא בריבוע לטאו בריבוע. והרי אמרנו שזה גם האומד שניקח כאומד סופי, התוחלת של ההתפלגות האפוסטריורית. וכשאנחנו מנסחים את זה ככה גם המשמעות של למדא מקבלת מימד נוסף. ככל שטאו בריבוע קטן יותר, אני מניח מראש שהמקדמים שלי מתפלגים עם שונות קטנה יותר מסביב לאפס, האילוץ חמור יותר, ככה גדלה למדא. ככל שטאו בריבוע גדלה יותר, אני מאפשר למקדמים לנוע חופשי סביב האפס ככה למדא קטנה יותר והאומד הסופי כבר לא שונה מהאומד של רגרסיה ליניארית רגילה. כלומר רגרסיה ליניארית רגילה מתקבלת עם מה שאנחנו קוראים פלאט פריור, פריור שטוח, למשל התפלגות נורמלית עם שונות גדולה מאוד או אינסופית.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="ridge-and-lasso-priors">Ridge and Lasso priors</h3>
<div id="6e3ab9c9" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-10-output-1.png" width="1430" height="325"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אבל בזה זה לא נגמר. אמרנו שהאומד לרגרסיה ליניארית רגילה מתקבל עם פריור שטוח, אין לי שום ציפיות מראש לפיזור המקדמים. האומד של רידג’ מתקבל עבור פריור נורמלי עם איזושהי שונות סופית. ומסתבר, שהאומד של לאסו מתקבל, אם הפריור שלנו הוא התפלגות עם שפיץ באפס שנקראת לפלאס או double exponential. כלומר אנחנו מניחים מראש שהמקדמים מתפלגים סימטרית סביב האפס אבל עם הרבה יותר צפיפות קרוב לאפס. ולכן גם כאן אנחנו רואים איך סביר בלאסו להיתקל במקדמים שהם פשוט אפס.</p>
<p>עד כאן לגבי שיטות רגולריזציה. נגיד שזה ממש לא הסוף, יש עוד מובנים גם לרידג’ וגם ללאסו, יש עוד שיטות לרגולריזציה שמנסות לשלב ביניהן. אבל הכי חשוב העיקרון של רגולריזציה הוא לא רק לרגרסיה ליניארית, הוא אפילו חשוב יותר, במודלים מורכבים יותר שתלמדו כמו עצי החלטה ורשתות נוירונים. הוא חשוב יותר ככל שהמודל גמיש יותר וצריך לאלף אותו.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="dimensionality-reduction-methods-pcr" class="slide level2 title-slide center">
<h2>Dimensionality Reduction Methods: PCR</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>הסוג האחרון של וריאציות לרגרסיה שנדבר עליהן מנסה להפחית קודם כל את המימד של הנתונים, ורק אז לבצע רגרסיה רגילה על הדאטא עם המימד הקטן יותר. אנחנו נתמקד רק ברגרסיה אחרי PCA או PCR אבל גם כאן יש עוד הרבה שיטות להורדת מימד. לבסוף כפי שעשינו עד כה נראה איך גם בשיטות האלה ניתן לראות כעוד גרסה של רגולריזציה, או לקיחת אילוץ על המקדמים.</p>
<p>לפני שנמשיך אזהרה: אנחנו עוד נלמד על PCA לעומק, כאן אנחנו עושים חזרה קצרה למי שכבר מכיר את הפרוצדורה. מי שלא – מומלץ לחזור לחלק הזה של השיעור רק לאחר שלמדנו על PCA.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="recall-pca">Recall: PCA</h3>
<p>PCA will find the best direction to project the data on, while preserving the maximum “information”:</p>
<div id="5916d1cd" class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-11-output-1.png" width="1034" height="449"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ניזכר מה אנחנו מנסים לעשות בPCA. נניח שיש לנו נתונים בשני מימדים X1 וX2, ואנחנו רוצים לתמצת אותם במימד אחד. במקום שני מספרים שיתארו כל תצפית, אנחנו רוצים מספר אחד. היינו רוצים לשמר את מירב האינפורמציה שיש בנתונים, ובמונחים סטטיסטיים זה אומר לשמר את מירב השונות שלהם. בPCA אנחנו עושים את זה באמצעות מציאת הכיוון של הכי הרבה שונות בדאטא. כאן זה די ברור, זה הכיוון הזה בקו האדום. ברגע שמצאנו את הכיוון הזה נטיל את הנתונים שלנו עליו, כלומר עבור כל תצפית נראה מה הערך שלה על הקו הזה כשאנחנו מסתכלים על המרחק הכי קצר אליו. והתוצאה היא ההטלה מימין, כאן על המימד החדש שנקרא T1 לכל תצפית יש רק ערך אחד, אין משמעות לציר הY, והטענה היא שככה שמרנו על הכי הרבה אינפורמציה במעבר מדו-מימד לחד-מימד.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-pca-problem">The PCA Problem</h3>
<div>
<ul>
<li class="fragment"><p>Goal: Find the <span class="math inline">\(q\)</span> direction(s) with the most dispersion</p></li>
<li class="fragment"><p>Projection is direction <span class="math inline">\(\mathbf{v}\)</span>: <span class="math inline">\(X\mathbf{v} \in \mathbb R^n.\)</span> Examples:</p>
<ul>
<li class="fragment"><span class="math inline">\(\mathbf{v} = (1,0,\dots,0)^T\)</span>: pick first coordinate from each observation</li>
<li class="fragment"><span class="math inline">\(\mathbf{v} = (1/\sqrt{p},1/\sqrt{p},\dots,1/\sqrt{p})^T\)</span>: project on diagonal (average all coordinates)</li>
</ul></li>
<li class="fragment"><p>Dispersion in direction <span class="math inline">\(\mathbf{v}\)</span>: <span class="math inline">\(||X\mathbf{v}||^2 = \mathbf{v}^T(X^TX)\mathbf{v}.\)</span></p></li>
<li class="fragment"><p>Finding the best direction which maximizes dispersion: <span class="math inline">\(\mathbf{v}_1 = \arg\max_{\mathbf{v}:\|\mathbf{v}\|^2 =1} \|X\mathbf{v}\|^2\)</span></p></li>
<li class="fragment"><p><span class="math inline">\(\mathbf{v}_1\)</span> is the first Principal Component direction: the best direction to project on!</p></li>
<li class="fragment"><p>Similarly find the next PC directions <span class="math inline">\(\mathbf{v}_2, \dots, \mathbf{v}_q\)</span> and stack them to matrix <span class="math inline">\(W_{p \times q}\)</span></p></li>
<li class="fragment"><p>Data with reduced dimensionality:</p>
<ul>
<li class="fragment"><span class="math inline">\(T_{n \times q} = X_{n \times p}W_{p \times q}\)</span> taking only the first <span class="math inline">\(q\)</span> principal directions</li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>באופן כללי יותר נגיד שהמטרה שלנו היא למצוא Q כיוונים כאלה ששומרים על מקסימום פיזור או שונות.</p>
<p>כיוון כזה כמו הוקטור האדום שלנו אפשר לסמן בווי, והטלה של הנתונים עליו היא מכפלה של X כפול V, מה שנותן וקטור חדש באורך n, קודם קראנו לו T1.</p>
<p>לדוגמא, ההטלה יכלה להיות הוקטור שיש לו 1 באלמנט הראשון שלו, ואפס בכל השאר – אם מכפילים את X בוקטור כזה זה בדיוק כמו לבחור רק את המשתנה הראשון בנתונים.</p>
<p>למשל, הוקטור וי יכול להיות וקטור שבכל האלמנטים שלו יש 1 חלקי שורש P. אם תעשו קצת אלגברה תראו שלהכפיל את X בוקטור כזה אומר עבור כל תצפית לקחת את הממוצע שלה על כל המשתנים.</p>
<p>מה זה פיזור של וקטור ההטלה שאנחנו רוצים למקסם? אנחנו ניקח את הנורמה בריבוע של וקטור ההטלה, אותה אפשר לרשום כv’X’Xv.</p>
<p>והנה הבעיה שלנו, למצוא את הכיוון, הוקטור v1 בעל נורמה 1, שממקסם את פיזור ההטלה. אנחנו עוד נלמד על PCA יותר לעומק ונראה למה אנחנו מוכרחים את האילוץ הזה על הוקטור וי.</p>
<p>כשנמצא את הוקטור הזה נקרא לו פרינסיפל קומפוננט דירקשן: זה הכיוון הכי טוב להטיל עליו על פי קריטריון המקסימום פיזור.</p>
<p>כעת נמצא את הכיוון הבא v2, v3 וכולי. נראה שהם צריכים להיות אורתוגונליים זה לזה. ונשים אותם זה לצד זה במטריצה W מסדר p שורות על q עמודות. זאת מטריצת ההטלה הסופית.</p>
<p>את הדאטא ממימד נמוך יותר נכתוב כT. הדאטא שלנו עבר הפחתת מימד מp שיכול להיות גבוה מאוד, לq שיכול להיות קטן מאוד, והוא מוכן לרגרסיה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="pca-regression-pcr">PCA regression (PCR)</h3>
<div>
<ul>
<li class="fragment">Standardize the features in <span class="math inline">\(X\)</span></li>
<li class="fragment">Find <span class="math inline">\(W_{p \times q}\)</span> (usually via SVD decomposition)</li>
<li class="fragment">Perform (regular) linear regression on <span class="math inline">\(T_{n \times q} = XW\)</span> <span class="math display">\[y_i = \theta_0 + \theta_1 \cdot t_{i1} + \dots + + \theta_q \cdot t_{iq} + \varepsilon_i \quad \text{ or } \quad y = T\theta + \varepsilon\]</span></li>
<li class="fragment"><span class="math inline">\(q\)</span> becomes a hyperparameter</li>
<li class="fragment"><span class="math inline">\(y = T\theta + \varepsilon = X(W\theta) + \varepsilon \to \beta = W\theta\)</span>, hence <span class="math inline">\(\beta\)</span> is still constrained</li>
<li class="fragment">SVD decomposition also shows similarities to Ridge regression</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נסכם איך אנחנו מבצעים PCR:</p>
<p>עושים סטנדרטיזציה על עמודות X.</p>
<p>מוצאים את מטריצת ההטלה W, בדרך כלל על-ידי פירוק SVD ועוד נחזור לזה כשנלמד PCA לעומק.</p>
<p>וכעת נבצע רגרסיה ליניארית עם המודל הליניארי הרגיל, על הדאטא אחרי הורדת מימד. אנחנו מחפשים עכשיו וקטור מקדמים תטא באורך Q ועוד 1, שכשנכפיל אותו פי הנתונים החדשים שלנו T נקבל בקירוב את המשתנה התלוי Y.</p>
<p>נשים לב שכמו שאר השיטות שלנו אין רק פתרון אחד, לא דיברנו על איך יודעים מהו Q, מה המימד הנכון לבחור? למעשה Q הופך להיפרפרמטר נוסף בדומה ללמדא, שצריך לבחור, למשל דרך קרוס ולידיישן.</p>
<p>עכשיו מה הקשר לשיטות שלמדנו: נביט שוב במודל שאנחנו מניחים ונראה שבהכרח זה אומר שאנחנו מחפשים וקטור מקדמים בטא ששווה לW כפול תטא. והנה אנחנו רואים שוב קשר לתמה הכללית שלנו, אפשר לראות בביטוי הזה כאילוץ על הבטאות, המקדמים בעצמם חייבים להיות צירוף ליניארי על וקטור מקדמים ממימד נמוך יותר!</p>
<p>וזה לא הקשר היחיד. כשלומדים על פירוק הSVD רואים אפילו קשר נוסף בין PCR לרגרסיית רידג’.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="pca-regression-pcr-1">PCA regression (PCR)</h3>
<div id="f9e03e68" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c08_feature_selection_regularization_files/figure-revealjs/cell-12-output-1.png" width="949" height="470"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בנתונים שלנו המימד P מלכתחילה לא כל כך גבוה, הוא 11, ולא ברור שצריך הורדת מימד באמצעות PCA או כל שיטה אחרת לפני הרצת רגרסיה.</p>
<p>ואכן, אם אנחנו בודקים מה הQ שמביא למינימום שגיאת חיזוי באמצעות קרוס ולידיישן כמו שעשינו עד עכשיו, אנחנו מקבלים שבנתונים שלנו לא כדאי כמעט להוריד מימד, הQ הטוב ביותר הוא 10. ואמנם לא רואים את זה בגרף אבל כן יש הפחתה בשורה התחתונה בMSE של הטסט לעומת רגרסיה רגילה, אם כי אחוזית זה הפרש די קטן.</p>
<p>נשים לב שברגע שמבינים שאפשר לחזור מוקטור המקדמים תטא אל וקטור המקדמים המקורי בטא באמצעות הכפלה פי מטריצת W, אפשר לראות את פרופיל המקדמים בטא כמו שעשינו עד עכשיו עם פרמטר הפנאלטי למדא. אנחנו רואים שגם בPCR המקדמים עוברים שרינקג’ שמזכיר את רידג’, ככל שQ קטן יותר או הורדת המימד היא אגרסיבית יותר. הכיוון כאן הפוך כי לא רצינו להפוך את ציר הX בצורה מלאכותית מ11 ל-1, אבל הרעיון זהה.</p>
<p>עד כאן בשיטות בחירה של משתנים ורגולריזציה. בשיעורים הבאים נלמד מודלים מורכבים וגמישים הרבה יותר, חלק מהם כל כך גמישים שהם באים בילט-אין עם רגולריזציה. העקרונות שראינו היום ישמשו אתכם לשיפור כל מודל מורכב שתראו בעתיד והם לא המלצה הם כבר חלק מהמודלים המודרנים ביותר שנמצאים בשימוש יומיומי, ולכן מומלץ להכיר אותם ואת החשיבות שלהם היטב.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="quarto-auto-generated-content">
<p><img src="../Intro2SL_logo_white.jpg" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://intro2statlearn.github.io/mooc/" target="_blank">Intro to Statistical Learning</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../libs/revealjs/plugin/search/search.js"></script>
  <script src="../libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>