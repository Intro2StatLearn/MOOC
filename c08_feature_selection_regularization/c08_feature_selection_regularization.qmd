---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Feature Selection and Regularization"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Feat. Selection and Regularization - Class 8

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Goals of Selection and Regularization {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Why select features? Why regularize?

E.g., did we not see the Gauss-Markov Theorem?

::: {.incremental}
- Improve prediction accuracy
  - Recall: the Bias-Variance Tradeoff $\Rightarrow$ allowing some bias might decrease variance!
  - Recall: $op \approx \mathcal{O}\left(\frac{p\sigma^2}{n}\right)$
  - If $p > n$: $X'X$ has no inverse, infinite solutions

- Improve interpretability
  - Discarding features with small "unlikely" coefficients
  
    $\Rightarrow$ lowering model complexity
    
    $\Rightarrow$ parsimony!
  - Often coincides with improving prediction accuracy

- In general: "don't believe *everything* the data says"

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Feature Selection and Regularization

We will focus on:

- Subset selection (best, stepwise, stagewise)
- Regularized regression (Ridge, Lasso)
- Dimensionality reduction (PCR)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Subset Selection {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Best subset selection

1. $M_0$ model: predict $\hat{y} = \hat{\beta}_0 = \bar{y}$
2. For $k = 1, \dots, p$:
    i. Fit all ${p \choose k}$ models containing $k$ features
    ii. Pick the best $M_k$ with $\min RSS$
3. Select the best model from $M_0, \dots, M_p$ with the $C_p$/$AIC$ criterion or CV

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Best subset selection

```{python}
#| echo: false

import pandas as pd
import numpy as np
import itertools
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0', 'Married'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Function to calculate RSS
def calculate_rss(y, y_pred):
    return np.sum((y - y_pred) ** 2)

# Perform best subset selection
rss_list = []
best_models = []

# Model M0: only intercept
X_intercept = np.ones((len(y), 1))
model = LinearRegression().fit(X_intercept, y)
rss = calculate_rss(y, model.predict(X_intercept))
rss_list.append((0, rss, 'Intercept'))
best_models.append((0, rss, []))

# Iterate over the number of predictors from 1 to 11
for k in range(1, X.shape[1] + 1):
    best_rss = np.inf
    best_model = None
    best_features = None
    for combo in itertools.combinations(X.columns, k):
        X_subset = X[list(combo)]
        model = LinearRegression().fit(X_subset, y)
        rss = calculate_rss(y, model.predict(X_subset))
        rss_list.append((k, rss, combo))
        if rss < best_rss:
            best_rss = rss
            best_model = model
            best_features = combo
    best_models.append((k, best_rss, best_features))

# Convert RSS list to a DataFrame
rss_df = pd.DataFrame(rss_list, columns=['k', 'RSS', 'Features'])

# Estimate sigma^2 from the full model
full_model = LinearRegression().fit(X, y)
full_rss = calculate_rss(y, full_model.predict(X))
sigma2_est = full_rss / (len(y) - X.shape[1] - 1)

# Calculate RSS + 2 * k * sigma2_est
rss_plus_penalty = [rss + 2 * k * sigma2_est for k, rss, _ in best_models]

# Find the minimum RSS + penalty and corresponding k
min_rss_plus_penalty = min(rss_plus_penalty)
best_k = rss_plus_penalty.index(min_rss_plus_penalty)

# Plot the RSS for all models
plt.figure(figsize=(10, 5))
for k in range(X.shape[1] + 1):
    k_rss = rss_df[rss_df['k'] == k]
    plt.scatter(k_rss['k'], k_rss['RSS'], color='blue', label='All models' if k == 0 else "")
best_rss_df = pd.DataFrame(best_models, columns=['k', 'RSS', 'Features'])
plt.plot(best_rss_df['k'], best_rss_df['RSS'], color='red', label='Best models')
plt.plot(best_rss_df['k'], rss_plus_penalty, color='green', linestyle='dashed', label='Best models + Cp penalty')
plt.axvline(x=best_k, color='orange', linestyle='dashed', label='Best Cp model')
plt.xlabel('Number of predictors (k)')
plt.ylabel('RSS')
plt.title('RSS for Best Subset Selection')
plt.legend()
plt.show()
```

::: {.fragment}
::: {.callout-note}
How many models are run?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Forward stepwise selection

1. $M_0$ model: predict $\hat{y} = \hat{\beta}_0 = \bar{y}$
2. For $k = 0, \dots, p - 1$:
    i. Fit all $p - k$ models adding 1 additional feature
    ii. Pick the best $M_{k + 1}$ with $\min RSS$
3. Select the best model from $M_0, \dots, M_p$ with the $C_p$/$AIC$ criterion or CV


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Forward stepwise selection

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Function to calculate RSS
def calculate_rss(y, y_pred):
    return np.sum((y - y_pred) ** 2)

# Forward stepwise selection
rss_list = []
best_models = []

# Model M0: only intercept
X_intercept = np.ones((len(y), 1))
model = LinearRegression().fit(X_intercept, y)
rss = calculate_rss(y, model.predict(X_intercept))
rss_list.append((0, rss, 'Intercept'))
best_models.append((0, rss, []))

current_features = []
remaining_features = list(X.columns)

# Iterate to add predictors from 1 to 11
for k in range(1, X.shape[1] + 1):
    best_rss = np.inf
    best_feature = None
    for feature in remaining_features:
        trial_features = current_features + [feature]
        X_subset = X[trial_features]
        model = LinearRegression().fit(X_subset, y)
        rss = calculate_rss(y, model.predict(X_subset))
        rss_list.append((k, rss, trial_features))
        if rss < best_rss:
            best_rss = rss
            best_feature = feature
    current_features.append(best_feature)
    remaining_features.remove(best_feature)
    best_models.append((k, best_rss, current_features.copy()))

# Convert RSS list to a DataFrame
rss_df = pd.DataFrame(rss_list, columns=['k', 'RSS', 'Features'])

# Estimate sigma^2 from the full model
full_model = LinearRegression().fit(X, y)
full_rss = calculate_rss(y, full_model.predict(X))
sigma2_est = full_rss / (len(y) - X.shape[1] - 1)

# Calculate RSS + 2 * k * sigma2_est
rss_plus_penalty = [rss + 2 * k * sigma2_est for k, rss, _ in best_models]

# Find the minimum RSS + penalty and corresponding k
min_rss_plus_penalty = min(rss_plus_penalty)
best_k = rss_plus_penalty.index(min_rss_plus_penalty)

# Plot the RSS for all models
plt.figure(figsize=(10, 5))
for k in range(X.shape[1] + 1):
    k_rss = rss_df[rss_df['k'] == k]
    plt.scatter(k_rss['k'], k_rss['RSS'], color='blue', label='All models' if k == 0 else "")
best_rss_df = pd.DataFrame(best_models, columns=['k', 'RSS', 'Features'])
plt.plot(best_rss_df['k'], best_rss_df['RSS'], color='red', label='Best models')
plt.plot(best_rss_df['k'], rss_plus_penalty, color='green', linestyle='dashed', label='Best models + Cp penalty')
plt.axvline(x=best_k, color='orange', linestyle='dashed', label='Best Cp model')
plt.xlabel('Number of predictors (k)')
plt.ylabel('RSS')
plt.title('RSS for Forward Stepwise Selection')
plt.legend()
plt.show()
```

::: {.fragment}
::: {.callout-note}
How many models are run?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Stepwise regression main disadvantage

| k | Best subset                           | Forward stepwise                    |
|---|---------------------------------------|-------------------------------------|
| 1 | {Rating}                              | {Rating}                            |
| 2 | {Rating, Income}                      | {Rating, Income}                    |
| 3 | {Rating, Income, Student}             | {Rating, Income, Student}           |
| 4 | {Income, Student, Limit, Cards}       | {Rating, Income, Student, Limit}    |

::: {.fragment}
::: {.callout-note}
What happens if $p > n$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Forward [stagewise]{style="color:red;"} selection

TODO

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Regularized Regression: Ridge {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Ridge regression

::: {.incremental}
- Instead of reducing the number of parameters $\to$ restrict their norm.
- With $\ell_2$ norm we get the penalized RSS criterion for some regularization/penalty parameter $\lambda$:
$$PRSS(\lambda) = \sum_{i = 1}^n \left(y_i - \beta_0 + \sum_{j = 1}^p \beta_j x_{ij}\right)^2 + \lambda\sum_{j = 1}^p \beta_j^2 = RSS + \lambda\|\beta^*\|^2_2$$

- Standardize the features in $X$, then:
  - $\hat{\beta}_0 = \bar{y}$
  - $\lambda$ punishes features of different scale comparably
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Ridge solution

$$PRSS(\lambda) = \|y - X\beta\|^2_2 + \lambda\|\beta\|^2_2$$

::: {.fragment}
$\frac{\partial PRSS}{\partial \beta} = -2X'y + 2X'X\beta +2\lambda\beta$
:::

::: {.fragment}
$2X'y - 2X'X\beta - 2\lambda\beta = 0$
:::
::: {.fragment}
$(X'X + \lambda I_p)\beta = X'y$
:::
::: {.fragment}
$\hat{\beta}(\lambda) = (X'X + \lambda I_p)^{-1}X'y$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Ridge (original) justification

$$\hat{\beta}(\lambda) = (X'X + \lambda I_p)^{-1}X'y$$

::: {.incremental}
- Numerical stability:
  - If $X$'s columns are correlated, $X'X$ is ill-conditioned and its inverse is unstable, variance of $\hat{\beta}$ is high -- but $X'X + \lambda I_p$ improves on this
- Feasibility:
  - If $X$'s columns are linearly dependent $X'X$ is not invertible but $X'X + \lambda I_p$ is!
  - If $p > n$ -- same!
- Guaranteed prediction error reduction for some $\lambda$: Bias increases but variance decreases *more*
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Choosing $\lambda$

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Define lambdas for Ridge regression
lambdas = np.logspace(-2, 3, 100)

# Cross-validation setup
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# Variables to store results
train_errors = []
test_errors = []
ridge_coefficients = []

# Regular linear regression for comparison
lr_model = LinearRegression()
lr_train_errors = []
lr_test_errors = []

# Ridge regression with cross-validation
for alpha in lambdas:
    ridge = make_pipeline(StandardScaler(), Ridge(alpha=alpha))
    
    fold_train_errors = []
    fold_test_errors = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        ridge.fit(X_train, y_train)
        y_train_pred = ridge.predict(X_train)
        y_test_pred = ridge.predict(X_test)
        
        fold_train_errors.append(((y_train - y_train_pred) ** 2).mean())
        fold_test_errors.append(((y_test - y_test_pred) ** 2).mean())

    train_errors.append(np.mean(fold_train_errors))
    test_errors.append(np.mean(fold_test_errors))
    ridge_coefficients.append(ridge.named_steps['ridge'].coef_)

# Fit regular linear regression and calculate mean train and test errors
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    scale = StandardScaler()
    X_train = scale.fit_transform(X_train)
    X_test = scale.transform(X_test)
    lr_model.fit(X_train, y_train)
    lr_train_errors.append(((y_train - lr_model.predict(X_train)) ** 2).mean())
    lr_test_errors.append(((y_test - lr_model.predict(X_test)) ** 2).mean())

mean_lr_train_error = np.mean(lr_train_errors)
mean_lr_test_error = np.mean(lr_test_errors)

# Find the best lambda
best_lambda_index = np.argmin(test_errors)
best_lambda = lambdas[best_lambda_index]
max_lam = 10
# Plot the results
plt.figure(figsize=(14, 5))

# Plot 1: Training and test errors vs lambda
plt.subplot(1, 2, 1)
plt.plot(lambdas[lambdas <= max_lam], np.array(train_errors)[lambdas <= max_lam], label='Train Error', color='blue')
plt.plot(lambdas[lambdas <= max_lam], np.array(test_errors)[lambdas <= max_lam], label='Test Error', color='red')
plt.axhline(y=mean_lr_train_error, color='blue', linestyle='dashed', label='Linear Regression Train Error')
plt.axhline(y=mean_lr_test_error, color='red', linestyle='dashed', label='Linear Regression Test Error')
plt.axvline(x=best_lambda, color='green', linestyle='dashed', label='Best Lambda')
plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Mean Squared Error')
plt.title('Ridge Regression: 5-Fold CV Mean MSE')
plt.legend()

# Plot 2: Ridge coefficients profile
plt.subplot(1, 2, 2)
ridge_coefficients = np.array(ridge_coefficients)
for i in range(ridge_coefficients.shape[1]):
    plt.plot(lambdas, ridge_coefficients[:, i], label=X.columns[i])
plt.axvline(x=best_lambda, color='green', linestyle='dashed', label='Best Lambda')
plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Coefficient Value')
plt.title('Ridge Coefficients Profile')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Regularized Regression: Lasso {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Lasso regression

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Lasso solution

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Shrinkage view of Ridge and Lasso

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Bayesian Viewpoint {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Bayesian Statistics

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Ridge: Gaussian prior

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Lasso: Laplace prior

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Dimensionality Reduction Methods: PCR {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Recall: PCA

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### PCA regression (PCR)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---
