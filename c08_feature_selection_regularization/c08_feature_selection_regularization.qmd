---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Feature Selection and Regularization"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Feat. Selection and Regularization - Class 8

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Goals of Selection and Regularization {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
השיעור שלפנינו עוסק בבחירת משתנים וברגולריזציה שלהם. אמנם אנחנו מתמקדים עדיין במודל הליניארי וספציפית ברגרסיה ליניארית, אבל העקרונות שנלמד נכונים גם לקלסיפיקציה ולמודלים מורכבים הרבה יותר, והם חיוניים להבנה שלנו בעתיד איך אפשר לשפר כל מודל באמצעות שליטה עליו, ולהימנע מאוברפיטינג.
:::
:::

---

### Why select features? Why regularize?

E.g., did we not see the Gauss-Markov Theorem?

::: {.incremental}
- Improve prediction accuracy
  - Recall: the Bias-Variance Tradeoff $\Rightarrow$ allowing some bias might decrease variance!
  - Recall: $op \approx \mathcal{O}\left(\frac{p\sigma^2}{n}\right)$
  - If $p > n$: $X'X$ has no inverse, infinite solutions

- Improve interpretability
  - Discarding features with small "unlikely" coefficients
  
    $\Rightarrow$ lowering model complexity
    
    $\Rightarrow$ parsimony!
  - Often coincides with improving prediction accuracy

- In general: "don't believe *everything* the data says"

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
חלק מהשיטות שנלמד לרגולריזציה משנות את המודל הליניארי, ונשאלת השאלה: למה לגעת בו? למה להחליט שאנחנו שמים משתנים מסוימים במודל ואחרים לא, או שתיכף נראה אנחנו מוסיפים אילוצים על המקדמים -- למשל, לא אמרנו שהאומד הליניארי שמצאנו בטא-האט לפי משפט גאוס-מרקוב הוא האומד הליניארי חסר ההטיה בעל השונות הקטנה ביותר?

אז מסתבר שאנחנו מרוויחים הרבה באמצעות רגולריזציה. בראש ובראשונה אנחנו משפרים את שגיאת החיזוי. אם ניזכר ששגיאת החיזוי למשל הריבועית מתפרקת לסכום של טעות אירדוסיבל ועוד ביאס בריבוע ועוד שונות המודל -- נראה היום שאפשר לשלם מעט בהעלאת הביאס, ולהרוויח הרבה בהורדת השונות, כך שבשורה התחתונה הטעות הריבועית תקטן.

ניזכר גם שראינו שאופטימיזם כפי שהגדרנו אותו במודל הליניארי, הוא סדר גודל של p מספר המשתנים חלקי n. כלומר באופן כללי זה עושה שכל להשתמש בכמה שפחות מהם, אם רוצים להקטין את האופטימיזם, את הפער בין הטעות שאנחנו רואים על מדגם הלמידה הספציפי שלנו, וטעות החיזוי הכללית על נתונים שלא ראינו.

אבל לא רק שאנחנו עשויים לשפר את שגיאת החיזוי, במצבים מסוימים המודל הליניארי פשוט לא פיזיבילי ואנחנו נראה שבאמצעות שיטות לבחירת משתנים או רגולריזציה פתאום כן אפשר למדל את הנתונים. זה קורה למשל כאשר יש יותר משתנים מתצפיות, למטריצה X'X אין הופכי, יש אינסוף פתרונות ואנחנו באים לתקן את זה.

היבט אחר שבו רגולריזציה מסייעת לנו הוא האינטרפרטביליות של המודל. בזה שאנחנו נפטרים ממקדמים שהם קטנים מדי לעומת הרעש, שלא סביר שהם שונים מאפס או שאין מספיק דאטא לאמוד אותם בצורה מדויקת -- המודל שלנו הופך לפחות מסובך ואנחנו משיגים פרסימוניה, המודל קומפקטי וברור יותר למי שרוצה ללמוד משהו.

נזכיר גם שהרבה פעמים שתי המטרות האלה הולכות יחד. נראה שלהיפטר ממשתנים בעלי השפעה קטנה שקשה לאמוד, הרבה פעמים מקטין גם את שגיאת החיזוי.

באופן כללי אפשר לסכם את השיעור שלנו באמירה השנויה במחלוקת: אל תאמינו לכל מה שהנתונים אומרים. מתוך הכרה שהנתונים שלנו הם מדגם מקרי בגודל סופי, אנחנו שמים על המודל מגבלות או מוסיפים קצת הטיה, ומרוויחים מזה בכל החזיתות.
:::
:::

---

### Feature Selection and Regularization

We will focus on:

- Subset selection (best, stepwise, stagewise)
- Regularized regression (Ridge, Lasso)
- Dimensionality reduction (PCR)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אנחנו נתמקד בשלוש משפחות של שיטות לבחירת משתנים ורגולריזציה: בחירת תת-קבוצה של משתנים ברגרסיה, בכל מיני דרכים. רגולריזציה של מקדמי הרגרסיה באמצעות שיטות מפורסמות כמו רידג' ולאסו. ולבסוף, הורדת מימד לנתונים ממימד גבוה באמצעות PCA, ורק אז ביצוע רגרסיה על הדאטא ממימד נמוך.
:::
:::

---

## Subset Selection {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל בבחירה של תת-קבוצה של משתנים. בשיטות אלה סך המשתנים שעומדים לפני הם רק מועמדים להיכנס לרגרסיה, יכול להיות שאבחר בהם ויכול להיות שלא. ראינו בשיעורים על מודל סלקשן מדדים טובים לאמוד את השגיאה ובהם נשתמש. בסופו של דבר יהיה לי סט של משתנים בהם בחרתי, ואני אריץ רגרסיה ליניארית רגילה על כל הדאטה שלי, רק על סט המשתנים הנבחר.
:::
:::

---

### Best subset selection

1. $M_0$ model: predict $\hat{y} = \hat{\beta}_0 = \bar{y}$
2. For $k = 1, \dots, p$:
    i. Fit all ${p \choose k}$ models containing $k$ features
    ii. Pick the best $M_k$ with $\min RSS$
3. Select the best model from $M_0, \dots, M_p$ with the $C_p$/$AIC$ criterion or CV

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל בגישה התמימה ביותר, שעוברת על כל האפשרויות -- בסט סאבסט סלקשן.

מתחילים עם מודל M0 שבו אין משתנים בכלל והחיזוי הוא ממוצע Y.

לאחר מכן בשלב k = 1 אני רוצה לבחון את כל המודלים עם משתנה אחד בלבד, ואני עובר על כל p המודלים האפשריים, כל פעם מכניס משתנה אחר, ובודק איזה משתנה נותן לי את מינימום השגיאה, במקרה שלנו של רגרסיה ליניארית מינימום RSS. למודל הזה אני קורא M1, ואז אני עובר לk = 2, עובר על כל האפשרויות של מודל עם 2 משתנים, בוחר את המודל M2 וכך הלאה עד המודל שמכניס את כל p המשתנים.

כעת אני צריך לבחור בין p + 1 מודלים, ואת זה ראינו בשיעור על מודל סלקשן איך לעשות, אני אבחר במדד שמתחשב במספר המשתנים כי אני יודע שזה מנפח את השגיאה האמיתית על תצפיות שלא ראיתי, מדד כמו הCp של מאלו או AIC, או ביצוע קרוס ולידיישן על המודלים. המודל הסופי יהיה המודל שמביא למינימום את הקריטריון שבחרתי.
:::
:::

---

### Best subset selection

```{python}
#| echo: false

import pandas as pd
import numpy as np
import itertools
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Function to calculate RSS
def calculate_rss(y, y_pred):
    return np.sum((y - y_pred) ** 2)

# Perform best subset selection
rss_list = []
best_models = []

# Model M0: only intercept
X_intercept = np.ones((len(y), 1))
model = LinearRegression().fit(X_intercept, y)
rss = calculate_rss(y, model.predict(X_intercept))
rss_list.append((0, rss, 'Intercept'))
best_models.append((0, rss, []))

# Iterate over the number of predictors from 1 to 11
for k in range(1, X.shape[1] + 1):
    best_rss = np.inf
    best_model = None
    best_features = None
    for combo in itertools.combinations(X.columns, k):
        X_subset = X[list(combo)]
        model = LinearRegression().fit(X_subset, y)
        rss = calculate_rss(y, model.predict(X_subset))
        rss_list.append((k, rss, combo))
        if rss < best_rss:
            best_rss = rss
            best_model = model
            best_features = combo
    best_models.append((k, best_rss, best_features))

# Convert RSS list to a DataFrame
rss_df = pd.DataFrame(rss_list, columns=['k', 'RSS', 'Features'])

# Estimate sigma^2 from the full model
full_model = LinearRegression().fit(X, y)
full_rss = calculate_rss(y, full_model.predict(X))
sigma2_est = full_rss / (len(y) - X.shape[1] - 1)

# Calculate RSS + 2 * k * sigma2_est
rss_plus_penalty = [rss + 2 * k * sigma2_est for k, rss, _ in best_models]

# Find the minimum RSS + penalty and corresponding k
min_rss_plus_penalty = min(rss_plus_penalty)
best_k = rss_plus_penalty.index(min_rss_plus_penalty)

# Plot the RSS for all models
plt.figure(figsize=(10, 5))
for k in range(X.shape[1] + 1):
    k_rss = rss_df[rss_df['k'] == k]
    plt.scatter(k_rss['k'], k_rss['RSS'], color='blue', label='All models' if k == 0 else "")
best_rss_df = pd.DataFrame(best_models, columns=['k', 'RSS', 'Features'])
plt.plot(best_rss_df['k'], best_rss_df['RSS'], color='red', label='Best models')
plt.plot(best_rss_df['k'], rss_plus_penalty, color='green', linestyle='dashed', label='Best models + Cp penalty')
plt.axvline(x=best_k, color='orange', linestyle='dashed', label='Best Cp model')
plt.xlabel('Number of predictors (k)')
plt.ylabel('RSS')
plt.title('RSS for Best Subset Selection')
plt.legend()
plt.show()
```

::: {.fragment}
::: {.callout-note}
How many models are run?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בואו נראה דוגמא של הרצת רגרסיה בסט סאבסט על נתוני אשראי. בדוגמה שלנו יש p = 11 משתנים כמו הכנסה, מגבלה על החשבון, האם בעל החשבון הוא סטודנט וכולי. ואנחנו מנסים לחזות את היתרה של בעל החשבון.

על ציר האיקס אנחנו רואים את k בכל שלב של האלגוריתם. בהתחלה k = 0 כלומר אנחנו עובדים רק עם חותך. ואז k = 1 ואנחנו מתאימים 11 מודלים עם משתנה אחד, כל נקודה כאן היא הRSS של אחד המודלים האלה. וכך ממשיכים עד המודל האחרון בו מכניסים את כל המשתנים.

עבור כל k אפשר לראות את המודל הנבחר שמשיג את הRSS הנמוך ביותר, הקו האדום מחבר ביניהם. באופן לא מפתיע הקו האדום יכול רק לרדת, את זה ראינו בשיעורים קודמים, הוספת משתנים לרגרסיה יכולה רק לשפר את הRSS.

לכן כשאנחנו באים לבחור באיזה מודל סופי להשתמש אנחנו יכולים לעשות קרוס ולידציה עם המודלים הנבחרים או להשתמש למשל בקריטריון הCp שמוסיף עונש לRSS שגדל ככל שמספר המשתנים עולה. כאן אפשר לראות שהמודל שהביא למינימום את הCp הוא המודל עם 6 משתנים, גם אם לא רואים את זה מהגרף. נזכיר שזה המודל עם 6 משתנים הטוב ביותר מכל האפשרויות למודלים עם 6 משתנים.

אז כמה מודלים הרצנו כאן? כל המודלים האפשריים, 2 בחזקת 11, שזה קצת יותר מאלפיים מודלים! ברור מייד שהגישה הזאת לא סבירה עבור p גדול הרבה יותר מ10, וגם אז עבור גדלי מדגם לא גדולים מדי.
:::
:::

---

### Forward stepwise selection

1. $M_0$ model: predict $\hat{y} = \hat{\beta}_0 = \bar{y}$
2. For $k = 0, \dots, p - 1$:
    i. Fit all $p - k$ models adding 1 additional feature
    ii. Pick the best $M_{k + 1}$ with $\min RSS$
3. Select the best model from $M_0, \dots, M_p$ with the $C_p$/$AIC$ criterion or CV


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ולכן הומצאו גישות אחרות של פורוורד או בקוורד סלקשן. הגישות האלה הרבה יותר קלות חישובית, אנחנו בשום פנים ואופן לא נריץ את כל האפשרויות למודל, אבל המחיר שמשלמים עבורן הוא לא קטן, כי הן מאוד גרידיות.

באלגוריתם הפורוורד סטפווייז, מתחילים כמו עם בסט סאבסט במודל שבו יש רק חותך והוא ממוצע Y. כעת k = 0 ואנחנו רוצים להוסיף רק משתנה אחד מתוך p המשתנים, מאוד מזכיר מה שעשינו קודם. נבחר את המשתנה שיביא למינימום RSS ונקרא למודל M1. כשk = 1 נכנס ההבדל, אנחנו לא לוקחים בחשבון את כל האפשרויות למודל עם 2 משתנים, אלא רוצים רק להוסיף משתנה אחד למודל שכבר קיים לנו. אז יש לנו p - 1 אפשרויות. ברגע שבחרנו את המשתנה השני אנחנו בוחרים את המשתנה השלישי וכך הלאה, בלי לגעת בסט המשתנים שכבר נבחרו.

בסוף בשלב השלישי כמו בבסט סאבסט אנחנו בוחרים את המודל הטוב ביותר מבין הקבוצה של מודלים שמצאנו בשלב 2, באמצעות קרוס ולידיצה או קריטריון ראוי אחר של מודל סלקשן.
:::
:::

---

### Forward stepwise selection

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Function to calculate RSS
def calculate_rss(y, y_pred):
    return np.sum((y - y_pred) ** 2)

# Forward stepwise selection
rss_list = []
best_models = []

# Model M0: only intercept
X_intercept = np.ones((len(y), 1))
model = LinearRegression().fit(X_intercept, y)
rss = calculate_rss(y, model.predict(X_intercept))
rss_list.append((0, rss, 'Intercept'))
best_models.append((0, rss, []))

current_features = []
remaining_features = list(X.columns)

# Iterate to add predictors from 1 to 11
for k in range(1, X.shape[1] + 1):
    best_rss = np.inf
    best_feature = None
    for feature in remaining_features:
        trial_features = current_features + [feature]
        X_subset = X[trial_features]
        model = LinearRegression().fit(X_subset, y)
        rss = calculate_rss(y, model.predict(X_subset))
        rss_list.append((k, rss, trial_features))
        if rss < best_rss:
            best_rss = rss
            best_feature = feature
    current_features.append(best_feature)
    remaining_features.remove(best_feature)
    best_models.append((k, best_rss, current_features.copy()))

# Convert RSS list to a DataFrame
rss_df = pd.DataFrame(rss_list, columns=['k', 'RSS', 'Features'])

# Estimate sigma^2 from the full model
full_model = LinearRegression().fit(X, y)
full_rss = calculate_rss(y, full_model.predict(X))
sigma2_est = full_rss / (len(y) - X.shape[1] - 1)

# Calculate RSS + 2 * k * sigma2_est
rss_plus_penalty = [rss + 2 * k * sigma2_est for k, rss, _ in best_models]

# Find the minimum RSS + penalty and corresponding k
min_rss_plus_penalty = min(rss_plus_penalty)
best_k = rss_plus_penalty.index(min_rss_plus_penalty)

# Plot the RSS for all models
plt.figure(figsize=(10, 5))
for k in range(X.shape[1] + 1):
    k_rss = rss_df[rss_df['k'] == k]
    plt.scatter(k_rss['k'], k_rss['RSS'], color='blue', label='All models' if k == 0 else "")
best_rss_df = pd.DataFrame(best_models, columns=['k', 'RSS', 'Features'])
plt.plot(best_rss_df['k'], best_rss_df['RSS'], color='red', label='Best models')
plt.plot(best_rss_df['k'], rss_plus_penalty, color='green', linestyle='dashed', label='Best models + Cp penalty')
plt.axvline(x=best_k, color='orange', linestyle='dashed', label='Best Cp model')
plt.xlabel('Number of predictors (k)')
plt.ylabel('RSS')
plt.title('RSS for Forward Stepwise Selection')
plt.legend()
plt.show()
```

::: {.fragment}
::: {.callout-note}
How many models are run?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בדוגמא שלנו זה ייראה כך. מתחילים כמו קודם במודל עם חותך בלבד, ממשיכים אותו דבר עם p אפשרויות להוספת משתנה אחד, ואז יש רק p - 1 אפשרויות להוספת משתנה שני, וכך הלאה עד אפשרות אחרת בלבד להוספת המשתנה האחרון.

כמו קודם הקו האדום הוא שמחבר בין המודלים הטובים ביותר והקו הירוק המקווקו הוא קריטריון הCp למודלים הטובים ביותר, שבוחר גם הפעם, במודל עם 6 משתנים.

אז לפני שנשווה בין הבחירה עצמה של משתנים -- נשאל כמה מודלים הרצנו כאן? p ועוד p - 1 ועוד p - 2... מי שיחשב את סכום הסדרה הזאת יגיע ל-1 ועוד p(p + 1)/2, כלומר סדר גודל של p בריבוע.
:::
:::

---

### Stepwise regression main disadvantage

| k | Best subset                           | Forward stepwise                    |
|---|---------------------------------------|-------------------------------------|
| 1 | {Rating}                              | {Rating}                            |
| 2 | {Rating, Income}                      | {Rating, Income}                    |
| 3 | {Rating, Income, Student}             | {Rating, Income, Student}           |
| 4 | {Income, Student, Limit, Cards}       | {Rating, Income, Student, Limit}    |

::: {.fragment}
::: {.callout-note}
What happens if $p > n$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אבל ההקלה בסיבוכיות לא באה בלי מחיר, על הגרידיות של האלגוריתם הסטפווייז. כאן אנחנו ממש משווים בין המשתנים עצמם שהבסט סאבסט חוזה מול הפורוורד סטפווייז. עד מודל עם 3 משתנים אין הבדל ביניהם. במודל עם 4 משתנים הבסט סאבסט לוקח בחשבון את כל האפשרויות ומגיע למסקנה שהמודל הכי טוב צריך לזנוח את משתנה הרייטינג, ולהוסיף שני משתנים אחרים. ואילו עבור מודל הפורוורד סטפוויז זאת לא אופציה בכלל! הוא לעולם לא יגיע לאיזור הזה במרחב החיפוש! וזאת יכולה להיות בעיה רצינית של האלגוריתם הזה.

לפני שנעבור לאלגוריתם האחרון בקבוצה, האם אפשר לטפל באמצעות הפורוורד סטפוויז בבעיות ממימד גבוה בהם p גדול מn, כלומר יש יותר משתנים מתצפיות? בודאי, מתחילים עם חותך ועוצרים עבור n - 1 מודלים, כלומר עד שהרגרסיה הליניארית כבר לא פיזיבילית.
:::
:::

---

### Forward [stagewise]{style="color:red;"} selection

0. Standardize all features, input some $\tau_{thresh} \in (0, 1)$ and $\varepsilon > 0$ step size
1. Residual $\mathbf{r} = \mathbf{y} - \bar{y}$, $\beta_1, \dots, \beta_p = 0$
2. Find the predictor $\mathbf{x}_j$ most correlated with $\mathbf{r}$, and let $\tau = Corr(\mathbf{r}, \mathbf{x}_j)$
3. While $|\tau| > \tau_{thresh}$:
    i. Update $\beta_j \leftarrow \beta_j + \delta_j$, where $\delta_j = \varepsilon \cdot \text{sign}(\tau)$
    ii. Update $\mathbf{r} \leftarrow \mathbf{r} - \delta_j\mathbf{x}_j$
    iii. Find the predictor $\mathbf{x}_j$ most correlated with $\mathbf{r}$, and let $\tau = Corr(\mathbf{r}, \mathbf{x}_j)$

::: {.fragment}
::: {.callout-note}
Why would we want to "slow-learn"?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
האלגוריתם האחרון שנלמד נראה בהתחלה יצור מוזר, ולא רק בגלל השם שלו שמזכיר את השם של האלגוריתם הסטפוויז. קוראים לו פורווד סטייג'וויז, והוא מציע לעשות דבר כזה:

נעשה סטנדרטיזציה למשתנים, ניקח איזשהו ערך סף לקורלציה שתיכף נשתמש בו, והכי חשוב - פרמטר אפסילון שייקרא סטפ-סייז, או קצב למידה.

נגדיר את השארית שבחיסור הממוצע של Y מכל התצפיות כR, ונציב אפס בכל מקדמי הרגרסיה.

כעת נמצא את המקדם הJ שיש לו את המתאים הגבוה ביותר עם השארית, כלומר זה המקדם שהכי נכון היה להכניס לרגרסיה, מעבר למודל הקיים.

כל עוד המתאם הזה גדול מאיזשהו ערך סף שאומר מה זה מתאם גבוה, בערך מוחלט, נעדכן את מקדם הרגרסיה של המשתנה הזה. אבל שימו לב לזה, לא נוסיף פשוט את המתאם עצמו, אלא נלך צעד קטן בכיוון שלו דלתא J, שהיא אפסילון בכיוון המתאם. כלומר נוסיף אפסילון לבטא-ג'יי של המשתנה או נחסיר בהתאם לגודל המתאם.

כעת נעדכן את השארית אחרי שחיסרנו ממנה את מה שמידלנו את המשתנה הJ כפול הדלתא J. ונחזור למצוא את המשתנה ש*עכשיו* הכי מתואם עם השארית.

נחזור לבדוק האם המתאם הזה גדול עדיין מספיק לעומת איזשהו ערך סף, וכך הלאה, עד שלא יישאר משתנה שמתואם מספיק עם השארית. המודל הסופי שלנו מגולם במקדמי הרגרסיה שמצאנו.

חשוב מאוד לשים לב! בכל שלב ניתן לבחור שוב משתנה שכבר נבחר! אפילו בשני שלבים רצופים.

אבל למה שנרצה לעשות את זה? יותר מהכל זה גם נראה למידה נורא איטית לא? אם גיליתם שלמשתנה יש קורלציה של 0.9 עם השארית, למה לא להוסיף אותו כמו שהוא לרגרסיה, למה רק אפסילון ממנו?

עוד נחזור לאלגוריתם המעניין הזה, אבל בינתיים נגיד, שמסתבר שלמידה איטית היא דבר טוב מאוד לשיפור שגיאת החיזוי, וזה עולה בקנה אחד עם העיקרון שאיתו פתחנו את השיעור -- לפעמים שווה לא ללכת עיוורים אחרי הדאטא, ולקחת בערבון מוגבל את הכיוון שהוא מוביל אותנו אליו. זה בדיוק מה שאנחנו עושים עם קצב למידה אפסילון.
:::
:::

---

## Regularized Regression: Ridge {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
את רוב השיעור נקדיש לשיטות של רגולריזציה, איך אנחנו ממתנים את המודל, מפקחים עליו ומגבילים אותו. השיטה הותיקה והמפורסמת ביותר, היא רגרסיית רידג'.
:::
:::

---

### Ridge regression

::: {.incremental}
- Instead of reducing the number of parameters $\to$ constrain them, penalize their norm
- With $\ell_2$ norm we get the penalized RSS criterion for some regularization/penalty parameter $\lambda > 0$:
$$PRSS(\lambda) = \sum_{i = 1}^n \left(y_i - \beta_0 + \sum_{j = 1}^p \beta_j x_{ij}\right)^2 + \lambda\sum_{j = 1}^p \beta_j^2 = RSS + \lambda\|\beta^*\|^2_2$$

- Standardize the features in $X$, then:
  - $\hat{\beta}_0 = \bar{y}$
  - $\lambda$ punishes features of different scale comparably
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אנחנו לא עושים משהו שונה מאוד. במקום להפחית את מספר הפרמטרים, אנחנו מגבילים אותם, אנחנו מחפשים אותם במרחב מצומצם יותר. ואת זה אנחנו משיגים על-ידי הוספת עונש, פנאלטי, על הנורמה שלהם.

אם הנורמה היא L2 זה נראה ככה: הקריטריון שלנו הוא פנאלייזד RSS או PRSS, כלומר סכום הריבועים הפחותים הרגיל שאנחנו מכירים, ועוד פרמטר למדא גדול מאפס כפול סכום על הריבוע של מקדמי הרגרסיה. העונש הזה שומר על האומדנים לפרמטרים שלא יהיו גדולים מדי, ותיכף נרחיב על איך בוחרים את למדא.

נשים לב שאת החותך אנחנו לא מענישים כי אין בזה היגיון, לכל Y יש פיזור טבעי ואין סיבה לא להתחיל מהממוצע שלו כמנבא בייסליין. אם נסמן את וקטור המקדמים בטא בלי בטא-אפס כבטא כוכב נקבל שאפשר לרשום את הקריטריון שלנו כRSS ועוד למדא כפול הנורמה בריבוע של וקטור בטא-האט.

שימו-לב שצריך לעשות סטנדרטיזציה של המשתנים, כלומר לחסר מכל עמודה של X את הממוצע ולחלק בסטיית התקן. אם מקפידים על זה אפשר לאמוד את בטא-אפס כממוצע של Y. ואז נקבל גם שלמדא משפיע בצורה דומה על כל המשתנים. אחרת שימוש בפרמטר למדא יהיה בעייתי, כי הוא יכול להיות מושפע מאוד ממשתנה ספציפי עם סקאלה מאוד רחבה.
:::
:::

---

### Ridge solution

$$PRSS(\lambda) = \|y - X\beta\|^2_2 + \lambda\|\beta\|^2_2$$

::: {.fragment}
$\frac{\partial PRSS}{\partial \beta} = -2X'y + 2X'X\beta +2\lambda\beta$
:::

::: {.fragment}
$2X'y - 2X'X\beta - 2\lambda\beta = 0$
:::
::: {.fragment}
$(X'X + \lambda I_p)\beta = X'y$
:::
::: {.fragment}
$\hat{\beta}(\lambda) = (X'X + \lambda I_p)^{-1}X'y$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נניח שאנחנו יודעים את למדא. איך נראה הפתרון של רידג'?

בואו נכתוב את הPRSS בצורה וקטורית, כאן כבר ויתרנו על הסימון בטא-כוכב, בטא לא כולל את החותך בטא-אפס.

אנחנו עושים בדיוק מה שעשינו ברגרסיה ליניארית, כלומר גוזרים את הPRSS לפי וקטור בטא, משווים לאפס, ומחלצים את בטא.

הפתרון שקיבלנו מזכיר מאוד את הפתרון של רגרסיה ליניארית, ואפשר כמובן לוודא שהוא נקודת מינימום. ההבדל הוא במטריצה שאנחנו הופכים, כאן נוסף לה הפרמטר למדא הקטן על האלכסון, ומכאן השם רידג', רכס באנגלית.
:::
:::

---

### Ridge (original) justification

$$\hat{\beta}(\lambda) = (X'X + \lambda I_p)^{-1}X'y$$

::: {.incremental}
- Numerical stability:
  - If $X$'s columns are correlated, $X'X$ is ill-conditioned and its inverse is unstable, variance of $\hat{\beta}$ is high -- but $X'X + \lambda I_p$ improves on this
- Feasibility:
  - If $X$'s columns are linearly dependent $X'X$ is not invertible but $X'X + \lambda I_p$ is!
  - If $p > n$ -- same!
- Guaranteed prediction error reduction for some $\lambda$:
  - Bias increases but variance decreases *more*
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מה זה נותן לנו, הרידג' הזה על האלכסון? מסתבר שלא מעט, כאן אנחנו נוגעים בהצדקה המקורית לשיטה הזאת.

קודם כל, למי שיש ניסיון בהפיכה של מטריצות סימטריות כמו X'X, יודע שלהוסיף למדא קטן לאלכסון של המטריצה עוזר ליציבות נומרית.

ספציפית כאן, כפי שראינו הסכנה היא שיש קורלציה בין העמודות של X, אז אנחנו קוראים למטריצה X'X איל קונדישונד, ההופכי שלה לא יציב מה שעלול להגדיל מאוד את שונות המקדמים כמו שדיברנו בשיעור על רגרסיה ליניארית. וההוספה הזאת של קבוע חיובי קטן לאלכסון של המטריצה הזאת משפרת מאוד את התופעה הזאת.

חוץ מזה כמו שאמרנו במצב שבו העמודות של X תלויות ליניארית, או במצב שיש יותר משתנים מתצפיות ההופכי של X'X בכלל לא קיים, ורגרסיית רידג' בכלל מאפשרת לנו פתרון לבעיה.

אבל אולי ההצדקה הכי מעניינת לרידג' זה גרנטי שיש לנו, בלי הוכחה כרגע, לשיפור טעות החיזוי הריבועית. האומד שלנו בטא-האט שתלוי בלמדא כבר לא חסר הטייה, אנחנו משלמים קצת בהטייה אבל מקבלים הפחתה ניכרת בשונות, ובסך הכל הפחתה בשגיאה הריבועית.
:::
:::

---

### Choosing $\lambda$

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Define lambdas for Ridge regression
lambdas = np.logspace(-2, 3, 100)

# Cross-validation setup
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# Variables to store results
train_errors = []
test_errors = []
ridge_coefficients = []

# Regular linear regression for comparison
lr_model = LinearRegression()
lr_train_errors = []
lr_test_errors = []

# Ridge regression with cross-validation
for alpha in lambdas:
    ridge = make_pipeline(StandardScaler(), Ridge(alpha=alpha))
    
    fold_train_errors = []
    fold_test_errors = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        ridge.fit(X_train, y_train)
        y_train_pred = ridge.predict(X_train)
        y_test_pred = ridge.predict(X_test)
        
        fold_train_errors.append(((y_train - y_train_pred) ** 2).mean())
        fold_test_errors.append(((y_test - y_test_pred) ** 2).mean())

    train_errors.append(np.mean(fold_train_errors))
    test_errors.append(np.mean(fold_test_errors))
    ridge_coefficients.append(ridge.named_steps['ridge'].coef_)

# Fit regular linear regression and calculate mean train and test errors
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    scale = StandardScaler()
    X_train = scale.fit_transform(X_train)
    X_test = scale.transform(X_test)
    lr_model.fit(X_train, y_train)
    lr_train_errors.append(((y_train - lr_model.predict(X_train)) ** 2).mean())
    lr_test_errors.append(((y_test - lr_model.predict(X_test)) ** 2).mean())

mean_lr_train_error = np.mean(lr_train_errors)
mean_lr_test_error = np.mean(lr_test_errors)

# Find the best lambda
best_lambda_index = np.argmin(test_errors)
best_lambda = lambdas[best_lambda_index]
max_lam = 10

# Plot the results
plt.figure(figsize=(10, 5))

# Plot 1: Training and test errors vs lambda
plt.subplot(1, 2, 1)
plt.plot(lambdas[lambdas <= max_lam], np.array(train_errors)[lambdas <= max_lam], label='Train Error', color='blue')
plt.plot(lambdas[lambdas <= max_lam], np.array(test_errors)[lambdas <= max_lam], label='Test Error', color='red')
plt.axhline(y=mean_lr_train_error, color='blue', linestyle='dashed', label='OLS Train Error')
plt.axhline(y=mean_lr_test_error, color='red', linestyle='dashed', label='OLS Test Error')
plt.axvline(x=best_lambda, color='green', linestyle='dashed', label='Best Lambda')
plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Mean Squared Error')
plt.title('Ridge Regression: 5-Fold CV Mean MSE')
plt.legend()

# Plot 2: Ridge coefficients profile
plt.subplot(1, 2, 2)
ridge_coefficients = np.array(ridge_coefficients)
for i in range(ridge_coefficients.shape[1]):
    plt.plot(lambdas, ridge_coefficients[:, i], label=X.columns[i])
plt.axvline(x=best_lambda, color='green', linestyle='dashed', label='Best Lambda')
plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Coefficient Value')
plt.title('Ridge Coefficients Profile')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה את רגרסיית רידג' בפעולה על הנתונים שלנו ועל הדרך נדגים גם איך בוחרים את הפרמטר למדא.

הפרמטר למדא הוא מה שקרוי היפרפרמטר, צריך לבחור אותו באמצעות בדיקה של אפשרויות שונות, בדרך כלל עם קרוס ולידיישן, ולראות איזו מהן נותנת את שגיאת החיזוי הטובה ביותר על מדגמים שהמודל לא ראה. כאן אנחנו בודקים את זה עם קרוס ולידיישן עם חמישה פולדים ומדווחים את ממוצע הMSE על הטריין ועל הטסט. אנחנו רואים כאן כמה דברים:

שבאופן צפוי השגיאה על הטריין נמוכה יותר מאשר השגיאה על הטסט. שהפרמטר למדא הטוב ביותר שמביא למינימום את השגיאה על הטסט הוא קצת פחות מ10 בחזקת 0 כלומר קצת פחות מ1. והכי חשוב, אנחונ רואים שאם אנחנו משווים לשגיאה של רגרסיה ליניארית רגילה, OLS, שהיא לא תלויה בלמדא בכלל כמובן ומופיעה כאן במקווקו, עבור הלמדא הנכון רידג' אכן מצליח להגיע לירידה בMSE ניכרת.

מצד ימין, אנחנו מראים איך נראים המקדמים הנאמדים של 11 המשתנים שלנו, בהתאם ללמדא. כשהלמדא הוא אפסי, אין ענישה בכלל כל אומדני המקדמים והם מקבלים את הערכים שהם היו מקבלים ברגרסיה רגילה. באופן צפוי, ככל שלמדא גדול יותר, העונש על הנורמה של וקטור בטא גדול יותר והאילוץ חמור יותר ומאפשר פחות ופחות חופש למודל והמקדמים הנאמדים נעשים קטנים יותר, הם מתכווצים לכיוון אפס. לתופעה כזאת של כיווץ אנחנו קוראים שרינקג', אפשר לראות שהיא עשויה מאוד לסייע להקטין את שגיאת החיזוי ונדבר עליה עוד בהמשך.
:::
:::

---

## Regularized Regression: Lasso {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נדבר עכשיו על שיטה אחרת ותיקה לעשות רגולריזציה למקדמי הרגרסיה. גם השיטה הזאת, כמו רידג' היא בעלת השלכות ושימושים נרחבים הרבה יותר מרגרסיה ליניארית בלבד, והיא נקראת לאסו.
:::
:::

---

### Lasso regression

$$PRSS(\lambda) = \sum_{i = 1}^n \left(y_i - \beta_0 + \sum_{j = 1}^p \beta_j x_{ij}\right)^2 + \lambda\sum_{j = 1}^p |\beta_j| = RSS + \lambda\|\beta^*\|$$

- As with Ridge, standardize the features in $X$, then:
  - $\hat{\beta}_0 = \bar{y}$
  - $\lambda$ punishes features of different scale comparably

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ברגרסיית לאסו, אנחנו גם מענישים הנורמה של מקדמי הרגרסיה כמו ברידג', אלא שכאן אנחנו מענישים את הנורמה L1, כלומר סכום המקדמים בערך מוחלט.

גם כאן בטא-אפס לא תחת אילוץ ואם אנחנו עושים סטנדרטיזציה של המשתנים אפשר לאמוד אותו כממוצע Y.

לקריטריון שלפנינו אין פתרון סגור. תיכף נגיד כמה מילים על איך פותרים את בעיית הלאסו. אבל קודם כל, למה שנרצה להעניש דווקא את נורמה L1 ובמה זה שונה מרידג'.
:::
:::

---

### The Lasso Path

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso, LinearRegression
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Define lambdas for Lasso regression
lambdas = np.logspace(-2, 3, 100)

# Cross-validation setup
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# Variables to store results
train_errors = []
test_errors = []
lasso_coefficients = []

# Regular linear regression for comparison
lr_model = LinearRegression()
lr_train_errors = []
lr_test_errors = []

# Lasso regression with cross-validation
for alpha in lambdas:
    lasso = make_pipeline(StandardScaler(), Lasso(alpha=alpha, max_iter=10000))
    
    fold_train_errors = []
    fold_test_errors = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        lasso.fit(X_train, y_train)
        y_train_pred = lasso.predict(X_train)
        y_test_pred = lasso.predict(X_test)
        
        fold_train_errors.append(((y_train - y_train_pred) ** 2).mean())
        fold_test_errors.append(((y_test - y_test_pred) ** 2).mean())
    
    train_errors.append(np.mean(fold_train_errors))
    test_errors.append(np.mean(fold_test_errors))
    lasso_coefficients.append(lasso.named_steps['lasso'].coef_)

# Fit regular linear regression and calculate mean train and test errors
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    scale = StandardScaler()
    X_train = scale.fit_transform(X_train)
    X_test = scale.transform(X_test)
    lr_model.fit(X_train, y_train)
    lr_train_errors.append(((y_train - lr_model.predict(X_train)) ** 2).mean())
    lr_test_errors.append(((y_test - lr_model.predict(X_test)) ** 2).mean())

mean_lr_train_error = np.mean(lr_train_errors)
mean_lr_test_error = np.mean(lr_test_errors)

# Find the best lambda
best_lambda_index = np.argmin(test_errors)
best_lambda = lambdas[best_lambda_index]
max_lam = 10

# Plot the results
plt.figure(figsize=(10, 5))

# Plot 1: Training and test errors vs lambda
plt.subplot(1, 2, 1)
plt.plot(lambdas[lambdas <= max_lam], np.array(train_errors)[lambdas <= max_lam], label='Train Error', color='blue')
plt.plot(lambdas[lambdas <= max_lam], np.array(test_errors)[lambdas <= max_lam], label='Test Error', color='red')
plt.axhline(y=mean_lr_train_error, color='blue', linestyle='dashed', label='OLS Train Error')
plt.axhline(y=mean_lr_test_error, color='red', linestyle='dashed', label='OLS Test Error')
plt.axvline(x=best_lambda, color='green', linestyle='dashed', label='Best Lambda')
plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Mean Squared Error')
plt.title('Lasso Regression: 5-Fold CV Mean MSE')
plt.legend()

# Plot 2: Lasso coefficients profile
plt.subplot(1, 2, 2)
lasso_coefficients = np.array(lasso_coefficients)
for i in range(lasso_coefficients.shape[1]):
    plt.plot(lambdas, lasso_coefficients[:, i], label=X.columns[i])
plt.axvline(x=best_lambda, color='green', linestyle='dashed', label='Best Lambda')
plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Coefficient Value')
plt.title('Lasso Coefficients Profile')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.tight_layout()
plt.show()
```

::: {.incremental}
- Lasso's selling point: *sparsity*!
- Highly useful, especially for $p > n$ datasets
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נעיף קודם מבט על שגיאת החיזוי. אפשר לראות שעבור הנתונים שלנו לאסו לא מאוד הועיל. הלמדא שהביא למינימום את שגיאת החיזוי על מדגמי הטסט בקרוס ולידיישן הוא למדא אפסי, ולכן האומדנים למקדמי הרגרסיה שהתקבלו עבור הלמדא האופטימלי הם כמעט ללא שינוי מרגרסיה ליניארית רגילה.

נראה שגם המקדמים של לאסו עוברים שרינקג' אבל הדפוס שבו הם מתכווצים הוא מאוד ייחודי. בעוד שברידג' המקדמים קטנים וקטנים לכיוון אפס, בלאסו בנקודה מסוימת מובטח לנו שהמקדמים מתאפסים, ממש. אפשר לראות כאן שזה קורה אחד אחרי השני.

התופעה הזאת היא ההצדקה המרכזית של שימוש דווקא ברגרסיית לאסו, באנגלית לאסו מביא למודלים ספרסיים עם מעט משתנים, בעברית אנחנו קוראים לזה מודלים דלילים.

כאן אנחנו לא רואים את היתרון של זה, מדובר בסך הכל ב11 משתנים, אבל הדבר יעיל מאוד ברגרסיה לנתונים ממימד גבוה כשP מספר המשתנים גדול מאוד ואולי אפילו גדול ממספר המשתנים. במקרה כזה לא סביר שכל המשתנים שונים באופן מהותי מאפס, ושימוש בלאסו משיג לנו מודל דליל, שהוא לא רק יותר אינטרפרטבילי אלא פעמים רבות מביא לשגיאת חיזוי נמוכה יותר כי הוא נמנע מאוברפיטינג.
:::
:::

---

### Lasso solution

- Quadratic Programming, LARS
- But also, surprisingly:

::: {.fragment}
```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import KFold

# Define the ForwardStagewiseRegression class
class ForwardStagewiseRegression:
    def __init__(self, n_iter=1000, step_size=0.01):
        self.n_iter = n_iter
        self.step_size = step_size
        self.coef_ = None
        self.coef_history_ = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.coef_ = np.zeros(n_features)
        self.coef_history_ = np.zeros((self.n_iter, n_features))
        self.intercept_ = np.mean(y)
        residuals = y - self.intercept_

        for i in range(self.n_iter):
            correlations = np.dot(X.T, residuals)
            best_feature = np.argmax(np.abs(correlations))
            direction = np.sign(correlations[best_feature])
            self.coef_[best_feature] += self.step_size * direction
            residuals = y - (np.dot(X, self.coef_) + self.intercept_)
            self.coef_history_[i] = self.coef_

        return self

    def predict(self, X):
        return np.dot(X, self.coef_) + self.intercept_

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Forward Stagewise Regression
n_iter = 20000
step_size = 0.1
fsr_model = ForwardStagewiseRegression(n_iter=n_iter, step_size=step_size)
fsr_model.fit(X_scaled, y)

# Lasso Regression with cross-validation
lambdas = np.logspace(-0.5, 2.6, 100) #np.linspace(10e-3, 10e3, 1000)
kf = KFold(n_splits=5, shuffle=True, random_state=1)
lasso_coefficients = []

for alpha in lambdas:
    lasso = make_pipeline(StandardScaler(), Lasso(alpha=alpha, max_iter=10000))
    fold_coefficients = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        lasso.fit(X_train, y_train)
        fold_coefficients.append(lasso.named_steps['lasso'].coef_)
    
    lasso_coefficients.append(np.mean(fold_coefficients, axis=0))

lasso_coefficients = np.array(lasso_coefficients)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=True)

# Plot 1: Lasso coefficients profile
for i in range(X.shape[1]):
    ax1.plot(lambdas, lasso_coefficients[:, i], label=X.columns[i])
ax1.set_xscale('log')
ax1.set_xlabel('Lambda')
ax1.set_ylabel('Coefficient Value')
ax1.set_title('Lasso Coefficients Profile')

# Plot 2: Forward Stagewise Regression coefficients profile
for i in range(X.shape[1]):
    ax2.plot(range(n_iter)[::-1], fsr_model.coef_history_[:, i], label=X.columns[i])
ax2.set_xlabel('Iteration')
ax2.set_ylabel('Coefficient Value')
ax2.set_title('Forward Stagewise Coefficient Path')
ax2.set_xticks(ticks=range(0, n_iter, n_iter//5))
ax2.set_xticklabels(range(n_iter, 0, -n_iter//5))
ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.tight_layout()
plt.show()
```

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בחלק הבא ננסה לתת תחושה מה עומד מאחורי המודלים הספרסיים שלאסו נותן. לפני זה נגיד כמה מילים על איך פותרים את בעיית לאסו, כי כפי שאמרנו פתרון סגור אין.

כשרק נוסחה הבעיה של לאסו, המחברים הצליחו להציג אותה כבעית אופטימיזציה די סטנדרטית, שמצריכה תכנות ריבועי או קוואדרטיק פרוגרמינג. מאוחר יותר נמצא פתרון יעיל יותר שנקרא least angle regression, או לארס, שלקח השראה מהתופעה הזאת שאפשר להוכיח, שכשלמדא הולך וקטן כל פעם נכנס משתנה אחר לרגרסיה, והמקדם שלו גדל בצורה ליניארית בקירוב, עד שנכנס המשתנה הבא.

אבל מה בעצם אמרנו כאן? איזה עוד אלגוריתם מכניס בזהירות כל פעם משתנה בצעד קטן עד שיימצא משתנה אחר שהכי מתואם עם Y שצריך להכניס לרגרסיה?

אני מדבר כמובן על אלגוריתם הפורוורד סטייג'וויז. אם ניקח אפסילון מספיק קטן, אפשר להראות שאלגוריתם הסטייג'וויז פותר בקירוב את בעיית לאסו. כאן אנחנו מראים את הדמיון המפתיע בין לאסו לפורוורד סטייג'וויז על הנתונים שלנו, באמצעות השוואה של הפרופילים של המקדמים.

נדגיש לסיום שפורוורד סטייג'וויז נחשבת לשיטה עם שגיאת חיזוי איכותית אבל היא מאוד איטית, ככה המחשב לא פותר את בעיית הלאסו אלא באמצעות שיטות אופטימיזציה. זה עדיין מעניין לראות את הקשר בין השיטות השונות ולנסות להבין מה משותף ומה שונה בהן, ועם הקו הזה נמשיך בחלקים הבאים.
:::
:::

---

## On Shrinkage and Sparsity {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
האם אנחנו יכולים להגיד עוד על האופן שבו המקדמים מתכווצים ברידג' ומתדללים בלאסו? האמת היא שיש הרבה עבודות בתחום, ומעניין לעבור על כמה תובנות.
:::
:::

---

### Dual criteria

There is 1-to-1 correspondnece between Ridge and Lasso's previous criteria and:

::: {.fragment}
- Ridge: $\min_{\beta} RSS \text{  s.t.  } \sum_{j = 1}^p\beta_j^2 \le s$
- Lasso: $\min_{\beta} RSS \text{  s.t.  } \sum_{j = 1}^p|\beta_j| \le s$
:::

::: {.fragment}
In this contest we can also write for Best Subset regression:

- $\min_{\beta} RSS \text{  s.t.  } \sum_{j = 1}^p\mathbb{1}\left[\beta_j \neq 0\right] \le s$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ראשית מעניין לציין שגם את רידג' וגם את לאסו ניתן לכתוב בצורה דואלית מעט שונה: ברידג' אנחנו רוצים להביא את הRSS למינימום כפוף לזה שסכום הריבועים של המקדמים או הנורמה L2 בריבוע, קטן או שווה לאיזשהו S.

בלאסו אנחנו רוצים להביא למינימום את הRSS באילוץ על סכום המקדמים בערך מוחלט שיהיה לכל היותר S.

אלה לא ניסוחים שונים כאמור, אפשר להראות שיש מיפוי 1 ל1 בין פרמטר למדא בבעיה המקורית לפרמטר S מתאים בבעיה הדואלית.

אבל הניסוח הזה מראה בצורה ברורה יותר למה יש שרינקג' ולכן השיטות האלה נקראות גם שיטות שרינקג'. הפרמטר S הוא כמו תקציב שהמקדמים צריכים לעמוד בו. גם מאוד ברור ככה למה כדאי לעשות סטנדרטיזציה על המשתנים, אם לא כל ה"תקציב" יכול להתבזבז על משתנה אחד עם סקאלה רחבה.

יותר מזה, צורת הכתיבה הזאת מאפשרת לנו גם לרשום את הקריטריון שבסט סאבסט מנסה להציג בכל אחד מהשלבים שלו: מינימום RSS כפוף לכך שלא יותר מS מקדמים יהיו שונים מאפס. אבל זו בעיה קומבינטורית די קשה לפתרון כמו שראינו. דרך אחרת זה לנסות להשיג את המינימום הזה עם שיטות פורוורד או בקוורד סטפווייז שראינו. דרך אחרת כמו שאנחנו רואים כאן היא לעשות רילקסציה על הקריטריון הקשה הזה. כלומר אנחנו יכולים לחשוב על רידג' ולאסו, כאלטרנטיבות לבסט סאבסט, שמציבות קריטריון קל יותר להשגה, כמו שקורה אגב בהרבה בעיות אחרות באופטימיזציה.
:::
:::

---

### Seeing Shrinkage and Sparsity (I)

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Define the grid for beta values
beta1 = np.linspace(-2, 2, 400)
beta2 = np.linspace(-2, 2, 400)
B1, B2 = np.meshgrid(beta1, beta2)

# Define the RSS function with identical contours for both lasso and ridge
RSS = (B1 - 1.0)**2 / 1 + (B2 - 1.57)**2 / 0.25

# Create the figure and subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

# Lasso constraint plot
ax1.contour(B1, B2, RSS, levels=[1.0, 2.0], colors='red')
lasso_constraint = plt.Polygon([[-1.07, 0], [0, 1.07], [1.07, 0], [0, -1.07]], color='cyan', alpha=0.5)
ax1.add_patch(lasso_constraint)
ax1.plot(1.0, 1.57, 'ko')  # Point representing the unbiased beta_hat
ax1.text(1.1, 1.67, r'$\hat{\beta}$', fontsize=12, color='black')
ax1.axhline(0, color='black', linewidth=1)
ax1.axvline(0, color='black', linewidth=1)
ax1.set_xlim([-2, 2])
ax1.set_ylim([-2, 2])
ax1.set_title('Lasso Constraint')
ax1.set_xlabel(r'$\beta_1$')
ax1.set_ylabel(r'$\beta_2$')

# Ridge constraint plot
ax2.contour(B1, B2, RSS, levels=[1.0, 2.0], colors='red')
circle = plt.Circle((0, 0), 1, color='cyan', alpha=0.5)
ax2.add_artist(circle)
ax2.plot(1.0, 1.57, 'ko')  # Point representing the unbiased beta_hat
ax2.text(1.1, 1.67, r'$\hat{\beta}$', fontsize=12, color='black')
ax2.axhline(0, color='black', linewidth=1)
ax2.axvline(0, color='black', linewidth=1)
ax2.set_xlim([-2, 2])
ax2.set_ylim([-2, 2])
ax2.set_title('Ridge Constraint')
ax2.set_xlabel(r'$\beta_1$')
ax2.set_ylabel(r'$\beta_2$')

plt.tight_layout()
plt.show()

```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נמשיך לפתח את זה. נאמר ויש לנו רגרסיה עם שני מקדמים בטא אחת ובטא שתיים. מה אומר האילוץ של רידג'? בטא אחת בריבוע ועוד בטא שתיים בריבוע צריך להיות קטן מאיזשהו S. כלומר יש איפשהו בטא-האט שמתאים למינימום RSS, ואנחנו מחפשים נקודה קצת ליד שנמצאת בתוך המעגל הזה, שעומדת בתקציב שלנו. כאן אנחנו רואים את הנקודה הזאת בחיתוך בין הקונטורים של RSS סביב בטא-האט הכי טובה, ברגע שהם נוגעים בתקציב. עכשיו התקציב בתלת-מימד יהיה כדור, בp מימד זה יהיה הייפרספיר.

לעומת זאת איך נראה האילוץ בלאסו? בטא אחת בערך מוחלט ועוד בטא שתיים בערך מוחלט קטן או שווה לאיזשהו תקציב S, זה יוצר לנו צורה של מעוין, עם שפיצים. בתלת מימד זה יהיה מעין יהלום, וככל שנעלה במימד נראה שזאת צורה עם הרבה שפיצים. כלומר כאן יש נטייה לקונטורים של הRSS להיפגש עם שפיצים של מעוין או יהלום. ומה המשמעות של לקבל פתרון באחד השפיצים? בדוגמא הזאת זה אומר שהמקדם בטא אחת מתאפס! מה שלא סביר שיקרה עם האילוץ של רידג'.

כלומר אם אנחנו מתאמצים אנחנו רואים גיאומטרית איך קורה שרינקג' לצפות למודלים דלילים עם לאסו.
:::
:::

---

### Seeing Shrinkage and Sparsity (II)

If $X$ has orthonormal columns ($X'X = I_p$) and $\hat{\beta}_j$ is the OLS estimator:

| Estimator              | $\tilde{\beta}_j$                                                                 |
|------------------------|-----------------------------------------------------------------------------------|
| Best $M$ subset        | $\hat{\beta}_j \cdot \mathbb{1}\left[|\hat{\beta}_j| \ge  |\hat{\beta}_{(M)}|\right]$ |
| Ridge                  | $\hat{\beta}_j / (1 + \lambda)$                                                   |
| Lasso                  | $\text{sign}(\hat{\beta}_j)(|\hat{\beta}_j| - \lambda)_+$                         |

::: {.fragment}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Generate OLS estimates
beta_ols = np.linspace(-3, 3, 400)

# Best subset estimator
M = 0.5
beta_best_subset = beta_ols * (np.abs(beta_ols) >= M)

# Ridge estimator
lambda_ridge = 0.5
beta_ridge = beta_ols / (1 + lambda_ridge)

# Lasso estimator
lambda_lasso = 0.5
beta_lasso = np.sign(beta_ols) * np.maximum(np.abs(beta_ols) - lambda_lasso, 0)

# Create the figure and subplots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(5 * 3, 3.5))

# Plot for best subset estimator
ax1.plot(beta_ols, beta_ols, color='blue', alpha=0.5)  # 45-degree line (OLS estimate)
ax1.plot(beta_ols, beta_best_subset, 'r--')  # Best subset estimate
ax1.axhline(0, color='gray', linewidth=0.5)
ax1.axvline(0, color='gray', linewidth=0.5)
ax1.set_title('Best Subset')
ax1.set_xlabel(r'$\hat{\beta}_j$')
ax1.set_ylabel(r'$\tilde{\beta}_j$')

# Plot for ridge estimator
ax2.plot(beta_ols, beta_ols, color='blue', alpha=0.5)  # 45-degree line (OLS estimate)
ax2.plot(beta_ols, beta_ridge, 'r--')  # Ridge estimate
ax2.axhline(0, color='gray', linewidth=0.5)
ax2.axvline(0, color='gray', linewidth=0.5)
ax2.set_title('Ridge')
ax2.set_xlabel(r'$\hat{\beta}_j$')

# Plot for lasso estimator
ax3.plot(beta_ols, beta_ols, color='blue', alpha=0.5)  # 45-degree line (OLS estimate)
ax3.plot(beta_ols, beta_lasso, 'r--')  # Lasso estimate
ax3.axhline(0, color='gray', linewidth=0.5)
ax3.axvline(0, color='gray', linewidth=0.5)
ax3.set_title('Lasso')
ax3.set_xlabel(r'$\hat{\beta}_j$')

plt.tight_layout()
plt.show()
```

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ואנחנו יכולים לשחק עם זה בצורה אפילו יותר מתוחכמת.

אם נניח שהעמודות של X הן אורתונורמליות, כלומר X'X שווה למטריצת היחידה, ונסמן את בטא-האט כאומד הOLS הקלאסי. אנחנו ממש יכולים לקבל נוסחה לכל מקדם בטא-J ולראות מתמטית גם את השרינקג', גם את הדלילות.

נצייר את הנוסחאות שרשומות כאן כדי לראות את זה טוב יותר.

ברגרסיית בסט סאבסט שבה אנחנו מחפשים M משתנים הכי טובים, ונסמן את המקדם הכי קטן כבטא-האט M, המקדם בטא-J לא נבחר או מתאפס כל עוד הוא קטן מהמקדם הזה. ברגע שיעבור אותו, יופיע כפי שהוא.

ברגרסיית רידג' הנה השרינקג', ככל שלמדא יהיה גדול יותר כך נכווץ את בטא-J המקורי יותר, לכיוון קו האפס, אבל למדא צריך להיות ממש גדול כדי ממש לאפס את המקדם הזה. אפשר גם לראות שהשרינקג' לא אחד לכל מקדם. אם מקדם גדול אז כיווץ פי 1 ועוד למדא משמעותי יותר מאשר אם המקדם קטן.

לבסוף ברגרסיית לאסו הסימון הזה אומר המקסימום בין הביטוי בסוגריים לבין אפס. כלומר עם בטא J בערך מוחלט לא גדול מלמדא הוא יקבל אפס. ואם הוא גדול אז נפחית ממנו את למדא, בצורה קבועה לא משנה מה הגודל שלו. אנחנו רואים גם את הדלילות וגם את השרינקג'.

נגיד שבעזרת אלגברה קצת יותר מסובכת אפשר להכליל את ההמחשות האלה גם למצבים מורכבים יותר, אבל אנחנו נעצור כאן, ונעבור לאפילו הצדקה נוספת לשימוש ברידג' ובלאסו.
:::
:::

---

## Bayesian Viewpoint {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נקודת מבט נוספת על רידג' ולאסו מגיעה מכיוון פחות צפוי, של סטטיסטיקה בייזיאנית. אנחנו נראה שהאומדים של רידג' ולאסו הם בדיוק מה שמתקבל עבור המודל הליניארי, תחת ציפיות מסוימות עוד לפני שראינו את הנתונים. אבל קודם נעשה הקדמה קצרה לגבי סטטיסטיקה בייזיאנית, נושא שאפשר לבלות רק עליו סמסטר שלם.
:::
:::

---

### Conditional Distribution

::: {.incremental}
- Recall Bayes Rule:
$$P(B | A) = \frac{P(A | B) \cdot P(B)}{P(A)} \text{     or     } \text{posterior} = \frac{\text{likelihood}\cdot\text{prior}}{\text{evidence}}$$

- For continuous distributions:
$$f_{Y|X}(y|x) = \frac{f_{X|Y}(x|y)f_Y(y)}{f_X(x)} \propto f_{X|Y}(x|y)f_Y(y)$$

- E.g. For normal distribution, if $X \sim \mathcal{N}(\mu_X, \sigma^2_X), Y \sim \mathcal{N}(\mu_Y, \sigma^2_Y), \rho = Corr(X, Y)$:
$$Y | X = x \sim \mathcal{N}\left(\mu_Y + \frac{\sigma_Y}{\sigma_X}\rho(x - \mu_X), (1 - \rho^2)\sigma_Y^2\right)$$

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בבסיס של סטטיסטיקה בייזיאנית עומד כלל בייז. ההסתברות למאורע B בהינתן שאני יודע שמאורע A קרה, היא הסתברות החיתוך חלקי הסתברות המאורע המתנה. והסתברות החיתוך ניתן לרשום כהסתברות המאורע B השולית, כפול ההסתברות המותנית ההפוכה של מאורע A בהינתן מאורע B. כלומר כלל בייז מקשר לנו בין ההסתברות האפריורית שהנחנו על מאורע B, לבין ההסתברות הפוסטריורית, בהינתן שאנחנו יודעים שמאורע A קרה. אנחנו נגיד שההסתברות האפוסטריורית שווה למכפלת ההסתברות האפריורית בסבירות או לייקליהוד לראות את מאורע A בהינתן מאורע B, חלקי ההסתברות השולית לראות את מאורע A או האבידנס.

עבור התפלגויות רציפות אין לנו פונקצית הסתברות יש לנו פונקצית צפיפות אבל העיקרון דומה. הצפיפות המותנית של משתנה Y בהינתן שראינו מהו משתנה X, היא הצפיפות השולית של Y כפול הצפיפות המותנית של X בהינתן Y, חלקי הצפיפות השולית של X. הרבה פעמים נרשום שהצפיפות המותנית היא פרופורציונלית למכפלת הצפיפות האפריורית והצפיפות המותנית ההפוכה. לא תמיד אנחנו מצליחים לזהות את ההתפלגות המותנית כהתפלגות שמוכרת לנו אבל הרבה פעמים כן.

למשל, אם X מתפלג בצורה שולית נורמלית עם תוחלת ושונות משלו, ו-Y מתפלג בצורה שולית נורמלית עם תוחלת ושונות משלו, הקורלציה ביניהם מסומנת ברו ואני יודע שX קרה, הוא שווה לX קטן. אז כעת אם אני ארשום את הצפיפות המותנית של Y בהינתן X לפי ההגדרה כאן ואעשה קצת אלגברה, אני אזהה בסופו של דבר שההתפלגות של Y בהינתן X היא התפלגות שמוכרת לי. היא עדיין התפלגות נורמלית, רק שהתוחלת והשונות האפוסטריורים התעדכנו. למשל התוחלת היא התוחלת המקורית של Y ועוד תוספת שתלויה ביחס בין סטיות התקן, בגודל וסימן של רו, ובמרחק של X שהתקבל מהתוחלת שלו. מעניין לציין שהשונות של Y בהינתן X, אלא אם כן רו שווה לאפס, בהכרח קטנה, מה שעושה שכל, כי צפיתי כבר במשתנה שמתואם עם Y, הציפיות שלי מתעדכנות ויש הרבה פחות שונות לערכים שY יכול לקבל.
:::
:::

---

### Bayesian Statistics

::: {.incremental}
- In Bayesian statistics a parameter $\theta$ isn't a *fixed* number but a RV having a [prior]{style="color:red;"} distribution $P(\theta)$
- In comes data sample $Y = y_1, \dots, y_n$ with distribution $P(Y | \theta)$ (likelihood)
- We calculate the [posterior]{style="color:red;"} distribution given the data $P(\theta|Y) \propto P(Y|\theta)P(\theta)$
- $\theta$'s final estimate is the mean or mode of $P(\theta|Y)$
:::

::: {.fragment}
::: {.callout-note}
Why would we take this approach?
:::
:::

::: {.fragment}
- For linear regression and normal prior:
    - Prior: $\beta \sim \mathcal{N}(0, \tau^2I_p)$
    - Data: $Y | \beta \sim \mathcal{N}(X\beta, \sigma^2I_n)$
    - Posterior: $\beta|Y \sim \mathcal{N}\left((X'X + \frac{\sigma^2}{\tau^2}I_p)^{-1}X'y, \Sigma\right)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בסטטיסטיקה בייזיאנית באופן כללי, לאו דוקא בבעיות רגרסיה, הפרמטר שאנחנו מנסים לאמוד תטא, הוא כבר לא פרמטר. הוא משתנה מקרי, שיש לו התפלגות אפריורית, פי של תטא. אם הוא רציף כמובן נדבר על צפיפויות, נהוג לסמן פי של תטא באופן כללי.

כעת מגיע משתנה אחר שאנחנו רואים, והוא מדגם מקרי של נתונים Y. למדגם הזה יש הסתברות מותנית או לייקליהוד.

וכעת, אחרי שראינו את הנתונים, אנחנו רוצים לעדכן את הידע שלנו לגבי תטא, את ההתפלגות האפוסטריורית של תטא בהינתן הנתונים, שהיא כאמור פרופורציונלית ללייקליהוד כפול הפריור. בדרך כלל נשתמש בפריור ובלייקליהוד נוחים ככה שנוכל לזהות את ההתפלגות האפוסטריורית בקלות.

ואם אנחנו רוצים לאמוד את תטא, נשתמש בתוחלת של ההתפלגות הזאת, או בשכיח.

עכשיו כדאי לשאול למה לעשות את זה, בפרט שצורת החשיבה הזאת מכריחה אותנו להוסיף עוד הנחות וציפיות, שלא ברור בכלל שיש לנו. והתשובה כמו תמיד בסטטיסטיקה נעוצה בהפחתת השונות של האומד, רמז לזה ראינו בדוגמא בשקף הקודם. אם יש לכם איזשהו ידע קודם על העולם, על הפרמטר הזה שאתם רוצים לאמוד, למשל שהוא תחום בין גבולות מסוימים, כמו מטבע שלא סביר שהסיכוי בו לעץ הוא נמוך יותר מ20 אחוז או גבוה יותר מ80 אחוז -- זה ידע שכדאי להשתמש בו כי האמידה הופכת לטובה ומדויקת יותר, מאשר אם באים לבעיה כזאת טאבולה ראסה. יש פחות סיכוי לדאטא לקחת אותנו איתו לכיוונים לא צפויים.

זה יכול להיות פרמטר אחד וזה יכול להיות וקטור של פרמטרים כמו בבעיה שלנו של רגרסיה ליניארית. ברגרסיה ליניארית למשל נוח להשתמש בפריור על המקדמים של התפלגות נורמלית. כלומר הוקטור עצמו בעל התפלגות רב נורמלית עם איזושהי שונות על אלכסון המטריצת קווריאנס שהיא נגיד טאו בריבוע. מראש אני מניח את זה. מגיע הדאטא עם הלייקליהוד המוכר של התפלגות נורמלית עם תוחלת שהיא הצירוף הליניארי ושונות סיגמא בריבוע.

וכעת מסתבר כמו מקודם שיש צורה סגורה ופשוטה להתפלגות האפוסטריורית של מקדמי הרגרסיה אחרי שראינו את הדאטא. הם מתפלגים נורמלית, עם איזושהי מטריצת שונות סיגמא שלא פרטנו כאן, ושימו לב לתוחלת. X'X + המנה של סיגמא בריבוע חלקי טאו בריבוע על האלכסון, בהופכי, X'y.

התוחלת המותנית הזאת מוכרת לנו! זה בדיוק פתרון רידג', אם נגדיר את למדא להיות היחס בין סיגמא בריבוע לטאו בריבוע. והרי אמרנו שזה גם האומד שניקח כאומד סופי, התוחלת של ההתפלגות האפוסטריורית. וכשאנחנו מנסחים את זה ככה גם המשמעות של למדא מקבלת מימד נוסף. ככל שטאו בריבוע קטן יותר, אני מניח מראש שהמקדמים שלי מתפלגים עם שונות קטנה יותר מסביב לאפס, האילוץ חמור יותר, ככה גדלה למדא. ככל שטאו בריבוע גדלה יותר, אני מאפשר למקדמים לנוע חופשי סביב האפס ככה למדא קטנה יותר והאומד הסופי כבר לא שונה מהאומד של רגרסיה ליניארית רגילה. כלומר רגרסיה ליניארית רגילה מתקבלת עם מה שאנחנו קוראים פלאט פריור, פריור שטוח, למשל התפלגות נורמלית עם שונות גדולה מאוד או אינסופית.
:::
:::

---

### Ridge and Lasso priors

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

# Define the x-axis range
beta_j = np.linspace(-5, 5, 400)

# Define the distributions
flat_prior = np.ones_like(beta_j) / len(beta_j)  # Uniform distribution
gaussian_prior = stats.norm.pdf(beta_j, 0, 1)    # Gaussian distribution
laplace_prior = stats.laplace.pdf(beta_j, 0, 1)  # Laplace distribution

# Create the plots
fig, axs = plt.subplots(1, 3, figsize=(5 * 3, 3.5), sharey=True)

# Plot flat prior (Linear Regression)
axs[0].plot(beta_j, flat_prior, label='Flat Prior')
axs[0].axvline(x=0, color='grey', linestyle='--')
axs[0].set_title('Flat Prior (Linear Regression)')
axs[0].set_xlabel(r'$\beta_j$')
axs[0].set_ylabel('Density')

# Plot Gaussian prior (Ridge Regression)
axs[1].plot(beta_j, gaussian_prior, label='Gaussian Prior')
axs[1].axvline(x=0, color='grey', linestyle='--')
axs[1].set_title('Gaussian Prior (Ridge Regression)')
axs[1].set_xlabel(r'$\beta_j$')

# Plot Laplace prior (Lasso Regression)
axs[2].plot(beta_j, laplace_prior, label='Laplace Prior')
axs[2].axvline(x=0, color='grey', linestyle='--')
axs[2].set_title('Laplace Prior (Lasso Regression)')
axs[2].set_xlabel(r'$\beta_j$')

# Adjust layout
plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אבל בזה זה לא נגמר. אמרנו שהאומד לרגרסיה ליניארית רגילה מתקבל עם פריור שטוח, אין לי שום ציפיות מראש לפיזור המקדמים. האומד של רידג' מתקבל עבור פריור נורמלי עם איזושהי שונות סופית. ומסתבר, שהאומד של לאסו מתקבל, אם הפריור שלנו הוא התפלגות עם שפיץ באפס שנקראת לפלאס או double exponential. כלומר אנחנו מניחים מראש שהמקדמים מתפלגים סימטרית סביב האפס אבל עם הרבה יותר צפיפות קרוב לאפס. ולכן גם כאן אנחנו רואים איך סביר בלאסו להיתקל במקדמים שהם פשוט אפס.

עד כאן לגבי שיטות רגולריזציה. נגיד שזה ממש לא הסוף, יש עוד מובנים גם לרידג' וגם ללאסו, יש עוד שיטות לרגולריזציה שמנסות לשלב ביניהן. אבל הכי חשוב העיקרון של רגולריזציה הוא לא רק לרגרסיה ליניארית, הוא אפילו חשוב יותר, במודלים מורכבים יותר שתלמדו כמו עצי החלטה ורשתות נוירונים. הוא חשוב יותר ככל שהמודל גמיש יותר וצריך לאלף אותו.
:::
:::

---

## Dimensionality Reduction Methods: PCR {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הסוג האחרון של וריאציות לרגרסיה שנדבר עליהן מנסה להפחית קודם כל את המימד של הנתונים, ורק אז לבצע רגרסיה רגילה על הדאטא עם המימד הקטן יותר. אנחנו נתמקד רק ברגרסיה אחרי PCA או PCR אבל גם כאן יש עוד הרבה שיטות להורדת מימד. לבסוף כפי שעשינו עד כה נראה איך גם בשיטות האלה ניתן לראות כעוד גרסה של רגולריזציה, או לקיחת אילוץ על המקדמים.
:::
:::

---

### Recall: PCA

PCA will find the best direction to project the data on, while preserving the maximum "information":

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

n = 50
X1 = np.random.normal(size=n)
X2 = 2 * X1 + 3 + np.random.normal(scale=1.0, size=n)
X = np.concatenate([X1[:, np.newaxis], X2[:, np.newaxis]], axis=1)
X = StandardScaler().fit_transform(X)
X = np.concatenate([X, [[3, 3]]], axis=0)
pca = PCA(n_components=1)
T = pca.fit_transform(X)

plt.figure(figsize =(13, 5))
plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1])
plt.plot([-3, 3], [-3, 3], linestyle='--', color='r')
plt.ylabel('X2')
plt.xlabel('X1')
plt.title('Best Direction')
plt.xlim((-3.5, 3.5))
plt.ylim((-3.5, 3.5))
plt.subplot(1, 2, 2)
plt.scatter(T, np.repeat(0, n + 1))
plt.ylabel('')
plt.xlabel('T1')
plt.tick_params(left = False , labelleft = False)
plt.title('Projected Data')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניזכר מה אנחנו מנסים לעשות בPCA. נניח שיש לנו נתונים בשני מימדים X1 וX2, ואנחנו רוצים לתמצת אותם במימד אחד. במקום שני מספרים שיתארו כל תצפית, אנחנו רוצים מספר אחד. היינו רוצים לשמר את מירב האינפורמציה שיש בנתונים, ובמונחים סטטיסטיים זה אומר לשמר את מירב השונות שלהם. בPCA אנחנו עושים את זה באמצעות מציאת הכיוון של הכי הרבה שונות בדאטא. כאן זה די ברור, זה הכיוון הזה בקו האדום. ברגע שמצאנו את הכיוון הזה נטיל את הנתונים שלנו עליו, כלומר עבור כל תצפית נראה מה הערך שלה על הקו הזה כשאנחנו מסתכלים על המרחק הכי קצר עליו. והתוצאה היא ההטלה מימין, כאן על המימד החדש שנקרא T1 לכל תצפית יש רק ערך אחד, אין משמעות לציר הY, והטענה היא שככה שמרנו על הכי הרבה אינפורמציה במעבר מדו-מימד לחד-מימד.
:::
:::

---

### The PCA Problem

::: {.incremental}

- Goal: Find the $q$ direction(s) with the most dispersion

- Projection is direction $\mathbf{v}$: $X\mathbf{v} \in \mathbb R^n.$ Examples: 
    - $\mathbf{v} = (1,0,\dots,0)'$: pick first coordinate from each observation
    - $\mathbf{v} = (1/\sqrt{p},1/\sqrt{p},\dots,1/\sqrt{p})'$: project on diagonal (average all coordinates)


- Dispersion in direction $\mathbf{v}$: $||X\mathbf{v}||^2 = \mathbf{v}'(X'X)\mathbf{v}.$

- Finding the best direction which maximizes dispersion: $\mathbf{v}_1 = \arg\max_{\mathbf{v}:\|\mathbf{v}\|^2 =1} \|X\mathbf{v}\|^2$

- $\mathbf{v}_1$ is the first Principal Component direction: the best direction to project on!

- Similarly find the next PC directions $\mathbf{v}_2, \dots, \mathbf{v}_q$ and stack them to matrix $W_{p \times q}$

- Data with reduced dimensionality:
    - $T_{n \times q} = X_{n \times p}W_{p \times q}$ taking only the first $q$ principal directions
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
באופן כללי יותר נגיד שהמטרה שלנו היא למצוא Q כיוונים כאלה ששומרים על מקסימום פיזור או שונות.

כיוון כזה כמו הוקטור האדום שלנו אפשר לסמן בווי, והטלה של הנתונים עליו היא מכפלה של X כפול V, מה שנותן וקטור חדש באורך n, קודם קראנו לו T1.

לדוגמא, ההטלה יכלה להיות הוקטור שיש לו 1 באלמנט הראשון שלו, ואפס בכל השאר -- אם מכפילים את X בוקטור כזה זה בדיוק כמו לבחור רק את המשתנה הראשון בנתונים.

למשל, הוקטור וי יכול להיות וקטור שבכל האלמנטים שלו יש 1 חלקי שורש P. אם תעשו קצת אלגברה תראו שלהכפיל את X בוקטור כזה אומר עבור כל תצפית לקחת את הממוצע של על כל המשתנים.

מה זה פיזור של וקטור ההטלה שאנחנו רוצים למקסם? אנחנו ניקח את הנורמה בריבוע של וקטור ההטלה, אותה אפשר לרשום כv'X'Xv.

והנה הבעיה שלנו, למצוא את הכיוון, הוקטור v1 בעל נורמה 1, שממקסם את פיזור ההטלה. אנחנו עוד נלמד על PCA יותר לעומר ונראה למה אנחנו מוכרחים את האילוץ הזה על הוקטור וי.

כשנמצא את הוקטור הזה נקרא לו פרינסיפל קומפוננט דירקשן: זה הכיוון הכי טוב להטיל עליו על פי קריטריון הפיזור.

כעת נמצא את הכיוון הבא v2, v3 וכולי. נראה שהם צריכים להיות אורתוגונליים זה לזה. ונשים אותם זה לצד זה במטריצה W מסדר p שורות על q עמודות. זאת מטריצת ההטלה הסופית.

את הדאטא ממימד נמוך יותר נכתוב כT. הדאטא שלנו עבר הפחתת מימד מp שיכול להיות גבוה מאוד, לq שיכול להיות קטן מאוד, והוא מוכן לרגרסיה.
:::
:::

---

### PCA regression (PCR)

::: {.incremental}
- Standardize the features in $X$
- Find $W_{p \times q}$ via SVD decomposition
- Perform (regular) linear regression on $T_{n \times q} = XW$
$$y_i = \theta_0 + \theta_1 \cdot t_{i1} + \dots + + \theta_q \cdot t_{iq} + \varepsilon_i \quad \text{ or } \quad y = T\theta + \varepsilon$$
- $q$ becomes a hyperparameter
- $\beta = W\theta$, hence $\beta$ is still constrained
- SVD decomposition also shows similarities to Ridge regression
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסכם איך אנחנו מבצעים PCR:

עושים סטנדרטיזציה על עמודות X.

מוצאים את מטריצת ההטלה W, בדרך כלל על-ידי פירוק SVD ועוד נחזור לזה כשנלמד PCA לעומק.

וכעת נבצע רגרסיה ליניארית עם המודל הליניארי הרגיל, על הדאטא אחרי הורדת מימד. אנחנו מחפשים עכשיו וקטור מקדמים תטא באורך Q, שכשנכפיל אותו פי הנתונים החדשים שלנו T נקבל בקירוב את המשתנה התלוי Y.

נשים לב שכמו שאר השיטות שלנו אין רק פתרון אחד, לא דיברנו על איך יודעים מהו Q, מה המימד הנכון לבחור? למעשה Q הופך להיפרפרמטר נוסף בדומה ללמדא, שצריך לבחור, למשל דרך קרוס ולידיישן.

עכשיו מה הקשר לשיטות שלמדנו: נביט שוב במודל שאנחנו מניחים ונראה שבהכרח זה אומר שאנחנו מחפשים וקטור מקדמים בטא ששווה לW כפול תטא. והנה אנחנו רואים שוב קשר לתמה הכללית שלנו, אפשר לראות בביטוי הזה כאילוץ על הבטאות, המקדמים בעצמם חייבים להיות צירוף ליניארי על וקטור מקדמים ממימד נמוך יותר!

וזה לא הקשר היחיד. כשנלמד על פירוק הSVD נראה אפילו קשר נוסף בין PCR לרגרסיית רידג'.
:::
:::

---

### PCA regression (PCR)

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import KFold

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Full linear regression baseline
kf = KFold(n_splits=5, shuffle=True, random_state=1)
linear_train_errors = []
linear_test_errors = []

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    scale = StandardScaler()
    X_train = scale.fit_transform(X_train)
    X_test = scale.transform(X_test)
    linear_model = LinearRegression()
    linear_model.fit(X_train, y_train)
    train_preds = linear_model.predict(X_train)
    test_preds = linear_model.predict(X_test)
    
    linear_train_errors.append(np.mean((y_train - train_preds)**2))
    linear_test_errors.append(np.mean((y_test - test_preds)**2))

mean_linear_train_error = np.mean(linear_train_errors)
mean_linear_test_error = np.mean(linear_test_errors)

# PCA regression
n_components_list = list(range(1, X.shape[1] + 1))
train_errors = []
test_errors = []
coef_profile = []

for n_components in n_components_list:
    pca_train_errors = []
    pca_test_errors = []
    pca_coefs = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        scale = StandardScaler()
        X_train = scale.fit_transform(X_train)
        X_test = scale.transform(X_test)
        pca = PCA(n_components=n_components)
        X_train_reduced = pca.fit_transform(X_train)
        X_test_reduced = pca.transform(X_test)
        
        reg_model = LinearRegression()
        reg_model.fit(X_train_reduced, y_train)
        train_preds = reg_model.predict(X_train_reduced)
        test_preds = reg_model.predict(X_test_reduced)
        
        pca_train_errors.append(np.mean((y_train - train_preds)**2))
        pca_test_errors.append(np.mean((y_test - test_preds)**2))
        pca_coefs.append(np.dot(pca.components_.T, reg_model.coef_))
    
    train_errors.append(np.mean(pca_train_errors))
    test_errors.append(np.mean(pca_test_errors))
    coef_profile.append(np.mean(pca_coefs, axis=0))

coef_profile = np.array(coef_profile)

# Plotting
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=False)

# Plot 1: Train and test errors
ax1.plot(n_components_list, train_errors, label='PCR Train Error', color='blue')
ax1.plot(n_components_list, test_errors, label='PCR Test Error', color='red')
ax1.axhline(y=mean_linear_train_error,linestyle='dashed', label='OLS Train Error', color='blue')
ax1.axhline(y=mean_linear_test_error, linestyle='dashed', label='OLS Test Error', color='red')
ax1.axvline(x=n_components_list[np.argmin(test_errors)], color='green', linestyle='dashed', label='Best q')
ax1.set_xlabel('Number of Components (q)')
ax1.set_ylabel('Mean Squared Error')
ax1.set_title('PCA Regression: 5-Fold CV Mean MSE')
ax1.legend(loc='lower left', bbox_to_anchor=(0, 0.2))

# Plot 2: Coefficients profile
for i in range(X.shape[1]):
    ax2.plot(n_components_list, coef_profile[:, i], label=X.columns[i])
ax2.set_xlabel('Number of Components (q)')
ax2.set_ylabel('Coefficient Value')
ax2.set_title('PCA Regression Coefficients Profile')
ax2.axvline(x=n_components_list[np.argmin(test_errors)], color='green', linestyle='dashed', label='Best q')
ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בנתונים שלנו המימד P מלכתחילה לא כל כך גבוה, הוא 11, ולא ברור שצריך הורדת מימד באמצעות PCA או כל שיטה אחרת לפני הרצת רגרסיה.

ואכן, אם אנחנו בודקים מה הQ שמביא למינימום שגיאת חיזוי באמצעות קרוס ולידיישן כמו שעשינו עד עכשיו, אנחנו מקבלים שבנתונים שלנו לא כדאי כמעט להוריד מימד, הQ הטוב ביותר הוא 10. ואמנם לא רואים את זה בגרף אבל כן יש הפחתה בשורה התחתונה בMSE של הטסט לעומת רגרסיה רגילה, אבל אחוזית זה הפרש די קטן.

נשים לב שברגע שמבינים שאפשר לחזור מוקטור המקדמים תטא אל וקטור המקדמים המקורי בטא באמצעות הכפלה פי מטריצת W, אפשר לראות את פרופיל המקדמים בטא כמו שעשינו עד עכשיו עם פרמטר הפנאלטי למדא. אנחנו רואים שגם בPCR המקדמים עוברים שרינקג' שמזכיר את רידג', ככל שQ קטן יותר או הורדת המימד היא אגרסיבית יותר. הכיוון כאן הפוך כי לא רצינו להפוך את ציר הX בצורה מלאכותית מ11 ל-1, אבל הרעיון זהה.

עד כאן בשיטות בחירה של משתנים ורגולריזציה. בשיעורים הבאים נלמד מודלים מורכבים וגמישים הרבה יותר, חלק מהם כל כך גמישים שהם באים בילט-אין עם רגולריזציה. העקרונות שראינו היום ישמשו אתכם לשיפור כל מודל מורכב שתראו בעתיד והם לא המלצה הם כבר חלק מהמודלים המודרנים ביותר שנמצאים בשימוש יומיומי, ולכן מומלץ להכיר אותם ואת החשיבות שלהם היטב.
:::
:::
