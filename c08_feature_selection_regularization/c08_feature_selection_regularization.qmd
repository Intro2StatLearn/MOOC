---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Feature Selection and Regularization"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Feat. Selection and Regularization - Class 8

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Goals of Selection and Regularization {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Why select features? Why regularize?

E.g., did we not see the Gauss-Markov Theorem?

::: {.incremental}
- Improve prediction accuracy
  - Recall: the Bias-Variance Tradeoff $\Rightarrow$ allowing some bias might decrease variance!
  - Recall: $op \approx \mathcal{O}\left(\frac{p\sigma^2}{n}\right)$
  - If $p > n$: $X'X$ has no inverse, infinite solutions

- Improve interpretability
  - Discarding features with small "unlikely" coefficients
  
    $\Rightarrow$ lowering model complexity
    
    $\Rightarrow$ parsimony!
  - Often coincides with improving prediction accuracy

- In general: "don't believe *everything* the data says"

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Feature Selection and Regularization

We will focus on:

- Subset selection (best, stepwise, stagewise)
- Regularized regression (Ridge, Lasso)
- Dimensionality reduction (PCR)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Subset Selection {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Best subset selection

1. $M_0$ model: predict $\hat{y} = \hat{\beta}_0 = \bar{y}$
2. For $k = 1, \dots, p$:
    i. Fit all ${p \choose k}$ models containing $k$ features
    ii. Pick the best $M_k$ with $\min RSS$
3. Select the best model from $M_0, \dots, M_p$ with the $C_p$/$AIC$ criterion or CV

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Best subset selection

```{python}
#| echo: false

import pandas as pd
import numpy as np
import itertools
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0', 'Married'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Function to calculate RSS
def calculate_rss(y, y_pred):
    return np.sum((y - y_pred) ** 2)

# Perform best subset selection
rss_list = []
best_models = []

# Model M0: only intercept
X_intercept = np.ones((len(y), 1))
model = LinearRegression().fit(X_intercept, y)
rss = calculate_rss(y, model.predict(X_intercept))
rss_list.append((0, rss, 'Intercept'))
best_models.append((0, rss, []))

# Iterate over the number of predictors from 1 to 11
for k in range(1, X.shape[1] + 1):
    best_rss = np.inf
    best_model = None
    best_features = None
    for combo in itertools.combinations(X.columns, k):
        X_subset = X[list(combo)]
        model = LinearRegression().fit(X_subset, y)
        rss = calculate_rss(y, model.predict(X_subset))
        rss_list.append((k, rss, combo))
        if rss < best_rss:
            best_rss = rss
            best_model = model
            best_features = combo
    best_models.append((k, best_rss, best_features))

# Convert RSS list to a DataFrame
rss_df = pd.DataFrame(rss_list, columns=['k', 'RSS', 'Features'])

# Estimate sigma^2 from the full model
full_model = LinearRegression().fit(X, y)
full_rss = calculate_rss(y, full_model.predict(X))
sigma2_est = full_rss / (len(y) - X.shape[1] - 1)

# Calculate RSS + 2 * k * sigma2_est
rss_plus_penalty = [rss + 2 * k * sigma2_est for k, rss, _ in best_models]

# Find the minimum RSS + penalty and corresponding k
min_rss_plus_penalty = min(rss_plus_penalty)
best_k = rss_plus_penalty.index(min_rss_plus_penalty)

# Plot the RSS for all models
plt.figure(figsize=(10, 5))
for k in range(X.shape[1] + 1):
    k_rss = rss_df[rss_df['k'] == k]
    plt.scatter(k_rss['k'], k_rss['RSS'], color='blue', label='All models' if k == 0 else "")
best_rss_df = pd.DataFrame(best_models, columns=['k', 'RSS', 'Features'])
plt.plot(best_rss_df['k'], best_rss_df['RSS'], color='red', label='Best models')
plt.plot(best_rss_df['k'], rss_plus_penalty, color='green', linestyle='dashed', label='Best models + Cp penalty')
plt.axvline(x=best_k, color='orange', linestyle='dashed', label='Best Cp model')
plt.xlabel('Number of predictors (k)')
plt.ylabel('RSS')
plt.title('RSS for Best Subset Selection')
plt.legend()
plt.show()
```

::: {.fragment}
::: {.callout-note}
How many models are run?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Forward stepwise selection

1. $M_0$ model: predict $\hat{y} = \hat{\beta}_0 = \bar{y}$
2. For $k = 0, \dots, p - 1$:
    i. Fit all $p - k$ models adding 1 additional feature
    ii. Pick the best $M_{k + 1}$ with $\min RSS$
3. Select the best model from $M_0, \dots, M_p$ with the $C_p$/$AIC$ criterion or CV


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Forward stepwise selection

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Function to calculate RSS
def calculate_rss(y, y_pred):
    return np.sum((y - y_pred) ** 2)

# Forward stepwise selection
rss_list = []
best_models = []

# Model M0: only intercept
X_intercept = np.ones((len(y), 1))
model = LinearRegression().fit(X_intercept, y)
rss = calculate_rss(y, model.predict(X_intercept))
rss_list.append((0, rss, 'Intercept'))
best_models.append((0, rss, []))

current_features = []
remaining_features = list(X.columns)

# Iterate to add predictors from 1 to 11
for k in range(1, X.shape[1] + 1):
    best_rss = np.inf
    best_feature = None
    for feature in remaining_features:
        trial_features = current_features + [feature]
        X_subset = X[trial_features]
        model = LinearRegression().fit(X_subset, y)
        rss = calculate_rss(y, model.predict(X_subset))
        rss_list.append((k, rss, trial_features))
        if rss < best_rss:
            best_rss = rss
            best_feature = feature
    current_features.append(best_feature)
    remaining_features.remove(best_feature)
    best_models.append((k, best_rss, current_features.copy()))

# Convert RSS list to a DataFrame
rss_df = pd.DataFrame(rss_list, columns=['k', 'RSS', 'Features'])

# Estimate sigma^2 from the full model
full_model = LinearRegression().fit(X, y)
full_rss = calculate_rss(y, full_model.predict(X))
sigma2_est = full_rss / (len(y) - X.shape[1] - 1)

# Calculate RSS + 2 * k * sigma2_est
rss_plus_penalty = [rss + 2 * k * sigma2_est for k, rss, _ in best_models]

# Find the minimum RSS + penalty and corresponding k
min_rss_plus_penalty = min(rss_plus_penalty)
best_k = rss_plus_penalty.index(min_rss_plus_penalty)

# Plot the RSS for all models
plt.figure(figsize=(10, 5))
for k in range(X.shape[1] + 1):
    k_rss = rss_df[rss_df['k'] == k]
    plt.scatter(k_rss['k'], k_rss['RSS'], color='blue', label='All models' if k == 0 else "")
best_rss_df = pd.DataFrame(best_models, columns=['k', 'RSS', 'Features'])
plt.plot(best_rss_df['k'], best_rss_df['RSS'], color='red', label='Best models')
plt.plot(best_rss_df['k'], rss_plus_penalty, color='green', linestyle='dashed', label='Best models + Cp penalty')
plt.axvline(x=best_k, color='orange', linestyle='dashed', label='Best Cp model')
plt.xlabel('Number of predictors (k)')
plt.ylabel('RSS')
plt.title('RSS for Forward Stepwise Selection')
plt.legend()
plt.show()
```

::: {.fragment}
::: {.callout-note}
How many models are run?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Stepwise regression main disadvantage

| k | Best subset                           | Forward stepwise                    |
|---|---------------------------------------|-------------------------------------|
| 1 | {Rating}                              | {Rating}                            |
| 2 | {Rating, Income}                      | {Rating, Income}                    |
| 3 | {Rating, Income, Student}             | {Rating, Income, Student}           |
| 4 | {Income, Student, Limit, Cards}       | {Rating, Income, Student, Limit}    |

::: {.fragment}
::: {.callout-note}
What happens if $p > n$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Forward [stagewise]{style="color:red;"} selection

0. Standardize all features, input some $\tau_{thresh} \in (0, 1)$ and $\varepsilon > 0$ step size
1. Residual $\mathbf{r} = \mathbf{x} - \bar{y}$, $\beta_1, \dots, \beta_p = 0$
2. Find the predictor $\mathbf{x}_j$ most correlated with $\mathbf{r}$, and let $\tau = Corr(\mathbf{r}, \mathbf{x}_j)$
3. While $|\tau| > \tau_{thresh}$:
    i. Update $\beta_j \leftarrow \beta_j + \delta_j$, where $\delta_j = \varepsilon \cdot \text{sign}(\tau)$
    ii. Update $\mathbf{r} \leftarrow \mathbf{r} - \delta_j\mathbf{x}_j$
    iii. Find the predictor $\mathbf{x}_j$ most correlated with $\mathbf{r}$, and let $\tau = Corr(\mathbf{r}, \mathbf{x}_j)$

::: {.fragment}
::: {.callout-note}
Why would we want to "slow-learn"?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Regularized Regression: Ridge {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Ridge regression

::: {.incremental}
- Instead of reducing the number of parameters $\to$ penalize their norm
- With $\ell_2$ norm we get the penalized RSS criterion for some regularization/penalty parameter $\lambda$:
$$PRSS(\lambda) = \sum_{i = 1}^n \left(y_i - \beta_0 + \sum_{j = 1}^p \beta_j x_{ij}\right)^2 + \lambda\sum_{j = 1}^p \beta_j^2 = RSS + \lambda\|\beta^*\|^2_2$$

- Standardize the features in $X$, then:
  - $\hat{\beta}_0 = \bar{y}$
  - $\lambda$ punishes features of different scale comparably
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Ridge solution

$$PRSS(\lambda) = \|y - X\beta\|^2_2 + \lambda\|\beta\|^2_2$$

::: {.fragment}
$\frac{\partial PRSS}{\partial \beta} = -2X'y + 2X'X\beta +2\lambda\beta$
:::

::: {.fragment}
$2X'y - 2X'X\beta - 2\lambda\beta = 0$
:::
::: {.fragment}
$(X'X + \lambda I_p)\beta = X'y$
:::
::: {.fragment}
$\hat{\beta}(\lambda) = (X'X + \lambda I_p)^{-1}X'y$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Ridge (original) justification

$$\hat{\beta}(\lambda) = (X'X + \lambda I_p)^{-1}X'y$$

::: {.incremental}
- Numerical stability:
  - If $X$'s columns are correlated, $X'X$ is ill-conditioned and its inverse is unstable, variance of $\hat{\beta}$ is high -- but $X'X + \lambda I_p$ improves on this
- Feasibility:
  - If $X$'s columns are linearly dependent $X'X$ is not invertible but $X'X + \lambda I_p$ is!
  - If $p > n$ -- same!
- Guaranteed prediction error reduction for some $\lambda$:
  - Bias increases but variance decreases *more*
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Choosing $\lambda$

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Define lambdas for Ridge regression
lambdas = np.logspace(-2, 3, 100)

# Cross-validation setup
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# Variables to store results
train_errors = []
test_errors = []
ridge_coefficients = []

# Regular linear regression for comparison
lr_model = LinearRegression()
lr_train_errors = []
lr_test_errors = []

# Ridge regression with cross-validation
for alpha in lambdas:
    ridge = make_pipeline(StandardScaler(), Ridge(alpha=alpha))
    
    fold_train_errors = []
    fold_test_errors = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        ridge.fit(X_train, y_train)
        y_train_pred = ridge.predict(X_train)
        y_test_pred = ridge.predict(X_test)
        
        fold_train_errors.append(((y_train - y_train_pred) ** 2).mean())
        fold_test_errors.append(((y_test - y_test_pred) ** 2).mean())

    train_errors.append(np.mean(fold_train_errors))
    test_errors.append(np.mean(fold_test_errors))
    ridge_coefficients.append(ridge.named_steps['ridge'].coef_)

# Fit regular linear regression and calculate mean train and test errors
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    scale = StandardScaler()
    X_train = scale.fit_transform(X_train)
    X_test = scale.transform(X_test)
    lr_model.fit(X_train, y_train)
    lr_train_errors.append(((y_train - lr_model.predict(X_train)) ** 2).mean())
    lr_test_errors.append(((y_test - lr_model.predict(X_test)) ** 2).mean())

mean_lr_train_error = np.mean(lr_train_errors)
mean_lr_test_error = np.mean(lr_test_errors)

# Find the best lambda
best_lambda_index = np.argmin(test_errors)
best_lambda = lambdas[best_lambda_index]
max_lam = 10

# Plot the results
plt.figure(figsize=(10, 5))

# Plot 1: Training and test errors vs lambda
plt.subplot(1, 2, 1)
plt.plot(lambdas[lambdas <= max_lam], np.array(train_errors)[lambdas <= max_lam], label='Train Error', color='blue')
plt.plot(lambdas[lambdas <= max_lam], np.array(test_errors)[lambdas <= max_lam], label='Test Error', color='red')
plt.axhline(y=mean_lr_train_error, color='blue', linestyle='dashed', label='OLS Train Error')
plt.axhline(y=mean_lr_test_error, color='red', linestyle='dashed', label='OLS Test Error')
plt.axvline(x=best_lambda, color='green', linestyle='dashed', label='Best Lambda')
plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Mean Squared Error')
plt.title('Ridge Regression: 5-Fold CV Mean MSE')
plt.legend()

# Plot 2: Ridge coefficients profile
plt.subplot(1, 2, 2)
ridge_coefficients = np.array(ridge_coefficients)
for i in range(ridge_coefficients.shape[1]):
    plt.plot(lambdas, ridge_coefficients[:, i], label=X.columns[i])
plt.axvline(x=best_lambda, color='green', linestyle='dashed', label='Best Lambda')
plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Coefficient Value')
plt.title('Ridge Coefficients Profile')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Regularized Regression: Lasso {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Lasso regression

$$PRSS(\lambda) = \sum_{i = 1}^n \left(y_i - \beta_0 + \sum_{j = 1}^p \beta_j x_{ij}\right)^2 + \lambda\sum_{j = 1}^p |\beta_j| = RSS + \lambda\|\beta^*\|$$

- As with Ridge, standardize the features in $X$, then:
  - $\hat{\beta}_0 = \bar{y}$
  - $\lambda$ punishes features of different scale comparably

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The Lasso Path

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso, LinearRegression
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Define lambdas for Lasso regression
lambdas = np.logspace(-2, 3, 100)

# Cross-validation setup
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# Variables to store results
train_errors = []
test_errors = []
lasso_coefficients = []

# Regular linear regression for comparison
lr_model = LinearRegression()
lr_train_errors = []
lr_test_errors = []

# Lasso regression with cross-validation
for alpha in lambdas:
    lasso = make_pipeline(StandardScaler(), Lasso(alpha=alpha, max_iter=10000))
    
    fold_train_errors = []
    fold_test_errors = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        lasso.fit(X_train, y_train)
        y_train_pred = lasso.predict(X_train)
        y_test_pred = lasso.predict(X_test)
        
        fold_train_errors.append(((y_train - y_train_pred) ** 2).mean())
        fold_test_errors.append(((y_test - y_test_pred) ** 2).mean())
    
    train_errors.append(np.mean(fold_train_errors))
    test_errors.append(np.mean(fold_test_errors))
    lasso_coefficients.append(lasso.named_steps['lasso'].coef_)

# Fit regular linear regression and calculate mean train and test errors
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    scale = StandardScaler()
    X_train = scale.fit_transform(X_train)
    X_test = scale.transform(X_test)
    lr_model.fit(X_train, y_train)
    lr_train_errors.append(((y_train - lr_model.predict(X_train)) ** 2).mean())
    lr_test_errors.append(((y_test - lr_model.predict(X_test)) ** 2).mean())

mean_lr_train_error = np.mean(lr_train_errors)
mean_lr_test_error = np.mean(lr_test_errors)

# Find the best lambda
best_lambda_index = np.argmin(test_errors)
best_lambda = lambdas[best_lambda_index]
max_lam = 10

# Plot the results
plt.figure(figsize=(10, 5))

# Plot 1: Training and test errors vs lambda
plt.subplot(1, 2, 1)
plt.plot(lambdas[lambdas <= max_lam], np.array(train_errors)[lambdas <= max_lam], label='Train Error', color='blue')
plt.plot(lambdas[lambdas <= max_lam], np.array(test_errors)[lambdas <= max_lam], label='Test Error', color='red')
plt.axhline(y=mean_lr_train_error, color='blue', linestyle='dashed', label='OLS Train Error')
plt.axhline(y=mean_lr_test_error, color='red', linestyle='dashed', label='OLS Test Error')
plt.axvline(x=best_lambda, color='green', linestyle='dashed', label='Best Lambda')
plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Mean Squared Error')
plt.title('Lasso Regression: 5-Fold CV Mean RSS')
plt.legend()

# Plot 2: Lasso coefficients profile
plt.subplot(1, 2, 2)
lasso_coefficients = np.array(lasso_coefficients)
for i in range(lasso_coefficients.shape[1]):
    plt.plot(lambdas, lasso_coefficients[:, i], label=X.columns[i])
plt.axvline(x=best_lambda, color='green', linestyle='dashed', label='Best Lambda')
plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Coefficient Value')
plt.title('Lasso Coefficients Profile')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.tight_layout()
plt.show()
```

::: {.incremental}
- Lasso's selling point: *sparsity*!
- Highly useful, especially for $p > n$ datasets
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Lasso solution

- Quadratic Programming, LARS
- But also, surprisingly:

::: {.fragment}
```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import KFold

# Define the ForwardStagewiseRegression class
class ForwardStagewiseRegression:
    def __init__(self, n_iter=1000, step_size=0.01):
        self.n_iter = n_iter
        self.step_size = step_size
        self.coef_ = None
        self.coef_history_ = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.coef_ = np.zeros(n_features)
        self.coef_history_ = np.zeros((self.n_iter, n_features))
        self.intercept_ = np.mean(y)
        residuals = y - self.intercept_

        for i in range(self.n_iter):
            correlations = np.dot(X.T, residuals)
            best_feature = np.argmax(np.abs(correlations))
            direction = np.sign(correlations[best_feature])
            self.coef_[best_feature] += self.step_size * direction
            residuals = y - (np.dot(X, self.coef_) + self.intercept_)
            self.coef_history_[i] = self.coef_

        return self

    def predict(self, X):
        return np.dot(X, self.coef_) + self.intercept_

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Forward Stagewise Regression
n_iter = 20000
step_size = 0.1
fsr_model = ForwardStagewiseRegression(n_iter=n_iter, step_size=step_size)
fsr_model.fit(X_scaled, y)

# Lasso Regression with cross-validation
lambdas = np.logspace(-0.5, 2.6, 100) #np.linspace(10e-3, 10e3, 1000)
kf = KFold(n_splits=5, shuffle=True, random_state=1)
lasso_coefficients = []

for alpha in lambdas:
    lasso = make_pipeline(StandardScaler(), Lasso(alpha=alpha, max_iter=10000))
    fold_coefficients = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        lasso.fit(X_train, y_train)
        fold_coefficients.append(lasso.named_steps['lasso'].coef_)
    
    lasso_coefficients.append(np.mean(fold_coefficients, axis=0))

lasso_coefficients = np.array(lasso_coefficients)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=True)

# Plot 1: Lasso coefficients profile
for i in range(X.shape[1]):
    ax1.plot(lambdas, lasso_coefficients[:, i], label=X.columns[i])
ax1.set_xscale('log')
ax1.set_xlabel('Lambda')
ax1.set_ylabel('Coefficient Value')
ax1.set_title('Lasso Coefficients Profile')

# Plot 2: Forward Stagewise Regression coefficients profile
for i in range(X.shape[1]):
    ax2.plot(range(n_iter)[::-1], fsr_model.coef_history_[:, i], label=X.columns[i])
ax2.set_xlabel('Iteration')
ax2.set_ylabel('Coefficient Value')
ax2.set_title('Forward Stagewise Coefficient Path')
ax2.set_xticks(ticks=range(0, n_iter, n_iter//5))
ax2.set_xticklabels(range(n_iter, 0, -n_iter//5))
ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.tight_layout()
plt.show()
```

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## On Shrinkage and Sparsity {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Dual criteria

There is 1-to-1 correspondnece between Ridge and Lasso's previous criteria and:

::: {.fragment}
- Ridge: $\min_{\beta} RSS \text{  s.t.  } \sum_{j = 1}^p\beta_j^2 \le s$
- Lasso: $\min_{\beta} RSS \text{  s.t.  } \sum_{j = 1}^p|\beta_j| \le s$
:::

::: {.fragment}
In this contest can also write for Best Subset regression:

- $\min_{\beta} RSS \text{  s.t.  } \sum_{j = 1}^p\mathbb{1}\left[\beta_j \neq 0\right] \le s$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Seeing Shrinkage and Sparsity (I)

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Define the grid for beta values
beta1 = np.linspace(-2, 2, 400)
beta2 = np.linspace(-2, 2, 400)
B1, B2 = np.meshgrid(beta1, beta2)

# Define the RSS function with identical contours for both lasso and ridge
RSS = (B1 - 1.0)**2 / 1 + (B2 - 1.57)**2 / 0.25

# Create the figure and subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

# Lasso constraint plot
ax1.contour(B1, B2, RSS, levels=[1.0, 2.0], colors='red')
lasso_constraint = plt.Polygon([[-1.07, 0], [0, 1.07], [1.07, 0], [0, -1.07]], color='cyan', alpha=0.5)
ax1.add_patch(lasso_constraint)
ax1.plot(1.0, 1.57, 'ko')  # Point representing the unbiased beta_hat
ax1.text(1.1, 1.67, r'$\hat{\beta}$', fontsize=12, color='black')
ax1.axhline(0, color='black', linewidth=1)
ax1.axvline(0, color='black', linewidth=1)
ax1.set_xlim([-2, 2])
ax1.set_ylim([-2, 2])
ax1.set_title('Lasso Constraint')
ax1.set_xlabel(r'$\beta_1$')
ax1.set_ylabel(r'$\beta_2$')

# Ridge constraint plot
ax2.contour(B1, B2, RSS, levels=[1.0, 2.0], colors='red')
circle = plt.Circle((0, 0), 1, color='cyan', alpha=0.5)
ax2.add_artist(circle)
ax2.plot(1.0, 1.57, 'ko')  # Point representing the unbiased beta_hat
ax2.text(1.1, 1.67, r'$\hat{\beta}$', fontsize=12, color='black')
ax2.axhline(0, color='black', linewidth=1)
ax2.axvline(0, color='black', linewidth=1)
ax2.set_xlim([-2, 2])
ax2.set_ylim([-2, 2])
ax2.set_title('Ridge Constraint')
ax2.set_xlabel(r'$\beta_1$')
ax2.set_ylabel(r'$\beta_2$')

plt.tight_layout()
plt.show()

```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Seeing Shrinkage and Sparsity (II)

If $X$ has orthonormal columns ($X'X = I_p$) and $\hat{\beta}_j$ is the OLS estimator:

| Estimator              | $\tilde{\beta}_j$                                                                 |
|------------------------|-----------------------------------------------------------------------------------|
| Best $M$ subset        | $\hat{\beta}_j \cdot \mathbb{1}\left[|\hat{\beta}_j| \ge  |\hat{\beta}_{(M)}|\right]$ |
| Ridge                  | $\hat{\beta}_j / (1 + \lambda)$                                                   |
| Lasso                  | $\text{sign}(\hat{\beta}_j)(|\hat{\beta}_j| - \lambda)_+$                         |

::: {.fragment}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Generate OLS estimates
beta_ols = np.linspace(-3, 3, 400)

# Best subset estimator
M = 0.5
beta_best_subset = beta_ols * (np.abs(beta_ols) >= M)

# Ridge estimator
lambda_ridge = 0.5
beta_ridge = beta_ols / (1 + lambda_ridge)

# Lasso estimator
lambda_lasso = 0.5
beta_lasso = np.sign(beta_ols) * np.maximum(np.abs(beta_ols) - lambda_lasso, 0)

# Create the figure and subplots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(5 * 3, 3.5))

# Plot for best subset estimator
ax1.plot(beta_ols, beta_ols, color='blue', alpha=0.5)  # 45-degree line (OLS estimate)
ax1.plot(beta_ols, beta_best_subset, 'r--')  # Best subset estimate
ax1.axhline(0, color='gray', linewidth=0.5)
ax1.axvline(0, color='gray', linewidth=0.5)
ax1.set_title('Best Subset')
ax1.set_xlabel(r'$\hat{\beta}_j$')
ax1.set_ylabel(r'$\tilde{\beta}_j$')

# Plot for ridge estimator
ax2.plot(beta_ols, beta_ols, color='blue', alpha=0.5)  # 45-degree line (OLS estimate)
ax2.plot(beta_ols, beta_ridge, 'r--')  # Ridge estimate
ax2.axhline(0, color='gray', linewidth=0.5)
ax2.axvline(0, color='gray', linewidth=0.5)
ax2.set_title('Ridge')
ax2.set_xlabel(r'$\hat{\beta}_j$')

# Plot for lasso estimator
ax3.plot(beta_ols, beta_ols, color='blue', alpha=0.5)  # 45-degree line (OLS estimate)
ax3.plot(beta_ols, beta_lasso, 'r--')  # Lasso estimate
ax3.axhline(0, color='gray', linewidth=0.5)
ax3.axvline(0, color='gray', linewidth=0.5)
ax3.set_title('Lasso')
ax3.set_xlabel(r'$\hat{\beta}_j$')

plt.tight_layout()
plt.show()
```

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Bayesian Viewpoint {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Conditional Distribution

::: {.incremental}
- Recall Bayes Rule:
$$P(B | A) = \frac{P(A | B) \cdot P(B)}{P(A)} \text{     or     } \text{posterior} = \frac{\text{likelihood}\cdot\text{prior}}{\text{evidence}}$$

- For continuous distributions:
$$f_{Y|X}(y|x) = \frac{f_{X|Y}(x|y)f_Y(y)}{f_X(x)} \propto f_{X|Y}(x|y)f_Y(y)$$

- E.g. For normal distribution, if $X \sim \mathcal{N}(\mu_X, \sigma^2_X), Y \sim \mathcal{N}(\mu_Y, \sigma^2_Y), \rho = Corr(X, Y)$:
$$Y | X = x \sim \mathcal{N}\left(\mu_Y + \frac{\sigma_Y}{\sigma_X}\rho(x - \mu_X), (1 - \rho^2)\sigma_Y^2\right)$$

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Bayesian Statistics

::: {.incremental}
- In Bayesian statistics a parameter $\theta$ isn't a *fixed* number but an RV having a [prior]{style="color:red;"} distribution $P(\theta)$
- In comes data sample $Y = y_1, \dots, y_n$ with distribution $P(Y | \theta)$
- We calculate the [posterior]{style="color:red;"} distribution given the data $P(\theta|Y)$
- $\theta$'s final estimate is the mean or mode of $P(\theta|Y)$
:::

::: {.fragment}
::: {.callout-note}
Why would we take this approach?
:::
:::

::: {.fragment}
- For linear regression and normal prior:
    - Prior: $\beta \sim \mathcal{N}(0, \tau^2I_p)$
    - Data: $Y | \beta \sim \mathcal{N}(X\beta, \sigma^2I_n)$
    - Posterior: $\beta|Y \sim \mathcal{N}\left((X'X + \frac{\sigma^2}{\tau^2}I_p)^{-1}X'y, \Sigma\right)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Ridge and Lasso priors

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

# Define the x-axis range
beta_j = np.linspace(-5, 5, 400)

# Define the distributions
flat_prior = np.ones_like(beta_j) / len(beta_j)  # Uniform distribution
gaussian_prior = stats.norm.pdf(beta_j, 0, 1)    # Gaussian distribution
laplace_prior = stats.laplace.pdf(beta_j, 0, 1)  # Laplace distribution

# Create the plots
fig, axs = plt.subplots(1, 3, figsize=(5 * 3, 3.5), sharey=True)

# Plot flat prior (Linear Regression)
axs[0].plot(beta_j, flat_prior, label='Flat Prior')
axs[0].axvline(x=0, color='grey', linestyle='--')
axs[0].set_title('Flat Prior (Linear Regression)')
axs[0].set_xlabel(r'$\beta_j$')
axs[0].set_ylabel('Density')

# Plot Gaussian prior (Ridge Regression)
axs[1].plot(beta_j, gaussian_prior, label='Gaussian Prior')
axs[1].axvline(x=0, color='grey', linestyle='--')
axs[1].set_title('Gaussian Prior (Ridge Regression)')
axs[1].set_xlabel(r'$\beta_j$')

# Plot Laplace prior (Lasso Regression)
axs[2].plot(beta_j, laplace_prior, label='Laplace Prior')
axs[2].axvline(x=0, color='grey', linestyle='--')
axs[2].set_title('Laplace Prior (Lasso Regression)')
axs[2].set_xlabel(r'$\beta_j$')

# Adjust layout
plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Dimensionality Reduction Methods: PCR {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Recall: PCA

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### PCA regression (PCR)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
