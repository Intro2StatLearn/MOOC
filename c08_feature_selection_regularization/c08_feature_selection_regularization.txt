=== 1. המטרות של סלקציה ורגולריזציה של משתנים ===

השיעור שלפנינו עוסק בבחירת משתנים וברגולריזציה שלהם. אמנם אנחנו מתמקדים עדיין במודל הליניארי וספציפית ברגרסיה ליניארית, אבל העקרונות שנלמד נכונים גם לקלסיפיקציה ולמודלים מורכבים הרבה יותר, והם חיוניים להבנה שלנו בעתיד איך אפשר לשפר כל מודל באמצעות שליטה עליו, ולהימנע מאוברפיטינג.

:::

חלק מהשיטות שנלמד לרגולריזציה משנות את המודל הליניארי, ונשאלת השאלה: למה לגעת בו? למה להחליט שאנחנו שמים משתנים מסוימים במודל ואחרים לא, או שתיכף נראה אנחנו מוסיפים אילוצים על המקדמים -- למשל, לא אמרנו שהאומד הליניארי שמצאנו בטא-האט לפי משפט גאוס-מרקוב הוא האומד הליניארי חסר ההטיה בעל השונות הקטנה ביותר?

אז מסתבר שאנחנו מרוויחים הרבה באמצעות רגולריזציה. בראש ובראשונה אנחנו משפרים את שגיאת החיזוי. אם ניזכר ששגיאת החיזוי למשל הריבועית מתפרקת לסכום של טעות אירדוסיבל ועוד ביאס בריבוע ועוד שונות המודל -- נראה היום שאפשר לשלם מעט בהעלאת הביאס, ולהרוויח הרבה בהורדת השונות, כך שבשורה התחתונה הטעות הריבועית תקטן.

ניזכר גם שראינו שאופטימיזם כפי שהגדרנו אותו במודל הליניארי, הוא סדר גודל של p מספר המשתנים חלקי n. כלומר באופן כללי זה עושה שכל להשתמש בכמה שפחות מהם, אם רוצים להקטין את האופטימיזם, את הפער בין הטעות שאנחנו רואים על מדגם הלמידה הספציפי שלנו, וטעות החיזוי הכללית על נתונים שלא ראינו.

אבל לא רק שאנחנו עשויים לשפר את שגיאת החיזוי, במצבים מסוימים המודל הליניארי פשוט לא פיזיבילי ואנחנו נראה שבאמצעות שיטות לבחירת משתנים או רגולריזציה פתאום כן אפשר למדל את הנתונים. זה קורה למשל כאשר יש יותר משתנים מתצפיות, למטריצה X'X אין הופכי, יש אינסוף פתרונות ואנחנו באים לתקן את זה.

היבט אחר שבו רגולריזציה מסייעת לנו הוא האינטרפרטביליות של המודל. בזה שאנחנו נפטרים ממקדמים שהם קטנים מדי לעומת הרעש, שלא סביר שהם שונים מאפס או שאין מספיק דאטא לאמוד אותם בצורה מדויקת -- המודל שלנו הופך לפחות מסובך ואנחנו משיגים פרסימוניה, המודל קומפקטי וברור יותר למי שרוצה ללמוד ממנו משהו.

נזכיר גם שהרבה פעמים שתי המטרות האלה הולכות יחד. נראה שלהיפטר ממשתנים בעלי השפעה קטנה שקשה לאמוד, הרבה פעמים מקטין גם את שגיאת החיזוי.

באופן כללי אפשר לסכם את השיעור שלנו באמירה השנויה במחלוקת: אל תאמינו לכל מה שהנתונים אומרים. מתוך הכרה שהנתונים שלנו הם מדגם מקרי בגודל סופי, אנחנו שמים על המודל מגבלות או מוסיפים קצת הטיה, ומרוויחים מזה בכל החזיתות.

:::

אנחנו נתמקד בשלוש משפחות של שיטות לבחירת משתנים ורגולריזציה: בחירת תת-קבוצה של משתנים ברגרסיה, בכל מיני דרכים. רגולריזציה של מקדמי הרגרסיה באמצעות שיטות מפורסמות כמו רידג' ולאסו. ולבסוף, הורדת מימד לנתונים ממימד גבוה באמצעות PCA, ורק אז ביצוע רגרסיה על הדאטא ממימד נמוך.

:::

=== 2. בחירת תת-קבוצה של משתנים ===

נתחיל בבחירה של תת-קבוצה של משתנים. בשיטות אלה סך המשתנים שעומדים לפני הם רק מועמדים להיכנס לרגרסיה, יכול להיות שאבחר בהם ויכול להיות שלא. ראינו בשיעורים על מודל סלקשן מדדים טובים לאמוד את השגיאה ובהם נשתמש. בסופו של דבר יהיה לי סט של משתנים בהם בחרתי, ואני אריץ רגרסיה ליניארית רגילה על כל הדאטה שלי, רק על סט המשתנים הנבחר.

:::

נתחיל בגישה התמימה ביותר, שעוברת על כל האפשרויות -- בסט סאבסט סלקשן.

מתחילים עם מודל M0 שבו אין משתנים בכלל והחיזוי הוא ממוצע Y.

לאחר מכן בשלב k = 1 אני רוצה לבחון את כל המודלים עם משתנה אחד בלבד, ואני עובר על כל p המודלים האפשריים, כל פעם מכניס משתנה אחר, ובודק איזה משתנה נותן לי את מינימום השגיאה, במקרה שלנו של רגרסיה ליניארית מינימום RSS. למודל הזה אני קורא M1, ואז אני עובר לk = 2, עובר על כל האפשרויות של מודל עם 2 משתנים, בוחר את המודל M2 וכך הלאה עד המודל שמכניס את כל p המשתנים.

כעת אני צריך לבחור בין p + 1 מודלים, ואת זה ראינו בשיעור על מודל סלקשן איך לעשות, אני אבחר במדד שמתחשב במספר המשתנים כי אני יודע שזה מנפח את השגיאה האמיתית על תצפיות שלא ראיתי, מדד כמו הCp של מאלו או AIC, או ביצוע קרוס ולידיישן על המודלים. המודל הסופי יהיה המודל שמביא למינימום את הקריטריון שבחרתי.

:::

בואו נראה דוגמא של הרצת רגרסיה בסט סאבסט על נתוני אשראי. בדוגמה שלנו יש p = 11 משתנים כמו הכנסה, מגבלה על החשבון, האם בעל החשבון הוא סטודנט וכולי. ואנחנו מנסים לחזות את היתרה של בעל החשבון.

על ציר האיקס אנחנו רואים את k בכל שלב של האלגוריתם. בהתחלה k = 0 כלומר אנחנו עובדים רק עם חותך. ואז k = 1 ואנחנו מתאימים 11 מודלים עם משתנה אחד, כל נקודה כאן היא הלוג של הRSS של אחד המודלים האלה. וכך ממשיכים עד המודל האחרון בו מכניסים את כל המשתנים.

עבור כל k אפשר לראות את המודל הנבחר שמשיג את הRSS הנמוך ביותר, הקו האדום מחבר ביניהם. באופן לא מפתיע הקו האדום יכול רק לרדת, את זה ראינו בשיעורים קודמים, הוספת משתנים לרגרסיה יכולה רק לשפר את הRSS.

לכן כשאנחנו באים לבחור באיזה מודל סופי להשתמש אנחנו יכולים לעשות קרוס ולידציה עם המודלים הנבחרים או להשתמש למשל בקריטריון הCp שמוסיף עונש לRSS שגדל ככל שמספר המשתנים עולה. כאן אפשר לראות שהמודל שהביא למינימום את הCp הוא המודל עם 6 משתנים, גם אם קצת קשה לראות את זה מהגרף. נזכיר שזה המודל עם 6 משתנים הטוב ביותר מכל האפשרויות למודלים עם 6 משתנים.

אז כמה מודלים הרצנו כאן? כל המודלים האפשריים, 2 בחזקת 11, שזה קצת יותר מאלפיים מודלים! ברור מייד שהגישה הזאת לא סבירה עבור p גדול הרבה יותר מ10, וגם אז עבור גדלי מדגם לא גדולים מדי.

:::

ולכן הומצאו גישות אחרות של פורוורד או בקוורד סלקשן. הגישות האלה הרבה יותר קלות חישובית, אנחנו בשום פנים ואופן לא נריץ את כל האפשרויות למודל, אבל המחיר שמשלמים עבורן הוא לא קטן, כי הן מאוד גרידיות.

באלגוריתם הפורוורד סטפווייז, מתחילים כמו עם בסט סאבסט במודל שבו יש רק חותך והוא ממוצע Y. כעת k = 0 ואנחנו רוצים להוסיף רק משתנה אחד מתוך p המשתנים, מאוד מזכיר מה שעשינו קודם. נבחר את המשתנה שיביא למינימום RSS ונקרא למודל M1. כשk = 1 נכנס ההבדל, אנחנו לא לוקחים בחשבון את כל האפשרויות למודל עם 2 משתנים, אלא רוצים רק להוסיף משתנה אחד למודל שכבר קיים לנו. אז יש לנו p - 1 אפשרויות. ברגע שבחרנו את המשתנה השני אנחנו בוחרים את המשתנה השלישי וכך הלאה, בלי לגעת בסט המשתנים שכבר נבחרו.

בסוף בשלב השלישי כמו בבסט סאבסט אנחנו בוחרים את המודל הטוב ביותר מבין הקבוצה של מודלים שמצאנו בשלב 2, באמצעות קרוס ולידיצה או קריטריון ראוי אחר של מודל סלקשן.

:::

בדוגמא שלנו זה ייראה כך. מתחילים כמו קודם במודל עם חותך בלבד, ממשיכים אותו דבר עם p אפשרויות להוספת משתנה אחד, ואז יש רק p - 1 אפשרויות להוספת משתנה שני, וכך הלאה עד אפשרות אחרת בלבד להוספת המשתנה האחרון.

כמו קודם הקו האדום הוא שמחבר בין המודלים הטובים ביותר והקו הירוק המקווקו הוא קריטריון הCp למודלים הטובים ביותר, שבוחר גם הפעם, במודל עם 6 משתנים.

אז לפני שנשווה בין הבחירה עצמה של משתנים -- נשאל כמה מודלים הרצנו כאן? p ועוד p - 1 ועוד p - 2... מי שיחשב את סכום הסדרה הזאת יגיע ל-1 ועוד p(p + 1)/2, כלומר סדר גודל של p בריבוע.

:::

אבל ההקלה בסיבוכיות לא באה בלי מחיר, על הגרידיות של האלגוריתם הסטפווייז. כאן אנחנו ממש משווים בין המשתנים עצמם שהבסט סאבסט חוזה מול הפורוורד סטפווייז. עד מודל עם 3 משתנים אין הבדל ביניהם. במודל עם 4 משתנים הבסט סאבסט לוקח בחשבון את כל האפשרויות ומגיע למסקנה שהמודל הכי טוב צריך לזנוח את משתנה הרייטינג, ולהוסיף שני משתנים אחרים. ואילו עבור מודל הפורוורד סטפוויז זאת לא אופציה בכלל! הוא לעולם לא יגיע לאיזור הזה במרחב החיפוש! וזאת יכולה להיות בעיה רצינית של האלגוריתם הזה.

לפני שנעבור לאלגוריתם האחרון בקבוצה, האם אפשר לטפל באמצעות הפורוורד סטפוויז בבעיות ממימד גבוה בהן p גדול מn, כלומר יש יותר משתנים מתצפיות? בודאי, מתחילים עם חותך ועוצרים עבור n - 1 מודלים, כלומר עד שהרגרסיה הליניארית כבר לא פיזיבילית.

:::

האלגוריתם בסט סאבסט האחרון שנלמד נראה בהתחלה יצור מוזר, ולא רק בגלל השם שלו שמזכיר את השם של האלגוריתם הסטפוויז. קוראים לו פורווד סטייג'וויז, והוא מציע לעשות דבר כזה:

נעשה סטנדרטיזציה למשתנים, ניקח איזשהו ערך סף לקורלציה שתיכף נשתמש בו, והכי חשוב - פרמטר אפסילון שייקרא סטפ-סייז, או קצב למידה.

נגדיר את השארית שבחיסור הממוצע של Y מכל התצפיות כR, ונציב אפס בכל מקדמי הרגרסיה.

כעת נמצא את המקדם הJ שיש לו את המתאם הגבוה ביותר עם השארית, כלומר זה המקדם שהכי נכון היה להכניס לרגרסיה, מעבר למודל הקיים.

כל עוד המתאם הזה גדול מאיזשהו ערך סף שאומר מה זה מתאם גבוה, בערך מוחלט, נעדכן את מקדם הרגרסיה של המשתנה הזה. אבל שימו לב לזה, לא נוסיף פשוט את המתאם עצמו, אלא נלך צעד קטן בכיוון שלו דלתא J, שהיא אפסילון בכיוון המתאם. כלומר נוסיף אפסילון לבטא-ג'יי של המשתנה או נחסיר בהתאם לגודל המתאם.

כעת נעדכן את השארית אחרי שחיסרנו ממנה את מה שמידלנו, את המשתנה הJ כפול הדלתא J. ונחזור למצוא את המשתנה ש*עכשיו* הכי מתואם עם השארית.

נחזור לבדוק האם המתאם הזה גדול עדיין מספיק לעומת איזשהו ערך סף, וכך הלאה, עד שלא יישאר משתנה שמתואם מספיק עם השארית. המודל הסופי שלנו מגולם במקדמי הרגרסיה שמצאנו.

חשוב מאוד לשים לב! בכל שלב ניתן לבחור שוב משתנה שכבר נבחר! אפילו בשני שלבים רצופים.

אבל למה שנרצה לעשות את זה? יותר מהכל זה גם נראה למידה נורא איטית לא? אם גיליתם שלמשתנה יש קורלציה של 0.9 עם השארית, למה לא להוסיף אותו כמו שהוא לרגרסיה, למה רק אפסילון ממנו?

עוד נחזור לאלגוריתם המעניין הזה, אבל בינתיים נגיד, שמסתבר שלמידה איטית היא דבר טוב מאוד לשיפור שגיאת החיזוי, וזה עולה בקנה אחד עם העיקרון שאיתו פתחנו את השיעור -- לפעמים שווה לא ללכת עיוורים אחרי הדאטא, ולקחת בערבון מוגבל את הכיוון שהוא מוביל אותנו אליו. זה בדיוק מה שאנחנו עושים עם קצב למידה אפסילון.

:::

=== 3. רגרסיית רידג' ===

את רוב השיעור נקדיש לשיטות של רגולריזציה, איך אנחנו ממתנים את המודל, מפקחים עליו ומגבילים אותו. השיטה הותיקה והמפורסמת ביותר, היא רגרסיית רידג'.

:::

אנחנו לא עושים משהו שונה מאוד. במקום להפחית את מספר הפרמטרים, אנחנו מגבילים אותם, אנחנו מחפשים אותם במרחב מצומצם יותר. ואת זה אנחנו משיגים על-ידי הוספת עונש, פנאלטי, על הנורמה שלהם.

אם הנורמה היא L2 זה נראה ככה: הקריטריון שלנו הוא פנאלייזד RSS או PRSS, כלומר סכום הריבועים הפחותים הרגיל שאנחנו מכירים, ועוד פרמטר למדא גדול מאפס כפול סכום על הריבוע של מקדמי הרגרסיה. העונש הזה שומר על האומדנים לפרמטרים שלא יהיו גדולים מדי, ותיכף נרחיב על איך בוחרים את למדא.

נשים לב שאת החותך אנחנו לא מענישים כי אין בזה היגיון, לכל Y יש פיזור טבעי ואין סיבה לא להתחיל מהממוצע שלו כמנבא בייסליין. אם נסמן את וקטור המקדמים בטא בלי בטא-אפס כבטא כוכב נקבל שאפשר לרשום את הקריטריון שלנו כRSS ועוד למדא כפול הנורמה בריבוע של וקטור בטא-האט.

שימו-לב שצריך לעשות סטנדרטיזציה של המשתנים, כלומר לחסר מכל עמודה של X את הממוצע ולחלק בסטיית התקן. אם מקפידים על זה אפשר לאמוד את בטא-אפס כממוצע של Y. ואז נקבל גם שלמדא משפיע בצורה דומה על כל המשתנים. אחרת שימוש בפרמטר למדא יהיה בעייתי, כי הוא יכול להיות מושפע מאוד ממשתנה ספציפי עם סקאלה מאוד רחבה.

:::

נניח שאנחנו יודעים את למדא. איך נראה הפתרון של רידג'?

בואו נכתוב את הPRSS בצורה וקטורית, כאן כבר ויתרנו על הסימון בטא-כוכב, בטא לא כולל את החותך בטא-אפס.

אנחנו עושים בדיוק מה שעשינו ברגרסיה ליניארית, כלומר גוזרים את הPRSS לפי וקטור בטא, משווים לאפס, ומחלצים את בטא.

הפתרון שקיבלנו מזכיר מאוד את הפתרון של רגרסיה ליניארית, ואפשר כמובן לוודא שהוא נקודת מינימום. ההבדל הוא במטריצה שאנחנו הופכים, כאן נוסף לה הפרמטר למדא הקטן על האלכסון, ומכאן השם רידג', רכס באנגלית.

:::

מה זה נותן לנו, הרידג' הזה על האלכסון? מסתבר שלא מעט, כאן אנחנו נוגעים בהצדקה המקורית לשיטה הזאת.

קודם כל, למי שיש ניסיון בהפיכה של מטריצות סימטריות כמו X'X, יודע שלהוסיף למדא קטן לאלכסון של המטריצה עוזר ליציבות נומרית.

ספציפית כאן, כפי שראינו הסכנה היא שיש קורלציה בין העמודות של X, אז אנחנו קוראים למטריצה X'X איל קונדישונד, ההופכי שלה לא יציב מה שעלול להגדיל מאוד את שונות המקדמים כמו שדיברנו בשיעור על רגרסיה ליניארית. וההוספה הזאת של קבוע חיובי קטן לאלכסון של המטריצה הזאת עוזרת מאוד להילחם בתופעה הזאת.

חוץ מזה כמו שאמרנו במצב שבו העמודות של X תלויות ליניארית, או במצב שיש יותר משתנים מתצפיות ההופכי של X'X בכלל לא קיים, ורגרסיית רידג' בכלל מאפשרת לנו פתרון לבעיה.

אבל אולי ההצדקה הכי מעניינת לרידג' זה גרנטי שיש לנו, בלי הוכחה כרגע, לשיפור טעות החיזוי הריבועית. האומד שלנו בטא-האט שתלוי בלמדא כבר לא חסר הטייה, אנחנו משלמים קצת בהטייה אבל מקבלים הפחתה ניכרת בשונות, ובסך הכל הפחתה בשגיאה הריבועית.

:::

נראה את רגרסיית רידג' בפעולה על הנתונים שלנו ועל הדרך נדגים גם איך בוחרים את הפרמטר למדא.

הפרמטר למדא הוא מה שקרוי היפרפרמטר. זה פרמטר שלא ניתן ללמוד ממדגם הלמידה, אלא צריך לבחור אותו באמצעות בדיקה של אפשרויות שונות, בדרך כלל עם קרוס ולידיישן, ולראות איזו מהן נותנת את שגיאת החיזוי הטובה ביותר על מדגמים שהמודל לא ראה. כאן אנחנו בודקים את זה עם קרוס ולידיישן עם חמישה פולדים ומדווחים את ממוצע הMSE על הטריין ועל הטסט. 

אנחנו רואים כאן כמה דברים:
שבאופן צפוי השגיאה על הטריין נמוכה יותר מאשר השגיאה על הטסט. שהפרמטר למדא הטוב ביותר שמביא למינימום את השגיאה על הטסט הוא קצת פחות מ10 בחזקת 0 כלומר קצת פחות מ1. והכי חשוב, אנחנו רואים שאם אנחנו משווים לשגיאה של רגרסיה ליניארית רגילה, OLS, שהיא לא תלויה בלמדא בכלל כמובן ומופיעה כאן במקווקו, עבור הלמדא הנכון רידג' אכן מצליח להגיע לירידה ניכרת בMSE.

מצד ימין, אנחנו מראים איך נראים המקדמים הנאמדים של 11 המשתנים שלנו, בהתאם ללמדא. כשהלמדא הוא אפסי, אין ענישה בכלל וכל אומדני המקדמים מקבלים את הערכים שהם היו מקבלים ברגרסיה רגילה. באופן צפוי, ככל שלמדא גדול יותר, העונש על הנורמה של וקטור בטא גדול יותר והאילוץ חמור יותר ומאפשר פחות ופחות חופש למודל והמקדמים הנאמדים נעשים קטנים יותר, הם מתכווצים לכיוון אפס. לתופעה כזאת של כיווץ אנחנו קוראים שרינקג', אפשר לראות שהיא עשויה מאוד לסייע להקטין את שגיאת החיזוי ונדבר עליה עוד בהמשך.

:::

=== 4. רגרסיית לאסו ===

נדבר עכשיו על שיטה אחרת ותיקה לעשות רגולריזציה למקדמי הרגרסיה. גם השיטה הזאת, כמו רידג' היא בעלת השלכות ושימושים נרחבים הרבה יותר מרגרסיה ליניארית בלבד, והיא נקראת לאסו.

:::

ברגרסיית לאסו, אנחנו גם מענישים את הנורמה של מקדמי הרגרסיה כמו ברידג', אלא שכאן אנחנו מענישים את הנורמה L1, כלומר סכום המקדמים בערך מוחלט.

גם כאן בטא-אפס לא תחת אילוץ ואם אנחנו עושים סטנדרטיזציה של המשתנים אפשר לאמוד אותו כממוצע Y, והעונש למדא ישפיע במידה דומה על כל הפיצ'רים.

לקריטריון שלפנינו אין פתרון סגור. תיכף נגיד כמה מילים על איך פותרים את בעיית הלאסו. אבל קודם כל, למה שנרצה להעניש דווקא את נורמה L1 ובמה זה שונה מרידג'.

:::

נעיף מבט על שגיאת החיזוי. אפשר לראות שעבור הנתונים שלנו לאסו לא מאוד הועיל. הלמדא שהביא למינימום את שגיאת החיזוי על מדגמי הטסט בקרוס ולידיישן הוא למדא אפסי, ולכן האומדנים למקדמי הרגרסיה שהתקבלו עבור הלמדא האופטימלי הם כמעט ללא שינוי מרגרסיה ליניארית רגילה.

נראה שגם המקדמים של לאסו עוברים שרינקג' אבל הדפוס שבו הם מתכווצים הוא מאוד ייחודי. בעוד שברידג' המקדמים קטנים וקטנים לכיוון אפס, בלאסו בנקודה מסוימת מובטח לנו שהמקדמים מתאפסים, ממש. אפשר לראות כאן שזה קורה אחד אחרי השני.

התופעה הזאת היא ההצדקה המרכזית של שימוש דווקא ברגרסיית לאסו, באנגלית לאסו מביא למודלים ספרסיים עם מעט משתנים, בעברית אנחנו קוראים לזה מודלים דלילים.

כאן אנחנו לא רואים את היתרון של זה, מדובר בסך הכל ב11 משתנים, אבל הדבר יעיל מאוד ברגרסיה לנתונים ממימד גבוה כשP מספר המשתנים גדול מאוד ואולי אפילו גדול ממספר התצפיות. במקרה כזה לא סביר שכל המשתנים שונים באופן מהותי מאפס, ושימוש בלאסו משיג לנו מודל דליל, שהוא לא רק יותר אינטרפרטבילי אלא פעמים רבות מביא לשגיאת חיזוי נמוכה יותר כי הוא מקטין אוברפיטינג.

:::

בחלק הבא ננסה לתת תחושה מה עומד מאחורי המודלים הספרסיים שלאסו נותן. לפני זה נגיד כמה מילים על איך פותרים את בעיית לאסו, כי כפי שאמרנו פתרון סגור אין.

כשרק נוסחה הבעיה של לאסו, המחברים הצליחו להציג אותה כבעית אופטימיזציה די סטנדרטית, שמצריכה תכנות ריבועי או קוואדרטיק פרוגרמינג. מאוחר יותר נמצא פתרון יעיל יותר שנקרא least angle regression, או לארס, שלקח השראה מהתופעה הזאת שאפשר להוכיח, שכשלמדא הולך וקטן כל פעם נכנס משתנה אחר לרגרסיה, והמקדם שלו גדל בצורה ליניארית בקירוב, עד שנכנס המשתנה הבא.

אבל מה בעצם אמרנו כאן? איזה עוד אלגוריתם מכניס בזהירות כל פעם משתנה בצעד קטן עד שיימצא משתנה אחר שהכי מתואם עם Y שצריך להכניס לרגרסיה?

אני מדבר כמובן על אלגוריתם הפורוורד סטייג'וויז. אם ניקח אפסילון מספיק קטן, אפשר להראות שאלגוריתם הסטייג'וויז פותר בקירוב את בעיית לאסו. כאן אנחנו מראים את הדמיון המפתיע בין לאסו לפורוורד סטייג'וויז על הנתונים שלנו, באמצעות השוואה של הפרופילים של המקדמים.

נדגיש לסיום שפורוורד סטייג'וויז נחשבת לשיטה עם שגיאת חיזוי איכותית אבל היא מאוד איטית, ככה המחשב לא פותר את בעיית הלאסו אלא באמצעות שיטות אופטימיזציה. זה עדיין מעניין לראות את הקשר בין השיטות השונות ולנסות להבין מה משותף ומה שונה בהן, ועם הקו הזה נמשיך בחלקים הבאים.

:::

=== 5. על שרינקג' ודלילות ===

האם אנחנו יכולים להגיד עוד על האופן שבו המקדמים מתכווצים ברידג' ומתדללים בלאסו? האמת היא שיש הרבה עבודות בתחום, ומעניין לעבור על כמה תובנות.

:::

ראשית מעניין לציין שגם את רידג' וגם את לאסו ניתן לכתוב בצורה דואלית מעט שונה: ברידג' אנחנו רוצים להביא את הRSS למינימום כפוף לזה שסכום הריבועים של המקדמים או הנורמה L2 בריבוע, קטן או שווה לאיזשהו S.

בלאסו אנחנו רוצים להביא למינימום את הRSS באילוץ על סכום המקדמים בערך מוחלט שיהיה לכל היותר S.

אלה לא ניסוחים שונים כאמור, אפשר להראות שיש מיפוי 1 ל1 בין פרמטר למדא בבעיה המקורית לפרמטר S מתאים בבעיה הדואלית.

אבל הניסוח הזה מראה בצורה ברורה יותר למה יש שרינקג' ולכן השיטות האלה נקראות גם שיטות שרינקג'. הפרמטר S הוא כמו תקציב שהמקדמים צריכים לעמוד בו. גם מאוד ברור ככה למה כדאי לעשות סטנדרטיזציה על המשתנים, אם לא נעשה, כל ה"תקציב" יכול להתבזבז על משתנה אחד עם סקאלה רחבה.

יותר מזה, צורת הכתיבה הזאת מאפשרת לנו גם לרשום את הקריטריון שבסט סאבסט מנסה להציג בכל אחד מהשלבים שלו: מינימום RSS כפוף לכך שלא יותר מS מקדמים יהיו שונים מאפס. אבל זו בעיה קומבינטורית די קשה לפתרון כמו שראינו. אז דרך אחת זה לנסות להשיג את המינימום הזה עם שיטות פורוורד או בקוורד סטפווייז שראינו. דרך אחרת כמו שאנחנו רואים כאן היא לעשות רילקסציה על הקריטריון הקשה הזה. כלומר אנחנו יכולים לחשוב על רידג' ולאסו, כאלטרנטיבות לבסט סאבסט, שמציבות קריטריון קל יותר להשגה, כמו שקורה אגב בהרבה בעיות אחרות באופטימיזציה.

:::

נמשיך לפתח את זה. נאמר ויש לנו רגרסיה עם שני מקדמים בטא אחת ובטא שתיים. מה אומר האילוץ של רידג'? בטא אחת בריבוע ועוד בטא שתיים בריבוע צריך להיות קטן מאיזשהו S. כלומר יש איפשהו בטא-האט שמתאים למינימום RSS, ואנחנו מחפשים נקודה קצת ליד שנמצאת בתוך המעגל הזה, שעומדת בתקציב שלנו. כאן אנחנו רואים את הנקודה הזאת בחיתוך בין הקונטורים של RSS סביב בטא-האט הכי טובה, ברגע שהם נוגעים בתקציב. עכשיו כאן, בשני משתנים התקציב יהיה עיגול, התקציב בתלת-מימד יהיה כדור, בp מימד זה יהיה הייפרספיר.

לעומת זאת איך נראה האילוץ בלאסו? בטא אחת בערך מוחלט ועוד בטא שתיים בערך מוחלט קטן או שווה לאיזשהו תקציב S, זה יוצר לנו צורה של מעוין, עם שפיצים. בתלת מימד זה יהיה מעין יהלום, וככל שנעלה במימד נראה שזאת צורה עם הרבה שפיצים. כלומר כאן יש נטייה לקונטורים של הRSS להיפגש עם שפיצים של מעוין או יהלום. ומה המשמעות של לקבל פתרון באחד השפיצים? בדוגמא הזאת זה אומר שהמקדם בטא אחת מתאפס! מה שלא סביר שיקרה עם האילוץ של רידג'.

כלומר אנחנו ממש יכולים לראות גיאומטרית איך קורה ששרינקג' של לאסו מביא למודלים דלילים.

:::

ואנחנו יכולים לשחק עם זה בצורה אפילו יותר מתוחכמת.

אם נניח שהעמודות של X הן אורתונורמליות, כלומר X'X שווה למטריצת היחידה, ונסמן את בטא-האט כאומד הOLS הקלאסי. אנחנו ממש יכולים לקבל נוסחה לכל מקדם בטא-J ולראות מתמטית גם את השרינקג', גם את הדלילות.

נצייר את הנוסחאות שרשומות כאן כדי לראות את זה טוב יותר. בכל הגרפים הקו הכחול הרציף מייצג את האומד בטא של רגרסיה רגילה, בטא-האט, והקו האדום המקווקו מייצג את בטא-טילדא, גודל הבטא החדש אם מפעילים את אחת השיטות לרגולריזציה שדיברנו עליהן.

ברגרסיית בסט סאבסט שבה אנחנו מחפשים M משתנים הכי טובים, ונסמן את המקדם הכי קטן כבטא-האט M, המקדם בטא-J לא נבחר או מתאפס כל עוד הוא קטן מהמקדם הזה. ברגע שיעבור אותו, יופיע כפי שהוא.

ברגרסיית רידג' הנה השרינקג', ככל שלמדא יהיה גדול יותר כך נכווץ את בטא-J המקורי יותר, לכיוון קו האפס, אבל למדא צריך להיות ממש גדול כדי ממש לאפס את המקדם הזה. אפשר גם לראות שהשרינקג' לא אחיד לכל מקדם. אם מקדם גדול אז כיווץ פי 1 ועוד למדא משמעותי יותר מאשר אם המקדם קטן.

לבסוף ברגרסיית לאסו הסימון הזה אומר המקסימום בין הביטוי בסוגריים לבין אפס. כלומר אם בטא J בערך מוחלט לא גדול מלמדא הוא יקבל אפס. ואם הוא גדול אז נפחית ממנו את למדא, בצורה קבועה לא משנה מה הגודל שלו. אנחנו רואים גם את הדלילות וגם את השרינקג'.

נגיד שבעזרת אלגברה קצת יותר מסובכת אפשר להכליל את ההמחשות האלה גם למצבים מורכבים יותר, אבל אנחנו נעצור כאן, ונעבור לאפילו הצדקה נוספת לשימוש ברידג' ובלאסו.

:::

=== 6. נקודת מבט בייזיאנית ===

נקודת מבט נוספת על רידג' ולאסו מגיעה מכיוון פחות צפוי, של סטטיסטיקה בייזיאנית. אנחנו נראה שהאומדים של רידג' ולאסו הם בדיוק מה שמתקבל עבור המודל הליניארי, תחת ציפיות מסוימות עוד לפני שראינו את הנתונים. אבל קודם נעשה הקדמה קצרה לגבי סטטיסטיקה בייזיאנית, נושא שאפשר לבלות רק עליו סמסטר שלם.

:::

בבסיס של סטטיסטיקה בייזיאנית עומד כלל בייז. ההסתברות למאורע B בהינתן שאני יודע שמאורע A קרה, היא הסתברות החיתוך חלקי הסתברות המאורע המתנה. והסתברות החיתוך ניתן לרשום כהסתברות המאורע B השולית, כפול ההסתברות המותנית ההפוכה של מאורע A בהינתן מאורע B. כלומר כלל בייז מקשר לנו בין ההסתברות האפריורית שהנחנו על מאורע B, לבין ההסתברות הפוסטריורית, בהינתן שאנחנו יודעים שמאורע A קרה. אנחנו נגיד שההסתברות האפוסטריורית שווה למכפלת ההסתברות האפריורית בסבירות או לייקליהוד לראות את מאורע A בהינתן מאורע B, חלקי ההסתברות השולית לראות את מאורע A או האבידנס.

עבור התפלגויות רציפות אין לנו פונקצית הסתברות יש לנו פונקצית צפיפות אבל העיקרון דומה. הצפיפות המותנית של משתנה Y בהינתן שראינו מהו משתנה X, היא הצפיפות השולית של Y כפול הצפיפות המותנית של X בהינתן Y, חלקי הצפיפות השולית של X. הרבה פעמים נרשום שהצפיפות המותנית היא פרופורציונלית למכפלת הצפיפות האפריורית והצפיפות המותנית ההפוכה. 

לא תמיד אנחנו מצליחים לזהות את ההתפלגות המותנית כהתפלגות שמוכרת לנו אבל הרבה פעמים כן.

:::

למשל, אם X מתפלג בצורה שולית נורמלית עם תוחלת ושונות משלו, ו-Y מתפלג בצורה שולית נורמלית עם תוחלת ושונות משלו, הקורלציה ביניהם מסומנת ברו ואני דוגם את X, הוא "קרה", הוא שווה לאיזשהו X קטן. אז כעת אם אני ארשום את הצפיפות המותנית של Y בהינתן X לפי ההגדרה שראינו ואעשה קצת אלגברה, אני אזהה בסופו של דבר שההתפלגות של Y בהינתן X היא התפלגות שמוכרת לי. היא עדיין התפלגות נורמלית, רק שהתוחלת והשונות האפוסטריורים התעדכנו. למשל התוחלת היא התוחלת המקורית של Y ועוד תוספת שתלויה ביחס בין סטיות התקן, בגודל וסימן של רו, ובמרחק של X שהתקבל מהתוחלת שלו. מעניין לציין שהשונות של Y בהינתן X, אלא אם כן רו שווה לאפס, בהכרח קטנה, מה שעושה שכל, כי צפיתי כבר במשתנה שמתואם עם Y, הציפיות שלי מתעדכנות ויש הרבה פחות שונות לערכים שY יכול לקבל.

ניתן לראות את זה בגרף שלפנינו. X וגם Y מתפלגים אפריורית נאמר נורמלית סטנדרטית, עם תוחלת אפס ושונות 1. אם אני יודע שX נדגם והיה שווה ל-1, ובין X לY יש קורלציה חיובית, נאמר חצי, זה אומר שאני מעדכן את הידע שלי על Y, התוחלת שלו זזה ימינה לעבר ערכים גבוהים יותר, במקרה הזה לחצי אם תציבו בנוסחה. והשונות שלו קטנה יותר. ואם הקורלציה היא גבוהה ממש, 0.9, אז בכלל אני מעדכן את ההתפלגות המותנית של Y להיות עם פיזור ממש קטן סביב התוחלת החדשה שמתקרבת ל-1.

:::

בסטטיסטיקה בייזיאנית באופן כללי, לאו דוקא בבעיות רגרסיה, הפרמטר שאנחנו מנסים לאמוד תטא, הוא כבר לא פרמטר. הוא משתנה מקרי, שיש לו התפלגות אפריורית, פי של תטא. אם הוא רציף כמובן נדבר על צפיפויות, נהוג לסמן פי של תטא באופן כללי.

כעת מגיע משתנה אחר שאנחנו רואים, והוא מדגם מקרי של נתונים Y. למדגם הזה יש הסתברות מותנית או לייקליהוד.

וכעת, אחרי שראינו את הנתונים, אנחנו רוצים לעדכן את הידע שלנו לגבי תטא, את ההתפלגות האפוסטריורית של תטא בהינתן הנתונים, שהיא כאמור פרופורציונלית ללייקליהוד כפול הפריור. בדרך כלל נשתמש בפריור ובלייקליהוד נוחים ככה שנוכל לזהות את ההתפלגות האפוסטריורית בקלות.

ואם אנחנו רוצים לאמוד את תטא, נשתמש בתוחלת של ההתפלגות הזאת, או בשכיח.

עכשיו כדאי לשאול למה לעשות את זה, בפרט שצורת החשיבה הזאת מכריחה אותנו להוסיף עוד הנחות וציפיות, שלא ברור בכלל שיש לנו. והתשובה כמו תמיד בסטטיסטיקה נעוצה בהפחתת השונות של האומד, רמז לזה ראינו בדוגמא בשקף הקודם. אם יש לכם איזשהו ידע קודם על העולם, על הפרמטר הזה שאתם רוצים לאמוד, למשל שהוא תחום בין גבולות מסוימים, כמו מטבע שלא סביר שהסיכוי בו לעץ הוא נמוך יותר מ20 אחוז או גבוה יותר מ80 אחוז -- זה ידע שכדאי להשתמש בו כי האמידה הופכת לטובה ומדויקת יותר, מאשר אם באים לבעיה כזאת טאבולה ראסה. יש פחות סיכוי לדאטא לקחת אותנו איתו לכיוונים לא צפויים.

זה יכול להיות פרמטר אחד וזה יכול להיות וקטור של פרמטרים כמו בבעיה שלנו של רגרסיה ליניארית. ברגרסיה ליניארית למשל נוח להשתמש בפריור על המקדמים של התפלגות נורמלית. כלומר הוקטור עצמו בעל התפלגות רב נורמלית עם איזושהי שונות על אלכסון המטריצת קווריאנס שהיא נגיד טאו בריבוע. מראש אני מניח את זה. מגיע הדאטא עם הלייקליהוד המוכר של התפלגות נורמלית עם תוחלת שהיא הצירוף הליניארי ושונות סיגמא בריבוע.

וכעת מסתבר כמו מקודם שיש צורה סגורה ופשוטה להתפלגות האפוסטריורית של מקדמי הרגרסיה אחרי שראינו את הדאטא. הם מתפלגים נורמלית, עם איזושהי מטריצת שונות סיגמא שלא פרטנו כאן, ושימו לב לתוחלת. X'X + המנה של סיגמא בריבוע חלקי טאו בריבוע על האלכסון, בהופכי, X'y.

התוחלת המותנית הזאת מוכרת לנו! זה בדיוק פתרון רידג', אם נגדיר את למדא להיות היחס בין סיגמא בריבוע לטאו בריבוע. והרי אמרנו שזה גם האומד שניקח כאומד סופי, התוחלת של ההתפלגות האפוסטריורית. וכשאנחנו מנסחים את זה ככה גם המשמעות של למדא מקבלת מימד נוסף. ככל שטאו בריבוע קטן יותר, אני מניח מראש שהמקדמים שלי מתפלגים עם שונות קטנה יותר מסביב לאפס, האילוץ חמור יותר, ככה גדלה למדא. ככל שטאו בריבוע גדלה יותר, אני מאפשר למקדמים לנוע חופשי סביב האפס ככה למדא קטנה יותר והאומד הסופי כבר לא שונה מהאומד של רגרסיה ליניארית רגילה. כלומר רגרסיה ליניארית רגילה מתקבלת עם מה שאנחנו קוראים פלאט פריור, פריור שטוח, למשל התפלגות נורמלית עם שונות גדולה מאוד או אינסופית.

:::

אבל בזה זה לא נגמר. אמרנו שהאומד לרגרסיה ליניארית רגילה מתקבל עם פריור שטוח, אין לי שום ציפיות מראש לפיזור המקדמים. האומד של רידג' מתקבל עבור פריור נורמלי עם איזושהי שונות סופית. ומסתבר, שהאומד של לאסו מתקבל, אם הפריור שלנו הוא התפלגות עם שפיץ באפס שנקראת לפלאס או double exponential. כלומר אנחנו מניחים מראש שהמקדמים מתפלגים סימטרית סביב האפס אבל עם הרבה יותר צפיפות קרוב לאפס. ולכן גם כאן אנחנו רואים איך סביר בלאסו להיתקל במקדמים שהם פשוט אפס.

עד כאן לגבי שיטות רגולריזציה. נגיד שזה ממש לא הסוף, יש עוד מובנים גם לרידג' וגם ללאסו, יש עוד שיטות לרגולריזציה שמנסות לשלב ביניהן. אבל הכי חשוב העיקרון של רגולריזציה הוא לא רק לרגרסיה ליניארית, הוא אפילו חשוב יותר, במודלים מורכבים יותר שתלמדו כמו עצי החלטה ורשתות נוירונים. הוא חשוב יותר ככל שהמודל גמיש יותר וצריך לאלף אותו.

:::

=== 7. רגרסיה לאחר PCA ===

הסוג האחרון של וריאציות לרגרסיה שנדבר עליהן מנסה להפחית קודם כל את המימד של הנתונים, ורק אז לבצע רגרסיה רגילה על הדאטא עם המימד הקטן יותר. אנחנו נתמקד רק ברגרסיה אחרי PCA או PCR אבל גם כאן יש עוד הרבה שיטות להורדת מימד. לבסוף כפי שעשינו עד כה נראה איך גם בשיטות האלה ניתן לראות כעוד גרסה של רגולריזציה, או לקיחת אילוץ על המקדמים.

לפני שנמשיך אזהרה: אנחנו עוד נלמד על PCA לעומק, כאן אנחנו עושים חזרה קצרה למי שכבר מכיר את הפרוצדורה. מי שלא -- מומלץ לחזור לחלק הזה של השיעור רק לאחר שלמדנו על PCA.

:::

ניזכר מה אנחנו מנסים לעשות בPCA. נניח שיש לנו נתונים בשני מימדים X1 וX2, ואנחנו רוצים לתמצת אותם במימד אחד. במקום שני מספרים שיתארו כל תצפית, אנחנו רוצים מספר אחד. היינו רוצים לשמר את מירב האינפורמציה שיש בנתונים, ובמונחים סטטיסטיים זה אומר לשמר את מירב השונות שלהם. בPCA אנחנו עושים את זה באמצעות מציאת הכיוון של הכי הרבה שונות בדאטא. כאן זה די ברור, זה הכיוון הזה בקו האדום. ברגע שמצאנו את הכיוון הזה נטיל את הנתונים שלנו עליו, כלומר עבור כל תצפית נראה מה הערך שלה על הקו הזה כשאנחנו מסתכלים על המרחק הכי קצר אליו. והתוצאה היא ההטלה מימין, כאן על המימד החדש שנקרא T1 לכל תצפית יש רק ערך אחד, אין משמעות לציר הY, והטענה היא שככה שמרנו על הכי הרבה אינפורמציה במעבר מדו-מימד לחד-מימד.

:::

באופן כללי יותר נגיד שהמטרה שלנו היא למצוא Q כיוונים כאלה ששומרים על מקסימום פיזור או שונות.

כיוון כזה כמו הוקטור האדום שלנו אפשר לסמן בווי, והטלה של הנתונים עליו היא מכפלה של X כפול V, מה שנותן וקטור חדש באורך n, קודם קראנו לו T1.

לדוגמא, ההטלה יכלה להיות הוקטור שיש לו 1 באלמנט הראשון שלו, ואפס בכל השאר -- אם מכפילים את X בוקטור כזה זה בדיוק כמו לבחור רק את המשתנה הראשון בנתונים.

למשל, הוקטור וי יכול להיות וקטור שבכל האלמנטים שלו יש 1 חלקי שורש P. אם תעשו קצת אלגברה תראו שלהכפיל את X בוקטור כזה אומר עבור כל תצפית לקחת את הממוצע שלה על כל המשתנים.

מה זה פיזור של וקטור ההטלה שאנחנו רוצים למקסם? אנחנו ניקח את הנורמה בריבוע של וקטור ההטלה, אותה אפשר לרשום כv'X'Xv.

והנה הבעיה שלנו, למצוא את הכיוון, הוקטור v1 בעל נורמה 1, שממקסם את פיזור ההטלה. אנחנו עוד נלמד על PCA יותר לעומק ונראה למה אנחנו מוכרחים את האילוץ הזה על הוקטור וי.

כשנמצא את הוקטור הזה נקרא לו פרינסיפל קומפוננט דירקשן: זה הכיוון הכי טוב להטיל עליו על פי קריטריון המקסימום פיזור.

כעת נמצא את הכיוון הבא v2, v3 וכולי. נראה שהם צריכים להיות אורתוגונליים זה לזה. ונשים אותם זה לצד זה במטריצה W מסדר p שורות על q עמודות. זאת מטריצת ההטלה הסופית.

את הדאטא ממימד נמוך יותר נכתוב כT. הדאטא שלנו עבר הפחתת מימד מp שיכול להיות גבוה מאוד, לq שיכול להיות קטן מאוד, והוא מוכן לרגרסיה.

:::

נסכם איך אנחנו מבצעים PCR:

עושים סטנדרטיזציה על עמודות X.

מוצאים את מטריצת ההטלה W, בדרך כלל על-ידי פירוק SVD ועוד נחזור לזה כשנלמד PCA לעומק.

וכעת נבצע רגרסיה ליניארית עם המודל הליניארי הרגיל, על הדאטא אחרי הורדת מימד. אנחנו מחפשים עכשיו וקטור מקדמים תטא באורך Q ועוד 1, שכשנכפיל אותו פי הנתונים החדשים שלנו T נקבל בקירוב את המשתנה התלוי Y.

נשים לב שכמו שאר השיטות שלנו אין רק פתרון אחד, לא דיברנו על איך יודעים מהו Q, מה המימד הנכון לבחור? למעשה Q הופך להיפרפרמטר נוסף בדומה ללמדא, שצריך לבחור, למשל דרך קרוס ולידיישן.

עכשיו מה הקשר לשיטות שלמדנו: נביט שוב במודל שאנחנו מניחים ונראה שבהכרח זה אומר שאנחנו מחפשים וקטור מקדמים בטא ששווה לW כפול תטא. והנה אנחנו רואים שוב קשר לתמה הכללית שלנו, אפשר לראות בביטוי הזה כאילוץ על הבטאות, המקדמים בעצמם חייבים להיות צירוף ליניארי על וקטור מקדמים ממימד נמוך יותר!

וזה לא הקשר היחיד. כשלומדים על פירוק הSVD רואים אפילו קשר נוסף בין PCR לרגרסיית רידג'.

:::

בנתונים שלנו המימד P מלכתחילה לא כל כך גבוה, הוא 11, ולא ברור שצריך הורדת מימד באמצעות PCA או כל שיטה אחרת לפני הרצת רגרסיה.

ואכן, אם אנחנו בודקים מה הQ שמביא למינימום שגיאת חיזוי באמצעות קרוס ולידיישן כמו שעשינו עד עכשיו, אנחנו מקבלים שבנתונים שלנו לא כדאי כמעט להוריד מימד, הQ הטוב ביותר הוא 10. ואמנם לא רואים את זה בגרף אבל כן יש הפחתה בשורה התחתונה בMSE של הטסט לעומת רגרסיה רגילה, אם כי אחוזית זה הפרש די קטן.

נשים לב שברגע שמבינים שאפשר לחזור מוקטור המקדמים תטא אל וקטור המקדמים המקורי בטא באמצעות הכפלה פי מטריצת W, אפשר לראות את פרופיל המקדמים בטא כמו שעשינו עד עכשיו עם פרמטר הפנאלטי למדא. אנחנו רואים שגם בPCR המקדמים עוברים שרינקג' שמזכיר את רידג', ככל שQ קטן יותר או הורדת המימד היא אגרסיבית יותר. הכיוון כאן הפוך כי לא רצינו להפוך את ציר הX בצורה מלאכותית מ11 ל-1, אבל הרעיון זהה.

עד כאן בשיטות בחירה של משתנים ורגולריזציה. בשיעורים הבאים נלמד מודלים מורכבים וגמישים הרבה יותר, חלק מהם כל כך גמישים שהם באים בילט-אין עם רגולריזציה. העקרונות שראינו היום ישמשו אתכם לשיפור כל מודל מורכב שתראו בעתיד והם לא המלצה הם כבר חלק מהמודלים המודרנים ביותר שנמצאים בשימוש יומיומי, ולכן מומלץ להכיר אותם ואת החשיבות שלהם היטב.

:::
