---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "The Bias-Variance Tradeoff"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### The Bias-Variance Tradeoff - Class 2

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## The Bias-Variance Tradeoff {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשפירקנו את הטעות הריבועית הנחנו לרגע שY האט הוא נתון, קיבלנו אותו מאיפשהו. בפועל ברור שזה לא פרקטי, Y האט הוא גם כן משתנה מקרי, הוא גם תוצר של תהליך שיש שיגידו שכולל אפילו את מי מידל את הנתונים, ואיך הוא בחר את המודל וכולי. לכל הפחות נרצה להוסיף את האקראיות של מדגם הלמידה, הרי ראינו מדגם אחד ויכולנו לראות מדגם אחר. בשיעור זה נראה פירוק מפורט יותר של הטעות הריבועית, שמוביל לטריידאוף חשוב, הביאס-וריאנס טריידאוף. נראה גם כיצד הוא בא לידי ביטוי בקלסיפיקציה, ונדגים זאת באמצעות KNN, שיטת השכנים הקרובים.
:::
:::

---

### Squared error decomposition

- For regression, take the standard model: $y = f(x) + \epsilon\;,\;\epsilon \sim (0,\sigma^2)$

- Modeling approach (e.g. OLS), given training data $T$, gives model $\hat{f}(x)$

::: {.fragment}
- Assume we want to predict at new point $x_0$, and understand our expected (squared) prediction error: 

$\mathbb{E}_{y_0, T}(y_0 - \hat{f}(x_0))^2 = \mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2$

- Note we treat both the training data $T$ (and hence $\hat{f}$) and the response $y_0$ as random variables in our expectations
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתמקד תחילה ברגרסיה, שם אנחנו מניחים שY הוא איזושהי פונקציה f של X ועוד רעש עם תוחלת אפס ושונות סיגמא בריבוע, לאו דוקא מהתפלגות נורמלית. מהו f אם ככה, אם התוחלת של אפסילון היא אפס? f הוא התוחלת המותנית של Y בהינתן הנתונים בX. הערך הנמדד של Y הוא לא בדיוק הערך הצפוי שלו, התוחלת, אלא התוחלת ועוד איזשהו רעש.

כעת אנחנו לוקחים טריינינג דאטא T וממנו לומדים את f. אנחנו יכולים עכשיו לבדוק את הביצועים של המודל על מדגם הטסט, אבל אני רוצה שנחשוב על כל התהליך הזה של דגימת נתונים, בניית מודל f, למידה שלו מהנתונים - כעל תהליך אקראי. ולהסתכל על התוחלת של הלוס שלנו, לדוגמא ברגרסיה השגיאה הריבועית, על פני ביצוע התהליך הזה הרבה פעמים, אם היה לי טריינינג דאטא קצת שונה כל פעם. רוצה לומר גם f_hat המודל שבנינו הוא משתנה מקרי שמבוסס על מקריות מדגם הלמידה שלנו!

אז תגיע תצפית חדשה X0 וכמו שאמרנו נסתכל על תוחלת הלוס שלה, כאן השגיאה בין Y0 האמיתי שלה, למודל שבנינו f_hat של X0.

כעת אני רושם את ההפרש כסכום של כמה אלמנטים. כל מה שאני עושה זה לחסר ולהוסיף את f של X0, שהוא התלות האמיתית של Y0 בX0, התוחלת המותנית. וגם מחסר ומוסיף את התוחלת של f_hat. שוב, למה זה דבר שקיים לנו? כי f_hat הוא בעצמו מעין משתנה מקרי שכאילו נדגם מתוך הרבה מודלים סופיים שנובעים ממדגמי למידה שונים.

כעת אנחנו פשוט שמים סוגריים ומסמנים אלמנט A, אלמנט B ואלמנט C. נרצה לפתח את הריבוע ולראות אכן ששגיאת החיזוי שלנו בתוחלת מתפרקת לביטויים מעניינים ולתת להם את השם המתמטי שלהם.
:::
:::

---

Which factors are random variables, dependent on $T$?

$\mathbb{E}(y_0 - \hat{f}(x_0))^2 = \mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2$

$$A = y_0 - f(x_0)$$

$$B = f(x_0) - \mathbb{E} (\hat{f}(x_0))$$

$$C = \mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אילו מהגורמים תלויים באקראיות מדגם הלמידה?

הגורם A - לא תלוי כלל במדגם הלמידה. הוא מבטא איזושהי אמת, רעש טבעי שקיים ולא נוכל להקטין, זהו בעצם האפסילון אפס כלומר כן משתנה מקרי אבל לא תלוי במדגם הלמידה.

הגורם B - נראה בהתחלה שתלוי במדגם הלמידה כי יש בו את המודל הנאמד f_hat, אבל יש כאן תוחלת, כלומר זה קבוע. כך שגורם זה הוא מספר, לא משתנה מקרי, ומה היינו מצפים שיהיה המספר הזה? אפס. אנחנו מקווים שהמודל שלנו עשיר מספיק שבסופו של דבר בתוחלת הוא קירוב טוב ליחס האמיתי f.

הגורם C - כאן בעצם יש את הגורם שתלוי במדגם הלמידה - המרחק של f_hat המודל שלמדנו מהתוחלת שלו. אם אני מעלה את זה בריבוע ולוקח תוחלת מה זה? שונות! שונות המודל f_hat.

כך שעוד לפני שהוכחנו, אתם כבר יכולים להבין שגורם A הוא איזושהי טעות שאין לי הרבה מה לעשות לגביה. גורם B הוא בעצם האפרוקסימיישן ארור, כמה המודל שלנו בתוחלת עשיר מספיק כדי לבטא את הקשר האמיתי בין X לY. וגורם C הוא האסטימיישן ארור, האם יש לי מספיק דאטא כדי שאם אחזור על התהליך הזה עם דאטא קצת אחר אקבל מודל דומה, כלומר כמה קטנה השונות של המודל עצמו.
:::
:::

---

### The bias-variance decomposition

$\mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2 =$<br><br>
$\;\;\;\;\;\;\;\;\;\;\;= \mathbb{E} A^2 + B^2 + \mathbb{E} C^2 + 2 B \cdot \mathbb{E} A + 2 \mathbb{E} (AC) + 2B \cdot\mathbb{E} C$<br><br>

::: {.fragment}
$\mathbb{E}(A^2) = \sigma^2$ the **Irreducible error** of a perfect model which knows the true $f$ 

$B^2 = \left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)^2$ is the **squared bias** --- a measure of approximation error (note $B$ is not a random variable)

$\mathbb{E}(C^2) = \mathbb{E} \left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0) \right)^2$ is the **variance** of the prediction --- a measure of estimation error

$B \cdot\mathbb{E} A = \mathbb{E} (AC) = B \cdot \mathbb{E} C = 0$ due to independence and mean-0 relations
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת כשאני מעלה את סכום שלושת הגורמים בריבוע אני מקבל את כל אחד מהם בריבוע ועוד 2 כפול מכפלה של כל זוג. כשאני לוקח תוחלת, אני טוען שאנחנו נשארים עם הביטוי שלפנינו. מדוע? נסתכל שוב על הגורמים שקיבלנו, בריבוע:

התוחלת של A בריבוע היא התוחלת של אפסילון בריבוע, כלומר היא השונות של אפסילון, שהיא סיגמא בריבוע. אנחנו קוראים לזה irreducible error, זה הרעש הקיים בטבע, שגם תחת מודל מדויק לא נצליח להפחית. אפסילון לא תלוי בדאטא שלנו, בהגדרה אין לנו דאטא לחזות אותו.

הגורם השני -- B בריבוע -- הוא כאמור קבוע, לא משתנה מקרי, והוא מבטא את האפרוקסימיישן ארור, עד כמה המודל שלנו בתוחלת עשיר מספיק וקרוב לf האמיתית. היינו רוצים שזה יהיה אפס, שלא תהיה הטיה בילט אין במודל שלנו. אבל אולי למשל לא הכנסנו את כל המשתנים שצריך, ויש הטיה, אז זאת מעין הטיה בריבוע, ואכן אנחנו קוראים לזה squared bias.

הגורם השלישי -- C בריבוע -- הוא כמו שאמרנו הvariance של החיזוי בנקודה X0, וזה מדד לאסטימיישן ארור. מודל עם שונות נמוכה, גם אם אקח דאטא קצת אחר אקבל חיזוי מאוד דומה, האמידה תהיה יציבה. מודל עם שונות גבוהה -- אם אקח דאטא קצת אחר אקבל חיזוי שונה, האמידה לא יציבה. מתכון לטיפול בטעות כזאת יכול להיות למשל להגדיל את מדגם הלמידה, כאן הוא בא לידי ביטוי.

מה נשאר לנו? להראות שכל שאר הביטויים של מכפלות הם אפס.

התוחלת של הגורם A, היא התוחלת של אפסילון, היא אפס. אז כל הביטוי של B כפול התוחלת של A הוא אפס.

התוחלת של C, היא התוחלת של f_hat פחות התוחלת של f_hat, כלומר גם היא 0 וכל הביטוי של B כפול התוחלת של C הוא אפס.

התוחלת של מכפלת A ו-C היא גם כן אפס. כי A ו-C הם משתנים מקריים בלתי תלויים לכן התוחלת של המכפלה שלהם היא מכפלת התוחלות והתוחלת של כל אחד מהם היא אפס.

:::
:::

---

### The bias-variance decomposition

$$\mathbb{E}(y_0 - \hat{f}(x_0))^2 = \text{irreducible error} + \text{squared bias} + \text{variance}$$

- Our general intuition: as model flexibility increases, bias (approximation error) decreases and variance (estimation error) increases 

- For many models we can calculate and show these effects on the bias and variance of the model

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כך שאנחנו רואים ששגיאת החיזוי שלנו בשורה התחתונה, על דאטא שהמודל לא ראה, היא סכום הגורמים הריבועיים: שגיאת רעש טבעי שאינה תלויה בדאטא או במודל. שגיאת הטייה ריבועית, שתלויה במודל אבל לא בדאטא. ושגיאת שונות המודל שתלויה גם במודל וגם בדאטא.

באופן כללי אנחנו רואים שהמתמטיקה מסתדרת עם האינטואיציה שלנו, ואפשר ממש לחשב את הטעויות האלה ולהראות שככל שהמודל מורכב יותר הביאס או אפרוקסימיישן יורד, והוריאנס או האסטימיישן עולה.

לדוגמא ברגרסיה ליניארית, ככל שנוסיף עוד משתנים המודל יהיה מורכב ומדויק יותר, ההטיה תרד ותרד. ומצד שני השונות תגדל ותגדל, האומדים שלנו ייהפכו יותר ויותר לא מדויקים ונצטרך יותר ויותר דאטא כדי שזה לא יתדרדר.

:::
:::

---

### Simulation example: bias, variance and prediction error

- Let's generate data according to the following model: $x \in \mathbb R^{20}$ has multivariate normal distribution, $y = \sum_{j=1}^{20} \sqrt{(21-j)/2} \times x_j + \epsilon\;,\;\epsilon \sim N(0, 1000)$ 

- So the true model is in fact linear with $\beta = (\sqrt{10},\sqrt{9.5},\dots,\sqrt{0.5})^t$

- We have $n=50$ training observations, and want to predict at $x_0 = (1,1,\dots,1)^t\; \Rightarrow\; y_0 = 43.6 + \epsilon$

- By generating many training sets and $\hat{f}$'s we can evaluate bias, variance and prediction error

- Complexity parameter: number of variables included in the model (only the first coordinate, first two, ...)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה כעת סימולציה שממחישה את הדפוס הזה יפה עם מודל ליניארי. יש לנו וקטור משתנים מסבירים X בגודל P = 20. והמקדמים שלו לפי הנוסחה הזאת, שורש 10, שורש 9.5 וכולי, כלומר הם הולכים וקטנים עד שורש חצי. המקדמים הראשונים הם הכי "חשובים, משפיעים" על Y והאחרונים פחות. זאת הf האמיתית שלנו, מודל ליניארי, אלה הם הבטאות.

כעת תעניין אותנו תצפית חדשה לחיזוי X0 שהיא בעצם 1 בכל המשתנים, אם נחשב את Y0 נראה שהוא צריך להיות 43.6 ועוד איזשהו רעש אפסילון שנדגם מהתפלגות הרעש, כאן היא נורמלית עם שונות אלף. ויהיה לנו תקציב של 50 תצפיות בלבד.

נייצר הרבה מדגמים עם היחס האמיתי הזה וכל פעם נבנה מודל f_hat אחר, על מדגמים שונים ועם מספר משתנים הולך וגדל, כלומר מורכב יותר ויותר. בצורה הזאת אנחנו יכולים לאמוד ממש אמפירית את שלושת הגדלים שחישבנו ולראות שהם מסתכמים בשגיאת החיזוי. מובן שאת הreducible error לא צריך לאמוד, היא ידועה בסימולציה.
:::
:::

---

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```

```{python}
#| code-line-numbers: "|1-7|9|10-12|13|14|15-16|18-20|"

ntr = 50
p = 20
sigma_sq = 1000
n_iter = 1000
beta = np.sqrt(np.array(range(20, 0, -1)) / 2)
yhat_0 = np.zeros((20, n_iter))
err = np.zeros((20, n_iter))

for iteration in range(n_iter):
    X = np.random.normal(loc=0.0, scale=1.0, size=(ntr, p))
    Y = X @ beta + np.random.normal(loc=0.0, scale=np.sqrt(sigma_sq), size=ntr)
    y0 = np.sum(beta) + np.random.normal(loc=0.0, scale=np.sqrt(sigma_sq), size=1)
    for pnow in range(1, p + 1):
        betahat = np.linalg.inv(X[:, :pnow].T @ X[:, :pnow]) @ X[:, :pnow].T @ Y
        yhat_0[pnow - 1, iteration] = np.sum(betahat)
        err[pnow - 1, iteration] = y0 - np.sum(betahat)
        
pred_err = np.mean(err**2, axis=1) # pred-error
pred_bias = (np.mean(yhat_0, axis=1) - np.sum(beta))**2 # E(B^2)
pred_var = np.var(yhat_0, axis=1) # E(C^2)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אני מאתחל את מספר התצפיות, את סיגמא בריבוע, מספר האיטרציות והבטאות לפי הנוסחא. אני מאתחל גם מערך של החיזויים ל-Y0 לכל אחד מ-20 המודלים לכל 1000 האיטרציות. ואותו גודל של מערך לשגיאת החיזוי הסופית.

כעת נחזור על 20 המודלים 1000 פעם כדי שנוכל לחשב ממוצע עליהם, כמו תוחלת.

אנחנו מגרילים X עם 50 תצפיות ו-20 משתנים מהתפלגות נורמלית סטנדרטית. מחשבים את Y באמצעות המודל האמיתי f שהוא המודל הליניארי ומוסיפים את האפסילונים מהתפלגות נורמלית עם שונות 1000. לבסוף אנחנו דוגמים תצפית אמיתית שלא תהיה חלק מהמודל, Y0. למה Y0 שווה לסכום הבטאות ועוד רעש אפסילון? כי אמרנו שX0, התצפית החדשה, תהיה וקטור של אחדות.

עכשיו אני בונה 20 מודלים, מתחיל ממשתנה אחד ומוסיף כל פעם משתנה למודל מורכב יותר ויותר עד 20 משתנים במודל.

לכל מודל כזה מחשב את בטא-האט על מדגם הלמידה לפי הנוסחה המוכרת לרגרסיה ליניארית, ואם לא מוכרת עוד נחזור עליה.

ואז מחשב את החיזוי לתצפית החדשה X0, זה כאמור סכום האומדנים לבטאות, הבטא-האט. ואת טעות החיזוי, Y0 האמיתית פחות Y0 החזוי.

עכשיו אני מחשב את תוחלת טעות החיזוי הריבועית. זה בעצם מיצוע של השגיאות בריבוע, לכל אחד מ20 המודלים על פני 1000 איטרציות. נחשב גם את ההטיה בריבוע, זה ממוצע החיזויים על פני 1000 איטרציות, פחות f0, התצפית האמיתית ללא רעש, וכל זה בריבוע. ולבסוף, התוחלת של C בריבוע זה בעצם שונות החיזוי כמו שראינו. איפה הגורם A בריבוע? אמרנו שלא צריך לאמוד אותו, זה סיגמא בריבוע שידועה בסימולציה, היא 1000.
:::
:::

---

```{python}
plt.plot(range(1, p + 1), pred_err, color='green', lw=2, label='prediction error' )
plt.plot([1, p + 1], [sigma_sq, sigma_sq], color='black', lw=1, label='$E(A^2)$:irreducible error')
plt.plot(range(1, p + 1), pred_bias, color='red', lw=2, label='$E(B^2)$:prediction bias squared')
plt.plot(range(1, p + 1), pred_var, color='blue', lw=2, label='$E(C^2)$:prediction variance')
plt.xlabel('dimension')
plt.ylabel('squared error')
plt.title('Error decomposition simulation')
plt.legend(loc="upper right")
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ובאמת, כשאנחנו מסרטטים את טעות החיזוי, הirreducible error, ההטיה הריבועית והשונות מתקבל הדפוס הצפוי.

הirreducible error היא כמובן קבוע, 1000, קו ישר. ההטיה בריבוע, הקו האדום, מתחיל מאוד גבוה ממודל פשטני מדי שכולל רק משתנה אחד ועד המודל הנכון עם 20 משתנים שיש לו הטיה אפס. אם אגב במודל רק 15 משתנים היו רלוונטיים, היינו מגיעים לאפס הזה מהר יותר, ב-15 משתנים. לבסוף שונות החיזוי, מתחילה מכמעט אפס עבור מודל פשטני עם משתנה אחד, 50 תצפיות מספיקות כדי להעריך את הפרמטר שלו במדויק. אבל השונות עולה ועולה ככל שהמודל מורכב יותר, 50 תצפיות פשוט לא מספיקות לשערך מודל עם 20 משתנים גם אם הוא נכון. שימו-לב שהיא עולה בצורה ליניארית, אנחנו נוכיח מאוחר יותר למה זה צפוי.

ומה עם שגיאת החיזוי הריבועית, בשורה התחתונה? אפשר לראות שבקירוב היא שווה לסכום שלוש השגיאות הריבועיות האחרות. והיא בעלת צורת U: עם מעט מדי משתנים שגיאת ההטיה גבוהה מאוד אבל השונות נמוכה ובסך הכל שגיאת החיזוי הריבועית גבוהה. איפשהו באמצע עם 10-12 משתנים ההטיה והשונות מספיק נמוכות ואנחנו משיגים מינימום בשגיאת החיזוי הריבועית. ועם כל המשתנים במודל, ההטיה אכן מגיעה לאפס, זה המודל הנכון, אבל אין מספיק תצפיות לאמוד אותו, יש רק 50 תצפיות. השונות גבוהה כל כך על פני הרבה מדגמי למידה, כך שבסך הכל גם שגיאת החיזוי על פני נתונים שהמודל לא ראה גבוהה מאוד.

שימו-לב שוב, זה נכון שהמודל עם 20 משתנים הוא הוא הנכון ביותר. אבל עם תקציב של 50 תצפיות, המסר הוא שניתן לשערך היטב מודל של כ12, משתנים בלבד.

עד כאן פירוק הביאס-וריאנס ברגרסיה שמסביר את עקומת הU שנראה בדרך כלל על מדגם הטסט שלנו, כפונקציה של גמישות המודל. ממודל פשטני מדי אבל שלא משתנה על פני הרבה מדגמי למידה, ועד מודל מורכב מדי שמסוגל לתאר יחס מורכב בין איקס לוואי, אבל היכולת שלנו לאמוד אותו מוגבלת בלי המון נתונים. העיקרון הזה נכון לא רק לרגרסיה אלא גם לקלסיפיקציה, ועל כך נדבר בהמשך.
:::
:::

---

## The Classification Setting {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה כעת כמה מהמושגים שדיברנו עליהם בסטינג של קלסיפיקציה, כשY הנחזה הוא בדיד, קטגוריאלי.
:::
:::

---

### Misclassification rate

::: {.incremental}
- Let $y \in \{0, 1, \dots, J - 1\}$
- For a classifier $\hat{f}(x)$, define the indicator $\mathbb{I}\left[y \neq \hat{y}\right]$
- For a sample $T = \{(x_1, y_1 ) \dots (x_n, y_n)\}$ the [training error]{style="color:red;"} (misclassification rate) is: $\frac{1}{n}\sum_i \mathbb{I}\left[y_i \neq \hat{y}_i\right]$
- For unknown observations $(x_0, y_0)$ we are interested in the *expected* error rate, but we look at [test error]{style="color:red;"}: $\frac{1}{m}\sum_i \mathbb{I}\left[y_{i,0} \neq \hat{y}_{i,0}\right]$
- This error rate can be decomposed into three terms as well!
:::

::: {.fragment}
We want a predictor which drives the error rate to minimum. What is that predictor?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשY הוא אחת מJ קטגוריות, הטעות האינטואיטיבית ביותר להסתכל עליה היא המיסקלסיפיקיישן רייט. עבור כל תצפית נספור "אחת" אם טעינו בחיזוי, ואפס אחרת, נסמן את זה באמצעות משתנה אינדיקטור.

עבור מדגם למידה T סך טעות החיזוי יהיה סכום של משתני האינדיקטורים האלה על n תצפיות.

כעת ברור שהיינו רוצים את תוחלת הטעות הזאת על נתונים שהמודל לא ראה, בפועל יש לנו מדגם טסט של m תצפיות נאמר, ואנחנו ננסה להביא למינימום את הטסט ארור, היא סכום האינדיקטורים על m התצפיות.

מסתבר שגם את הטעות הזאת ניתן לפרק לשלושה גורמים אם כי לא נעשה את זה כאן.

נשאל מהו החיזוי שיביא למינימום את הטסט ארור הזה?
:::
:::

---

### Bayes decision boundary

Assume the conditional probability $P(Y = 1 | X)$ is a nice, slowly changing function of $X$:

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Define a more complex logistic function for P(Y = 1 | X1, X2) to create a wavy decision boundary
def logistic_function(x1, x2):
    return 1 / (1 + np.exp(- (np.sin(x1) + np.cos(x2) + 0.5 * x1 * x2 - 1)))

# Create a grid of X1 and X2 values
x1 = np.linspace(-3, 3, 300)
x2 = np.linspace(-3, 3, 300)
X1, X2 = np.meshgrid(x1, x2)

# Compute P(Y = 1 | X1, X2) over the grid
P = logistic_function(X1, X2)

# Plot the heatmap
plt.figure(figsize=(8, 5))
plt.contourf(X1, X2, P, levels=50, cmap='viridis')
plt.colorbar(label='$P(Y = 1 | X1, X2)$')

# Plot the decision boundary where P(Y = 1 | X1, X2) = 0.5
contour = plt.contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2)
plt.clabel(contour, fmt = '%.1f', colors = 'red')

# Labels and title
plt.xlabel('$X1$')
plt.ylabel('$X2$')
plt.show()
```

::: {.fragment}
The [Bayes decision boundary]{style="color:red;"} is where $P(Y = 1 | X) = 0.5$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתמקד רגע במצב שבו Y הוא בינארי, אפס או אחת. נניח שY בהינתן X מתפלג ברנולי עם הסתברות מותנית להיות 1, P. ונניח שההסתברות הזאת היא פונקציה חלקה יחסית כמו כאן של מרחב X. כמו שניתן לראות כאן הפונקציה היא לא ליניארית באיקס אבל היא כאן משתנה לאט, יש לה גבעות ועמקים.

קו תיאורטי שיעניין אותנו מאוד יהיה הקו שמסמל את גובה חצי, כלומר איפה ההסתברות להיות כל אחד משני הקלאסים היא חצי בדיוק. הבאונדרי הזה נקרא הבייז דסיז'ן באונדרי. למה דסיז'ן? כי נראה נכון מצד אחד של הגבול לסווג תצפיות כ1 ומהצד השני כ0.
:::
:::

---

### The Bayes classifier

- For a $J$-class classification problem, assume we know the probabilities $P(Y = j | X = x) \space \forall j, x$
- The *best* classifier is the [Bayes classifier]{style="color:red;"}: $\hat{f}(x) = \arg\max_j P(Y = j | X = x)$
- For a $2$-class problem we can use the Bayes decision boundary and reach a simpler notation: $\hat{f}(x) = 1 \space \forall x \space s.t. \space P(Y = 1 | X = x) > 0.5 \text{ otherwise } 0$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
זה בדיוק מה שיעשה הקלסיפייר התיאורטי של בייז, הבייז קלסיפייר.

באופן כללי עבור J קלאסים בייז יבחר בקלאס J שיביא למקסימום את ההסתברות שY שווה לJ. במצב של שני קלאסים זה פשוט אומר להסתכל על הבאונדרי שראינו, מצד אחד שלו עבור הסתברות גדולה מחצי לחזות 1 ומהצד השני 0.
:::
:::

---

### How is Bayes classifier "best"?

$E_Y(\mathbb{I}\left[y \neq \hat{y}\right]| X) = P(Y = 1 | X) \cdot \mathbb{I}\left[\hat{y} = 0\right] + P(Y = 0 | X) \cdot \mathbb{I}\left[\hat{y} = 1\right]$

::: {.incremental}
  - Suppose $P(Y = 1 | X) = 0.7$, between $\hat{y} \in \{0, 1\} \Rightarrow$ choose $\hat{y} = 1$ to minimize $E_Y(\mathbb{I}\left[y \neq \hat{y}\right]| X)$
  - Suppose $P(Y = 1 | X) = 0.3$, between $\hat{y} \in \{0, 1\} \Rightarrow$ choose $\hat{y} = 0$ to minimize $E_Y(\mathbb{I}\left[y \neq \hat{y}\right]| X)$
- Put differently the Bayes classifier $\hat{f}(x) = \arg\max_j P(Y = j | X = x)$ minimizes expected error rate!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אנחנו טוענים שהבייס קלסיפייר, אם מפת ההסתברות הזאת ידועה, הוא הקלסיפייר האופטימלי. אופטימלי באיזה מובן? במובן שהוא מביא למינימום את תוחלת המיסקלסיפיקיישן רייט. הרי מהי אותה תוחלת?

בהסתברות שY שווה לאחת נקבל טעות אם ננבא אפס, ובהסתברות שY שווה לאפס, נקבל טעות אם ננבא אחת. ושוב, אנחנו מניחים שההסתברות ידועה!

אז אם עבור נקודה מסוימת ההסתברות להיות 1 שווה למשל 0.7, איך נביא את הביטוי הזה למינימום? נבחר לחזות 1 והטעות תהיה 0.3.

ואם עבור נקודה מסוימת ההסתברות להיות 1 שווה ל0.3, איך נביא את הביטוי הזה למינימום? נבחר לחזות 0 והטעות תהיה 0.3

אבל מה שתיארנו כאן זה בדיוק הבייס קלסיפייר עבור שני קלאסים, כלומר לבחור את הקלאס הJ שמביא למקסימום את ההסתברות שY שווה לJ, מביא למינימום את תוחלת המיסקלסיפיקיישן רייט, עבור נקודה ספציפית.

ומה הבעיה שוב?
:::
:::

---

### The Bayes classifier is purely theoretical!

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Define a more complex logistic function for P(Y = 1 | X1, X2) to create a wavy decision boundary
def logistic_function(x1, x2):
    return 1 / (1 + np.exp(- (np.sin(x1) + np.cos(x2) + 0.5 * x1 * x2 - 1)))

# Create a grid of X1 and X2 values
x1 = np.linspace(-3, 3, 300)
x2 = np.linspace(-3, 3, 300)
X1, X2 = np.meshgrid(x1, x2)

# Compute P(Y = 1 | X1, X2) over the grid
P = logistic_function(X1, X2)

# Simulate random points within the grid for the training set
np.random.seed(42)  # For reproducibility
X1_train = np.random.uniform(-3, 3, 500)
X2_train = np.random.uniform(-3, 3, 500)

# Calculate the probability for each training sample point
P_train = logistic_function(X1_train, X2_train)

# Generate Y values based on the probabilities for the training set
Y_train = np.random.binomial(1, P_train)

# Plot the heatmap
plt.figure(figsize=(8, 5))
heatmap = plt.contourf(X1, X2, P, levels=50, cmap='viridis')
plt.colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

# Plot the decision boundary where P(Y = 1 | X1, X2) = 0.5
decision_boundary = plt.contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
plt.clabel(decision_boundary, fmt = '%.1f', colors = 'red')

# Plot the simulated training points
plt.scatter(X1_train[Y_train == 1], X2_train[Y_train == 1], color='yellow', label='Y = 1 (Train)', edgecolor='black')
plt.scatter(X1_train[Y_train == 0], X2_train[Y_train == 0], color='blue', label='Y = 0 (Train)', edgecolor='black')

# Labels and title
plt.xlabel('$X1$')
plt.ylabel('$X2$')
plt.show()
```

::: {.fragment}
Even for this training data, the Bayes classifier will have an *irreducible* error rate! (classes overlap)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הבייס קלסיפייר הוא אורקל, מונח תיאורטי בלבד, שלו היינו יודעים את מפת ההסתברות הוא היה אופטימלי. ונשים לב שבפועל נקודות נדגמות על-פי הסתברות, המתחמים של הקלאסים הם לא דטרמיניסטיים עבור דאטא טיפוסי ויש ביניהם חפיפה. וגם עבור דאטה שהמודל רואה ויודע את מפת ההסתברות ובוחר בבייס קלסיפייר - יש טעויות. זה מאוד מזכיר אירדוסיבל ארור.
:::
:::

---

### The Bayes error rate

::: {.incremental}
- What is the Bayes classifier error?
- For a given $x$: $BE = 0 \cdot P(correct) + 1 \cdot P(error) = 1 - \max_j P(Y = j | X = x)$
- Therefore for a test set the expected error rate is given by: $1 - E\left(\max_j P(Y = j | X = x)\right)$
- This bound is the *lowest* error rate achievable (under our assumptions), similar to the irreducible error in regression
- Sampling additional test observations in the example we reach test error rate of 0.205
:::

::: {.fragment}
::: {.callout-note}
But in practice the conditional probability is unknown. How can we approximate it?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מהי טעות החיזוי של בייס קלסיפייר?

אם אני צודק, בקריטריון שלנו אני "משלם" 0, ואם אני טועה אני משלם 1. לפיכך הטעות היא הסיכוי שלי לטעות שהוא 1 פחות הסיכוי שהביא אותי מלכתחילה לבחור בקלאס, כלומר 1 פחות הסיכוי המקסימלי.

אם ניקח על זה תוחלת נראה ביטוי די פשוט לתוחלת הטעות של הבייס קליספייר. עבור תצפית שבה הסיכוי המקסימלי הזה הוא 0.6, כלומר תצפיות שקרובות לגבול ההחלטה, הטעות הזאת בתוחלת תהיה די גדולה, 0.4. אבל עבור תצפית שבה הסיכוי המקסימלי הזה הוא 0.9, כלומר תצפיות רחוקות מהגבול, הטעות הזאת בתוחלת תהיה די קטנה, 0.1.

אבל בכל מקרה, גם בתוחלת, התצפיות אינן דטרמניסטיות ותהיה לנו טעות, אפילו עבור הבייס קלסיפייר! הדבר מאוד דומה לטעות האירדוסיבל שראינו ברגרסיה.

למעשה, בדוגמא שראינו אם נחשב את טעות החיזוי של הבייס קלסיפייר, נראה שהטעות האמפירית עליהן היא בערך 20 אחוז.

אבל כל הדיון הוא דיון תיאורטי הרי, כי הבייס קלסיפייר הוא אורקל, ובפועל אנחנו צריכים לאמוד את מפת ההסתברות הזאת, בהנחה שהיא קיימת. איך נאמוד אותה? בחלק האחרון של השיעור נזכיר את שיטת הKNN, שיטת השכנים הקרובים ונראה שתחת ההנחה שהמפה הזו, פונקצית ההסתברות הזאת משתנה לאט, היא עושה עבודה לא רעה באמידה, וגבול ההחלטה שלה מתקרב מאוד לבייס קלסיפייר האופטימלי.
:::
:::

---

## The KNN Classifier {.title-slide}

---

### K-Nearest Neighbors

- Let $K$ be a positive integer, and let $T$ be the training set
- Classification rule for new observation $x_0$:
  - Let the $K$-neighborhood $\mathcal{N}(x_0)$ the $K$ points closest to $x_0$ in $T$
  - $\hat{f}(x_0) = \arg\max_j \left[\sum \mathbb{I}\left[y(x) = j\right] | x' \in \mathcal{N}(x_0)\right]$

::: {.fragment}
- Under what conditions is it an approximation to the Bayes classifier?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נזכיר כעת כיצד מתבצעת קלסיפיקציה בשיטת KNN, שיטה א-פרמטרית ומאוד אינטואיטיבית. אנחנו רוצים לשערך את פונקצית ההסתברות של Y בכל שכונה של X? אז בואו נגדיר לכל X "שכונה", כמספר שכנים K הכי קרובים אליו במדגם הלמידה.

החיזוי הסופי של הקלאס J של הנקודה X יהיה הJ שיקבל מהשכנים בשכונה את מירב הקולות, או יביא למקסימום את הביטוי שרשום כאן.

KNN זו אחת השיטות האינטואיטיביות ביותר לקלסיפיקציה, והיא למעשה "נטולת למידה", אין אלגוריתם איטרטיבי או לא שצריך להפעיל על הנתונים, רק לשמור בזיכרון את מדגם הלמידה וגמרנו. אבל נזכיר שוב תחת איזו הנחה KNN תעבוד טוב ותקרב את הבייס קלסיפייר? תחת ההנחה שבכלל קיימת פונקציה או מפת הסתברות כזאת שמשתנה לאט, וניתן לאמוד אותה באמצעות שיטת השכנים. אם לדוגמא הקלאסים משתנים באמצעות כלל משוגע אחר, שכל נקודה "מחליטה לעצמה" ואין בכלל חלקות בנוף ההסתברות הזאת, KNN לא יעשה עבודה טובה.
:::
:::

---

### KNN vs. Bayes classifier


```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Define a logistic function with a single wavy decision boundary
def logistic_function(x1, x2):
    return 1 / (1 + np.exp(- (np.sin(x1) + np.cos(x2) + 0.5 * x1 * x2 - 1)))

# Create a grid of X1 and X2 values
x1 = np.linspace(-3, 3, 300)
x2 = np.linspace(-3, 3, 300)
X1, X2 = np.meshgrid(x1, x2)

# Compute P(Y = 1 | X1, X2) over the grid
P = logistic_function(X1, X2)

# Simulate 300 random points within the grid for the training set
np.random.seed(42)  # For reproducibility
X1_train = np.random.uniform(-3, 3, 500)
X2_train = np.random.uniform(-3, 3, 500)

# Calculate the probability for each training sample point
P_train = logistic_function(X1_train, X2_train)

# Generate Y values based on the probabilities for the training set
Y_train = np.random.binomial(1, P_train)

# Plot the heatmap
plt.figure(figsize=(8, 5))
heatmap = plt.contourf(X1, X2, P, levels=50, cmap='viridis')
plt.colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

# Plot the decision boundary where P(Y = 1 | X1, X2) = 0.5
decision_boundary = plt.contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
plt.clabel(decision_boundary, fmt = '%.1f', colors = 'red')

# Plot the simulated training points
plt.scatter(X1_train[Y_train == 1], X2_train[Y_train == 1], color='yellow', label='Y = 1 (Train)', edgecolor='black')
plt.scatter(X1_train[Y_train == 0], X2_train[Y_train == 0], color='blue', label='Y = 0 (Train)', edgecolor='black')

# Labels and title
plt.xlabel('$X1$')
plt.ylabel('$X2$')

# Generate 100 random points within the grid for the testing set
X1_test = np.random.uniform(-3, 3, 200)
X2_test = np.random.uniform(-3, 3, 200)

# Calculate the probability for each testing sample point
P_test = logistic_function(X1_test, X2_test)

# Generate Y values based on the probabilities for the testing set
Y_test = np.random.binomial(1, P_test)

# Predict Y values for the testing set based on the decision rule
Y_pred = (P_test >= 0.5).astype(int)

# Prepare the training and testing data for KNN
X_train = np.vstack((X1_train, X2_train)).T
X_test = np.vstack((X1_test, X2_test)).T

# Fit a KNN classifier with K = 10
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(np.column_stack((X1_train, X2_train)), Y_train)

# Predict probabilities for each point on the grid
Z = knn.predict_proba(np.column_stack((X1.ravel(), X2.ravel())))[:, 1]
Z = Z.reshape(X1.shape)

# Plot the decision boundary created by the KNN classifier
plt.contour(X1, X2, Z, levels=[0.5], colors='orange', linewidths=2)

plt.show()
```

(For $K = 10$ test error rate 0.270, only slightly worse than Bayes error rate!)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נניח שנבחר בK = 10, כלומר עבור כל נקודה חדשה המודל חוזה עבורה את הקלאס שזוכה לרוב הקולות מבין 10 השכנים הקרובים אליה ביותר במקרה הזה במרחק אוקלידי, אפשר לחשוב על מרחקים אחרים.

בבעיה שלנו עם 2 קלאסים אפשר לחזות עבור כל נקודה גם את שיעור השכנים שלה שחוזים את הקלאס 1, ואז אפשר לסרטט את גבול ההחלטה של KNN, כלומר הקו שעבורו היא חוזה הסתברות חצי, ולראות שהוא מאוד מזכיר את הבאונדרי של הבייס קליספייר, ולא במקרה, ההנחות שלנו נכונות. למעשה, עבור מדגם טסט של עוד כמה מאות תצפיות, טעות החיזוי כאן היא כ-27 אחוז, קצת מעל טעות החיזוי של בייס, כפי שהיינו מצפים.

אבל איך הגענו לK = 10?
:::
:::

---

### KNN and bias-variance tradeoff

```{python}
#| echo: false

knn1 = KNeighborsClassifier(n_neighbors=1)
knn1.fit(np.column_stack((X1_train, X2_train)), Y_train)

knn2 = KNeighborsClassifier(n_neighbors=200)
knn2.fit(np.column_stack((X1_train, X2_train)), Y_train)

Z1 = knn1.predict_proba(np.column_stack((X1.ravel(), X2.ravel())))[:, 1]
Z1 = Z1.reshape(X1.shape)

Z2 = knn2.predict_proba(np.column_stack((X1.ravel(), X2.ravel())))[:, 1]
Z2 = Z2.reshape(X1.shape)

fig, axes = plt.subplots(1, 2, figsize = (2 * 5, 3))

heatmap = axes[0].contourf(X1, X2, P, levels=50, cmap='viridis')
# axes[0].colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

decision_boundary = axes[0].contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
axes[0].clabel(decision_boundary, fmt = '%.1f', colors = 'red')

axes[0].scatter(X1_train[Y_train == 1], X2_train[Y_train == 1], color='yellow', label='Y = 1 (Train)', edgecolor='black')
axes[0].scatter(X1_train[Y_train == 0], X2_train[Y_train == 0], color='blue', label='Y = 0 (Train)', edgecolor='black')

axes[0].set_xlabel('$X1$')
axes[0].set_ylabel('$X2$')
axes[0].set_title('K = 1')
axes[0].contour(X1, X2, Z1, levels=[0.5], colors='orange', linewidths=2)

heatmap = axes[1].contourf(X1, X2, P, levels=50, cmap='viridis')
# axes[1].colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

decision_boundary = axes[1].contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
axes[1].clabel(decision_boundary, fmt = '%.1f', colors = 'red')

axes[1].scatter(X1_train[Y_train == 1], X2_train[Y_train == 1], color='yellow', label='Y = 1 (Train)', edgecolor='black')
axes[1].scatter(X1_train[Y_train == 0], X2_train[Y_train == 0], color='blue', label='Y = 0 (Train)', edgecolor='black')

axes[1].set_xlabel('$X1$')
axes[1].set_ylabel('$X2$')
axes[1].set_title('K = 200')
axes[1].contour(X1, X2, Z2, levels=[0.5], colors='orange', linewidths=2)

plt.show()
```
- How can we efficiently find a neighborhood of an observation?
  - For small $K$: classifier is flexible but has high variance
  - For large $K$: classifier is stable but suffers from high bias
- There is no way to escape this bias-variance tradeoff

::: {.fragment}
- So how to choose $K$?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אנחנו רואים כאן את אותו גבול החלטה עבור K = 1 ועבור K = 200. ולמעשה עוד המחשה של הביאס-וריאנס טריידאוף שקיים גם בקלסיפיקציה.

כשK קטן, הקלאסיפייר גמיש מאוד ומסוגל לתאר קוי החלטה שונים ומשונים. אבל הם מושפעים מאוד, כנראה יותר מדי, ממדגם הלמידה ומביאים לשונות גבוהה, במובן שאם יגיע מדגם למידה אחר הם יכולים להשתנות לחלוטין.

כשK גדול קו ההחלטה יציב יותר, הוא יחזה נקודה צהובה רק כשהוא ממש בטוח, אבל יש לו ביאס מאוד גבוה כי הפונקציה שהוא מסוגל לתאר היא מוגבלת מאוד ולא גמישה.

אז מה בכל זאת אנחנו עושים כדי לבחור את K?
:::
:::

---

### How to choose $K$?

```{python}
#| echo: false

# Function to simulate data
def simulate_data(n_samples):
    X1 = np.random.uniform(-3, 3, n_samples)
    X2 = np.random.uniform(-3, 3, n_samples)
    P = logistic_function(X1, X2)
    Y = np.random.binomial(1, P)
    return np.column_stack((X1, X2)), Y

# Number of repetitions
n_repeats = 10

# Values of K to evaluate
K_values = [1, 2, 3, 5, 10, 30, 300]

# Initialize lists to store error rates
train_errors = {K: [] for K in K_values}
test_errors = {K: [] for K in K_values}

# Loop over repetitions
for _ in range(n_repeats):
    # Generate training and testing data
    X_train, y_train = simulate_data(500)
    X_test, y_test = simulate_data(200)
    
    # Loop over different values of K
    for K in K_values:
        knn = KNeighborsClassifier(n_neighbors=K)
        knn.fit(X_train, y_train)
        
        # Predict on training and testing data
        y_train_pred = knn.predict(X_train)
        y_test_pred = knn.predict(X_test)
        
        # Compute error rates
        train_error = 1 - accuracy_score(y_train, y_train_pred)
        test_error = 1 - accuracy_score(y_test, y_test_pred)
        
        # Store error rates
        train_errors[K].append(train_error)
        test_errors[K].append(test_error)

# Compute mean and standard error of the mean for each K
train_error_means = [np.mean(train_errors[K]) for K in K_values]
test_error_means = [np.mean(test_errors[K]) for K in K_values]
train_error_stds = [np.std(train_errors[K]) / np.sqrt(n_repeats) for K in K_values]
test_error_stds = [np.std(test_errors[K]) / np.sqrt(n_repeats) for K in K_values]

# Plotting the results
plt.figure(figsize=(10, 5))
plt.errorbar(1/np.array(K_values), train_error_means, yerr=train_error_stds, label='Train Error', marker='o', capsize=5)
plt.errorbar(1/np.array(K_values), test_error_means, yerr=test_error_stds, label='Test Error', marker='o', capsize=5)
plt.axhline(y=0.205, color='k', linestyle='--')
plt.text(0.8, 0.22, 'Bayes error rate', fontsize=12, va='center', ha='center')
plt.xticks(1/np.array(K_values), ['1/' + str(K) for K in K_values], rotation='vertical')
plt.xlabel('1/K')
plt.ylabel('Error Rate')
plt.legend()
plt.grid(True)
plt.show()
```

::: {.fragment}
- Sometimes we have prior knowledge, can choose $K$ from theoretical considerations
- In most real-life problems, choosing $K$ requires some additional data we use to validate our model
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
לפעמים יש לנו הבנה חזקה יותר בדומיין של הדאטה שבו אנחנו עוסקים ויכולות להיות לנו סיבות טובות לבחור בK ספציפי, עם משמעות אינהרנטית לבעיה, כמו למשל אם היינו צריכים לחזות תכונה גנטית של אדם והשכונה שהיינו בוחרים היתה המשפחה שלו.

ברוב המקרים אנחנו נשתמש באסטרטגיה של חלוקת הדאטה לטריין ולטסט כמו כאן, ונבחר את הK שמביא למינימום את טעות החיזוי על מדגם הטסט. במקרה הזה אנחנו חוזרים על הניסוי התיאורטי שמייצר את הנתונים שרצינו מספר פעמים, מחלקים את הדאטא לטריין וטסט, לומדים על הטריין, מחשבים מיסקלסיפיקיישן על הטסט, וממצעים.

כאן אפשר לראות את טעות החיזוי כפונקציה של 1 חלקי K, כי אנחנו רגילים לראות את הגרף הזה שנע ממודל מאוד פשוט ולא מורכב, ועד מודל מאוד גמיש ומורכב מדי, כלומר אנחנו רוצים לראות את K יורד. שני דברים שאנחנו מצפים לראות בגרף החיזוי של KNN ומתקיימים כאן:

הארור של מדגם הלמידה, לא הטסט, עבור K = 1 יורד לאפס! כי עבור כל תצפית נחזה את הקלאס שלה עצמה. מה שממחיש את הבעייתיות בהסתכלות על מדגם הלמידה.

ועוד תופעה שצפינו, KNN במיטבו מתקרב לטעות החיזוי של הבייס קלסיפיר שמסומנת כאן בקו שחור מקווקוו, אבל לא מצליח לרדת ממנה. זו טעות אירדוסיבל. בכל מקרה כאן נראה שהK הטוב ביותר הוא בסביבות 30, עבורו היינו מקבלים טעות חיזוי של כ-25 אחוזים.

עד כאן שני השיעורים הראשונים שלנו שעוסקים במושגים כלליים של למידה כמו טעות חיזוי, ביאס-וריאנס ואוברפיטינג. בשיעורים הבאים נתמקד במודל הליניארי, אולי מודל הלמידה הישן ביותר והנחקר ביותר בספרות, מודל שצריך לשמש כבייסליין בכל פרויקט של למידה סטטיסטית מנתונים.
:::
:::
