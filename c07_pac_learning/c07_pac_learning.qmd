---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "PAC Learning"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### PAC Learning - Class 7

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## What is learnable? {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Previously on Model Selection

::: {.incremental}
- For linear regression, squared error loss, Fixed $X$:
$$op = \mathbb{E}_{y}\left[Err_{in} - \overline{err}|X\right] = \mathbb{E}_{y}\left[\frac{1}{n}\sum_{i=1}^n\mathbb{E}_{y_0}\left[L(y_0, \hat{f}(x_i))|T\right] - \frac{1}{n}\sum_{i=1}^{n} L(y_i, \hat{f}(x_i))\right] = \frac{2d\sigma^2}{n}$$

- For Fixed $X$, for any $\varepsilon > 0$, if $n \ge \frac{2d\sigma^2}{\varepsilon}$, $\mathbb{E}_{y}\left[\overline{err}\right]$ is up to $\varepsilon$ smaller than $\mathbb{E}_{y}\left[Err_{in}\right]$!

- Can we learn *any* $f$ with *any* large-enough training sample, ensuring $\overline{err}$ is up to $\varepsilon$ smaller than $Err = \mathbb{E}_{x_0, y_0, T}\left[L(y_0, \hat{f}(x_0))\right]$ (no overfitting)?

- Spoiler: no, but [learnability]{style="color:red;"} is key concept:
    - under what conditions is $f$ learnable from a training sample?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשיעור הקודם, הגדרנו את מושג האופטימיזם: עד כמה מדגם הלמידה אופטימי בהערכת הטעות, אם נניח שX נתון, הוא קבוע, פיקסד, ואנחנו מושכים עוד תצפיות שהמודל לא רואה, מאותם איקסים. ברגרסיה ליניארית ראינו שאפשר להגיע לביטוי סגור, שכולל את d מספר הפרמטרים במודל, ואת סיגמא בריבוע, כשאם אנחנו לא יודעים אותה אפשר לאמוד אותה.

אם ככה, מתבקש לרצות שהאופטימיזם יהיה קטן כרצוננו. ומאחר שאין לנו שליטה על סיגמא בריבוע, עבור מודל עם מספר פרמטרים נתון, כל שנותר לנו לעשות זה להגדיל את גודל המדגם, את n. אם נרצה שהאופטימיזם יהיה קטן מאיזשהו מספר אפסילון קטן כרצוננו, אפשר להגיע לחסם תחתון על n, n צריך להיות גדול או שווה לפעמיים d סיגמא בריבוע חלקי אותו אפסילון.

האם אנחנו יכולים להמשיך את הקו הזה? האם אנחנו יכולים למצוא מדגם גדול מספיק כדי ללמוד לא רק פונקציה ליניארית אלא כל פונקציה f, ולוודא שהטעות של מדגם הלמידה תהיה קטנה עד כדי כל אפסילון שנבחר מהטעות האמיתית שסימנו כארר, על פני כל נתון שהמודל לא יראה?

אז הנה ספוילר להמשך, התשובה היא שלא. בשביל להשיג את החסם הזה השתמשנו במספר הנחות כמו המודל הליניארי, ואם זורקים את כולן מחוץ לחלון, קשה להבטיח שהטעות של מדגם הלמידה תהיה קרובה ככל שנרצה לשגיאה האמיתית. אבל השאלה הזאת מביאה אותנו לקונספט חשוב נורא והוא: לרנביליטי, או: למידות.

בשיעור הזה נטעם קצת מתיאוריית הלמידה, וננסה לענות, או להתחיל לענות על השאלה הפשוטה מה למיד, עבור איזו פונקציה f ובאילו תנאים, אפשר לדגום ממדגם הלמידה, ולצפות שהטעות שלו תשקף את השגיאה האמיתית עבור כל נתון שהמודל לא רואה.
:::
:::

---

### Learning theory ingredients

- $x \sim P_X$ on $\mathcal{X}$
- Label set: $y \in \mathcal{Y}$  (start with $\mathcal{Y}  = \{0, 1\}$)
- True relation: $f: \mathcal{X} \to \mathcal{Y}$
- Classifier: $h: \mathcal{X} \to \mathcal{Y}$ (our $\hat{f}$)
- Training data: $T = \{x_1, \dots, x_n\}$, i.i.d, where $y_i = f(x_i)$
- Risk function: $L(f(x), h(x))$ (start with 0-1 loss)
- Error: $Err_{P_X, f}(h) = \mathbb{E}_{X \sim P_X}\left[L(f(x), h(x))\right] = P_X\left[f(x) \neq h(x)\right]$

::: {.fragment}
Most startling assumption:

- [Realizability]{style="color:red;"}: $y_i = f(x_i)$, i.e. for $Err_{P_X, f}(f) = 0$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נגביל את עצמנו לעולם מצומצם עם חוקים והנחות די מחמירות, ואחר-כך ננסה להיפטר מהמגבלות.

בעולם שלנו, מרחב ההסתברות כולל רק את X, המשתנים המסבירים, על המרחב הזה יש התפלגות Px מתוכה אנחנו דוגמים את האיקסים.

הוואי שלנו בא ממרחב וואי מסוים, ואנחנו נתחיל עם קלסיפיקציה לשתי רמות, כלומר וואי הוא אפס או אחת.

שימו-לב, אנחנו מניחים שיש קשר אמיתי, פונקציה f שלכל X מתאימה Y. ואת המודל שלנו שעד כה סימנו כf_hat נסמן כh. נקרא לו קלאסיפייר, אפשר גם כלל חיזוי, אפשר היפותזה.

כלומר המדגם המקרי בגודל n שאנחנו לוקחים הוא מדגם T מתוך האיקסים, שעבור כל X יש מיפוי לY דרך פונקציה F אבסולוטית.

לפונקצית ההפסד שלנו נקרא ריסק, סיכון, ונתחיל עם פונקצית ההפסד הטבעית לבעיה שהיא האפס-אחת לוס.

ואז הטעות שלנו, היא תוחלת הריסק הזה. תוחלת על פני מה? על פני מרחב ההסתברות של X שהוא המשתנה המקרי היחיד פה. זה משתנה אינדיקטור, אם חזינו נכון נקבל 0 ואם טעינו נקבל 1. ותוחלת של משתנה מקרי אינדיקטור היא פשוט ההסתברות לקבל 1, כלומר הטעות שלנו היא ההסתברות לחיזוי שגוי, שההיפותזה שלנו h על כל X, תהיה שונה מf האמיתית. נסמן את הטעות הזאת כארר על P_X ו-f, כפונקציה של היפותזה h.

אז אם עקבתם, הרכיב שהכי שונה מאיך שראינו את העולם עד עכשיו בלמידה סטטיסטית, הוא ההנחה המחמירה שלכל X מתאים רק Y ספציפי, ואין סטוכסטיות בקשר הזה. הרי תחת המודל שלנו עד עכשיו Y שווה לאיזושהי פונקציה f של X ועוד איזה משתנה מקרי של רעש אפסילון, ככה הסברנו איך המודל שלנו מסביר שעבור שני סטודנטים שונים מאותו מגדר, אותו ניסיון, אותם ציונים קודמים ואותו זמן התכוננות למבחן - יכול לצאת ציון שונה. וכאן - לא! זה דטרמיניסטי, אין דגימה ממרחב Y.

יתרה מזה, זה אומר שיש מושג כזה של טעות אפס, אם נגיע למודל האמיתי f, וזאת אפשרות, הסיכוי לשגיאה הוא אפס, כלומר הארר שלנו, הוא אפס. להנחה הזאת קוראים ריאלייזביליטי, היתכנות, אפשר להגיע לשגיאה אפס. זאת חתיכת הנחה, ננסה להוריד אותה בהמשך.

עוד הנחה שממנה כפי שאמרנו בעבר לעולם לא ניפטר, מסתתרת מאחורי הביטוי iid, כלומר המדגם נמשך בצורה זהה, עם החזרה, כל תצפית בלתי תלויה בשניה. זה גם אומר שהמדגם שאנחנו רואים קשור לעולם, אנחנו לא נמדדים בנתונים שלא ראינו על עולם אחר - אחרת, היינו עוצרים כאן.
:::
:::

---

## PAC Learning {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בעולם המחמיר שהגדרנו, ננסה לענות על השאלה האם תמיד אפשר ללמוד ממדגם הלמידה, ואם לא תמיד מה כן אפשר לצפות.
:::
:::

---

### Empirical Risk Minimization (ERM)

- $\overline{err}$ is termed the [empirical risk]{style="color:red;"}, a.k.a the training error:

$$\overline{err}_T(h) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, h(x_i)) = \frac{1}{n}\sum_{i=1}^{n} \mathbb{I}\left[f(x_i) \neq h(x_i)\right]$$

::: {.incremental}
- A method which outputs $h_T$ by minimizing $\overline{err}_T(h)$ is called empirical risk minimization ([ERM]{style="color:red;"})

- If $f,h \in \mathcal{H}$, from [Realizability]{style="color:red;"}: $\overline{err}_T(h_T) = 0$

- Can we *always* find $h$ for which $Err_{P_X, f}(h) = 0$ via ERM (= by minimizing $\overline{err}_T(h)$)?

- Counter-example:
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניתן קודם שם למה זה אומר ללמוד ממדגם הלמידה. אם הסיכון הוא התוחלת על פני כל האיקסים האפשריים, נסמן בארר עם קו למעלה כמו שעשינו את הסיכון האמפירי שמחושב על מדגם הלמידה, שמחושב על מדגם ספציפי T ועבור מודל ספציפי h. במקרה של סיכון אפס-אחת מדובר בשיעור החיזוי השגוי על תצפיות מדגם הלמידה, מתוך n תצפיות. אנחנו קראנו לזה בעבר הטריינינג ארור.

פרדיגמה שלומדת איזשהו hT על-ידי הבאה למינימום של הכמות הזאת נקראת אמפיריקל ריסק מינימיזיישן, או ERM בקיצור.

עכשיו נשים לב לפרט עדין וחשוב: נניח שהמודל האמיתי f והמודל שERM מחפש h באים מאותו מרחב H שעוד נדבר עליו -- כלומר ERM מחפש במרחב הנכון -- מהנחת הריאלייזביליטי, שאומרת שעבור כל X מתאים רק Y ספציפי, האם זה מאתגר למצוא מודל שיגיע לאמפיריקל ריסק אפס? בודאי שלא. תחת הנחת הריאלייזביליטי, אנחנו תמיד יכולים לאפס (!) את האמפיריקל ריסק אם אנחנו מחפשים במרחב הנכון, כי אין רעש, אין תצפיות שלפעמים הY שלהן אחת ולפעמים אפס. אני רק אזכיר שעד כאן חשדנו מאוד ממצב כזה שקראנו לו אוברפיטינג, ונשאלת השאלה:

מה זה למידות? האם אפשר לצפות להביא את שגיאת החיזוי, על פני נתונים שלא ראינו, לאפס, אם אנחנו מסתכלים רק על החלון הזה של מדגם למידה, כלומר לומדים עם ERM? זאת יכולה להיות ההגדרה של למידות?

כל מה שצריך זה להביא דוגמא נגדית כדי להראות שזה לא יכול להתקיים תמיד. בדוגמא הפשוטה ביותר אפשר להניח שכל מרחב האיקסים כולל רק שתי אפשרויות, X1 וX2. ההסתברות PX על המרחב הזה היא לקבל את X2 באיזשהו סיכוי קטן אפסילון, ואת X1 בסיכוי 1 פחות אפסילון. נניח גם שהמיפוי האמיתי f מביא את X1 לY = 0 ואת X2 לY = 1. מה זה אומר להיכשל במצב כל כך פשוט? מספיק אף פעם לא לראות בכלל את X2 כדי להיכשל, להגיע למודל שאומר שעבור כל האיקסים וואי שווה 0, האפשרות היחידה שאנחנו מכירים. האם יש סיכוי שאף פעם לא נראה את X2 וניכשל? בודאי. הסיכוי הזה הוא 1 פחות אפסילון בחזקת n. שמתנהג כמו כמו אקספוננט בחזקת מינוס אפסילון n. והגודל הזה לא בהכרח שואף לאפס ככל שn גדל, אם אפסילון קטן מאוד, הרבה יותר קטן מ1 חלקי n הוא לא ישאף לאפס.

כלומר יש סיכוי סביר שנקבל פשוט מדגם רע, שמביא את האמפיריקל ריסק לאפס אבל מביא לשגיאת חיזוי של אפסילון בהכרח על נתונים שהוא לא ראה. והשורה התחתונה היא שלהביא את שגיאת החיזוי לאפס היא קריטריון מחמיר מדי ללמידות, חייבים להוסיף פה איזשהו מימד הסתברותי. אנחנו צריכים להבין שתמיד יש סיכוי למדגם גרוע, ושגם מדגם טוב במרכאות לא בהכרח מביא לשגיאה אפס על נתונים שהמודל לא ראה.
:::
:::

---

### Probably Approximately Correct (PAC)

::: {.incremental}
We introduce two relaxations:

1. [Accuracy]{style="color:red;"} $\varepsilon$: Find $h$ via ERM for which $Err_{P_X, f}(h) \leq \varepsilon$ ("approximately correct")
2. [Confidence]{style="color:red;"} $\delta$: With probability $\geq 1 - \delta$ ("probably correct", on $P_X$)
:::

::: {.fragment}
PAC learning, interim summary:
:::

::: {.incremental}
- Assume $P_X, f$
- Input: $\varepsilon, \delta \in (0,1)$
- Take i.i.d training sample $T$ of size $n \geq n(\varepsilon, \delta)$, where $n(\varepsilon, \delta): [0, 1]^2 \to \mathbb{N}$
- Output prediction rule $h$ s.t. w.p. at least $1 - \delta$ it holds that $Err_{P_X, f}(h) \leq \varepsilon$:
$$P_X\left[T: Err_{P_X, f}(h) \leq \varepsilon\right] \geq 1 - \delta$$
:::

::: {.fragment}
::: {.callout-note}
This is a definition! Importantly, no guarantee of existence, no way of finding algorithm $A$ to minimize ERM.
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אנחנו עושים שתי רילקסציות, ודורשים קצת פחות: רילקסציה ראשונה מבטאת את זה שאנחנו יודעים שאנחנו לא יכולים להביא את שגיאת החיזוי לאפס מוחלט, אנחנו רוצים להביא אותה עד לאיזשהו פרמטר אקיורסי קטן כרצוננו, אפסילון. כלומר אנחנו רוצים להיות אפרוקסימטלי קורקט.

הרילקסציה השניה מבטאת את זה שאנחנו יודעים שבסיכוי מסוים גם מדגם מקרי יכול להיות גרוע או לא מייצג, ואנחנו רוצים לחסום את הסיכוי הזה להיות 1 פחות איזשהו פרמטר קונפידנס דלתא. כלומר אנחנו רוצים להיות פרובבלי קורקט.

ללמידות כזאת, פרובבלי אפרוקסימטלי קורקט, קוראים בקיצור פאק לרנביליטי או לרנינג, למידה מסוג פאק. נסכם:

אנחנו מניחים מרחב הסתברות PX ואיזשהו כלל החלטה או מיפוי ממנו אל מרחב Y.

אנחנו מספקים שני פרמטרים של אקיורסי וקונפידנס בין 0 ל1 קטנים כרצוננו.

אנחנו לוקחים מדגם מקרי iid בגודל n כאשר השליטה היחידה שלנו היא על גודל המדגם, אנחנו נדאג שיהיה גדול או שווה לאיזשהו סף שתלוי באפסילון ודלתא. את הסף הזה אפשר לראות כפונקציה מהמרחב של אפסילון ודלתא, מרחב האפס עד אחת בריבוע, אל המספרים השלמים.

ונצפה שתהליך הלמידה יחזיר לנו מודל, שבסיכוי לפחות אחת פחות דלתא, שגיאת החיזוי על פני נתונים שהוא לא ראה היא לכל היותר אפסילון. שזה בדיוק מה שרשום בביטוי שלפנינו.

נשים לב שזאת הגדרה. אנחנו עדיין לא יודעים אם קיים מודל שיבטיח לנו את הקריטריון הזה, לכל בעיה או לחלק מהבעיות, וגם אם כן לא יודעים איך למצוא אותו, או להוכיח שנמצא אותו דווקא ע"י ERM וגם אם כן איך לעשות מינימיזציה על האמפיריקל ריסק, כלומר למצוא את האלגוריתם עצמו שימצא את h.
:::
:::

---

### No Free Lunch Theorem (NFL)

Is there an algorithm $A$, that for every i.i.d training sample of size $n$ from every distribution $P_X$, there is a high chance ("probably") it outputs a predictor $h$ that has low risk ("approximately")?

A "universal learner"?

::: {.fragment}
::: {.callout-tip}
Theorem (No Free Lunch, binary classification):

For any $\delta \in (0,1), \varepsilon < 1/2$, for any learner $A$ and training sample $T$ of size $n$, there exist $P_X, f$ s.t. w.p. at least $\delta$: $Err_{P_X, f}(A(T)) \geq \varepsilon$
:::
:::

::: {.incremental}
- That is, no universal learner: no algorithm $A$ which is "best" for any classification task.
- We can always *fail* a learner $A$, with some distribution $P_X$.
- How can we prevent such failures?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עכשיו נשאל: יש אלגוריתם כזה? לומד אוניברסלי, שכל מה שאנחנו צריכים זה להביא לו מדגם מקרי, מכל התפלגות PX, והוא ישיג לנו את הקריטריון של PAC.

משפט מפורסם בשם No free lunch, אומר לנו שלא. יש הרבה גרסאות למשפט הזה, גם בניסוחים עממיים יותר, כאן אנחנו מביאים ניסוח קצת יותר מדויק עבור הבעיה שלנו שהיא עדיין קלאסיפיקציה בינארית:

לכל דלתא בין 0 ל1, לכל אפסילון קטן מחצי, לכל לרנר או אלגוריתם A שפועל על מדגם מקרי בגודל n, קיימים התפלגות PX וכלל החלטה F, כך שבהסתברות לפחות דלתא שגיאת החיזוי של מודל שמושג באמצעות האלגוריתם הזה, תהיה גדולה מאפסילון. במילים אחרות לא מושג קריטריון פאק.

בניסוח מעט עממי יותר נהוג לומר שאין לומד אוניברסלי, שברור שהוא הוא האלגוריתם שהכי טוב לכל בעיה. לכל אלגוריתם אני יכול למצוא מקרים שבהם הוא ייכשל להבטיח קריטריון פאק.

לא נוכיח את זה כאן אבל בשורה התחתונה קריטריון פאק הוא עדיין מאוד מחמיר ואנחנו יכולים להכשיל כל לומד שינסה להשיג אותו באמצעות התפלגות מסוימת על המרחב של X. לכן אני שואל, איך נמנעים מכישלונות כאלה, או שוב, מה אנחנו כן יכולים לדרוש בצורה ריאלית, כנראה עם עוד רילקסציות.
:::
:::

---

## Learning in Finite Hypothesis Classes {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Use Prior Knowledge

::: {.incremental}
- Assume $f \in \mathcal{H}$, where $\mathcal{H}$ is a [finite hypothesis class]{style="color:red;"}
- Example: "logistic regression" (not really)
    - intercept and slope $\beta_0, \beta_1$, each can take $1K$ possibilities $\to |\mathcal{H}| = 1M$
- The learner knows $\mathcal{H}$
- ERM restricted to this class only: $ERM_\mathcal{H}(T) \in \arg\min_{h \in \mathcal{H}}\overline{err}_T(h)$
:::

::: {.fragment}
::: {.callout-tip}
Theorem:

For any $\varepsilon, \delta \in (0,1)$, if $n \geq \frac{\log(|\mathcal{H}| / \delta)}{\varepsilon}$, then for every $P_X, f$, w.p. at least $1 - \delta$ over the choice of training sample $T$ of size $n$, $Err_{P_X, f}(ERM_\mathcal{H}(T)) \leq \varepsilon$
:::
:::

::: {.incremental}
- Example: "logistic regression" (not really), for $\delta = \varepsilon = 0.01$:
  - $\Rightarrow n \geq 800$ guarantees PAC correct: w.p. $0.99$ $Err_{P_X, f}(ERM_\mathcal{H}(T)) \leq 0.01$
- Important: this says nothing about how to find $ERM_\mathcal{H}$ or about its efficency
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז כאמור, אנחנו מגיעים למסקנה שלא נוכל להגיע לקריטריון של למידת פאק עבור כל מרחב של קשרים אמיתיים בין X לY. נהיה חייבים להוסיף איזה ידע מוקדם, להגביל את עצמנו בסוג המרחבים שניתנים ללמידת פאק. הגבלה אפשרית אחת, היא שהמרחבים האלה יהיו סופיים.

למה הכוונה? לדוגמא, אם אנחנו מדברים על קלסיפיקציה בינארית, אני יכול לחשוב על מודל ליניארי בסגנון רגרסיה לוגיסטית. כאן אני שם אותו במרכאות כי אני לא מתכוון ממש למודל של רגרסיה לוגיסטית שלמדנו, עם הנחה של התפלגות ברנולי, אלא למשהו כללי הרבה יותר שעדיין מערב מקדמים של רגרסיה, לדוגמא חותך ושיפוע. ובואו נניח שכל אחד מהם לוקח אלף אפשרויות. עוד מעט נגיד למה זה לא כל כך מופרך כשמגיעים למימוש מודל כזה. מכל מקום מרחב המודלים או ההשערות שאני מציע אם ככה, הוא סופי, יש בדיוק אלף בריבוע אפשרויות למודל, או מיליון.

הלומד או האלגוריתם מודע למרחב הזה, שם הוא מחפש, זה חלק מהאינפוט, והERM מביא למינימום את השגיאה על מדגם הלמידה, רק במרחב הזה, נסמן את המודל שיצא כERM H של T.

והנה משפט די מדהים: תחת כל ההנחות שהיו לנו עד עכשיו, בתוספת ההנחה שמרחב ההשערות H הוא סופי, ישנו חסם תחתון על מספר התצפיות במדגם הלמידה, כך שאם נעבור אותו, מובטח לנו הקריטריון של פאק. בסיכוי לפחות 1 פחות למדא, שגיאת החיזוי, ההכללה על תצפיות שהמודל לא ראה, תהיה עד אפסילון. בפרט זה אומר שמרחבים סופיים, עם הנחת הריאלייזביליטי, ניתנים ללמידת PAC, כשמספר התצפיות הדרוש מתנהג כמו לוג של גודל המרחב.

אם נחזור לדוגמא של רגרסיה לוגיסטית במרכאות, עם חותך ושיפוע, ונבקש למדא ואפסילון של 0.01 ונציב את גודל המרחב שלנו בחסם התחתון של המשפט, נגיע לזה שאנחנו צריכים לפחות 800 תצפיות במדגם הלמידה כדי להשיג את קריטריון פאק, כלומר בסיכוי לפחות 99 אחוז להיות טועים, בתוחלת, על תצפיות שלא ראינו לכל היותר באחוז אחד.

ועוד דגש חשוב לפני ההוכחה: שימו לב שהמשפט לא אומר איך למצוא את האלגוריתם הזה, ולא מבטיח שום דבר על הסיבוכיות או היעילות שלו באופן כללי. כלומר הכל בתנאי שמצאנו אלגוריתם ERM שאפשר לעבוד איתו.
:::
:::

---

### Proof:

::: {.incremental}
- We want to prove that if $n \geq \frac{\log(|\mathcal{H}| / \delta)}{\varepsilon}$:
$$P_X\left[T: Err_{P_X, f}(ERM_\mathcal{H}(T)) > \varepsilon\right] \leq \delta$$

- Let $\mathcal{H}_B$ be the set of "bad" hypotheses: $\mathcal{H}_B = \{h \in \mathcal{H}: Err_{P_X, f}(h) > \varepsilon\}$
- Let $M$ be the set of "misleading" samples: $\{T: \text{ there is } h \in \mathcal{H}_B \text{ for which } \overline{err}_T(h) = 0\}$ (why?)
:::

::: {.fragment}
- Note I:
$$\{T: Err_{P_X, f}(ERM_\mathcal{H}(T)) > \varepsilon\} \subseteq M$$
- Note II:
$$M = \bigcup_{h \in \mathcal{H}_B} \{T:  \overline{err}_T(h) = 0\}$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נוכיח את המשפט המדהים הזה.

אנחנו רוצים להוכיח את מה שכתוב כאן בדרך קצת אחרת, עם המאורע המשלים, ההסתברות ששגיאת חיזוי מאלגוריתם שנתן הERM, על מדגם הלמידה במרחב סופי, תהיה יותר מאיזשהו אפסילון, היא לכל היותר דלתא, אם נשתמש במדגם למידה גדול מספיק.

נגדיר את HB כקבוצת ההיפותזות או המודלים הרעים, כלומר כל המודלים שעבורם שגיאת החיזוי היא יותר ממה שאנחנו רוצים, מאפסילון.

נגדיר את M להיות קבוצת כל המדגמים שהם מטעים. מה זה אומר מטעים? מדגם הלמידה יכול להראות על אחד מהמודלים הגרועים, שגיאה אפס. כלומר מדגם הלמידה עלול להוביל אותנו למודל גרוע. למה בהכרח אפס? נזכור שתחת הנחת הריאלייזיביליטי שהיא עדיין איתנו, זה בדיוק מה שERM מביא אותנו, לא חוזים לX מסוים Y מסוים ויכול להיות Y אחר בטעות, אפשר בקלות להגיע לשגיאה אפס.

אז שני דברים חשובים לשים לב אליהם:

אחד, זה שמה שכתוב לנו בביטוי הההסתברות למעלה הוא בעצם מוכל בM.  קבוצת כל מדגמי הלמידה שמביאים לשגיאת חיזוי גדולה יותר ממה שרצינו עם ERM, כלומר מטעים את ERM, מוכלת או שווה לקבוצת המדגמים המטעים בכלל. מה זה נותן לי? זה אומר שאני יכול לרשום שהסיכוי שאנחנו רוצים הוא קטן או שווה לסיכוי של M, הסיכוי לפגוש מדגם מטעה.

דבר שני, זה שאפשר לרשום את קבוצת המדגמים המטעים כאיחוד. איחוד כל המדגמים שמביאים לשגיאת למידה אפס, על פני כל המודלים הגרועים. מה זה נותן לי? בהסתברות אנחנו אוהבים להשתמש בחסם מפורסם שנקרא חסם האיחוד, היוניון באונד. ההסתברות לאיחוד שני מאורעות A וB קטנה או שווה לסכום ההסתברויות של המאורעות האלה. וכאן ההסתברות של הקבוצה M שהיא איחוד מאורעות, תהיה קטנה או שווה לסכום ההסתברויות של המאורעות האלה.
:::
:::

---

### Proof:

$\begin{aligned}
P_X\left[T: Err_{P_X, f}(ERM_\mathcal{H}(T)) > \varepsilon\right] &\leq P_X\left[M\right] \\
&= P_X\left[ \bigcup_{h \in \mathcal{H}_B} \{T:  \overline{err}_T(h) = 0\} \right] \\
&\leq \sum_{h \in \mathcal{H}_B} P_X\left[ T:  \overline{err}_T(h) = 0 \right] \quad \text{(union bound)}\\
&= \sum_{h \in \mathcal{H}_B}\prod_{i = 1}^n P_X\left[x_i: h(x_i) = f(x_i)\right] \\
&= \sum_{h \in \mathcal{H}_B}\prod_{i = 1}^n \left[1 - Err_{P_X, f}(h)\right] \\
&\leq |\mathcal{H}_B|(1 - \varepsilon)^n \\
&\leq |\mathcal{H}|e^{-\varepsilon n} \to \leq \delta
\end{aligned}$

::: {.fragment}
$\Rightarrow n \geq \frac{\log(|\mathcal{H}| / \delta)}{\varepsilon}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז נסביר מה קורה כאן שלב אחר שלב:

המעבר הראשון נובע מההערה הראשונה, המאורע ההסתברותי שלנו מוכל בM לכן ההסתברות שלנו קטנה או שווה להסתברות הקבוצה M.

זאת בעצם הסתברות איחוד על פני כל המודלים הרעים.

ומהיוניון באונד אפשר לחסום את הסתברות האיחוד בסכום על פני כל המודלים הרעים לראות מדגמים עם שגיאת למידה אפס.

אבל מה זו ההסתברות לראות מדגם למידה עם שגיאה אפס, תחת הנחת הריאלייזביליטי והiid זה אומר מכפלת כל הסיכויים שכל תצפית במדגם הגרוע הזה שווה בדיוק לקלסיפיקציה של X, לf האמיתי.

כל אחת מההסתברויות האלה היא על פי הגדרה 1 פחות שגיאת החיזוי, או 1 פחות התוחלת המותנית של המשתנה אינדיקטור שמקבל 1 אם טועים ואפס אם צודקים.

אבל השגיאה הזאת מחושבת רק על מודלים רעים, היא על פי הגדרה גדולה יותר מאפסילון, זה מה שהופך אותם למודלים רעים, כלומר כל המכפלה היא לכל היותר 1 פחות אפסילון בחזקת n. וכעת השגיאה הזאת לא תלויה בh, היא כמות קבועה כפול כל מספר המודלים הגרועים.

עכשיו אני חוסם את מספר המודלים הגרועים בגודל של כל מחלקת המודלים, הגודל של H. וגם אנחנו משתמשים שוב בעובדה ש1 פחות אפסילון בחזקת n מתנהג כאקספוננט בחזקת מינוס אפסילון כפול n.

והדבר הזה היינו רוצים שיהיה קטן או שווה לדלתא, זאת הדרישה המקורית.

 יש כאן אי-שוויון די פשוט, מעברים אגפים, לוקחים לוג, ומקבלים בדיוק את החסם התחתון על n מהמשפט.
:::
:::

---

### Do we believe realizability?

::: {.fragment}
![](error_decomposition.png)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ועדיין אנחנו לא לגמרי מרוצים. גם אנחנו יודעים שעולם הלמידה מורכב מעוד בעיות חוץ מקלסיפיקציה בינארית, גם יש לנו אישיו עם העובדה שמגבילים אותנו למרחבים סופיים של השערות, הרי אפילו ברגרסיה ליניארית פשוטה כל אחד מהמקדמים יכול להיות על פני כל הישר הממשי, שהוא מרחב אינסופי של אפשרויות. אבל אולי הכי אקוטי, אנחנו מתקשים להאמין להנחת הריאלייזביליטי, האם אנחנו מצופים להאמין שתמיד תמיד, שתי מהנדסות עם אותו ניסיון, אותו גיל, השכלה, תפקיד ואפילו אותו מקום עבודה יקבלו אותו שכר?

המודל שאנחנו רגילים לראות לדוגמא עם פירוק ביאס-וריאנס, הוא יותר משהו מהסגנון שאנחנו רואים בסכמה הזאת. הפונקציה האמיתית f קודם כל לא חייבת להיות במרחב שבו אנחנו מחפשים את המודל h. היא יכולה להיות מחוצה לו. עכשיו אם היא מחוצה לו, זה כבר אומר שיש איזה מודל אופטימלי שניתן להשיג בתוך מרחב H שמגביל אותנו, ובין השניים יש איזושהי שגיאת קירוב או ביאס. וגם אם אנחנו מחפשים דווקא בתוך מרחב H, אנחנו מובלים על-ידי המדגם שלנו אל מודל hT, כשטי מגיע מהמדגם למידה הספציפי שלנו, טריינינג. למה המודל שהמדגם הספציפי שלנו יהיה רחוק מהמודל האופטימלי שאפשר להגיע אליו במרחב H? שגיאת אמידה, או: שונות. המהדרין יוסיפו כאן אפילו עוד h עם סימון alg מלשון אלגוריתם, כלומר יוסיפו בחשבון שגם האלגוריתם עצמו שבחרנו כדי לעשות מינימום לשגיאה יוסיף רעש, למשל אם אנחנו חייבים לבחור באלגוריתם יעיל על הנתונים שלנו, שיחזיר תשובה גם אם הוא לא אופטימלי. לבסוף, אנחנו גם סקפטים שיש פונקציה f אמיתית שמתאימה לכל ערך X את ערך הY שלו, או שאנחנו רוצים לקחת בחשבון שאין לנו את היכולת לאמוד אותה כל כך ספציפי, כלומר שגם סביב f האמיתית יש איזשהו רעש שלא נצליח למדל. לרעש הזה קראנו האירדוסיבל ארור, ברגרסיה סימנו אותו כסיגמה בריבוע. 

אז היינו רוצים לראות מושג של למידות או לרנביליטי קצת פחות נוקשה, שמאפשר לדבר בדיוק במושגים האלה, וזה מה שנראה בחלק הבא.
:::
:::

---

## Agnostic PAC Learning {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בחלק הזה נעשה כמה רילקסציות לטפל בחלק מהבעיות שהעלינו, בניסיון להביא את כל התיאוריה הזאת למטה, לאדמה, לטיפול בבעיות דאטא אמיתיות ריאליסטיות. בראש ובראשונה ניפטר מהנחת הריאלייזביליטי, ונדרוש קריטריון פחות מחמיר מפאק, קריטריון זה נקרא פאק אגנוסטי, מייד נראה אגנוסטי למה.
:::
:::

---

### Agnostic PAC Learning

::: {.fragment}
|               | PAC                                                       | [Agnostic]{style="color:red;"} PAC                               |
|---------------|-----------------------------------------------------------|------------------------------------------------------------------|
| Realizability | Yes                                                       | No                                                                          |
| Distribution  | $P_X$ over $\mathcal{X}$                                  | $P_{XY}$ over $\mathcal{X}\times\mathcal{Y}$                     |
| Training $T$  |$(x_1, \dots, x_n) \sim P_X \quad \forall i, y_i = f(x_i)$ | $((x_1, y_1), \dots, (x_n, y_n)) \sim P_{XY}$                    |
| $\mathcal{Y}$ | $\{0, 1\}$                                                | Any                                                                         |
| Truth         | $f \in \mathcal{H}$                                       | not in class or doesn't exist                                                                       |
| Risk          | $Err_{P_X, f}(h) = P_X\left[f(x) \neq h(x)\right]$        | $Err_{P_{XY}}(h) = \mathbb{E}_{P_{XY}}\left[L(h, (x, y))\right]$ |
| Probably      | $P_X\left[T: \text{Approximately}\right] \geq 1 - \delta$ | $P_{XY}\left[T: \text{Approximately}\right] \geq 1 - \delta$     |
| Approximately | $Err_{P_X, f}(A(T)) \leq \varepsilon$                     | $Err_{P_{XY}}(A(T)) \leq \min_{h \in \mathcal{H}}Err_{P_{XY}}(h) + \varepsilon$ |

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נעבור שורה שורה ונראה מה מוסיף לנו המעבר למודל פאק הכללי, ללמידת פאק אגנוסטי, אחרי שנפטרנו מהנחת הריאלייזביליטי, ואנחנו מוכנים לקבל שיש רעש קטן בנתונים.

המרחב שאנחנו דוגמים ממנו הוא כבר לא מרחב של איקסים, אלא מרחב משותף של איקסים ווואים, מאיזושהי התפלגות משותפת XY.

כלומר אנחנו לא דוגמים יותר איקסים ומכילים עליהם איזשהו יחס f אבסולוטי כדי לקבל את הוואי, אנחנו דוגמים זוגות של תצפיות איקס וואי מתוך התפלגות משותפת.

יתרה מזה, המרחב של וואי לא חייב להיות בינארי או אפילו סופי, אנחנו יכולים לטפל במרחבים שונים כולל הישר הממשי לבעיות רגרסיה.

אנחנו לא מניחים כאמור שקיים איזשהו מודל אחד או פונקציה f מתוך קבוצה של מודלים אפשריים וזאת האמת, יכול להיות שהוא לא קיים ויכול להיות שהוא קיים ואנחנו מחפשים מודל לא בקלאס H הנכון, למשל המודל האמיתי הוא לא-ליניארי ואנחנו מגבילים את עצמנו למודל ליניארי. בשפה שלנו אנחנו מאפשרים הטיה.

קודם הריסק שלנו, או שגיאת החיזוי היתה רק הסיכוי להיות טועים, כלומר התוחלת של שגיאת אפס-אחת. עכשיו הריסק שלנו, או שגיאת החיזוי, זה תוחלת של כל הפסד L סביר, שלוקח זוג X וY, לוקח השערה או מודל נאמד ומחזיר מספר אי-שלילי בדרך כלל.

ההבדל החשוב ביותר נמצא בשתי השורות האחרונות של הטבלה: המשמעות של פאק משתנה קצת. הפרובבלי נשאר אותו דבר, אנחנו מודעים לזה שיכול להיות שנקבל מדגם פשוט גרוע, ואנחנו רוצים להיות צודקים אפרוקסימטלי, כלומר לקבל מדגם טוב, בסיכוי לכל הפחות אחת פחות דלתא.

מה זה אומר להיות צודקים אפרוקסימטלי משתנה: אנחנו רוצים לקבל שגיאת חיזוי לא אבסולוטית קטנה או שווה מאיזשהו אפסילון, אלא קטנה או שווה מהשגיאה הכי טובה שאפשר לקבל במרחב H שבו אנחנו מחפשים, ועוד אפסילון. כאן נעוצה המשמעות של המילה אגנוסטי, לא אכפת לי באיזה מרחב אתה מחפש, אני מבקש שבמרחב הזה, תגיע עד כדי אפסילון לשגיאה הכי טובה שאפשר להגיע אליה.
:::
:::

---

### More formally

::: {.callout-caution}
Definition:

A hypothesis class $\mathcal{H}$ is agnostic *PAC learnable* with respect to a set $T$ from $\mathcal{X}\times\mathcal{Y}$ and a loss function $L: \mathcal{H}\times\mathcal{X}\times\mathcal{Y} \to \mathbb{R}_+$, if there exists a function $n_\mathcal{H}: (0, 1)^2 \to \mathbb{N}$ and a learning algorithm $A$ with the following property:

For every $\varepsilon, \delta \in (0,1), n \geq n_\mathcal{H}(\varepsilon, \delta)$, and distribution $P_{XY}$ over $\mathcal{X}\times\mathcal{Y}$,

$$P_{XY}\left[T: Err_{P_{XY}}(A(T)) \leq \min_{h \in \mathcal{H}}Err_{P_{XY}}(h) + \varepsilon\right] \geq 1 - \delta$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עכשיו אפשר להציג את ההגדרה הרשמית למה הופך מרחב של מודלים ללמיד, בקריטריון של פאק אגנוסטי.

יש לנו מדגם למידה טי שנדגם מהמרחב המשותף של X וY. יש לנו פונקציית הפסד מהמרחב המשותף ומהמרחב של המודלים האפשריים אל איזושהי כמות אי-שלילית. ואם נצליח למצוא חסם תחתון nH לכל המרחב הזה לגודל המדגם, ואיזשהו אלגוריתם A שמשיג את עיקרון הפאק החדש, אז נגיד שמרחב או קלאס המודלים הזה, הוא למיד מסוג פאק אגנוסטי.

ומהו הקריטריון? להיות צודקים בערך, בסבירות גבוהה. שהסבירות לדגום מדגם T טוב, שיביא לאלגוריתם A שמשיג את השגיאה הטובה ביותר שאפשר עד כדי אפסילון, תהיה לכל הפחות אחת פחות דלתא. כשאפסילון ודלתא הם שני שברים קטנים שאנחנו נותנים.
:::
:::

---

### What is (agnostic) PAC learnable? (I)

::: {.fragment}
Not so fast, need two definitions:
:::

::: {.fragment}
::: {.callout-caution}
Definition:

A training sample $T$ is called [*$\varepsilon$-representative*]{style="color:red;"} if:
$$\forall h \in \mathcal{H} \quad | \overline{err}_T(h) - Err_{P_{XY}}(h)| \leq \varepsilon$$
:::
:::

::: {.fragment}
::: {.callout-caution}
Definition:

$\mathcal{H}$ has [*uniform convergence*]{style="color:red;"} if there exists a function $n_\mathcal{H}^{UC}: (0, 1)^2 \to \mathbb{N}$, such that for every $\varepsilon, \delta \in (0,1), n \geq n_\mathcal{H}^{UC}(\varepsilon, \delta)$, for every $P_{XY}$ over $\mathcal{X}\times\mathcal{Y}$:
$$P_{XY}\left[T: T \text{ is } \varepsilon \text{-representative}\right] \geq 1 - \delta$$
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כמו קודם, זאת היתה רק הגדרה, ועכשיו נשאל, האם אנחנו יודעים לאפיין מחלקות מסוימות של מודלים כלמידות.

התשובה היא כן, אבל אנחנו צריכים קודם שתי הגדרות בשביל זה:

הגדרה ראשונה היא של מדגם אפסילון-מייצג. מדגם אפסילון-מייצג, זאת הגדרה ביחס לאיזושהי מחלקה של מודלים או השערות, וזה אומר שאני יכול להבטיח שהטעות על מדגם הלמידה תהיה רחוקה מהטעות הכללית, שגיאת החיזוי בתוחלת, עד כדי אפסילון, לכל מודל h.

הגדרה נוספת שנצטרך שמשתמשת בהגדרה של מדגם אפסילון מייצג, היא של יוניפורם קונברג'נס - התכנסות אחידה. נאמר שמחלקה של מודלים H יש לה את תכונת ההתכנסות האחידה, אם קיים חסם תחתון שנסמן בnHUC, כך שעבור כל אפסילון ודלתא קטנים, אם גודל המדגם שלנו גדול מהחסם, אז לכל התפלגות משותפת PXY, מובטח לנו שהמדגם הוא אפסילון-מייצג בסיכוי לפחות 1 פחות דלתא.

ההתפלגות האחידה כאן היא על כל התפלגות PXY, ועל כל מודל H מהמשפחה, מובטח לנו שמדגם גדול מספיק הוא מייצג, כלומר ששגיאת הלמידה שלו תהיה רחוקה מהשגיאה בתוחלת עד כדי אפסילון, בהסתברות 1 פחות דלתא.
:::
:::

---

### What is (agnostic) PAC learnable? (II)

::: {.fragment}
Still not done, need "an algorithm $A$":
:::

::: {.fragment}
::: {.callout-important}
Claim:

If $T$ is $\frac{\varepsilon}{2}$-representative, then for any solution $h_T$ of $ERM_\mathcal{H}(T) \in \arg\min_{h \in \mathcal{H}}\overline{err}_T(h)$:
$$Err_{P_{XY}}(h_T) \leq \min_{h \in \mathcal{H}}Err_{P_{XY}}(h) + \varepsilon$$
:::
:::

::: {.fragment}
Proof:
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אוקי, עדיין לא גמרנו, אם תיזכרו בהגדרה של מה זה למידות פאק אגנוסטי זה כולל גם איזשהו אלגוריתם להשיג את המודל הזה שמביא לשגיאה קטנה, והטענה היא שאלגוריתם כזה הוא בדיוק אלגוריתם שעושה מינימיזציה לשגיאה של מדגם הלמידה, מה שסימנו כERMH.

ספציפית, הטענה אומרת שמספיק שמדגם הלמידה טי הוא חצי-אפסילון מייצג, אז מובטח שERMH משיג את הקריטריון האפרוקסימטלי של פאק, שהשגיאה האמיתית של המודל שנלמד מERM, בתוחלת, על פני נתונים שלא ראינו, היא שווה לשגיאה הטובה ביותר שאפשר להשיג על מחלקה H עד כדי אפסילון.

נוכיח את זה בזריזות, אף על פי שזה נשמע די הגיוני:

על פי ההגדרה של מדגם חצי-אפסילון מייצג, השגיאה האמיתית של המודל הספציפי שקיבלנו hT, קטנה או שווה לשגיאת מדגם הלמידה של הלומד הספציפי הזה ועוד חצי אפסילון, זה המקסימום שהיא יכולה להתרחק. אבל איך הגענו למודל הספציפי הזה? עשינו מינימום לשגיאת מדגם הלמידה, כלומר שגיאת מדגם הלמידה היא קטנה או שווה לשגיאה של כל לומד H על מדגם הלמידה הספציפי הזה. ושוב בחזרה לשגיאה האמיתית, התוחלת, בגלל ההגדרה של מדגם חצי-אפסילון מייצג היא רחוקה עד כדי חצי אפסילון מהשגיאה האמיתית. חצי אפסילון ועוד חצי אפסילון זה אפסילון ומקבלים את האי-שוויון הרצוי, השגיאה האמיתית על הלומד הספציפי שנלמד מERM היא קטנה או שווה לשגיאה האמיתית לכל לומד, ועוד אפסילון.
:::
:::

---

### What is (agnostic) PAC learnable? (III)

Uniform convergence with $\frac{\varepsilon}{2}$ $\rightarrow$ $T$ is $\frac{\varepsilon}{2}$-representative $\rightarrow$ $ERM_\mathcal{H}(T)$ gets PAC bound

::: {.fragment}
::: {.callout-tip}
Corollary:

- If $\mathcal{H}$ has uniform convergence with a function $n_\mathcal{H}^{UC}$ with some $\frac{\varepsilon}{2}$ then $\mathcal{H}$ is agnostic PAC learnable, with $n \geq n_\mathcal{H}(\varepsilon, \delta)$
- Furthermore, $ERM_\mathcal{H}(T)$ is a succesful PAC learner for $\mathcal{H}$
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
סוף סוף אנחנו יכולים לתת אפיון מסוים למחלקות של מודלים שלמידים פאק אגנוסטי, ולא רק זה שהם למידים אלא איך ללמוד אותם.

אם אפשר להראות שמודלים כאלה הם בעלי התכונה של התכנסות אחידה עם חצי אפסילון, זה אומר על-פי הגדרה שעבור כל התפלגות משותפת המדגם שניקח יהיה חצי-אפסילון מייצג בהסתברות לפחות 1 פחות למדא. וזה אומר על-פי הטענה שפרידגמת ERM על המחלקה הזאת, תשיג את חסם פאק.

או בצורה רשמית יותר, מצאנו שאם למחלקה של מודלים H יש את תכונת ההתכנסות האחידה עם חסם תחתון nHUC ביחס לחצי אפסילון, המחלקה היא למידה פאק אגנוסטי, עם מדגם גדול מספיק מחסם תחתון nH שהגדרנו עם אפסילון שלם.

יתרה מזו, דרך אפשרית ללמוד את האלגוריתם A שישיג את חסם פאק היא עם אמפיריקל ריסק מינימיזיישן, ERM.

נשים לב שזה תנאי מספיק ללהגיד שמחלקה של מודלים היא למידה-פאק-אגנוסטי, אבל הוא לא הכרחי, בהחלט יכולות להיות מחלקות שאין להן את התכונה הזאת והן למידות-פאק, עד כמה שאנחנו יודעים. אבל זה שלב חשוב, כי אם נוכל לאפיין אילו מחלקות מקיימות את התנאי הזה של התכנסות אחידה נדע מייד שהן למידות, ואפילו איך ומה צריך להיות גודל המדגם כדי ללמוד אותן. זה בדיוק מה שנעשה בחלק הבא.
:::
:::

---

## Finite Classes are Angnostic PAC Learnable {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What has uniform convergence?

::: {.fragment}
::: {.callout-tip}
Theorem:

Assume $\mathcal{H}$ is **finite** and the range of the loss function is bounded in $[0, 1]$. Then:

- $\mathcal{H}$ has uniform convergence with function $n_\mathcal{H}^{UC} \leq \Bigl\lceil\frac{\log(2|\mathcal{H}|/\delta)}{2\varepsilon^2}\Bigr\rceil$
- Therefore, $\mathcal{H}$ is agnostic PAC learnable using the $ERM_\mathcal{H}$ algorithm with sample size $n \geq \Bigl\lceil\frac{2\log(2|\mathcal{H}|/\delta)}{\varepsilon^2}\Bigr\rceil$
:::
:::

::: {.fragment}
Comments:

- Use the Hoeffding's Inequality:
  - Let $X_1, \dots, X_n$ in $[a, b]$ i.i.d RVs with $\mathbb{E}(X_i) = \mu$, then for any $\varepsilon > 0$:
  $$P\left[|\bar{X} - \mu| > \varepsilon\right] \leq 2\exp\left(-2n\varepsilon^2/(b - a)^2\right)$$
- So the loss can be bounded in any range
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז הבנו שאם נוכל להגיד על מחלקה מסוימת של מודלים שיש לה התכנסות אחידה עבור חצי אפסילון, זה אומר שהיא למידה פאק אגנוסטי עבור אפסילון שלם. כלומר כל שניתן לנו לשאול זה לאיזו מחלקה של מודלים יש את התכונה הזאת של התכנסות אחידה.

מה שמחזיר אותנו שוב למחלקות סופיות. משפט חשוב אומר שאם מחלקה היא סופית, ואנחנו משתמשים בפונקצית הפסד שיכולה להחזיר ערכים בין 0 ל-1, אז יש לH התכנסות אחידה עם חסם תחתון על גודל מדגם הלמידה שהוא לכל היותר לוגריתמי בגודל המחלקה עם הביטוי שכאן.

ולכן, H היא מחלקה של מודלים למידה פאק אגנוסטי באמצעות ERM, עם חסם תחתון על גודל המדגם בביטוי שמופיע כאן. פשוט תראו באפסילון של ההתכנסות האחידה כחצי אפסילון, כשהוא עולה בריבוע מקבלים אפסילון בריבוע חלקי 4, מצטמצם עם 2, ו-2 עולה למונה.

אז לא נוכיח כאן מדוע למחלקה סופית יש תכונה של התכנסות אחידה עם החסם שמופיע כאן, רק נגיד שמוכיחים זאת באמצעות אי-שוויון הופדינג, שאומר שעבור מדגם מקרי בגודל n של משתנה שחסום בין מספרים A וB, ממוצע המדגם מתרחק מהתוחלת עד כדי כל אפסילון שנרצה, בסיכוי שאפשר לחסום.

במקרה שלנו המשתנה המקרי הוא הלוס שחסום בין אפס לאחת, ואנחנו מראים שממוצע הלוס לא יכול להתרחק מהתוחלת יותר מדי בסיכוי מסוים, כלומר למחלקה H יש את תכונת ההתכנסות האחידה ולכן היא למידת פאק אגנוסטי.
:::
:::

---

### Linear Model PAC Learnability

::: {.incremental}
- Now suppose $y_i = f(x_i) + \tau_i$, specifically $y_i = \beta_0 + \beta_1x_{i1} + \dots \beta_px_{ip} + \tau_i$
- Suppose our squared error loss can be bounded (in $[0, 1]$, but can manage $[0, a]$ for some large $a$)
- What is $\mathcal{H}$? Is it finite?
- Discretization Trick: if every coefficient $\beta_j$ is represented as a 64 bit float:
  - Let $d = p + 1$
  - $|\mathcal{H}| = 2^{64d}$
- We reach agnostic PAC learnability for any $\varepsilon, \delta \in (0,1)$!
  - With $ERM_\mathcal{H}(T) = \hat{\beta} = (X^TX)^{-1}X^Ty$
  - $n \geq \Bigl\lceil\frac{2\log(2|\mathcal{H}|/\delta)}{\varepsilon^2}\Bigr\rceil \approx \frac{128d + 2\log(2/\delta)}{\varepsilon^2}$
  - Compare this to the Fixed-$X$ optimism bound: $n \geq \frac{2d\sigma^2}{\varepsilon}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז על אילו בעיות אנחנו יכולים להכיל את החסם הזה?

למשל הבעיה שלנו של רגרסיה ליניארית, היא לא באופן מיידי עונה להגדרות של מחלקה H של מודלים סופית ופונקצית הפסד בין אפס לאחת.

אבל אם נניח שאפשר איכשהו לחסום את ההפסד הריבועי של רגרסיה ליניארית בין 0 ל-1, והאמת שגם אם הוא חסום בין 0 לאיזשהו קבוע A החסם פשוט יגדל - 

מהי H? האם היא סופית?

H היא מרחב כל הפרמטרים של הרגרסיה. יש P + 1 כאלה, כל אחד יכול להיות על כל הישר הממשי, כלומר R בP + 1. שהוא בודאי לא מרחב סופי.

אבל בכל זאת אפשר לבצע כאן טריק שנקרא הדיסקרטיזיישן טריק, דיסקרטיזציה. בעידן המחשב בכל זאת מספר הוא בדיד, ונגיד שאפשר לייצג אותו עם 64 ביטים.

אז אם D = P + 1 פרמטרים של בטא, גודל המחלקה שלנו הוא 2 בחזקת 64 D, שזה גודל עצום אבל היא כן סופית.

ותחת התנאים האלה, מובטחת לנו למידות פאק אגנוסטי עבור איזשהם אפסילון ודלתא קטנים שנספק.

לא רק זה, אנחנו כבר יודעים את הדרך למצוא מודל כזה, פתרון ERM במקרה כזה הוא פתרון הריבועים הפחותים שלמדנו, בטא האט, אף על פי שנזכור שהשגנו אותו עם הפסד ריבועי כללי ולא בהכרח הפסד חסום עד 1.

אבל בשורה התחתונה קיבלנו חסם לאיזה גודל מדגם אנחנו צריכים כדי להבטיח למידות פאק עם אפסילון ודלתא שלנו, והוא לוגריתמי בגודל המחלקה. אם נציב את המחלקה שלנו נקבל שאנחנו צריכים מדגם גדול לפחות כמו 128 כפול מספר הפרמטרים D, ועוד קבוע שתלוי בדלתא חלקי אפסילון בריבוע.

מעניין להשוות את החסם הזה, שהתקבל עם הרבה פחות הנחות, לחסם שהגענו אליו מחישובי האופטימיזם, עבור סטינג של פיקסד איקס, כלומר עבור איקס קבוע. באופן אינטואיטיבי החסם שמניח הרבה פחות הוא גם גדול הרבה יותר בפועל. אנחנו גם מכפילים את D במספר הביטים שאפשר לייצג מספר שזה בימינו לא פחות מ32, וגם מחלקים באפסילון קטן הרבה יותר, מחלקים באפסילון בריבוע.
:::
:::

---

## The Bias-Complexity Tradeoff {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The Bias-Complexity Tradeoff

::: {.incremental}
- Let $h_T$ be the model reached by $ERM_\mathcal{H}(T)$, that is: $h_T \in \arg\min_{h \in \mathcal{H}}\overline{err}_T(h)$.

- Its "true" error over unseen data: $Err_{P_{XY}}(h_T)$

- The actual minimum error achievable in $\mathcal{H}$: $\min_{h \in \mathcal{H}} Err_{P_{XY}}(h)$

- So: $Err_{P_{XY}}(h_T) = \min_{h \in \mathcal{H}} Err_{P_{XY}}(h) + \tau = \varepsilon_{\text{app}} + \varepsilon_{\text{est}}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בתיאוריית הלמידה הכללית ממנה אנחנו טועמים בשיעור הזה, מופיע גם כן טריידאוף על שגיאת החיזוי כתוצאה מפירוק. כאן נהוג לקרוא לו הביאס-קומפלקסיטי טריידאוף, כלומר אין התייחסות ישירה לאלמנט הוריאנס אלא רק דרך הקומפלקסיטי, המסובכות הכללית של המודל.

נגדיר שוב את hT כהמודל שקיבלנו בסוף כל התהליך של ERM, כלומר המודל שעל פני מחלקה H הביא למינימום את השגיאה על מדגם הלמידה.

הטעות שמעניינת אותנו היא השגיאה שאפשר להכליל על כל הנתונים, תוחלת השגיאה של hT על נתונים גם שהוא לא ראה.

נרשום גם את המינימום שגיאת חיזוי שניתן להגיע אליו עם כל מודל ממחלקה H.

ברור שהשגיאה של המודל שלנו, שווה למינימום הזה ועוד איזשהו משתנה שנסמן כטאו.

עכשיו נסמן את המינימום כאפסילון-אפפ, ואת טאו כאפסילון-אסט.

ממה נובע אפסילון-אפפ? מהמחלקה שבחרתי H. אם קיים מודל אמיתי אבל הוא בכלל לא בH שבחרתי, לדוגמא יש קשר לא-ליניארי בין X לY ואני מגביל את עצמי למודל ליניארי. אז זו שגיאת קירוב שיכולה להיות די גדולה אבל היא לא תלויה בכלל במדגם הלמידה, ובגודל שלו n. איך קראנו לשגיאה כזאת? הטייה, ביאס.

וממה נובע אפסילון-אסט? למה שהמודל שאני השגתי עם מדגם הלמידה ישיג שגיאה שרחוקה מהשגיאה הכי טובה במחלקה? זה כבר תלוי במדגם הלמידה ובגודל שלו n, ובמחלקה עצמה ועד כמה היא מסובכת. זו שגיאת אמידה, אנחנו קראנו לזה שונות.
:::
:::

---

### The Bias-Complexity Tradeoff

$$Err_{P_{XY}}(h_T) = \varepsilon_{\text{app}} + \varepsilon_{\text{est}}$$

::: {.fragment}
- Approximation Error $\varepsilon_{\text{app}}$:
  - Caused by restricting class $\mathcal{H}$
  - Does not depend on $T$
  - Decreases with $|\mathcal{H}|$
  - Model bias!
- Estimation Error $\varepsilon_{\text{est}}$:
  - Caused by ERM with specific $T$
  - increases (logarithmically) with $|\mathcal{H}|$, decreases with sample size $n$
  - Model complexity (our "variance")
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נסכם: את שגיאת החיזוי, תוחלת ההפסד על פני כל התצפיות האפשריות, אפשר לחלק לשניים:

שגיאת קירוב אפסילון-אפפ, היא ההטיה. היא מושפעת מההגבלה שלנו לאיזושהי מחלקה של מודלים. היא לא תלויה במדגם הלמידה והיא יורדת ככל שהמחלקה גדלה, ככל שאנחנו מאפשרים מודל מורכב יותר ויותר. זאת ההטיה של המודל.

מצד שני יש לנו שגיאת אמידה אפסילון-אסט, שמתרחשת בגלל השימוש במדגם למידה וERM לאמוד את השגיאה האמיתית. ספציפית היא תלויה מאוד בגודל של מדגם הלמידה. ככל שנדגום יותר תצפיות היא תפחת. ומצד שני ככל שהמחלקה תגדל כך טעות האמידה תגדל ונצטרך מדגם גדול יותר לאמוד את המודל הנכון.

קראנו לזה שונות, כאן הפוקוס הוא על הסיבוכיות או גודל מחלקת המודלים.
:::
:::

---

### Where did "irreducible" go?

::: {.incremental}
- Recall: $Err_{P_{XY}}(h_T) = \min_{h \in \mathcal{H}} Err_{P_{XY}}(h) + \tau = \varepsilon_{\text{app}} + \varepsilon_{\text{est}}$
- The $\min_{h \in \mathcal{H}} Err_{P_{XY}}(h)$ could be decomposed itself to:
$$\min_{h \in \mathcal{H}} Err_{P_{XY}}(h) = \min_{h \in \mathcal{Y}^\mathcal{X}} Err_{P_{XY}}(h) + \left[\min_{h \in \mathcal{H}} Err_{P_{XY}}(h) - \min_{h \in \mathcal{Y}^\mathcal{X}} Err_{P_{XY}}(h)\right]$$
:::

::: {.fragment}
- That is, the best possible ("Bayes optimal") error for any $\mathcal{H}$ + excess over that which is what we know as bias
  - Under realizability $\min_{h \in \mathcal{Y}^\mathcal{X}} Err_{P_{XY}}(h) = 0$
  - Under agnostic model it could be $> 0$, like our $\sigma^2$

![](error_decomposition.png){fig-align="right" height=230}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אבל בפירוק הביאס-וריאנס שראינו היה עוד אלמנט של טעות, לא? טעות האירדוסיבל. כאן נדמה שהוא לא קיים, אבל אם נדייק נמצא גם אותו.

האמת היא שכשאנחנו רושמים את המינימום שגיאה על פני כל מחלקה H, אנחנו יכולים גם אותו לפרק לטעות שלא ניתן להפחית, ועוד טעות שהיא ההטייה שלנו.

נפחית ונחסר מהמינימום טעות את המינימום טעות על פני המרחב H הכי כללי שאפשר להעלות על הדעת, שממנו התחלנו, בלי שום הגבלות.

כעת נוסיף סוגריים ונראה שהטעות הזאת מתפרקת לטעות שלא ניתן להפחית, זה מינימום על המרחב הכי גדול שאפשר לחשוב, וואי באיקס. זה ממש כמו האירדוסיבל. וטעות שהיא קרובה יותר להטיה כפי שניסחנו אותה. הפער הבילט-אין, בלי קשר למדגם, בין הטעות במודל הכי טוב שניתן להגיע אליו, למודל שמוגבל להיות במחלקה שבחרנו.

לטעות הזאת המינימלית ביותר על פני כל המחלקות האפשריות, נהוג גם לכנות בייס אופטימל או בייס ארור, זאת טעות תיאורטית שהזכרנו בקצרה כשדיברנו על קלסיפיקציה. תחת ריאלייזביליטי כאמור הטעות הזאת היא אפס, אין סטוכסטיות. ותחת המודל הכללי יותר האגנוסטי, היא יכולה להיות גדולה מאפס, באופן שמזכיר מאוד את הסיגמה בריבוע שלנו, שם זה פרמטר שונות, או רעש.

עד כאן הטעימה שלנו מתיאורית הלמידה. ניכר שהשימוש כאן בכלים מתמטיים כמו טענות, הוכחות ומסקנות הוא רב יותר, מה שמראה כמה זה מורכב לדרוש תיאורטית שנצליח בבעית למידה, אפילו אם אנחנו מגבילים את עצמנו למודל פשוט כמו המודל הליניארי. אבל העקרונות שתיאורית הלמידה מגיעה אליהם הם לא שונים מהעקרונות שהגענו אליהם עד כה, ספציפית הטריידאוף של ביאס-קומפלקסיטי מתריע בפנינו על האיזון שאנחנו צריכים לשמור בין לחפש את המודל שלנו במשפחה מצומצמת ולקבל הטייה רחוקה מהאמת, לבין לחפש אותו במשפחה מורכבת מדי, שמצריכה מדגם גדול ועלולה להביא לשונות גבוהה ולפער בין השגיאה של מדגם הלמידה לשגיאה הצפויה בעולם האמיתי.
:::
:::
