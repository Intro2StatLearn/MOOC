---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "PAC Learning"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### PAC Learning - Class 7

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## What is learnable? {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Previously on Model Selection

::: {.incremental}
- For linear regression, squared error loss, Fixed $X$:
$$op = \mathbb{E}_{y}\left[Err_{in} - \overline{err}|X\right] = \mathbb{E}_{y}\left[\frac{1}{n}\sum_{i=1}^n\mathbb{E}_{y_0}\left[L(y_0, \hat{f}(x_i))|T\right] - \frac{1}{n}\sum_{i=1}^{n} L(y_i, \hat{f}(x_i))\right] = \frac{2d\sigma^2}{n}$$

- For Fixed $X$, for any $\varepsilon > 0$, if $n \ge \frac{2d\sigma^2}{\varepsilon}$, $\mathbb{E}_{y}\left[\overline{err}\right]$ is up to $\varepsilon$ smaller than $\mathbb{E}_{y}\left[Err_{in}\right]$!

- Can we learn *any* $f$ with *any* large-enough training sample, ensuring $\overline{err}$ is up to $\varepsilon$ smaller than $Err = \mathbb{E}_{x_0, y_0, T}\left[L(y_0, \hat{f}(x_0))\right]$ (no overfitting)?

- Spoiler: no, but [learnability]{style="color:red;"} is key concept:
    - under what conditions is $f$ learnable from a training sample?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשיעור הקודם, הגדרנו את מושג האופטימיזם: עד כמה מדגם הלמידה אופטימי בהערכת הטעות, אם נניח שX נתון, הוא קבוע, פיקסד, ואנחנו מושכים עוד תצפיות שהמודל לא רואה, מאותם איקסים. ברגרסיה ליניארית ראינו שאפשר להגיע לביטוי סגור, שכולל את d מספר הפרמטרים במודל, ואת סיגמא בריבוע, כשאם אנחנו לא יודעים אותה אפשר לאמוד אותה.

אם ככה, מתבקש לרצות שהאופטימיזם יהיה קטן כרצוננו. ומאחר שאין לנו שליטה על סיגמא בריבוע, עבור מודל עם מספר פרמטרים נתון, כל שנותר לנו לעשות זה להגדיל את גודל המדגם, את n. אם נרצה שהאופטימיזם יהיה קטן מאיזשהו מספר אפסילון קטן כרצוננו, אפשר להגיע לחסם תחתון על n, n צריך להיות גדול או שווה לפעמיים d סיגמא בריבוע חלקי אותו אפסילון שיכול להיות קטן כרצוננו.

האם אנחנו יכולים להמשיך את הקו הזה? האם אנחנו יכולים למצוא מדגם גדול מספיק כדי ללמוד לא רק פונקציה ליניארית אלא כל פונקציה f, ולוודא שהטעות של מדגם הלמידה תהיה קטנה עד כדי כל אפסילון שנבחר מהטעות האמיתית שסימנו כארר, על פני כל נתון שהמודל לא יראה?

אז הנה ספוילר להמשך, התשובה היא שלא. בשביל להשיג את החסם הזה השתמשנו במספר הנחות כמו המודל הליניארי, ואם זורקים את כולן מחוץ לחלון, קשה להבטיח שהטעות של מדגם הלמידה תהיה קרובה ככל שנרצה לשגיאה האמיתית. אבל השאלה הזאת מביאה אותנו לקונספט חשוב נורא והוא: לרנביליטי, או: למידות.

בשיעור הזה נטעם קצת מתיאוריית הלמידה, וננסה לענות, או להתחיל לענות על השאלה הפשוטה מה למיד, עבור איזו פונקציה f ובאילו תנאים, אפשר לדגום ממדגם הלמידה, אפשר לצפות שהטעות שלו תשקף את השגיאה האמיתית עבור כל נתון שהמודל לא רואה.
:::
:::

---

### Learning theory ingredients

- $x \sim P_X$ on $\mathcal{X}$
- Label set: $y \in \mathcal{Y}$  (start with $\mathcal{Y}  = \{0, 1\}$)
- True relation: $f: \mathcal{X} \to \mathcal{Y}$
- Classifier: $h: \mathcal{X} \to \mathcal{Y}$ (our $\hat{f}$)
- Training data: $T = \{x_1, \dots, x_n\}$, $i.i.d$, where $y_i = f(x_i)$
- Risk function: $L(f(x), h(x))$ (start with 0-1 loss)
- Error: $Err_{P_X, f}(h) = \mathbb{E}_{X \sim P_X}\left[L(f(x), h(x))\right] = P_X\left[f(x) \neq h(x)\right]$

::: {.fragment}
Most startling assumption:

- [Realizability]{style="color:red;"}: $y_i = f(x_i)$, i.e. for $Err_{P_X, f}(f) = 0$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נגביל את עצמנו לעולם מצומצם עם חוקים והנחות די מחמירות, ואחר-כך ננסה להיפטר מהמגבלות.

בעולם שלנו, מרחב ההסתברות כולל רק את X, המשתנים המסבירים, על המרחב הזה יש התפלגות Px מתוכה אנחנו דוגמים את האיקסים.

הוואי שלנו בא ממרחב וואי מסוים, ואנחנו נתחיל עם קלסיפיקציה לשתי רמות, כלומר וואי הוא אפס או אחת.

שימו-לב, אנחנו מניחים שיש קשר אמיתי, פונקציה f שלכל X מתאימה Y. ואת המודל שלנו שעד כה סימנו כf_hat נסמן כh. נקרא לו קלאסיפייר, אפשר גם כלל חיזוי, אפשר היפותזה.

כלומר המדגם המקרי בגודל n שאנחנו לוקחים הוא מדגם T מתוך האיקסים, שעבור כל X יש מיפוי לY דרך פונקציה F.

לפונקצית ההפסד שלנו נקרא ריסק, סיכון, ונתחיל עם פונקצית ההפסד הטבעית לבעיה שהיא האפס-אחת לוס.

ואז הטעות שלנו, היא תוחלת הריסק הזה. תוחלת על מה? על פני מרחב ההסתברות של X. זה משתנה אינדיקטור, אם חזינו נכון נקבל 0 ואם טעינו נקבל 1. ותוחלת של משתנה מקרי אינדיקטור היא פשוט ההסתברות לקבל 1, כלומר הטעות שלנו היא ההסתברות לחיזוי שגוי, שההיפותזה שלנו h על כל X, תהיה שונה מf האמיתית. נסמן את הטעות הזאת כארר על P_X ו-f, כפונקציה של היפותזה h.

אז אם עקבתם, הרכיב שהכי שונה מאיך שראינו את העולם עד עכשיו בלמידה סטטיסטית, הוא ההנחה המחמירה שלכל X מתאים רק Y ספציפי, ואין סטוכסטיות בקשר הזה. הרי תחת המודל שלנו עד עכשיו Y שווה לאיזושהי פונקציה f של X ועוד איזה משתנה מקרי של רעש אפסילון, ככה הסברנו איך המודל שלנו מסביר שעבור שני סטודנטים שונים מאותו מגדר, אותו ניסיון, אותם ציונים קודמים ואותו זמן התכוננות למבחן - יכול לצאת ציון שונה. וכאן - לא! זה דטרמיניסטי, אין דגימה ממרחב Y.

יתרה מזה, זה אומר שיש מושג כזה של טעות אפס, אם נגיע למודל האמיתי f, וזאת אפשרות, הסיכוי לשגיאה הוא אפס, כלומר הארר שלנו, הוא אפס. להנחה הזאת קוראים ריאלייזביליטי, היתכנות, אפשר להגיע לשגיאה אפס. זאת חתיכת הנחה, ננסה להוריד אותה בהמשך.

עוד הנחה שממנה כפי שאמרנו בעבר לעולם לא ניפטר, מסתתרת מאחורי הביטוי iid, כלומר המדגם נמשך בצורה זהה, עם החזרה, כל תצפית בלתי תלויה בשניה. זה גם אומר שהמדגם שאנחנו רואים קשור לעולם, אנחנו לא נמדדים בנתונים שלא ראינו על עולם אחר - אחרת, היינו עוצרים כאן.
:::
:::

---

## PAC Learning {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בעולם המחמיר שהגדרנו, ננסה לענות על השאלה האם תמיד אפשר ללמוד ממדגם הלמידה, ואם לא תמיד מה כן אפשר לצפות.
:::
:::

---

### Empirical Risk Minimization (ERM)

- $\overline{err}$ is termed the [empirical risk]{style="color:red;"}:

$$\overline{err}_T(h) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, h(x_i)) = \frac{1}{n}\sum_{i=1}^{n} \mathbb{1}\left[f(x_i) \neq h(x_i)\right]$$

::: {.incremental}
- A method which outputs $h_T$ by minimizing $\overline{err}_T(h)$ is called empirical risk minimization [ERM]{style="color:red;"}

- From [Realizability]{style="color:red;"}: $\overline{err}_T(h_T) = 0$

- Can we *always* find $h$ for which $Err_{P_X, f}(h) = 0$ via ERM (= by minimizing $\overline{err}_T(h)$)?

- Counter-example:
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניתן קודם שם למה זה אומר ללמוד ממדגם הלמידה. אם הסיכון הוא התוחלת על פני כל האיקסים האפשריים, נסמן בארר עם קו למעלה כמו שעשינו את הסיכון האמפירי שמחושב על מדגם הלמידה, שמחושב על מדגם ספציפי T ועבור מודל ספציפי h. במקרה של סיכון אפס-אחת מדובר בשיעור החיזוי השגוי על תצפיות מדגם הלמידה, מתוך n תצפיות.

פרדיגמה שלומדת איזשהו hT על-ידי הבאה למינימום של הכמות הזאת נקראת אמפיריקל ריסק מינימיזיישן, או ERM בקיצור.

עכשיו נשים לב לפרט עדין וחשוב: מהנחת הריאלייזביליטי, שאומרת שעבור כל X מתאים רק Y ספציפי, האם זה מאתגר למצוא מודל שיגיע לאמפיריקל ריסק אפס? בודאי שלא. תחת הנחת הריאלייזביליטי, אנחנו תמיד יכולים לאפס (!) את האמפיריקל ריסק, כי אין רעש, אין תצפיות שלפעמים הY שלהן אחת ולפעמים אפס. אני רק אזכיר שעד כאן חשדנו מאוד ממצב כזה שקראנו לו אוברפיטינג, ונשאלת השאלה:

מה זה למידות? האם אפשר לצפות להביא את שגיאת החיזוי, על פני נתונים שלא ראינו, לאפס, אם אנחנו מסתכלים רק על החלון הזה של מדגם למידה, כלומר לומדים עם ERM? זאת יכולה להיות ההגדרה של למידות?

כל מה שצריך זה להביא דוגמא נגדית כדי להראות שזה לא יכול להתקיים תמיד. בדוגמא הפשוטה ביותר אפשר להניח שכל מרחב האיקסים כולל רק שתי אפשרויות, X1 וX2. ההסתברות PX על המרחב הזה היא לקבל את X2 באיזשהו סיכוי קטן אפסילון, ואת X2 בסיכוי 1 פחות אפסילון. נניח גם שהמיפוי האמיתי f מביא את X1 לY = 0 ואת X2 לY = 1. מה זה אומר להיכשל במצב כל כך פשוט? מספיק אף פעם לא לראות בכלל את X2 כדי להיכשל, להגיע למודל שאומר שעבור כל האיקסים וואי שווה 0, האפשרות היחידה שאנחנו מכירים. האם יש סיכוי שאף פעם לא נראה את X2 וניכשל? בודאי. הסיכוי הזה הוא 1 פחות אפסילון בחזקת n. שמתנהג כמו כמו אקספוננט בחזקת מינוס אפסילון n. והגודל הזה לא בהכרח שואף לאפס ככל שn גדל, אם אפסילון קטן מאוד, הרבה יותר קטן מ1 חלקי n הוא לא ישאף לאפס.

כלומר יש סיכוי סביר שנקבל פשוט מדגם רע, שמביא את האמפיריקל ריסק לאפס אבל מביא לשגיאת חיזוי של אפסילון בהכרח על נתונים שהוא לא ראה. והשורה התחתונה היא שלהביא את שגיאת החיזוי לאפס היא קריטריון מחמיר מדי ללמידות, חייבים להוסיף פה איזשהו מימד הסתברותי. אנחנו צריכים להבין שתמיד יש סיכוי למדגם גרוע, ושגם מדגם טוב במרכאות לא בהכרח מביא לשגיאה אפס.
:::
:::

---

### Probably Approximately Correct (PAC)

::: {.incremental}
We introduce two relaxations:

1. [Accuracy]{style="color:red;"} $\varepsilon$: Find $h$ via ERM for which $Err_{P_X, f}(h) \leq \varepsilon$ ("approximately correct")
2. [Confidence]{style="color:red;"} $\delta$: With probability $\geq 1 - \delta$ ("probably correct", on $P_X$)
:::

::: {.fragment}
PAC learning, interim summary:
:::

::: {.incremental}
- Assume $P_X, f$
- Input: $\varepsilon, \delta \in (0,1)$
- Take i.i.d training sample $T$ of size $n \geq n(\varepsilon, \delta)$, where $n(\varepsilon, \delta): [0, 1]^2 \to \mathbb{N}$
- Output prediction rule $h$ s.t. w.p. at least $1 - \delta$ it holds that $Err_{P_X, f}(h) \leq \varepsilon$:
$$P_X\left[T: Err_{P_X, f}(h) \leq \varepsilon\right] \geq 1 - \delta$$
:::

::: {.fragment}
::: {.callout-note}
This is a definition! Importantly, no guarantee of existence, no way of finding algorithm $A$ to minimize ERM.
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אנחנו עושים שתי רילקסציות, ודורשים קצת פחות: רילקסציה ראשונה מבטאת את זה שאנחנו יודעים שאנחנו לא יכולים להביא את שגיאת החיזוי לאפס מוחלט, אנחנו רוצים להביא אותה עד לאיזשהו פרמטר אקיורסי קטן כרצוננו, אפסילון. כלומר אנחנו רוצים להיות אפרוקסימטלי קורקט.

הרילקסציה השניה מבטאת את זה שאנחנו יודעים שבסיכוי מסוים גם מדגם מקרי יכול להיות גרוע או לא מייצג, ואנחנו רוצים לחסום את הסיכוי הזה להיות 1 פחות איזשהו פרמטר קונפידנס דלתא. כלומר אנחנו רוצים להיות פרובבלי קורקט.

ללמידות כזאת, פרובבלי אפרוקסימטלי קורקט, קוראים בקיצור פאק לרנביליטי או לרנינג, למידה מסום פאק. נסכם:

אנחנו מניחים מרחב הסתברות PX ואיזשהו כלל החלטה או מיפוי ממנו אל מרחב Y.

אנחנו מספקים שני פרמטרים של אקיורסי וקונפידנס בין 0 ל1 קטנים כרצוננו.

אנחנו לוקחים מדגם מקרי iid בגודל n כאשר השליטה היחידה שלנו היא על גודל המדגם, אנחנו נדאג שיהיה גדול או שווה לאיזשהו סף שתלוי באפסילון ודלתא. את הסף הזה אפשר לראות כפונקציה מהמרחב של אפסילון ודלתא, מרחב האפס עד אחת בריבוע, אל המספרים השלמים.

ונצפה שתהליך הלמידה יחזיר לנו מודל, שבסיכוי לפחות אחת פחות דלתא, שגיאת החיזוי על פני נתונים שהוא לא ראה היא לכל היותר אפסילון. שזה בדיוק מה שרשום בביטוי שלפנינו.

נשים לב שזאת הגדרה. אנחנו עדיין לא יודעים אם קיים מודל שיבטיח לנו את הקריטריון הזה, לכל בעיה או לחלק מהבעיות, וגם אם כן לא יודעים איך למצוא אותו, או להוכיח שנמצא אותו דווקא ע"י ERM וגם אם כן איך לעשות מינימיזציה על האמפיריקל ריסק, כלומר למצוא את האלגוריתם עצמו שימצא את h.
:::
:::

---

### No Free Lunch Theorem (NFL)

Is there an algorithm $A$, that for every i.i.d training sample of size $n$ from every distribution $P_X$, there is a high chance ("probably") it outputs a predictor $h$ that has low risk ("approximately")?

A "universal learner"?

::: {.fragment}
::: {.callout-tip}
Theorem (No Free Lunch, binary classification):

For any $\delta \in (0,1), \varepsilon < 1/2$, for any learner $A$ and training sample $T$ of size $n$, there exist $P_X, f$ s.t. w.p. at least $\delta$: $Err_{P_X, f}(A(T)) \geq \varepsilon$
:::
:::

::: {.incremental}
- That is, no universal learner: no algorithm $A$ which is "best" for any classification task.
- We can always *fail* a learner $A$, with some distribution $P_X$.
- How can we prevent such failures?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כלומר מתבקשת השאלה: יש אלגוריתם כזה? לומד אוניברסלי, שכל מה שאנחנו צריכים זה להביא לו מדגם מקרי, מכל התפלגות PX, והוא ישיג לנו את הקריטריון של PAC.

משפט מפורסם בשם No free lunch, אומר לנו שלא. יש הרבה גרסאות למשפט הזה, גם בניסוחים עממיים יותר, כאן אנחנו מביאים ניסוח קצת יותר מדייק עבור הבעיה שלנו שהיא עדיין קלאסיפיקציה בינארית:

לכל דלתא, לכל אפסילון קטן מחצי, לכל לרנר או אלגוריתם A שפועל על מדגם מקרי בגודל n, קיימים התפלגות PX וכלל החלטה F, כך שבהסתברות לפחות דלתא שגיאת החיזוי של מודל שמושד באמצעות האלגוריתם הזה, תהיה גדולה מאפסילון. במילים אחרות לא מושג קריטריון פאק.

בניסוח מעט עממי יותר נהוג לומר שאין לומד אוניברסלי, שברור שהוא הוא האלגוריתם שהכי טוב לכל בעיה. לכל אלגוריתם אני יכול למצוא מקרים שבהם הוא ייכשל להבטיח קריטריון פאק.

לא נוכיח את זה כאן אבל בשורה התחתונה קריטריון פאק הוא עדיין מאוד מחמיר ואנחנו יכולים להכשיל כל לומד שינסה להשיג אותו באמצעות התפלגות מסוימת על המרחב של X. לכן נשאלת השאלה איך נמנעים מכישלונות כאלה, או שוב, מה אנחנו כן יכולים לדרוש בצורה ריאלית, כנראה עם עוד רילקסציות.
:::
:::

---

## Learning in Finite Hypothesis Classes {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Use Prior Knowledge

- Assume $f \subset \mathcal{H}$, where $\mathcal{H}$ is a [finite hypothesis class]{style="color:red;"}
- Example: "logistic regression" (not really)
    - intercept and slope $\beta_0, \beta_1$, each can take $1K$ possibilities $\to |\mathcal{H}| = 1M$
- The learner knows $\mathcal{H}$
- ERM restricted to this class only: $ERM_\mathcal{H}(T) \in \arg\min_{h \in \mathcal{H}}\overline{err}_T(h)$

::: {.fragment}
::: {.callout-tip}
Theorem:

For any $\varepsilon, \delta \in (0,1)$, if $n \geq \frac{\log(|\mathcal{H}| / \delta)}{\varepsilon}$, then for every $P_X, f$, w.p. at least $1 - \delta$ over the choice of training sample $T$ of size $n$, $Err_{P_X, f}(ERM_\mathcal{H}(T)) \leq \varepsilon$
:::
:::

::: {.fragment}
- Example: "logistic regression" (not really)
    - For $\delta = \varepsilon = 0.01 \to n \geq 800$ guarantees PAC correct
    - w.p. $0.99$ $Err_{P_X, f}(ERM_\mathcal{H}(T)) \leq 0.01$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Proof:

::: {.incremental}
- We want to prove:
$$P_X\left[T: Err_{P_X, f}(ERM_\mathcal{H}(T)) > \varepsilon\right] \leq \delta$$

- Let $\mathcal{H}_B$ be the set of "bad" hypotheses: $\mathcal{H}_B = \{h \in \mathcal{H}: Err_{P_X, f}(h_S) > \varepsilon\}$
- Let $M$ be the set of "misleading" samples: $\{T: \text{ there is } h \in \mathcal{H}_B \text{ for which } \overline{err}_T(h) = 0\}$ (why?)
:::

::: {.fragment}
- Note I:
$$\{T: Err_{P_X, f}(ERM_\mathcal{H}(T)) > \varepsilon\} \subseteq M$$
- Note II:
$$M = \bigcup_{h \in \mathcal{H}_B} \{T:  \overline{err}_T(h) = 0\}$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Proof:

$\begin{aligned}
P_X\left[T: Err_{P_X, f}(ERM_\mathcal{H}(T)) > \varepsilon\right] &\leq P_X\left[M\right] \\
&= P_X\left[ \bigcup_{h \in \mathcal{H}_B} \{T:  \overline{err}_T(h) = 0\} \right] \\
&\leq \sum_{h \in \mathcal{H}_B} P_X\left[ T:  \overline{err}_T(h) = 0 \right] \\
&= \sum_{h \in \mathcal{H}_B}\prod_{i = 1}^n P_X\left[x_i: h(x_i) = f(x_i)\right] \\
&= \sum_{h \in \mathcal{H}_B}\prod_{i = 1}^n \left[1 - Err_{P_X, f}(h)\right] \\
&\leq |\mathcal{H}_B|(1 - \varepsilon)^n \\
&\leq |\mathcal{H}|e^{-\varepsilon n} \to \leq \delta
\end{aligned}$

::: {.fragment}
$\Rightarrow n \geq \frac{\log(|\mathcal{H}| / \delta)}{\varepsilon}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Agnostic PAC Learning {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Agnostic PAC Learning

::: {.fragment}
|               | PAC                                                       | [Agnostic]{style="color:red;"} PAC                               |
|---------------|-----------------------------------------------------------|------------------------------------------------------------------|
| Distribution  | $P_X$ over $\mathcal{X}$                                  | $P_{XY}$ over $\mathcal{X}\times\mathcal{Y}$                     |
| $\mathcal{Y}$ | $\{0, 1\}$                                                 | Any                                                              |
| Truth         | $f \in \mathcal{H}$                                       | not in class or doesn't exist                                    |
| Training $T$  |$(x_1, \dots, x_n) \sim P_X \quad \forall i, y_i = f(x_i)$ | $((x_1, y_1), \dots, (x_n, y_n)) \sim P_{XY}$                    |
| Risk          | $Err_{P_X, f}(h) = P_X\left[f(x) \neq h(x)\right]$        | $Err_{P_{XY}}(h) = \mathbb{E}_{P_{XY}}\left[L(h, (x, y))\right]$ |
| Approximately | $Err_{P_X, f}(A(T)) \leq \varepsilon$                     | $Err_{P_{XY}}(A(T)) \leq \min_{h \in \mathcal{H}}Err_{P_{XY}}(h) + \varepsilon$ |
| Probably      | $P_X\left[T: \text{Approximately}\right] \geq 1 - \delta$ | $P_{XY}\left[T: \text{Approximately}\right] \geq 1 - \delta$     |

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### More formally

::: {.callout-caution}
Definition:

A hypothesis class $\mathcal{H}$ is agnostic *PAC learnable* with respect to a set $T$ from $\mathcal{X}\times\mathcal{Y}$ and a loss function $L: \mathcal{H}\times\mathcal{X}\times\mathcal{Y} \to \mathbb{R}_+$, if there exists a function $n_\mathcal{H}: (0, 1)^2 \to \mathbb{N}$ and a learning algorithm $A$ with the following property:

For every $\varepsilon, \delta \in (0,1), n \geq n_\mathcal{H}(\varepsilon, \delta)$, and distribution $P_{XY}$ over $\mathcal{X}\times\mathcal{Y}$,

$$P_{XY}\left[T: Err_{P_{XY}}(A(T)) \leq \min_{h \in \mathcal{H}}Err_{P_{XY}}(h) + \varepsilon\right] \geq 1 - \delta$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What is (agnostic) PAC learnable?

::: {.fragment}
Not so fast, need two definitions:
:::

::: {.fragment}
::: {.callout-caution}
Definition:

A training sample $T$ is called [*$\varepsilon$-representative*]{style="color:red;"} if:
$$\forall h \in \mathcal{H} \quad | \overline{err}_T(h) - Err_{P_{XY}}(h)| \leq \varepsilon$$
:::
:::

::: {.fragment}
::: {.callout-caution}
Definition:

$\mathcal{H}$ has [*uniform convergence*]{style="color:red;"} if there exists a function $n_\mathcal{H}^{UC}: (0, 1)^2 \to \mathbb{N}$, such that for every $\varepsilon, \delta \in (0,1), n \geq n_\mathcal{H}^{UC}(\varepsilon, \delta)$, for every $P_{XY}$ over $\mathcal{X}\times\mathcal{Y}$:
$$P_{XY}\left[T: T \text{ is } \varepsilon \text{-representative}\right] \geq 1 - \delta$$
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What is (agnostic) PAC learnable?

::: {.fragment}
Still not done, need "an algorithm $A$":
:::

::: {.fragment}
::: {.callout-important}
Claim:

If $T$ is $\frac{\varepsilon}{2}$-representative, then for any solution $h_T$ of $ERM_\mathcal{H}(T) \in \arg\min_{h \in \mathcal{H}}\overline{err}_T(h)$:
$$Err_{P_{XY}}(h_T) \leq \min_{h \in \mathcal{H}}Err_{P_{XY}}(h) + \varepsilon$$
:::
:::

::: {.fragment}
Proof:
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What is (agnostic) PAC learnable? How?

Uniform convergence with $\frac{\varepsilon}{2}$ $\rightarrow$ $T$ is $\frac{\varepsilon}{2}$-representative $\rightarrow$ $ERM_\mathcal{H}(T)$ gets PAC bound

::: {.fragment}
::: {.callout-tip}
Corollary:

- If $\mathcal{H}$ has uniform convergence with a function $n_\mathcal{H}^{UC}$ with some $\frac{\varepsilon}{2}$ then $\mathcal{H}$ is agnostic PAC learnable, with $n \geq n_\mathcal{H}(\varepsilon, \delta)$
- Furthermore, $ERM_\mathcal{H}(T)$ is a succesful PAC learner for $\mathcal{H}$
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Finite Classes are Angnostic PAC Learnable {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What has uniform convergence?

::: {.fragment}
::: {.callout-tip}
Theorem:

Assume $\mathcal{H}$ is **finite** and the range of the loss function is bounded in $[0, 1]$. Then:

- $\mathcal{H}$ has uniform convergence with function $n_\mathcal{H}^{UC} \leq \Bigl\lceil\frac{\log(2|\mathcal{H}|/\delta)}{2\varepsilon^2}\Bigr\rceil$
- Therefore, $\mathcal{H}$ is agnostic PAC learnable using the $ERM_\mathcal{H}$ algorithm with sample size $n \geq \Bigl\lceil\frac{2\log(2|\mathcal{H}|/\delta)}{\varepsilon^2}\Bigr\rceil$
:::
:::

::: {.fragment}
Comments:

- Use the Hoeffding's Inequality:
  - Let $X_1, \dots, X_n$ in $[a, b]$ i.i.d RVs with $\mathbb{E}(X_i) = \mu$, then for any $\varepsilon > 0$:
  $$P\left[|\bar{X} - \mu| > \varepsilon\right] \leq 2\exp\left(-2n\varepsilon^2/(b - a)^2\right)$$
- So the loss can be bounded in any $[a, b]$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Linear Model PAC Learnability

::: {.incremental}
- Now suppose $y_i = f(x_i) + \tau_i$, specifically $y_i = \beta_0 + \beta_1x_{i1} + \dots \beta_px_{ip} + \tau_i$
- Suppose our squared error loss can be bounded (in $[0, 1]$, but can manage $[0, a]$ for some large $a$)
- What is $\mathcal{H}$? Is it finite?
- Discretization Trick: if every coefficient $\beta_j$ is represented as a 64 bit float:
  - Let $d = p + 1$
  - $|\mathcal{H}| = 2^{64d}$
- We reach agnostic PAC learnability for any $\varepsilon, \delta \in (0,1)$!
  - With $ERM_\mathcal{H}(T) = \hat{\beta} = (X'X)^{-1}X'y$
  - $n \geq \Bigl\lceil\frac{2\log(2|\mathcal{H}|/\delta)}{\varepsilon^2}\Bigr\rceil \approx \frac{128d + 2\log(2/\delta)}{\varepsilon^2}$
  - Compare this to the Fixed-$X$ optimism bound: $n \geq \frac{2d\sigma^2}{\varepsilon}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## The Bias-Complexity Tradeoff {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The Bias-Complexity Tradeoff

::: {.incremental}
- Let $h_T$ be the model reached by $ERM_\mathcal{H}(T)$, that is: $h_T \in \arg\min_{h \in \mathcal{H}}\overline{err}_T(h)$.

- Its "true" error over unseen data: $Err_{P_{XY}}(h_T)$

- The actual minimum error achievable in $\mathcal{H}$: $\min_{h \in \mathcal{H}} Err_{P_{XY}}(h)$

- So: $Err_{P_{XY}}(h_T) = \min_{h \in \mathcal{H}} Err_{P_{XY}}(h) + \tau = \varepsilon_{\text{app}} + \varepsilon_{\text{est}}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The Bias-Complexity Tradeoff

$$Err_{P_{XY}}(h_T) = \varepsilon_{\text{app}} + \varepsilon_{\text{est}}$$

::: {.fragment}
- Approximation Error $\varepsilon_{\text{app}}$:
  - Caused by restricting class $\mathcal{H}$
  - Does not depend on $T$
  - Decreases with $|\mathcal{H}|$
  - Model bias!
- Estimation Error $\varepsilon_{\text{est}}$:
  - Caused by ERM with specific $T$
  - increases (logarithmically) with $|\mathcal{H}|$, decreases with sample size $n$
  - Model complexity (our "variance")
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Where did "irreducible" go?

::: {.incremental}
- Recall: $Err_{P_{XY}}(h_T) = \min_{h \in \mathcal{H}} Err_{P_{XY}}(h) + \tau = \varepsilon_{\text{app}} + \varepsilon_{\text{est}}$
- The $\min_{h \in \mathcal{H}} Err_{P_{XY}}(h)$ could be decomposed itself to:
$$\min_{h \in \mathcal{H}} Err_{P_{XY}}(h) = \min_{h \in \mathcal{Y}^\mathcal{X}} Err_{P_{XY}}(h) + \left[\min_{h \in \mathcal{H}} Err_{P_{XY}}(h) - \min_{h \in \mathcal{Y}^\mathcal{X}} Err_{P_{XY}}(h)\right]$$
:::

<br></br>

::: {.incremental}
- That is, the best possible ("Bayes optimal") error for any $\mathcal{H}$ + excess over that which is what we know as bias
  - Under realizability $\min_{h \in \mathcal{Y}^\mathcal{X}} Err_{P_{XY}}(h) = 0$
  - Under agnostic model it could be $> 0$, like our $\sigma^2$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
