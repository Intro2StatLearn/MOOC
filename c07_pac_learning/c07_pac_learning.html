<!DOCTYPE html>
<html lang="en"><head>
<script src="../libs/clipboard/clipboard.min.js"></script>
<script src="../libs/quarto-html/tabby.min.js"></script>
<script src="../libs/quarto-html/popper.min.js"></script>
<script src="../libs/quarto-html/tippy.umd.min.js"></script>
<link href="../libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.554">

  <title>PAC Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="../slides_quarto.css">
  <link href="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  

    <link rel="icon" href="../Intro2SL_logo.jpg" type="image/jpg"> 

    <link rel="shortcut icon" href="../Intro2SL_logo.jpg" type="image/jpg">

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css">

  </head>

<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section id="section" class="slide level2 logo-slide">
<h2></h2>
</section>
<section id="introduction-to-statistical-learning" class="slide level2 title-slide center">
<h2>Introduction to Statistical Learning</h2>
<h3 id="pac-learning---class-7">PAC Learning - Class 7</h3>
<h3 id="giora-simchoni">Giora Simchoni</h3>
<h4 id="gsimchonigmail.com-and-add-intro2sl-in-subject"><code>gsimchoni@gmail.com</code> and add <code>#intro2sl</code> in subject</h4>
<h3 id="stat.-and-or-department-tau">Stat. and OR Department, TAU</h3>
</section>
<section id="what-is-learnable" class="slide level2 title-slide center">
<h2>What is learnable?</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">

</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="previously-on-model-selection">Previously on Model Selection</h3>
<div>
<ul>
<li class="fragment"><p>For linear regression, squared error loss, Fixed <span class="math inline">\(X\)</span>: <span class="math display">\[op = \mathbb{E}_{y}\left[Err_{in} - \overline{err}|X\right] = \mathbb{E}_{y}\left[\frac{1}{n}\sum_{i=1}^n\mathbb{E}_{y_0}\left[L(y_0, \hat{f}(x_i))|T\right] - \frac{1}{n}\sum_{i=1}^{n} L(y_i, \hat{f}(x_i))\right] = \frac{2d\sigma^2}{n}\]</span></p></li>
<li class="fragment"><p>For Fixed <span class="math inline">\(X\)</span>, for any <span class="math inline">\(\varepsilon &gt; 0\)</span>, if <span class="math inline">\(n \ge \frac{2d\sigma^2}{\varepsilon}\)</span>, <span class="math inline">\(\mathbb{E}_{y}\left[\overline{err}\right]\)</span> is up to <span class="math inline">\(\varepsilon\)</span> smaller than <span class="math inline">\(\mathbb{E}_{y}\left[Err_{in}\right]\)</span>!</p></li>
<li class="fragment"><p>Can we learn <em>any</em> <span class="math inline">\(f\)</span> with <em>any</em> large-enough training sample, ensuring <span class="math inline">\(\overline{err}\)</span> is up to <span class="math inline">\(\varepsilon\)</span> smaller than <span class="math inline">\(Err = \mathbb{E}_{x_0, y_0, T}\left[L(y_0, \hat{f}(x_0))\right]\)</span> (no overfitting)?</p></li>
<li class="fragment"><p>Spoiler: no, but <span style="color:red;">learnability</span> is key concept:</p>
<ul>
<li class="fragment">under what conditions is <span class="math inline">\(f\)</span> learnable from a training sample?</li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בשיעור הקודם, הגדרנו את מושג האופטימיזם: עד כמה מדגם הלמידה אופטימי בהערכת הטעות, אם נניח שX נתון, הוא קבוע, פיקסד, ואנחנו מושכים עוד תצפיות שהמודל לא רואה, מאותם איקסים. ברגרסיה ליניארית ראינו שאפשר להגיע לביטוי סגור, שכולל את d מספר הפרמטרים במודל, ואת סיגמא בריבוע, כשאם אנחנו לא יודעים אותה אפשר לאמוד אותה.</p>
<p>אם ככה, מתבקש לרצות שהאופטימיזם יהיה קטן כרצוננו. ומאחר שאין לנו שליטה על סיגמא בריבוע, עבור מודל עם מספר פרמטרים נתון, כל שנותר לנו לעשות זה להגדיל את גודל המדגם, את n.&nbsp;אם נרצה שהאופטימיזם יהיה קטן מאיזשהו מספר אפסילון קטן כרצוננו, אפשר להגיע לחסם תחתון על n, n צריך להיות גדול או שווה לפעמיים d סיגמא בריבוע חלקי אותו אפסילון.</p>
<p>האם אנחנו יכולים להמשיך את הקו הזה? האם אנחנו יכולים למצוא מדגם גדול מספיק כדי ללמוד לא רק פונקציה ליניארית אלא כל פונקציה f, ולוודא שהטעות של מדגם הלמידה תהיה קטנה עד כדי כל אפסילון שנבחר מהטעות האמיתית שסימנו כארר, על פני כל נתון שהמודל לא יראה?</p>
<p>אז הנה ספוילר להמשך, התשובה היא שלא. בשביל להשיג את החסם הזה השתמשנו במספר הנחות כמו המודל הליניארי, ואם זורקים את כולן מחוץ לחלון, קשה להבטיח שהטעות של מדגם הלמידה תהיה קרובה ככל שנרצה לשגיאה האמיתית. אבל השאלה הזאת מביאה אותנו לקונספט חשוב נורא והוא: לרנביליטי, או: למידות.</p>
<p>בשיעור הזה נטעם קצת מתיאוריית הלמידה, וננסה לענות, או להתחיל לענות על השאלה הפשוטה מה למיד, עבור איזו פונקציה f ובאילו תנאים, אפשר לדגום ממדגם הלמידה, ולצפות שהטעות שלו תשקף את השגיאה האמיתית עבור כל נתון שהמודל לא רואה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="learning-theory-ingredients">Learning theory ingredients</h3>
<ul>
<li><span class="math inline">\(x \sim P_X\)</span> on <span class="math inline">\(\mathcal{X}\)</span></li>
<li>Label set: <span class="math inline">\(y \in \mathcal{Y}\)</span> (start with <span class="math inline">\(\mathcal{Y}  = \{0, 1\}\)</span>)</li>
<li>True relation: <span class="math inline">\(f: \mathcal{X} \to \mathcal{Y}\)</span></li>
<li>Classifier: <span class="math inline">\(h: \mathcal{X} \to \mathcal{Y}\)</span> (our <span class="math inline">\(\hat{f}\)</span>)</li>
<li>Training data: <span class="math inline">\(T = \{x_1, \dots, x_n\}\)</span>, <span class="math inline">\(i.i.d\)</span>, where <span class="math inline">\(y_i = f(x_i)\)</span></li>
<li>Risk function: <span class="math inline">\(L(f(x), h(x))\)</span> (start with 0-1 loss)</li>
<li>Error: <span class="math inline">\(Err_{P_X, f}(h) = \mathbb{E}_{X \sim P_X}\left[L(f(x), h(x))\right] = P_X\left[f(x) \neq h(x)\right]\)</span></li>
</ul>
<div class="fragment">
<p>Most startling assumption:</p>
<ul>
<li><span style="color:red;">Realizability</span>: <span class="math inline">\(y_i = f(x_i)\)</span>, i.e.&nbsp;for <span class="math inline">\(Err_{P_X, f}(f) = 0\)</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נגביל את עצמנו לעולם מצומצם עם חוקים והנחות די מחמירות, ואחר-כך ננסה להיפטר מהמגבלות.</p>
<p>בעולם שלנו, מרחב ההסתברות כולל רק את X, המשתנים המסבירים, על המרחב הזה יש התפלגות Px מתוכה אנחנו דוגמים את האיקסים.</p>
<p>הוואי שלנו בא ממרחב וואי מסוים, ואנחנו נתחיל עם קלסיפיקציה לשתי רמות, כלומר וואי הוא אפס או אחת.</p>
<p>שימו-לב, אנחנו מניחים שיש קשר אמיתי, פונקציה f שלכל X מתאימה Y. ואת המודל שלנו שעד כה סימנו כf_hat נסמן כh. נקרא לו קלאסיפייר, אפשר גם כלל חיזוי, אפשר היפותזה.</p>
<p>כלומר המדגם המקרי בגודל n שאנחנו לוקחים הוא מדגם T מתוך האיקסים, שעבור כל X יש מיפוי לY דרך פונקציה F אבסולוטית.</p>
<p>לפונקצית ההפסד שלנו נקרא ריסק, סיכון, ונתחיל עם פונקצית ההפסד הטבעית לבעיה שהיא האפס-אחת לוס.</p>
<p>ואז הטעות שלנו, היא תוחלת הריסק הזה. תוחלת על פני מה? על פני מרחב ההסתברות של X שהוא המשתנה המקרי היחיד פה. זה משתנה אינדיקטור, אם חזינו נכון נקבל 0 ואם טעינו נקבל 1. ותוחלת של משתנה מקרי אינדיקטור היא פשוט ההסתברות לקבל 1, כלומר הטעות שלנו היא ההסתברות לחיזוי שגוי, שההיפותזה שלנו h על כל X, תהיה שונה מf האמיתית. נסמן את הטעות הזאת כארר על P_X ו-f, כפונקציה של היפותזה h.</p>
<p>אז אם עקבתם, הרכיב שהכי שונה מאיך שראינו את העולם עד עכשיו בלמידה סטטיסטית, הוא ההנחה המחמירה שלכל X מתאים רק Y ספציפי, ואין סטוכסטיות בקשר הזה. הרי תחת המודל שלנו עד עכשיו Y שווה לאיזושהי פונקציה f של X ועוד איזה משתנה מקרי של רעש אפסילון, ככה הסברנו איך המודל שלנו מסביר שעבור שני סטודנטים שונים מאותו מגדר, אותו ניסיון, אותם ציונים קודמים ואותו זמן התכוננות למבחן - יכול לצאת ציון שונה. וכאן - לא! זה דטרמיניסטי, אין דגימה ממרחב Y.</p>
<p>יתרה מזה, זה אומר שיש מושג כזה של טעות אפס, אם נגיע למודל האמיתי f, וזאת אפשרות, הסיכוי לשגיאה הוא אפס, כלומר הארר שלנו, הוא אפס. להנחה הזאת קוראים ריאלייזביליטי, היתכנות, אפשר להגיע לשגיאה אפס. זאת חתיכת הנחה, ננסה להוריד אותה בהמשך.</p>
<p>עוד הנחה שממנה כפי שאמרנו בעבר לעולם לא ניפטר, מסתתרת מאחורי הביטוי iid, כלומר המדגם נמשך בצורה זהה, עם החזרה, כל תצפית בלתי תלויה בשניה. זה גם אומר שהמדגם שאנחנו רואים קשור לעולם, אנחנו לא נמדדים בנתונים שלא ראינו על עולם אחר - אחרת, היינו עוצרים כאן.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="pac-learning" class="slide level2 title-slide center">
<h2>PAC Learning</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בעולם המחמיר שהגדרנו, ננסה לענות על השאלה האם תמיד אפשר ללמוד ממדגם הלמידה, ואם לא תמיד מה כן אפשר לצפות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="empirical-risk-minimization-erm">Empirical Risk Minimization (ERM)</h3>
<ul>
<li><span class="math inline">\(\overline{err}\)</span> is termed the <span style="color:red;">empirical risk</span>, a.k.a the training error:</li>
</ul>
<p><span class="math display">\[\overline{err}_T(h) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, h(x_i)) = \frac{1}{n}\sum_{i=1}^{n} \mathbb{1}\left[f(x_i) \neq h(x_i)\right]\]</span></p>
<div>
<ul>
<li class="fragment"><p>A method which outputs <span class="math inline">\(h_T\)</span> by minimizing <span class="math inline">\(\overline{err}_T(h)\)</span> is called empirical risk minimization <span style="color:red;">ERM</span></p></li>
<li class="fragment"><p>If <span class="math inline">\(f,h \in \mathcal{H}\)</span>, from <span style="color:red;">Realizability</span>: <span class="math inline">\(\overline{err}_T(h_T) = 0\)</span></p></li>
<li class="fragment"><p>Can we <em>always</em> find <span class="math inline">\(h\)</span> for which <span class="math inline">\(Err_{P_X, f}(h) = 0\)</span> via ERM (= by minimizing <span class="math inline">\(\overline{err}_T(h)\)</span>)?</p></li>
<li class="fragment"><p>Counter-example:</p></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ניתן קודם שם למה זה אומר ללמוד ממדגם הלמידה. אם הסיכון הוא התוחלת על פני כל האיקסים האפשריים, נסמן בארר עם קו למעלה כמו שעשינו את הסיכון האמפירי שמחושב על מדגם הלמידה, שמחושב על מדגם ספציפי T ועבור מודל ספציפי h. במקרה של סיכון אפס-אחת מדובר בשיעור החיזוי השגוי על תצפיות מדגם הלמידה, מתוך n תצפיות. אנחנו קראנו לזה בעבר הטריינינג ארור.</p>
<p>פרדיגמה שלומדת איזשהו hT על-ידי הבאה למינימום של הכמות הזאת נקראת אמפיריקל ריסק מינימיזיישן, או ERM בקיצור.</p>
<p>עכשיו נשים לב לפרט עדין וחשוב: נניח שהמודל האמיתי f והמודל שERM מחפש h באים מאותו מרחב H שעוד נדבר עליו – כלומר ERM מחפש במרחב הנכון – מהנחת הריאלייזביליטי, שאומרת שעבור כל X מתאים רק Y ספציפי, האם זה מאתגר למצוא מודל שיגיע לאמפיריקל ריסק אפס? בודאי שלא. תחת הנחת הריאלייזביליטי, אנחנו תמיד יכולים לאפס (!) את האמפיריקל ריסק אם אנחנו מחפשים במרחב הנכון, כי אין רעש, אין תצפיות שלפעמים הY שלהן אחת ולפעמים אפס. אני רק אזכיר שעד כאן חשדנו מאוד ממצב כזה שקראנו לו אוברפיטינג, ונשאלת השאלה:</p>
<p>מה זה למידות? האם אפשר לצפות להביא את שגיאת החיזוי, על פני נתונים שלא ראינו, לאפס, אם אנחנו מסתכלים רק על החלון הזה של מדגם למידה, כלומר לומדים עם ERM? זאת יכולה להיות ההגדרה של למידות?</p>
<p>כל מה שצריך זה להביא דוגמא נגדית כדי להראות שזה לא יכול להתקיים תמיד. בדוגמא הפשוטה ביותר אפשר להניח שכל מרחב האיקסים כולל רק שתי אפשרויות, X1 וX2. ההסתברות PX על המרחב הזה היא לקבל את X2 באיזשהו סיכוי קטן אפסילון, ואת X1 בסיכוי 1 פחות אפסילון. נניח גם שהמיפוי האמיתי f מביא את X1 לY = 0 ואת X2 לY = 1. מה זה אומר להיכשל במצב כל כך פשוט? מספיק אף פעם לא לראות בכלל את X2 כדי להיכשל, להגיע למודל שאומר שעבור כל האיקסים וואי שווה 0, האפשרות היחידה שאנחנו מכירים. האם יש סיכוי שאף פעם לא נראה את X2 וניכשל? בודאי. הסיכוי הזה הוא 1 פחות אפסילון בחזקת n.&nbsp;שמתנהג כמו כמו אקספוננט בחזקת מינוס אפסילון n.&nbsp;והגודל הזה לא בהכרח שואף לאפס ככל שn גדל, אם אפסילון קטן מאוד, הרבה יותר קטן מ1 חלקי n הוא לא ישאף לאפס.</p>
<p>כלומר יש סיכוי סביר שנקבל פשוט מדגם רע, שמביא את האמפיריקל ריסק לאפס אבל מביא לשגיאת חיזוי של אפסילון בהכרח על נתונים שהוא לא ראה. והשורה התחתונה היא שלהביא את שגיאת החיזוי לאפס היא קריטריון מחמיר מדי ללמידות, חייבים להוסיף פה איזשהו מימד הסתברותי. אנחנו צריכים להבין שתמיד יש סיכוי למדגם גרוע, ושגם מדגם טוב במרכאות לא בהכרח מביא לשגיאה אפס על נתונים שהמודל לא ראה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="probably-approximately-correct-pac">Probably Approximately Correct (PAC)</h3>
<div>
<p>We introduce two relaxations:</p>
<ol type="1">
<li class="fragment"><span style="color:red;">Accuracy</span> <span class="math inline">\(\varepsilon\)</span>: Find <span class="math inline">\(h\)</span> via ERM for which <span class="math inline">\(Err_{P_X, f}(h) \leq \varepsilon\)</span> (“approximately correct”)</li>
<li class="fragment"><span style="color:red;">Confidence</span> <span class="math inline">\(\delta\)</span>: With probability <span class="math inline">\(\geq 1 - \delta\)</span> (“probably correct”, on <span class="math inline">\(P_X\)</span>)</li>
</ol>
</div>
<div class="fragment">
<p>PAC learning, interim summary:</p>
</div>
<div>
<ul>
<li class="fragment">Assume <span class="math inline">\(P_X, f\)</span></li>
<li class="fragment">Input: <span class="math inline">\(\varepsilon, \delta \in (0,1)\)</span></li>
<li class="fragment">Take i.i.d training sample <span class="math inline">\(T\)</span> of size <span class="math inline">\(n \geq n(\varepsilon, \delta)\)</span>, where <span class="math inline">\(n(\varepsilon, \delta): [0, 1]^2 \to \mathbb{N}\)</span></li>
<li class="fragment">Output prediction rule <span class="math inline">\(h\)</span> s.t. w.p. at least <span class="math inline">\(1 - \delta\)</span> it holds that <span class="math inline">\(Err_{P_X, f}(h) \leq \varepsilon\)</span>: <span class="math display">\[P_X\left[T: Err_{P_X, f}(h) \leq \varepsilon\right] \geq 1 - \delta\]</span></li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>This is a definition! Importantly, no guarantee of existence, no way of finding algorithm <span class="math inline">\(A\)</span> to minimize ERM.</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז אנחנו עושים שתי רילקסציות, ודורשים קצת פחות: רילקסציה ראשונה מבטאת את זה שאנחנו יודעים שאנחנו לא יכולים להביא את שגיאת החיזוי לאפס מוחלט, אנחנו רוצים להביא אותה עד לאיזשהו פרמטר אקיורסי קטן כרצוננו, אפסילון. כלומר אנחנו רוצים להיות אפרוקסימטלי קורקט.</p>
<p>הרילקסציה השניה מבטאת את זה שאנחנו יודעים שבסיכוי מסוים גם מדגם מקרי יכול להיות גרוע או לא מייצג, ואנחנו רוצים לחסום את הסיכוי הזה להיות 1 פחות איזשהו פרמטר קונפידנס דלתא. כלומר אנחנו רוצים להיות פרובבלי קורקט.</p>
<p>ללמידות כזאת, פרובבלי אפרוקסימטלי קורקט, קוראים בקיצור פאק לרנביליטי או לרנינג, למידה מסוג פאק. נסכם:</p>
<p>אנחנו מניחים מרחב הסתברות PX ואיזשהו כלל החלטה או מיפוי ממנו אל מרחב Y.</p>
<p>אנחנו מספקים שני פרמטרים של אקיורסי וקונפידנס בין 0 ל1 קטנים כרצוננו.</p>
<p>אנחנו לוקחים מדגם מקרי iid בגודל n כאשר השליטה היחידה שלנו היא על גודל המדגם, אנחנו נדאג שיהיה גדול או שווה לאיזשהו סף שתלוי באפסילון ודלתא. את הסף הזה אפשר לראות כפונקציה מהמרחב של אפסילון ודלתא, מרחב האפס עד אחת בריבוע, אל המספרים השלמים.</p>
<p>ונצפה שתהליך הלמידה יחזיר לנו מודל, שבסיכוי לפחות אחת פחות דלתא, שגיאת החיזוי על פני נתונים שהוא לא ראה היא לכל היותר אפסילון. שזה בדיוק מה שרשום בביטוי שלפנינו.</p>
<p>נשים לב שזאת הגדרה. אנחנו עדיין לא יודעים אם קיים מודל שיבטיח לנו את הקריטריון הזה, לכל בעיה או לחלק מהבעיות, וגם אם כן לא יודעים איך למצוא אותו, או להוכיח שנמצא אותו דווקא ע”י ERM וגם אם כן איך לעשות מינימיזציה על האמפיריקל ריסק, כלומר למצוא את האלגוריתם עצמו שימצא את h.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="no-free-lunch-theorem-nfl">No Free Lunch Theorem (NFL)</h3>
<p>Is there an algorithm <span class="math inline">\(A\)</span>, that for every i.i.d training sample of size <span class="math inline">\(n\)</span> from every distribution <span class="math inline">\(P_X\)</span>, there is a high chance (“probably”) it outputs a predictor <span class="math inline">\(h\)</span> that has low risk (“approximately”)?</p>
<p>A “universal learner”?</p>
<div class="fragment">
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Theorem (No Free Lunch, binary classification):</p>
<p>For any <span class="math inline">\(\delta \in (0,1), \varepsilon &lt; 1/2\)</span>, for any learner <span class="math inline">\(A\)</span> and training sample <span class="math inline">\(T\)</span> of size <span class="math inline">\(n\)</span>, there exist <span class="math inline">\(P_X, f\)</span> s.t. w.p. at least <span class="math inline">\(\delta\)</span>: <span class="math inline">\(Err_{P_X, f}(A(T)) \geq \varepsilon\)</span></p>
</div>
</div>
</div>
</div>
<div>
<ul>
<li class="fragment">That is, no universal learner: no algorithm <span class="math inline">\(A\)</span> which is “best” for any classification task.</li>
<li class="fragment">We can always <em>fail</em> a learner <span class="math inline">\(A\)</span>, with some distribution <span class="math inline">\(P_X\)</span>.</li>
<li class="fragment">How can we prevent such failures?</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כלומר מתבקשת השאלה: יש אלגוריתם כזה? לומד אוניברסלי, שכל מה שאנחנו צריכים זה להביא לו מדגם מקרי, מכל התפלגות PX, והוא ישיג לנו את הקריטריון של PAC.</p>
<p>משפט מפורסם בשם No free lunch, אומר לנו שלא. יש הרבה גרסאות למשפט הזה, גם בניסוחים עממיים יותר, כאן אנחנו מביאים ניסוח קצת יותר מדויק עבור הבעיה שלנו שהיא עדיין קלאסיפיקציה בינארית:</p>
<p>לכל דלתא בין 0 ל1, לכל אפסילון קטן מחצי, לכל לרנר או אלגוריתם A שפועל על מדגם מקרי בגודל n, קיימים התפלגות PX וכלל החלטה F, כך שבהסתברות לפחות דלתא שגיאת החיזוי של מודל שמושג באמצעות האלגוריתם הזה, תהיה גדולה מאפסילון. במילים אחרות לא מושג קריטריון פאק.</p>
<p>בניסוח מעט עממי יותר נהוג לומר שאין לומד אוניברסלי, שברור שהוא הוא האלגוריתם שהכי טוב לכל בעיה. לכל אלגוריתם אני יכול למצוא מקרים שבהם הוא ייכשל להבטיח קריטריון פאק.</p>
<p>לא נוכיח את זה כאן אבל בשורה התחתונה קריטריון פאק הוא עדיין מאוד מחמיר ואנחנו יכולים להכשיל כל לומד שינסה להשיג אותו באמצעות התפלגות מסוימת על המרחב של X. לכן נשאלת השאלה איך נמנעים מכישלונות כאלה, או שוב, מה אנחנו כן יכולים לדרוש בצורה ריאלית, כנראה עם עוד רילקסציות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="learning-in-finite-hypothesis-classes" class="slide level2 title-slide center">
<h2>Learning in Finite Hypothesis Classes</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">

</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="use-prior-knowledge">Use Prior Knowledge</h3>
<div>
<ul>
<li class="fragment">Assume <span class="math inline">\(f \in \mathcal{H}\)</span>, where <span class="math inline">\(\mathcal{H}\)</span> is a <span style="color:red;">finite hypothesis class</span></li>
<li class="fragment">Example: “logistic regression” (not really)
<ul>
<li class="fragment">intercept and slope <span class="math inline">\(\beta_0, \beta_1\)</span>, each can take <span class="math inline">\(1K\)</span> possibilities <span class="math inline">\(\to |\mathcal{H}| = 1M\)</span></li>
</ul></li>
<li class="fragment">The learner knows <span class="math inline">\(\mathcal{H}\)</span></li>
<li class="fragment">ERM restricted to this class only: <span class="math inline">\(ERM_\mathcal{H}(T) \in \arg\min_{h \in \mathcal{H}}\overline{err}_T(h)\)</span></li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Theorem:</p>
<p>For any <span class="math inline">\(\varepsilon, \delta \in (0,1)\)</span>, if <span class="math inline">\(n \geq \frac{\log(|\mathcal{H}| / \delta)}{\varepsilon}\)</span>, then for every <span class="math inline">\(P_X, f\)</span>, w.p. at least <span class="math inline">\(1 - \delta\)</span> over the choice of training sample <span class="math inline">\(T\)</span> of size <span class="math inline">\(n\)</span>, <span class="math inline">\(Err_{P_X, f}(ERM_\mathcal{H}(T)) \leq \varepsilon\)</span></p>
</div>
</div>
</div>
</div>
<div>
<ul>
<li class="fragment">Example: “logistic regression” (not really), for <span class="math inline">\(\delta = \varepsilon = 0.01\)</span>:
<ul>
<li class="fragment"><span class="math inline">\(\Rightarrow n \geq 800\)</span> guarantees PAC correct: w.p. <span class="math inline">\(0.99\)</span> <span class="math inline">\(Err_{P_X, f}(ERM_\mathcal{H}(T)) \leq 0.01\)</span></li>
</ul></li>
<li class="fragment">Important: this says nothing about how to find <span class="math inline">\(ERM_\mathcal{H}\)</span> or about its efficency</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז כאמור, אנחנו מגיעים למסקנה שלא נוכל להגיע לקריטריון של למידת פאק עבור כל מרחב של קשרים אמיתיים בין X לY. נהיה חייבים להוסיף איזה ידע מוקדם, להגביל את עצמנו בסוג המרחבים שניתנים ללמידת פאק. הגבלה אפשרית אחת, היא שהמרחבים האלה יהיו סופיים.</p>
<p>למה הכוונה? לדוגמא, אם אנחנו מדברים על קלסיפיקציה בינארית, אני יכול לחשוב על מודל ליניארי בסגנון רגרסיה לוגיסטית. כאן אני שם אותו במרכאות כי אני לא מתכוון ממש למודל של רגרסיה לוגיסטית שלמדנו, עם הנחה של התפלגות ברנולי, אלא למשהו כללי הרבה יותר שעדיין מערב מקדמים של רגרסיה, לדוגמא חותך ושיפוע. ובואו נניח שכל אחד מהם לוקח אלף אפשרויות. עוד מעט נגיד למה זה לא כל כך מופרך כשמגיעים למימוש מודל כזה. מכל מקום מרחב המודלים או ההשערות שאני מציע אם ככה, הוא סופי, יש בדיוק אלף בריבוע אפשרויות למודל, או מיליון.</p>
<p>הלומד או האלגוריתם מודע למרחב הזה, שם הוא מחפש, זה חלק מהאינפוט, והERM מביא למינימום את השגיאה על מדגם הלמידה, רק במרחב הזה, נסמן את זה כERM H.</p>
<p>והנה משפט די מדהים: תחת כל ההנחות שהיו לנו עד עכשיו, בתוספת ההנחה שמרחב ההשערות H הוא סופי, ישנו חסם תחתון על מספר התצפיות במדגם הלמידה, כך שאם נעבור אותו, מובטח לנו הקריטריון של פאק. בסיכוי לפחות 1 פחות למדא, שגיאת החיזוי, ההכללה על תצפיות שהמודל לא ראה, תהיה עד אפסילון. בפרט זה אומר שמרחבים סופיים, עם הנחת הריאלייזביליטי, ניתנים ללמידת PAC, כשמספר התצפיות הדרוש מתנהג כמו לוג של גודל המרחב.</p>
<p>אם נחזור לדוגמא של רגרסיה לוגיסטית במרכאות, עם חותך ושיפוע, ונבקש למדא ואפסילון של 0.01 ונציב את גודל המרחב שלנו בחסם התחתון של המשפט, נגיע לזה שאנחנו צריכים לפחות 800 תצפיות במדגם הלמידה כדי להשיג את קריטריון פאק, כלומר בסיכוי לפחות 99 אחוז להיות טועים, בתוחלת, על תצפיות שלא ראינו לכל היותר באחוז אחד.</p>
<p>ועוד דגש חשוב לפני ההוכחה: שימו לב שהמשפט לא אומר איך למצוא את האלגוריתם הזה, ולא מבטיח שום גבר על הסיבוכיות או היעילות שלו באופן כללי. כלומר הכל בתנאי שמצאנו אלגוריתם ERM שאפשר לעבוד איתו.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="proof">Proof:</h3>
<div>
<ul>
<li class="fragment"><p>We want to prove that if <span class="math inline">\(n \geq \frac{\log(|\mathcal{H}| / \delta)}{\varepsilon}\)</span>: <span class="math display">\[P_X\left[T: Err_{P_X, f}(ERM_\mathcal{H}(T)) &gt; \varepsilon\right] \leq \delta\]</span></p></li>
<li class="fragment"><p>Let <span class="math inline">\(\mathcal{H}_B\)</span> be the set of “bad” hypotheses: <span class="math inline">\(\mathcal{H}_B = \{h \in \mathcal{H}: Err_{P_X, f}(h) &gt; \varepsilon\}\)</span></p></li>
<li class="fragment"><p>Let <span class="math inline">\(M\)</span> be the set of “misleading” samples: <span class="math inline">\(\{T: \text{ there is } h \in \mathcal{H}_B \text{ for which } \overline{err}_T(h) = 0\}\)</span> (why?)</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Note I: <span class="math display">\[\{T: Err_{P_X, f}(ERM_\mathcal{H}(T)) &gt; \varepsilon\} \subseteq M\]</span></li>
<li>Note II: <span class="math display">\[M = \bigcup_{h \in \mathcal{H}_B} \{T:  \overline{err}_T(h) = 0\}\]</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נוכיח את המשפט המדהים הזה.</p>
<p>אנחנו רוצים להוכיח את מה שכתוב כאן בדרך קצת אחרת, ההסתברות ששגיאת חיזוי מאלגוריתם שנתן הERM על מדגם הלמידה תהיה יותר מאיזשהו אפסילון, היא לכל היותר דלתא, אם נשתמש במדגם למידה גדול מספיק.</p>
<p>נגדיא את HB כקבוצת ההיפותזות או המודלים הרעים, כלומר כל המודלים שעבורם שגיאת החיזוי היא יותר ממה שאנחנו רוצים, מאפסילון.</p>
<p>נגדיר את M להיות קבוצת כל המדגמים שהם מטעים. מה זה אומר מטעים? מדגם הלמידה יכול להראות על אחד מהמודלים הגרועים, שגיאה אפס. כלומר מדגם הלמידה עלול להוביל אותנו למודל גרוע. למה בהכרח אפס? נזכור שתחת הנחת הריאלייזיביליטי שהיא עדיין איתנו, זה בדיוק מה שERM מביא אותנו, לא חוזים לX מסוים Y מסוים ויכול להיות Y אחר בטעות, אפשר בקלות להגיע לשגיאה אפס.</p>
<p>אז שני דברים חשובים לשים לב אליהם:</p>
<p>אחד, זה שמה שכתוב לנו בביטוי הההסתברות למעלה הוא בעצם מוכל בM. קבוצת כל מדגמי הלמידה שמביאים לשגיאת חיזוי גדולה יותר ממה שרצינו מוכלת או שווה לקבוצת המדגמים המטעים. מה זה נותן לי? זה אומר שאני יכול לרשום שהסיכוי שאנחנו רוצים הוא קטן או שווה לסיכוי של M, הסיכוי לפגוש מדגם מטעה.</p>
<p>דבר שני, זה שאפשר לרשום את קבוצת המדגמים המטעים כאיחוד. איחוד כל המדגמים שמביאים לשגיאת למידה אפס, על פני כל המודלים הגרועים. מה זה נותן לי? בהסתברות אנחנו אוהבים להשתמש בחסם מפורסם שנקרא חסם האיחוד, היוניון באונד. ההסתברות לאיחוד שני מאורעות A וB קטן או שווה לסכום ההסתברויות של המאורעות האלה. וכאן ההסתברות של הקבוצה M שהיא איחוד מאורעות, יהיה קטן או שווה לסכום ההסתברויות של המאורעות האלה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="proof-1">Proof:</h3>
<p><span class="math inline">\(\begin{aligned}
P_X\left[T: Err_{P_X, f}(ERM_\mathcal{H}(T)) &gt; \varepsilon\right] &amp;\leq P_X\left[M\right] \\
&amp;= P_X\left[ \bigcup_{h \in \mathcal{H}_B} \{T:  \overline{err}_T(h) = 0\} \right] \\
&amp;\leq \sum_{h \in \mathcal{H}_B} P_X\left[ T:  \overline{err}_T(h) = 0 \right] \quad \text{(union bound)}\\
&amp;= \sum_{h \in \mathcal{H}_B}\prod_{i = 1}^n P_X\left[x_i: h(x_i) = f(x_i)\right] \\
&amp;= \sum_{h \in \mathcal{H}_B}\prod_{i = 1}^n \left[1 - Err_{P_X, f}(h)\right] \\
&amp;\leq |\mathcal{H}_B|(1 - \varepsilon)^n \\
&amp;\leq |\mathcal{H}|e^{-\varepsilon n} \to \leq \delta
\end{aligned}\)</span></p>
<div class="fragment">
<p><span class="math inline">\(\Rightarrow n \geq \frac{\log(|\mathcal{H}| / \delta)}{\varepsilon}\)</span></p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז נסביר מה קורה כאן שלב אחר שלב:</p>
<p>המעבר הראשון נובע מההערה הראשונה, המאורע ההסתברותי שלנו מוכל בM לכן ההסתברות שלנו קטנה או שווה להסתברות הקבוצה M.</p>
<p>זאת בעצם הסתברות איחוד על פני כל המודלים הרעים.</p>
<p>ומהיוניון באונד אפשר לחסום את הסתברות האיחוד בסכום על פני כל המודלים הרעים לראות מדגמים עם שגיאת למידה אפס.</p>
<p>אבל מה זו ההסתברות לראות מדגם למידה עם שגיאה אפס, תחת הנחת הריאלייזביליטי והiid זה אומר מכפלת כל הסיכויים שהמודל הגרוע הזה שווה בדיוק לקלסיפיקציה של X, לf האמיתי.</p>
<p>כל אחת מההסתברויות האלה היא על פי הגדרה 1 פחות שגיאת החיזוי, או 1 פחות התוחלת המותנית של המשתנה אינדיקטור שמקבל 1 אם טועים ואפס אם צודקים.</p>
<p>אבל השגיאה הזאת מחושבת רק על מודלים רעים, היא על פי הגדרה גדולה יותר מאפסילון, זה מה שהופך אותם למודלים רעים, כלומר כל המכפלה היא לכל היותר 1 פחות אפסילון בחזקת n.&nbsp;וכעת השגיאה הזאת לא תלויה בh, היא כמות קבועה כפול כל מספר המודלים הגרועים.</p>
<p>עכשיו אני חוסם את מספר המודלים הגרועים בגודל של כל מחלקת המודלים, הגודל של H. וגם אנחנו משתמשים שוב בעובדה ש1 פחות אפסילון בחזקת n מתנהג כאקספוננט בחזקת מינוס אפסילון כפול n.</p>
<p>והדבר הזה היינו רוצים שיהיה קטן או שווה לדלתא, זאת הדרישה המקורית.</p>
<p>יש כאן אי-שוויון די פשוט, מעברים אגפים, לוקחים לוג, ומקבלים בדיוק את החסם התחתון על n מהמשפט.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="do-we-believe-realizability">Do we believe realizability?</h3>
<div class="fragment">
<p><img data-src="error_decomposition.png"></p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ועדיין אנחנו לא לגמרי מרוצים. גם אנחנו יודעים שעולם הלמידה מורכב מעוד בעיות חוץ מקלסיפיקציה בינארית, גם יש לנו אישיו עם העובדה שמגבילים אותנו למרחבים סופיים של השערות, הרי אפילו ברגרסיה ליניארית פשוטה כל אחד מהמקדמים יכול להיות על פני כל הישר הממשי, שהוא מרחב אינסופי של אפשרויות. אבל אולי הכי אקוטי, אנחנו מתקשים להאמין להנחת הריאלייזביליטי, האם אנחנו מצופים להאמין שתמיד תמיד, שתי מהנדסות עם אותו ניסיון, אותו גיל, השכלה, תפקיד ואפילו אותו מקום עבודה יקבלו אותו שכר?</p>
<p>המודל שאנחנו רגילים לראות לדוגמא עם פירוק ביאס-וריאנס, הוא יותר משהו מהסגנון שאנחנו רואים בסכמה הזאת. הפונקציה האמיתית f קודם כל לא חייבת להיות במרחב שבו אנחנו מחפשים את המודל h. היא יכולה להיות מחוצה לו. עכשיו אם היא מחוצה לו, זה כבר אומר שיש איזה מודל אופטימלי שניתן להשיג בתוך מרחב H שמגביל אותנו, ובין השניים יש איזושהי שגיאת קירוב או ביאס. וגם אם אנחנו מחפשים דווקא בתוך מרחב H, אנחנו מובלים על-ידי המדגם שלנו אל מודל hT, כשטי מגיע מהמדגם למידה הספציפי שלנו, טריינינג. למה המודל שהמדגם הספציפי שלנו יהיה רחוק מהמודל האופטימלי שאפשר להגיע אליו במרחב H? שגיאת אמידה, או: שונות. המהדרין יוסיפו כאן אפילו עוד h עם סימון alg מלשון אלגוריתם, כלומר יוסיפו בחשבון שגם האלגוריתם עצמו שבחרנו כדי לעשות מינימום לשגיאה יוסיף רעש, למשל אם אנחנו חייבים לבחור באלגוריתם יעיל על הנתונים שלנו, שיחזיר תשובה גם אם הוא לא אופטימלי. לבסוף, אנחנו גם סקפטים שיש פונקציה f אמיתית שמתאימה לכל ערך X את ערך הY שלו, או שאנחנו רוצים לקחת בחשבון שאין לנו את היכולת לאמוד אותה כל כך ספציפי, כלומר שגם סביב f האמיתית יש איזשהו רעש שלא נצליח למדל. לרעש הזה קראנו האירדוסיבל ארור, ברגרסיה סימנו אותו כסיגמה בריבוע.</p>
<p>אז היינו רוצים לראות מושג של למידות או לרנביליטי קצת פחות נוקשה, שמאפשר לדבר בדיוק במושגים האלה, וזה מה שנראה בחלק הבא.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="agnostic-pac-learning" class="slide level2 title-slide center">
<h2>Agnostic PAC Learning</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בחלק הזה נעשה כמה רילקסציות לטפל בחלק מהבעיות שהעלינו, בניסיון להביא את כל התיאוריה הזאת למטה, לאדמה, לטיפול בבעיות דאטא אמיתיות ריאליסטיות. בראש ובראשונה ניפטר מהנחת הריאלייזביליטי, ונדרוש קריטריון פחות מחמיר מפאק, קריטריון זה נקרא פאק אגנוסטי, מייד נראה אגנוסטי למה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="agnostic-pac-learning-1">Agnostic PAC Learning</h3>
<div class="fragment">
<table>
<colgroup>
<col style="width: 10%">
<col style="width: 42%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>PAC</th>
<th><span style="color:red;">Agnostic</span> PAC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Realizability</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="even">
<td>Distribution</td>
<td><span class="math inline">\(P_X\)</span> over <span class="math inline">\(\mathcal{X}\)</span></td>
<td><span class="math inline">\(P_{XY}\)</span> over <span class="math inline">\(\mathcal{X}\times\mathcal{Y}\)</span></td>
</tr>
<tr class="odd">
<td>Training <span class="math inline">\(T\)</span></td>
<td><span class="math inline">\((x_1, \dots, x_n) \sim P_X \quad \forall i, y_i = f(x_i)\)</span></td>
<td><span class="math inline">\(((x_1, y_1), \dots, (x_n, y_n)) \sim P_{XY}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathcal{Y}\)</span></td>
<td><span class="math inline">\(\{0, 1\}\)</span></td>
<td>Any</td>
</tr>
<tr class="odd">
<td>Truth</td>
<td><span class="math inline">\(f \in \mathcal{H}\)</span></td>
<td>not in class or doesn’t exist</td>
</tr>
<tr class="even">
<td>Risk</td>
<td><span class="math inline">\(Err_{P_X, f}(h) = P_X\left[f(x) \neq h(x)\right]\)</span></td>
<td><span class="math inline">\(Err_{P_{XY}}(h) = \mathbb{E}_{P_{XY}}\left[L(h, (x, y))\right]\)</span></td>
</tr>
<tr class="odd">
<td>Probably</td>
<td><span class="math inline">\(P_X\left[T: \text{Approximately}\right] \geq 1 - \delta\)</span></td>
<td><span class="math inline">\(P_{XY}\left[T: \text{Approximately}\right] \geq 1 - \delta\)</span></td>
</tr>
<tr class="even">
<td>Approximately</td>
<td><span class="math inline">\(Err_{P_X, f}(A(T)) \leq \varepsilon\)</span></td>
<td><span class="math inline">\(Err_{P_{XY}}(A(T)) \leq \min_{h \in \mathcal{H}}Err_{P_{XY}}(h) + \varepsilon\)</span></td>
</tr>
</tbody>
</table>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נעבור שורה שורה ונראה מה מוסיף לנו המעבר למודל פאק הכללי, ללמידת פאק אגנוסטי, אחרי שנפטרנו מהנחת הריאלייזביליטי, ואנחנו מוכנים לקבל שיש רעש קטן בנתונים.</p>
<p>המרחב שאנחנו דוגמים ממנו הוא כבר לא מרחב של איקסים, אלא מרחב משותף של איקסים ווואים, מאיזושהי התפלגות משותפת XY.</p>
<p>כלומר אנחנו לא דוגמים יותר איקסים ומכילים עליהם איזשהו יחס f אבסולוטי כדי לקבל את הוואי, אנחנו דוגמים זוגות של תצפיות איקס וואי מתוך התפלגות משותפת.</p>
<p>יתרה מזה, המרחב של וואי לא חייב להיות בינארי או אפילו סופי, אנחנו יכולים לטפל במרחבים שונים כולל הישר הממשי לבעיות רגרסיה.</p>
<p>אנחנו לא מניחים כאמור שקיים איזשהו מודל אחד או פונקציה f מתוך קבוצה של מודלים אפשריים וזאת האמת, יכול להיות שהוא לא קיים ויכול להיות שהוא קיים ואנחנו מחפשים מודל לא בקלאס H הנכון, למשל המודל האמיתי הוא לא-ליניארי ואנחנו מגבילים את עצמנו למודל ליניארי. בשפה שלנו אנחנו מאפשרים הטיה.</p>
<p>קודם הריסק שלנו, או שגיאת החיזוי היתה רק הסיכוי להיות טועים, כלומר התוחלת של שגיאת אפס-אחת. עכשיו הריסק שלנו, או שגיאת החיזוי, זה תוחלת של כל הפסד L סביר, שלוקח זוג X וY, לוקח השערה או מודל נאמד ומחזיר מספר אי-שלילי בדרך כלל.</p>
<p>ההבדל החשוב ביותר נמצא בשתי השורות האחרונות של הטבלה: המשמעות של פאק משתנה קצת. הפרובבלי נשאר אותו דבר, אנחנו מודעים לזה שיכול להיות שנקבל מדגם פשוט גרוע, ואנחנו רוצים להיות צודקים אפרוקסימטלי, כלומר לקבל מדגם טוב, בסיכוי לכל הפחות אחת פחות דלתא.</p>
<p>מה זה אומר להיות צודקים אפרוקסימטלי משתנה: אנחנו רוצים לקבל שגיאת חיזוי לא אבסולוטית קטנה או שווה מאיזשהו אפסילון, אלא קטנה או שווה מהשגיאה הכי טובה שאפשר לקבל במרחב H שבו אנחנו מחפשים, ועוד אפסילון. כאן נעוצה המשמעות של המילה אגנוסטי, לא אכפת לי באיזה מרחב אתה מחפש, אני מבקש שבמרחב הזה, תגיע עד כדי אפסילון לשגיאה הכי טובה שאפשר להגיע אליה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="more-formally">More formally</h3>
<div class="callout callout-caution callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Definition:</p>
<p>A hypothesis class <span class="math inline">\(\mathcal{H}\)</span> is agnostic <em>PAC learnable</em> with respect to a set <span class="math inline">\(T\)</span> from <span class="math inline">\(\mathcal{X}\times\mathcal{Y}\)</span> and a loss function <span class="math inline">\(L: \mathcal{H}\times\mathcal{X}\times\mathcal{Y} \to \mathbb{R}_+\)</span>, if there exists a function <span class="math inline">\(n_\mathcal{H}: (0, 1)^2 \to \mathbb{N}\)</span> and a learning algorithm <span class="math inline">\(A\)</span> with the following property:</p>
<p>For every <span class="math inline">\(\varepsilon, \delta \in (0,1), n \geq n_\mathcal{H}(\varepsilon, \delta)\)</span>, and distribution <span class="math inline">\(P_{XY}\)</span> over <span class="math inline">\(\mathcal{X}\times\mathcal{Y}\)</span>,</p>
<p><span class="math display">\[P_{XY}\left[T: Err_{P_{XY}}(A(T)) \leq \min_{h \in \mathcal{H}}Err_{P_{XY}}(h) + \varepsilon\right] \geq 1 - \delta\]</span></p>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>עכשיו אפשר להציג את ההגדרה הרשמית למה הופך מרחב של מודלים ללמיד, בקריטריון של פאק אגנוסטי.</p>
<p>יש לנו מדגם למידה טי שנדגם מהמרחב המשותף של X וY. יש לנו פונקציית הפסד מהמרחב המשותף ומהמרחב של המודלים האפשריים אל איזושהי כמות אי-שלילית. ואם נצליח למצוא חסם תחתון nH לכל המרחב הזה לגודל המדגם, ואיזשהו אלגוריתם A שמשיג את עיקרון הפאק החדש, אז נגיד שמרחב או קלאס המודלים הזה, הוא למיד מסוג פאק אגנוסטי.</p>
<p>ומהו הקריטריון? להיות צודקים בערך, בסבירות גבוהה. שהסבירות לדגום מדגם T טוב, שיביא לאלגוריתם A שמשיג את השגיאה הטובה ביותר שאפשר עד כדי אפסילון, תהיה לכל הפחות אחת פחות דלתא. כשאפסילון ודלתא האם שני שברים קטנים שאנחנו נותנים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="what-is-agnostic-pac-learnable">What is (agnostic) PAC learnable?</h3>
<div class="fragment">
<p>Not so fast, need two definitions:</p>
</div>
<div class="fragment">
<div class="callout callout-caution callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Definition:</p>
<p>A training sample <span class="math inline">\(T\)</span> is called <span style="color:red;"><em><span class="math inline">\(\varepsilon\)</span>-representative</em></span> if: <span class="math display">\[\forall h \in \mathcal{H} \quad | \overline{err}_T(h) - Err_{P_{XY}}(h)| \leq \varepsilon\]</span></p>
</div>
</div>
</div>
</div>
<div class="fragment">
<div class="callout callout-caution callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Definition:</p>
<p><span class="math inline">\(\mathcal{H}\)</span> has <span style="color:red;"><em>uniform convergence</em></span> if there exists a function <span class="math inline">\(n_\mathcal{H}^{UC}: (0, 1)^2 \to \mathbb{N}\)</span>, such that for every <span class="math inline">\(\varepsilon, \delta \in (0,1), n \geq n_\mathcal{H}^{UC}(\varepsilon, \delta)\)</span>, for every <span class="math inline">\(P_{XY}\)</span> over <span class="math inline">\(\mathcal{X}\times\mathcal{Y}\)</span>: <span class="math display">\[P_{XY}\left[T: T \text{ is } \varepsilon \text{-representative}\right] \geq 1 - \delta\]</span></p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כמו קודם, זאת היתה רק הגדרה, ועכשיו נשאל, האם אנחנו יודעים לאפיין מחלקות מסוימות של מודלים כלמידות.</p>
<p>התשובה היא כן, אבל אנחנו צריכים קודם שתי הגדרות בשביל זה:</p>
<p>הגדרה ראשונה היא של מדגם אפסילון-מייצג. מדגם אפסילון-מייצג, זאת הגדרה ביחס לאיזושהי מחלקה של מודלים או השערות, וזה אומר שאני יכול להבטיח שהטעות על מדגם הלמידה תהיה רחוקה מהטעות הכללית, שגיאת החיזוי בתוחלת, עד כדי אפסילון.</p>
<p>הגדרה נוספת שנצטרך שמשתמשת בהגדרה של מדגם אפסילון מייצג, היא של יוניפורם קונברג’נס - התכנסות אחידה. נאמר שמחלקה של מודלים H יש לה את תכונת ההתכנסות האחידה, אם קיים חסם תחתון שנסמן בnHUC, כך שעבור כל אפסילון ודלתא קטנים, אם גודל המדגם שלנו גדול מהחסם, אז לכל התפלגות משותפת PXY, מובטח לנו שהמדגם הוא אפסילון-מייצג בסיכוי לפחות 1 פחות דלתא.</p>
<p>ההתפלגות האחידה כאן היא על כל התפלגות PXY, ועל כל מודל H מהמשפחה, מובטח לנו שמדגם גדול מספק הוא מייצג, כלומר ששגיאת הלמידה שלו תהיה רחוקה מהשגיאה בתוחלת עד כדי אפסילון, בהסתברות 1 פחות דלתא.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="what-is-agnostic-pac-learnable-1">What is (agnostic) PAC learnable?</h3>
<div class="fragment">
<p>Still not done, need “an algorithm <span class="math inline">\(A\)</span>”:</p>
</div>
<div class="fragment">
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Claim:</p>
<p>If <span class="math inline">\(T\)</span> is <span class="math inline">\(\frac{\varepsilon}{2}\)</span>-representative, then for any solution <span class="math inline">\(h_T\)</span> of <span class="math inline">\(ERM_\mathcal{H}(T) \in \arg\min_{h \in \mathcal{H}}\overline{err}_T(h)\)</span>: <span class="math display">\[Err_{P_{XY}}(h_T) \leq \min_{h \in \mathcal{H}}Err_{P_{XY}}(h) + \varepsilon\]</span></p>
</div>
</div>
</div>
</div>
<div class="fragment">
<p>Proof:</p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אוקי, עדיין לא גמרנו, אם תיזכרו בהגדרה של מה זה למידות פאק אגנוסטי זה כולל גם איזשהו אלגוריתם להשיג את המודל הזה שמביא לשגיאה קטנה, והטענה היא שאלגוריתם כזה הוא בדיוק אלגוריתם שעושה מינימיזציה לשגיאה של מדגם הלמידה, מה שסימנו כERMH.</p>
<p>ספציפית, הטענה אומרת שמספיק שמדגם הלמידה טי הוא חצי-אפסילון מייצג, אז מובטח שERMH משיג את הקריטריון האפרוקסימטלי של פאק, שהשגיאה האמיתית של המודל שנלמד מERM, בתוחלת, על פני נתונים שלא ראינו, היא שווה לשגיאה הטובה ביותר שאפשר להשיג על מחלקה H עד כדי אפסילון.</p>
<p>נוכיח את זה בזריזות, אף על פי שזה נשמע די הגיוני:</p>
<p>על פי ההגדרה של מדגם חצי-אפסילון מייצג, השגיאה האמיתית של המודל הספציפי שקיבלנו hT, קטנה או שווה לשגיאת מדגם הלמידה של הלומד הספציפי הזה ועוד חצי אפסילון. אבל איך הגענו למודל הספציפי הזה? עשינו מינימום לשגיאת מדגם הלמידה, כלומר שגיאת מדגם הלמידה היא קטנה או שווה לשגיאה של כל לומד H על מדגם הלמידה הספציפי הזה. ושוב בחזרה לשגיאה האמיתית, התוחלת, בגלל ההגדרה של מדגם חצי-אפסילון מייצג היא רחוקה עד כדי חצי אפסילון מהשגיאה האמיתית. חצי אפסילון ועוד חצי אפסילון זה אפסילון ומקבלים את האי-שוויון הרצוי, השגיאה האמיתית על הלומד הספציפי שנלמד מERM היא קטנה או שווה לשגיאה האמיתית לכל לומד, ועוד אפסילון.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="what-is-agnostic-pac-learnable-how">What is (agnostic) PAC learnable? How?</h3>
<p>Uniform convergence with <span class="math inline">\(\frac{\varepsilon}{2}\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(T\)</span> is <span class="math inline">\(\frac{\varepsilon}{2}\)</span>-representative <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(ERM_\mathcal{H}(T)\)</span> gets PAC bound</p>
<div class="fragment">
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Corollary:</p>
<ul>
<li>If <span class="math inline">\(\mathcal{H}\)</span> has uniform convergence with a function <span class="math inline">\(n_\mathcal{H}^{UC}\)</span> with some <span class="math inline">\(\frac{\varepsilon}{2}\)</span> then <span class="math inline">\(\mathcal{H}\)</span> is agnostic PAC learnable, with <span class="math inline">\(n \geq n_\mathcal{H}(\varepsilon, \delta)\)</span></li>
<li>Furthermore, <span class="math inline">\(ERM_\mathcal{H}(T)\)</span> is a succesful PAC learner for <span class="math inline">\(\mathcal{H}\)</span></li>
</ul>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>סוף סוף אנחנו יכולים לתת אפיון מסוים למחלקות של מודלים שלמידים פאק אגנוסטי, ולא רק זה שהם למידים אלא איך ללמוד אותם.</p>
<p>אם אפשר להראות שמודלים כאלה הם בעלי התכונה של התכנסות אחידה עם חצי אפסילון, זה אומר על-פי הגדרה שעבור כל התפלגות משותפת המדגם שניקח יהיה חצי-אפסילון מייצג בהסתברות לפחות 1 פחות למדא. וזה אומר על-פי הטענה שפרידגמת ERM על המחלקה הזאת, תשיג את חסם פאק.</p>
<p>או בצורה רשמית יותר, מצאנו שאם למחלקה של מודלים H יש את תכונת ההתכנסות האחידה עם חסם תחתון nHUC ביחס לחצי אפסילון, המחלקה היא למידה פאק אגנוסטי, עם מדגם גדול מספיק מחסם תחתון nH שהגדרנו עם אפסילון שלם.</p>
<p>יתרה מזו, דרך אפשרית ללמוד את האלגוריתם A שישיג את חסם פאק היא עם אמפיריקל ריסק מינימיזיישן, ERM.</p>
<p>נשים לב שזה תנאי מספיק להגיד שמחלקה של מודלים היא למידה-פאק-אגנוסטי, אבל הוא לא הכרחי, בהחלט יכולות להיות מחלקות שאין להן את התכונה הזאת והן למידות-פאק, עד כמה שאנחנו יודעים. אבל זה שלב חשוב, כי אם נוכל לאפיין אילו מחלקות מקיימות את התנאי הזה של התכנסות אחידה נדע מייד שהן למידות, ואפילו איך ומה צריך להיות גודל המדגם כדי ללמוד אותן. זה בדיוק מה שנעשה בחלק הבא.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="finite-classes-are-angnostic-pac-learnable" class="slide level2 title-slide center">
<h2>Finite Classes are Angnostic PAC Learnable</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">

</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="what-has-uniform-convergence">What has uniform convergence?</h3>
<div class="fragment">
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Theorem:</p>
<p>Assume <span class="math inline">\(\mathcal{H}\)</span> is <strong>finite</strong> and the range of the loss function is bounded in <span class="math inline">\([0, 1]\)</span>. Then:</p>
<ul>
<li><span class="math inline">\(\mathcal{H}\)</span> has uniform convergence with function <span class="math inline">\(n_\mathcal{H}^{UC} \leq \Bigl\lceil\frac{\log(2|\mathcal{H}|/\delta)}{2\varepsilon^2}\Bigr\rceil\)</span></li>
<li>Therefore, <span class="math inline">\(\mathcal{H}\)</span> is agnostic PAC learnable using the <span class="math inline">\(ERM_\mathcal{H}\)</span> algorithm with sample size <span class="math inline">\(n \geq \Bigl\lceil\frac{2\log(2|\mathcal{H}|/\delta)}{\varepsilon^2}\Bigr\rceil\)</span></li>
</ul>
</div>
</div>
</div>
</div>
<div class="fragment">
<p>Comments:</p>
<ul>
<li>Use the Hoeffding’s Inequality:
<ul>
<li>Let <span class="math inline">\(X_1, \dots, X_n\)</span> in <span class="math inline">\([a, b]\)</span> i.i.d RVs with <span class="math inline">\(\mathbb{E}(X_i) = \mu\)</span>, then for any <span class="math inline">\(\varepsilon &gt; 0\)</span>: <span class="math display">\[P\left[|\bar{X} - \mu| &gt; \varepsilon\right] \leq 2\exp\left(-2n\varepsilon^2/(b - a)^2\right)\]</span></li>
</ul></li>
<li>So the loss can be bounded in any range</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז הבנו שאם נוכל להגיד על מחלקה מסוימת של מודלים שיש לה התכנסות אחידה עבור חצי אפסילון, זה אומר שהיא למידה פאק אגנוסטי עבור אפסילון שלם. כלומר כל שניתן לנו לשאול זה לאיזו מחלקה של מודלים יש את התכונה הזאת של התכנסות אחידה.</p>
<p>מה שמחזיר אותנו שוב למחלקות סופיות. משפט חשוב אומר שאם מחלקה היא סופית, ואנחנו משתמשים בפונקצית הפסד שיכולה להחזיר ערכים בין 0 ל-1, אז יש לH התכנסות אחידה עם חסם תחתון על גודל מדגם הלמידה שהוא לכל היותר לוגריתמי בגודל המחלקה עם הביטוי שכאן.</p>
<p>ולכן, H היא מחלקה של מודלים למידה פאק אגנוסטי באמצעות ERM, עם חסם תחתון על גודל המדגם בביטוי שמופיע כאן. פשוט תראו באפסילון של ההתכנסות האחידה כחצי אפסילון, כשהוא עולה בריבוע מקבלים אפסילון בריבוע חלקי 4, מצטמצם עם 2, ו-2 עולה למונה.</p>
<p>אז לא נוכיח כאן מדוע למחלקה סופית יש תכונה של התכנסות אחידה עם החסם שמופיע כאן, רק נגיד שמוכיחים זאת באמצעות אי-שוויון הופדינג, שאומר שעבור מדגם מקרי בגודל n של משתנה שחסום בין מספרים A וB, ממוצע המדגם מתרחק מהתוחלת עד כדי כל אפסילון שנרצה, בסיכוי שאפשר לחסום.</p>
<p>במקרה שלנו המשתנה המקרי הוא הלוס שחסום בין אפס לאחת, ואנחנו מראים שממוצע הלוס לא יכול להתרחק מהתוחלת יותר מדי בסיכוי מסוים, כלומר המדגם אפסילון מייצג וזה אומר שלמחלקה H יש את תכונת ההתכנסות האחידה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="linear-model-pac-learnability">Linear Model PAC Learnability</h3>
<div>
<ul>
<li class="fragment">Now suppose <span class="math inline">\(y_i = f(x_i) + \tau_i\)</span>, specifically <span class="math inline">\(y_i = \beta_0 + \beta_1x_{i1} + \dots \beta_px_{ip} + \tau_i\)</span></li>
<li class="fragment">Suppose our squared error loss can be bounded (in <span class="math inline">\([0, 1]\)</span>, but can manage <span class="math inline">\([0, a]\)</span> for some large <span class="math inline">\(a\)</span>)</li>
<li class="fragment">What is <span class="math inline">\(\mathcal{H}\)</span>? Is it finite?</li>
<li class="fragment">Discretization Trick: if every coefficient <span class="math inline">\(\beta_j\)</span> is represented as a 64 bit float:
<ul>
<li class="fragment">Let <span class="math inline">\(d = p + 1\)</span></li>
<li class="fragment"><span class="math inline">\(|\mathcal{H}| = 2^{64d}\)</span></li>
</ul></li>
<li class="fragment">We reach agnostic PAC learnability for any <span class="math inline">\(\varepsilon, \delta \in (0,1)\)</span>!
<ul>
<li class="fragment">With <span class="math inline">\(ERM_\mathcal{H}(T) = \hat{\beta} = (X'X)^{-1}X'y\)</span></li>
<li class="fragment"><span class="math inline">\(n \geq \Bigl\lceil\frac{2\log(2|\mathcal{H}|/\delta)}{\varepsilon^2}\Bigr\rceil \approx \frac{128d + 2\log(2/\delta)}{\varepsilon^2}\)</span></li>
<li class="fragment">Compare this to the Fixed-<span class="math inline">\(X\)</span> optimism bound: <span class="math inline">\(n \geq \frac{2d\sigma^2}{\varepsilon}\)</span></li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז על אילו בעיות אנחנו יכולים להכיל את החסם הזה?</p>
<p>למשל הבעיה שלנו של רגרסיה ליניארית, היא לא באופן מיידי עונה להגדרות של מחלקה H של מודלים סופית ופונקצית הפסד בין אפס לאחת.</p>
<p>אבל אם נניח שאפשר איכשהו לחסום את ההפסד הריבועי של רגרסיה ליניארית בין 0 ל-1, והאמת שגם אם הוא חסום בין 0 לאיזשהו קבוע A החסם פשוט יגדל -</p>
<p>מהי H? האם היא סופית?</p>
<p>H היא מרחב כל הפרמטרים של הרגרסיה. יש P + 1 כאלה, כל אחד יכול להיות על כל הישר הממשי, כלומר R בP + 1. שהוא בודאי לא מרחב סופי.</p>
<p>אבל בכל זאת אפשר לבצע כאן טריק שנקרא הדיסקרטיזיישן טריק, דיסקרטיזציה. בעידן המחשב בכל זאת מספר הוא בדיד, ונגיד שאפשר לייצג אותו עם 64 ביטים.</p>
<p>אז אם D = P + 1 פרמטרים של בטא, גודל המחלקה שלנו הוא 2 בחזקת 64 D, שזה גודל עצום אבל היא כן סופית.</p>
<p>ותחת התנאים האלה, מובטחת לנו למידות פאק אגנוסטי עבור איזשהם אפסילון ודלתא קטנים שנספק.</p>
<p>לא רק זה, אנחנו כבר יודעים את הדרך למצוא את מודל כזה, פתרון ERM במקרה כזה הוא פתרון הריבועים הפחותים שלמדנו, בטא האט, אף על פי שנזכור שהשגנו אותו עם הפסד ריבועי כללי ולא בהכרח הפסד חסום עד 1.</p>
<p>אבל בשורה התחתונה קיבלנו חסם לאיזה גודל מדגם אנחנו צריכים כדי להבטיח למידות פאק עם אפסילון ודלתא שלנו, והוא לוגריתמי בגודל המחלקה. אם נציב את המחלקה שלנו נקבל שאנחנו צריכים מדגם גדול לפחות כמו 128 כפול מספר הפרמטרים D, ועוד קבוע שתלוי בדלתא חלקי אפסילון בריבוע.</p>
<p>מעניין להשוות את החסם הזה, שהתקבל עם הרבה פחות הנחות, לחסם שהגענו אליו מחישובי האופטימיזם, עבור סטינג של פיקסד איקס, כלומר עבור איקס קבוע. באופן אינטואיטיבי החסם שמניח הרבה פחות הוא גם גדול הרבה יותר בפועל. אנחנו גם מכפילים את D במספר הביטים שאפשר לייצג מספר שזה בימינו לא פחות מ32, וגם מחלקים באפסילון קטן הרבה יותר, מחלקים באפסילון בריבוע.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-bias-complexity-tradeoff" class="slide level2 title-slide center">
<h2>The Bias-Complexity Tradeoff</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">

</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-bias-complexity-tradeoff-1">The Bias-Complexity Tradeoff</h3>
<div>
<ul>
<li class="fragment"><p>Let <span class="math inline">\(h_T\)</span> be the model reached by <span class="math inline">\(ERM_\mathcal{H}(T)\)</span>, that is: <span class="math inline">\(h_T \in \arg\min_{h \in \mathcal{H}}\overline{err}_T(h)\)</span>.</p></li>
<li class="fragment"><p>Its “true” error over unseen data: <span class="math inline">\(Err_{P_{XY}}(h_T)\)</span></p></li>
<li class="fragment"><p>The actual minimum error achievable in <span class="math inline">\(\mathcal{H}\)</span>: <span class="math inline">\(\min_{h \in \mathcal{H}} Err_{P_{XY}}(h)\)</span></p></li>
<li class="fragment"><p>So: <span class="math inline">\(Err_{P_{XY}}(h_T) = \min_{h \in \mathcal{H}} Err_{P_{XY}}(h) + \tau = \varepsilon_{\text{app}} + \varepsilon_{\text{est}}\)</span></p></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בתיאוריית הלמידה הכללית ממנה אנחנו טועמים בשיעור הזה, מופיע גם כן טריידאוף על שגיאת החיזוי כתוצאה מפירוק. כאן נהוג לקרוא לו הביאס-קומפלקסיטי טריידאוף, כלומר אין התייחסות ישירה לאלמנט הוריאנס אלא רק דרך הקומפלקסיטי, המסובכות הכללית של המודל.</p>
<p>נגדיר שוב את hT כהמודל שקיבלנו בסוף כל התהליך של ERM, כלומר המודל שעל פני מחלקה H הביא למינימום את השגיאה על מדגם הלמידה.</p>
<p>הטעות שמעניינת אותנו היא השגיאה שאפשר להכליל על כל הנתונים, תוחלת השגיאה של hT על נתונים גם שהוא לא ראה.</p>
<p>נרשום גם את המינימום שגיאת חיזוי שניתן להגיע אליו עם כל מודל ממחלקה H.</p>
<p>ברור שהשגיאה של המודל שלנו, שווה למינימום הזה ועוד איזשהו משתנה שנסמן כטאו.</p>
<p>עכשיו נסמן את המינימום כאפסילון-אפפ, ואת טאו כאפסילון-אסט.</p>
<p>ממה נובע אפסילון-אפפ? מהמחלקה שבחרתי H. אם קיים מודל אמיתי אבל הוא בכלל לא בH שבחרתי, לדוגמא יש קשר לא-ליניארי בין X לY ואני מגביל את עצמי למודל ליניארי. אז זו שגיאת קירוב שיכולה להיות די גדולה אבל היא לא תלויה בכלל במדגם הלמידה, ובגודל שלו n.&nbsp;איך קראנו לשגיאה כזאת? הטייה, ביאס.</p>
<p>וממה נובע אפסילון-אסט? למה שהמודל שאני השגתי עם מדגם הלמידה ישיג שגיאה שרחוקה מהשגיאה הכי טובה במחלקה? זה כבר תלוי במדגם הלמידה ובגודל שלו n, ובמחלקה עצמה ועד כמה היא מסובכת. זו שגיאת אמידה, אנחנו קראנו לזה שונות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-bias-complexity-tradeoff-2">The Bias-Complexity Tradeoff</h3>
<p><span class="math display">\[Err_{P_{XY}}(h_T) = \varepsilon_{\text{app}} + \varepsilon_{\text{est}}\]</span></p>
<div class="fragment">
<ul>
<li>Approximation Error <span class="math inline">\(\varepsilon_{\text{app}}\)</span>:
<ul>
<li>Caused by restricting class <span class="math inline">\(\mathcal{H}\)</span></li>
<li>Does not depend on <span class="math inline">\(T\)</span></li>
<li>Decreases with <span class="math inline">\(|\mathcal{H}|\)</span></li>
<li>Model bias!</li>
</ul></li>
<li>Estimation Error <span class="math inline">\(\varepsilon_{\text{est}}\)</span>:
<ul>
<li>Caused by ERM with specific <span class="math inline">\(T\)</span></li>
<li>increases (logarithmically) with <span class="math inline">\(|\mathcal{H}|\)</span>, decreases with sample size <span class="math inline">\(n\)</span></li>
<li>Model complexity (our “variance”)</li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נסכם: את שגיאת החיזוי, תוחלת ההפסד על פני כל התצפיות האפשריות, אפשר לחלק לשניים:</p>
<p>שגיאת קירוב אפסילון-אפפ, היא ההטיה. היא מושפעת מההגבלה שלנו לאיזושהי מחלקה של מודלים. היא לא תלויה במדגם הלמידה והיא יורדת ככל שהמחלקה גדלה, ככל שאנחנו מאפשרים מודל מורכב יותר ויותר. זאת ההטיה של המודל.</p>
<p>מצד שני יש לנו שגיאת אמידה אפסילון-אסט, שמתרחשת בגלל השימוש במדגם למידה וERM לאמוד את השגיאה האמיתית. ספציפית היא תלויה מאוד בגודל של מדגם הלמידה. ככל שנדגום יותר תצפיות היא תפחת. ומצד שני ככל שהמחלקה תגדל כך טעות האמידה תגדל ונצטרך מדגם גדול יותר לאמוד את המודל הנכון.</p>
<p>קראנו לזה שונות, כאן הפוקוס הוא על הסיבוכיות או גודל מחלקת המודלים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="where-did-irreducible-go">Where did “irreducible” go?</h3>
<div>
<ul>
<li class="fragment">Recall: <span class="math inline">\(Err_{P_{XY}}(h_T) = \min_{h \in \mathcal{H}} Err_{P_{XY}}(h) + \tau = \varepsilon_{\text{app}} + \varepsilon_{\text{est}}\)</span></li>
<li class="fragment">The <span class="math inline">\(\min_{h \in \mathcal{H}} Err_{P_{XY}}(h)\)</span> could be decomposed itself to: <span class="math display">\[\min_{h \in \mathcal{H}} Err_{P_{XY}}(h) = \min_{h \in \mathcal{Y}^\mathcal{X}} Err_{P_{XY}}(h) + \left[\min_{h \in \mathcal{H}} Err_{P_{XY}}(h) - \min_{h \in \mathcal{Y}^\mathcal{X}} Err_{P_{XY}}(h)\right]\]</span></li>
</ul>
</div>
<p><br><br></p>
<div>
<ul>
<li class="fragment">That is, the best possible (“Bayes optimal”) error for any <span class="math inline">\(\mathcal{H}\)</span> + excess over that which is what we know as bias
<ul>
<li class="fragment">Under realizability <span class="math inline">\(\min_{h \in \mathcal{Y}^\mathcal{X}} Err_{P_{XY}}(h) = 0\)</span></li>
<li class="fragment">Under agnostic model it could be <span class="math inline">\(&gt; 0\)</span>, like our <span class="math inline">\(\sigma^2\)</span></li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אבל בפירוק הביאס-וריאנס שראינו היה עוד אלמנט של טעות, לא? טעות האירדוסיבל. כאן נדמה שהוא לא קיים, אבל אם נדייק נמצא גם אותו.</p>
<p>האמת היא שכשאנחנו רושמים את המינימום שגיאה על פני כל מחלקה H, אנחנו יכולים גם אותו לפרק לטעות שלא ניתן להפחית, ועוד טעות שהיא ההטייה שלנו.</p>
<p>נפחית ונחסר מהמינימום טעות את המינימום טעות על פני המרחב H הכי כללי שאפשר להעלות על הדעת, שממנו התחלנו, בלי שום הגבלות.</p>
<p>כעת נוסיף סוגריים ונראה שהטעות הזאת מתפרקת לטעות שלא ניתן להפחית, זה מינימום על המרחב הכי גדול שאפשר לחשוב, וואי באיקס. זה ממש כמו האירדוסיבל. וטעות שהיא קרובה יותר להטיה כפי שניסחנו אותה. הפער הבילט-אין, בלי קשר למדגם, בין הטעות במודל הכי טוב שניתן להגיע אליו, למודל שמוגבל להיות במחלקה שבחרנו.</p>
<p>לטעות הזאת המינימלית ביותר על פני כל המחלקות האפשריות, נהוג גם לכנות בייס אופטימל או בייס ארור, זאת טעות תיאורטית שהזכרנו בקצרה כשדיברנו על קלסיפיקציה. תחת ריאלייזביליטי כאמור הטעות הזאת היא אפס, אין סטוכסטיות. ותחת המודל הכללי יותר האגנוסטי, היא יכולה להיות גדולה מאפס, באופן שמזכיר מאוד את הסיגמה בריבוע שלנו, שם זה פרמטר שונות, או רעש.</p>
<p>עד כאן הטעימה שלנו מתיאורית הלמידה. ניכר שהשימוש כאן בכלים מתמטיים כמו טענות, הוכחות ומסקנות הוא רב יותר, מה שמראה כמה זה מורכב לדרוש תיאורטית שנצליח בבעית למידה, אפילו אם אנחנו מגבילים את עצמנו למודל פשוט כמו המודל הליניארי. אבל העקרונות שתיאורית הלמידה מגיעה אליהם הם לא שונים מהעקרונות שהגענו אליהם עד כה, ספציפית הטריידאוף של ביאס-קומפלקסיטי מתריע בפנינו על האיזון שאנחנו צריכים לשמור בין לחפש את המודל שלנו במשפחה מצומצמת ולקבל הטייה רחוקה מהאמת, לבין לחפש אותו במשפחה מורכבת מדי, שמצריכה מדגם גדול ועלולה להביא לשונות גבוהה ולפער בין השגיאה של מדגם הלמידה לשגיאה הצפויה בעולם האמיתי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="quarto-auto-generated-content">
<p><img src="../Intro2SL_logo_white.jpg" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://intro2statlearn.github.io/mooc/" target="_blank">Intro to Statistical Learning</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../libs/revealjs/plugin/search/search.js"></script>
  <script src="../libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>