<!DOCTYPE html>
<html lang="en"><head>
<script src="../libs/clipboard/clipboard.min.js"></script>
<script src="../libs/quarto-html/tabby.min.js"></script>
<script src="../libs/quarto-html/popper.min.js"></script>
<script src="../libs/quarto-html/tippy.umd.min.js"></script>
<link href="../libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.554">

  <title>Intro. to Statistical Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="../slides_quarto.css">
  <link href="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  

    <link rel="icon" href="../Intro2SL_logo.jpg" type="image/jpg"> 

    <link rel="shortcut icon" href="../Intro2SL_logo.jpg" type="image/jpg">

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css">

  </head>

<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section id="section" class="slide level2 logo-slide">
<h2></h2>
</section>
<section id="introduction-to-statistical-learning" class="slide level2 title-slide center">
<h2>Introduction to Statistical Learning</h2>
<h3 id="intro-to-statistical-learning---class-1">Intro to Statistical Learning - Class 1</h3>
<h3 id="giora-simchoni">Giora Simchoni</h3>
<h4 id="gsimchonigmail.com-and-add-intro2sl-in-subject"><code>gsimchoni@gmail.com</code> and add <code>#intro2sl</code> in subject</h4>
<h3 id="stat.-and-or-department-tau">Stat. and OR Department, TAU</h3>
</section>
<section id="what-is-statistical-learning" class="slide level2 title-slide center">
<h2>What is Statistical Learning?</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>שלום. וברוכים הבאים לשיעור הראשון בקורס מבוא ללמידה סטטיסטית. בשיעור הזה נחזור על מושגים חשובים בלמידה, כמו אוברפיטינג, הביאס-וריאנס טריידאוף וטעות החיזוי. נראה איך המושגים האלה באים לידי ביטוי ברגרסיה ובקלסיפיקציה. אבל קודם כל ננסה להגדיר: מהי בכלל למידה סטטיסטית?</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="what-is-statistical-learning-1">What is Statistical Learning?</h3>
<div>
<ul>
<li class="fragment">Statistical learning is the task of understanding data, and making predictions based on data</li>
<li class="fragment">It is a sub-domain of machine learning</li>
<li class="fragment">It uses statistics to build models that approximate the data</li>
<li class="fragment">There is a diverse set of tools for this task, where different problems calls for different tools</li>
<li class="fragment">We will focus on the statistical aspect of the task, and rarely mention computational aspects</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מטרות העל של למידה סטטיסטית הן הבנה של הנתונים שאנחנו מביטים בהם, וחיזוי על סמך נתונים אלה.</p>
<p>נהוג לומר שלמידה סטטיסטית היא תת-תחום בעולם למידת המכונה, שהרי ההסתכלות הזאת של למידה מתוך נתונים היא לא האפשרות היחידה. מודלים של בינה מלאכותית קיימים גם מחוץ ללמידה סטטיסטית, כמו למשל למידת חיזוק, שהמודל הנלמד הוא תוצאה של ניסיון מתמשך מול העולם. ויש גם ניסיון ללמוד מודלים על-ידי הגדרת כללים ותרשימי זרימה, לאו דוקא על סמך נתונים.</p>
<p>אבל בלמידה סטטיסטית הדגש הוא על נתונים, וכשמה כן היא, הכלי העיקרי שבו אנחנו משתמשים לבניית מודלים הוא סטטיסטיקה והסתברות.</p>
<p>ועדיין, תת העולם שאנחנו מסתכלים עליו הוא עצום, וקיימים כלים רבים ומגוונים ללמידת מודלים על סמך הנתונים. בקורס זה נלמד את הכלים האלה בראייה סטטיסטית ולאו דוקא חישובית, ונדבר מעט יחסית על הסיבוכיות שלהם, ונשים דגש על איך בוחרים נכון את הכלי לטיפול בנתונים. הכלי הנכון לטיפול בנתונים נובע מהנתונים עצמם.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="key-concepts">Key Concepts</h3>
<div>
<ul>
<li class="fragment">The data is a set of features (covariates/ independent variables/ predictors):
<ul>
<li class="fragment">Continuous</li>
<li class="fragment">Ordered categorical (discrete)</li>
<li class="fragment">Unordered categorical</li>
</ul></li>
<li class="fragment">Using them we construct a model (learner) of the data</li>
<li class="fragment">Usually we use the model to predict a goal:
<ul>
<li class="fragment">When the goal is continuous the model is a <span style="color:red;">regression model</span></li>
<li class="fragment">When the goal is categorical the model is a <span style="color:red;">classification model</span> (classifier)</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Question: Can one use regression for classification and vice versa? We will get back to it later.</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בואו נחזור על כמה מושגי יסוד. נתייחס לדאטא שלנו כקבוצה של פיצ’רים, במקומות אחרים נראה גם שמות כמו משתנים מנבאים או בלתי תלויים. המשתנים האלה יכולים להיות רציפים גמו לדוגמא גובה, משתנים בדידים אורדינליים כמו למשל מספר הכוכבים של מלון. והם יכולים להיות גם בדידים קטגוריאליים ללא משמעות לסדר, כמו צבע עיניים.</p>
<p>בבעיות אמיתיות החלוקה הזאת לא תמיד ברורה, וגם הבחירה בסוג המשתנה שעומד מולי ואיך לייצג אותו, יכולה להיות תלויה במטרה הסופית של הפרויקט עליו אני עובד, לדוגמא להקטין טעות חיזוי. הרי אם מזינים גובה של אנשים בסנטימטרים שלמים, הוא לא באמת רציף, הוא בדיד ואורדינלי, אבל קשה לחשוב על יישום שבו יהיה יתרון להתייחס לגובה כמשתנה בדיד. מצד שני אם הזכרנו את מספר הכוכבים של בית מלון, יכול להיות שאפשר היה להתייחס אל משתנה כזה כרציף? פשוט להחליף את מספר הכוכבים ל1 עד 5? אולי, אבל כאן תסתתר הנחה שיש מרווחים קבועים בין הכוכבים, הנחה שהיא לא בטוח נכונה. כל מי שבדק פעם מלונות באינטרנט יודע שמלונות עד 3 כוכבים הם פחות טובים ושיש קפיצה משמעותית באיכות המלון כשמדברים על 4 ו-5 כוכבים. למעשה אולי נכון ביישומים מסוימים להתייחס לאיכות המלון כמשתנה בינארי - 3 כוכבים ומטה או 4 ומעלה.</p>
<p>ואפילו בדוגמא של צבע העיניים, צבע עצמו הוא לא בהכרח משתנה בדיד! הרי הצבע שאנחנו רואים הוא תוצר של הספקטרום האלקטרומגנטי, ותדרי גל אלקטרומגנטי שונים יוצרים צבעים שונים. סגול מתאים לגלים קצרים, ואדום לגלים ארוכים. זה משהו שכדאי לחשוב עליו בעבודה מעשית עם נתונים.</p>
<p>את כל הפיצ’רים האלה נבחן כדי לבנות מודל או לרנר בדרך כלל למטרת חיזוי. כשאם החיזוי הוא של משתנה רציף אנחנו קוראים לסטינג הזה רגרסיה, ואם החיזוי הוא של משתנה בדיד אנחנו קוראים לסטינג הזה קלסיפיקציה, ולמודל קלסיפייר.</p>
<p>שאלה חשובה: האם אפשר להשתמש במודלים לרגרסיה עבור בעיות קלסיפיקציה ולהיפך? התשובה המפתיעה היא שכן, וזה תלוי שוב במטרת הפרויקט, ומה עובד לנתונים שלכם. נחזור לדילמה הזאת מאוחר יותר בקורס.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="supervised-and-unsupervised-learning" class="slide level2 title-slide center">
<h2>Supervised and Unsupervised Learning</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נהוג להפריד את הלמידה הסטטיסטית לשני סוגים עיקריים: למידה מפוקחת שבה יש משתנה מטרה ברור שאת האופי שלו אנחנו רוצים להבין או לחזות כתלות בנתונים, ולמידה בלתי מפוקחת, שבה אין משתנה מטרה, הנתונים עצמם הם לב העניין.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="supervised-learning">Supervised Learning</h3>
<div>
<ul>
<li class="fragment">Definition:
<ul>
<li class="fragment">Input: Let <span class="math inline">\(x\)</span> be a vector of length <span class="math inline">\(n\)</span>, and let <span class="math inline">\(y = f(x) + \varepsilon\)</span>, where <span class="math inline">\(E(\varepsilon) = 0\)</span>, <span class="math inline">\(Var(\varepsilon) = \sigma^2\)</span>, <span class="math inline">\((\varepsilon, x)\)</span> are independent</li>
<li class="fragment">Goal: learn <span class="math inline">\(f\)</span> given a set of inputs <span class="math inline">\((X, y)\)</span></li>
</ul></li>
<li class="fragment">In most cases we can only find a function <span class="math inline">\(\hat{f} \approx f\)</span>, an estimator</li>
<li class="fragment">The approximation is measured relative to some loss function <span class="math inline">\(L(y, \hat{y})\)</span>
<ul>
<li class="fragment">Regression example: MSE (This will be our common choice), MAE</li>
<li class="fragment">Classification example: misclassification rate, recall and precision</li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מהי למידה מפוקחת?</p>
<p>אם איקס הוא וקטור באורך n של נתונים, אנחנו מניחים שוואי, וקטור אחר שאנחנו רואים הוא תוצאה של איזושהי פונקציה f ועוד משתנה רעש אפסילון. במודל הכי פשוט לרעש הזה יש תוחלת אפס ואיזושהי שונות סיגמא בריבוע, והוא לא תלוי בתצפיות, כלומר הוא קבוע. ברור שאפשר לחשוב על מודל מסובך יותר, כמו למשל שהרעש כן יהיה תלוי בנתונים, אבל נתחיל בזה.</p>
<p>מה לומדים בלמידה מפוקחת? את f, כיצד X משפיע על Y, כאשר נהוג לחשוב על המשתנה התלוי Y כ”המפקח”, כי ההשתנות שלו תקבע את הלמידה שלנו.</p>
<p>ברוב המקרים אין לנו דרך ללמוד את f האמיתית, אנחנו יודעים שאנחנו מקבלים רק אומד, f האט, וטיב האמידה שלנו ייקבע באמצעות פונקצית הפסד או לוס, שנסמן בL, מטריקה בין Y ל-Y הנחזה שנסמן כY האט, שנרצה להביא למינימום.</p>
<p>בסטינג של רגרסיה הלוס הנפוץ ביותר הוא השגיאה הריבועית או הממוצע שלה על פני התצפיות, הMSE, אבל לא רק. אפשר לחשוב למשל על ממוצע השגיאה האבסולוטית, בערך מוחלט, הMAE. לדוגמא ביישום שנוגע לחיזוי מחיר, כשY הוא כסף, למה שארצה להעניש בעונש ריבועי תצפיות שרחוקות מהאמת. כסף הוא כסף, ואולי אעדיף שגיאה בערך מוחלט.</p>
<p>בקלסיפיקציה הלוס הנפוץ הוא המיסקלסיפיקיישן רייט, שגיאת החיזוי שעוד נדבר עליה בשיעור זה. אבל יש הרבה בעיות עם הלוס הזה ביישומים אמיתיים, למשל בחיזוי מחלות נדירות. עבור מחלה שמופיעה בסיכוי נדיר של 1 אחוז, מהו קלסיפייר פשוט שיביא לשגיאת חיזוי נמוכה של 1 אחוז? לחזות שכולם בריאים. אז ביישומים כגון זה נעדיף אולי להסתכל על פרסיז’ן או ריקול. פרסיז’ן זה המדד שאומד את ההסתברות לחיזוי נכון של פציינט בהינתן שהמודל חוזה שהוא חולה. ריקול זה המדד שאומד את ההסתברות לחיזוי נכון של פציינט בהינתן שהוא אכן חולה. ובדוגמא שלנו, מודל שחוזה שכולם בריאים במחלה נדירה, יביא לריקול אפס, ולפרסיז’ן לא מוגדר.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="supervised-learning-examples">Supervised Learning: examples</h3>
<ul>
<li>Goal: Predict the total sales of a product in a given day
<ul>
<li>Covariates: price, geographical region, day of week, holiday, …</li>
</ul></li>
<li>Goal: Predict if a client of a cellular company will churn
<ul>
<li>Covariates: usage, age, socio-economical status, phone type, …</li>
</ul></li>
<li>Goal: Personalized medicine (e.g.&nbsp;what is the effective medicine dosage for a <em>specific</em> patient)
<ul>
<li>Covariates: genes profile, effectiveness on cell tissues, …</li>
<li>Drug cost is a part of the loss function <span class="math inline">\(L\)</span>!</li>
</ul></li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>דוגמאות ללמידה מפוקחת יש הרבה:</p>
<p>Y יהיה סך המכירות של מוצר ביום נתון, המשתנים בX יהיו מחיר המוצר, המיקום הגאוגרפי של המכירות, האם יש יום חופש.</p>
<p>Y יהיה האם לקוח של חברת סלולר יעזוב את החברה, מונח שנקרא צ’רן. המשתנים בX יהיו כמות השימוש בסלולרי שלו, גיל, סוג הפלאפון ועוד.</p>
<p>יישום מודרני יותר יכול להיות ברפואה ממוקדת. Y יהיה המינון המתאים לתרופה של חולה ספציפי, והמשתנים יהיו הפרופיל הגנטי שלו, האפקטיביות של טיפול כפי שנמדדה על רקמות תאים, ואם נהיה ציניים נוכל לכלול אפילו את מחיר התרופה עבור החולה ועבור קופת החולים. אבל אם המחיר הכלכלי כל כך מהותי לאפליקציה הזאת, אפשר גם לנסח את פונקצית ההפסד באופן שיכלול את המינימיזציה שלו, או כאילוץ.</p>
<p>כך שאנחנו רואים שהמושג של למידה מפוקחת כולל בתוכו שלל אתגרים מכל תחומי המחקר.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="supervised-learning-wage-prediction">Supervised Learning: wage prediction</h3>
<div id="4e18ca42" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c01_intro_to_intro_files/figure-revealjs/cell-2-output-1.png" width="1188" height="282"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>גם פונקצית ה-f שאנחנו אומדים, יכולים להיות לה פרופילים רבים. כאן אנחנו רואים נתונים אמיתיים של שכר שנתי של כ3000 גברים בארה”ב, כל פעם כפונקציה של משתנה אחר.</p>
<p>שכר שנתי כפונקציה של גיל מתאר דפוס עולה עד לשיא בגילאי 40-50 ואז יורד קצת.</p>
<p>שכר כפונקציה של שנה, מתאר דפוס של עלייה ליניארית עם שיפוע ממש קטן, כמעט בלתי נראה. כאן למרות שקשה להבחין בזה השכר החציוני עולה בכאלף דולר כל שנה!</p>
<p>בגרף השלישי מתוארת ההשתנות של Y כפונקציה של רמת ההשכלה. רמת ההשכלה המתוארת כאן היא מאנשים שלא סיימו תיכון, ועד אנשים עם תואר שני ומעלה. זאת דוגמא מצוינת למשתנה שאפשר היה לחשוב עליו כמשתנה רציף, כמו שנה, אבל לא ברור אם היחס בין כל שתי רמות נשמר ולכן טבעי יותר להתיחס אליו כמשתנה בדיד אורדינלי, ולצייר בוקספלוטים במקום תרשים פיזור. כך או כך, ברור שהשכר החציוני עולה ככל שההשכלה עולה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="unsupervised-learning">Unsupervised Learning</h3>
<p>What if there is no <span class="math inline">\(y\)</span>?</p>
<div>
<ul>
<li class="fragment">Given set of features <span class="math inline">\(x\)</span> find a model that describes properties of the data:
<ul>
<li class="fragment">Clusters</li>
<li class="fragment">Dependencies</li>
<li class="fragment">Correlations</li>
<li class="fragment">Common factors (PCA)</li>
</ul></li>
<li class="fragment">Problems in which we model the <em>covariates data</em> are called <span style="color:red;">unsupervised</span></li>
<li class="fragment">Most of the course: we will focus on supervised learning</li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p><span style="color:red;">Semi-supervised</span>: there exists a goal but it is partially labeled</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מה עם אין לנו מפקח? אין משתנה תלוי. יש לנו רק דאטא X, ואנחנו רוצים ללמוד תכונות שלו.</p>
<p>היינו רוצים לדעת על קלאסטרים בתוך הדאטא, לדוגמא למטרות שיווקיות. אנחנו אתר מכירות עם הרבה לקוחות והיינו רוצים להקצות איש מכירות שיתמחה בפלח אחר של הלקוחות שלנו. אבל לא היינו רוצים לנסות להגדיר לבד את הקבוצות השונות של לקוחות, אלא היינו רוצים שהדאטא יראה לנו קלאסטרים טבעיים בנתוני הלקוחות. איזה דאטא? כל דבר החל ממין וגיל, ועד דפוסי גלישה ורכישה.</p>
<p>תחום אחר שנחשב ללמידה בלתי מפוקחת ינסה למצוא מתאמים בין הפיצ’רים, ותלויות.</p>
<p>אנחנו יודעים למצוא קורלציה בין שני משתנים אבל הרי לא נעבור על כל הזוגות האפשריים של פיצ’רים ונחשב קורלציה, נרצה גם לדעת על קורלציות בין שילובי משתנים, כמו קבוצה של 3 שאלות במבחן כלשהו עם 4 שאלות אחרות. יחסים כאלה נקראים פקטורים והכלי הכי מוכר למצוא אותם הוא PCA.</p>
<p>כל הבעיות האלה שבהן אנחנו ממדלים את השתנות הפיצ’רים של הדאטא ואין לנו Y מפקח הן בעיות אנסופרוויזד. נעסוק בהן בקורס אבל פחות, רוב הקורס יתמקד בלמידה סופרוויזד.</p>
<p>מילה אחרונה לגבי סוג למידה נוסף או ביניים, שפופולרי במיוחד בעידן הביג-דאטא. בהרבה יישומים יש לי תרחיש של הרבה מאוד נתונים בלתי-מפוקחים, כלומר אין לי לגביהם את המשתנה התלוי הY או הלייבל, וסט נתונים קטן יותר שלגביהם יש לי את Y, הלייבל. זה קורה הרבה פעמים מסיבות של תקציב, אפשר לחשוב על אתר כמו אינסטגרם עם מיליוני תמונות של משתמשים. היינו רוצים לתת כותרת למה מתארת התמונה, ומסיבות של תקציב אנחנו יכולים לעשות את זה בצורה ידנית רק לכ10 אלף תמונות. אבל יש עוד מיליוני תמונות! למידה סמי-סופרוויזד או “חצי-מפוקחת”, מאפשרת לממדל להשתמש בדאטא האנלייבלד על מנת לשפר את המודל שנבנה עם הדאטא הלייבלד. כנראה שנוכל למדל טוב יותר את ההסתברות של Y בהינתן הנתונים X אם נדע טוב יותר את ההתפלגות של הנתונים עצמם X. כשמדברים על רשתות נוירונים ההבנה הזאת מפיקה ארכיטקטורה טבעית של רשת לבעיה, אבל רשתות נוירונים הם לא בסקופ של הקורס שלנו ונעצור את זה כאן.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="parametric-and-non-parametric-models" class="slide level2 title-slide center">
<h2>Parametric and Non-Parametric Models</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נבחין כעת בין מודלים פרמטרים למודלים א-פרמטרים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="why-estimate-f">Why estimate <span class="math inline">\(f\)</span>?</h3>
<div class="fragment">
<ul>
<li>For <span style="color:red;">prediction</span>: Given a new instance vector <span class="math inline">\(x\)</span>, predict: <span class="math inline">\(\hat{y} = \hat{f}(x)\)</span>
<ul>
<li>This seems like a reasonable choice since the noise is zero-mean</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>For <span style="color:red;">inference</span>: Suppose that we learned an estimator <span class="math inline">\(\hat{f}\)</span>, we can use it to learn properties of the input, such as:
<ul>
<li>Which variables of <span class="math inline">\(x\)</span> affects <span class="math inline">\(y\)</span>?</li>
<li>Given a subset of variables <span class="math inline">\(X^{'} \subset X\)</span>, is a variable <span class="math inline">\(X^{''} \not\subset X^{'}\)</span> informative for <span class="math inline">\(f(y | X')\)</span>?</li>
<li>How well can we approximate <span class="math inline">\(y\)</span>?</li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>סוג המודל שנבחר קשור קשר הדוק למהי המטרה שלנו. אז… מהי המטרה שלנו? למה לאמוד את f?</p>
<p>בגדול יש שתי מטרות. המטרה שנראית לעיתים כחזות הכל היא פרדיקציה, בהינתן תצפית חדשה איקס, נרצה לאמוד את הוואי המתאים לה, הוא המודל הנאמד f האט, כי תוחלת הרעש היא אפס.</p>
<p>אבל פרדיקציה היא לא חזות הכל! בעולם המחקר פעמים רבות המטרה היא לא בהכרח לחזות בצורה הטובה ביותר עבור תצפית חדשה, אלא מה שקרוי בסטטיסטיקה הסקה - להבין כיצד הנתונים בX משפיעים על Y. בדוגמא שראינו למשל, אולי התובנה שההשפעה של גיל על שכר היא לא רק שלא ליניארית אלא שהיא אפילו לא מונוטונית - אולי התובנה הזאת היא מטרת המחקר.</p>
<p>שאלה אחרת יכולה להיות ההשפעה של משתנים מסוימים על Y בהינתן משתנים אחרים. כלומר אם אני יודע מצב סוציו-אקונומי על סטודנט, האם המוצא שלו משפיע על הציון? רק התשובה לשאלה הזאת יכולה להיות שווה פרסום.</p>
<p>ואולי השאלה היא בכלל לא על אופן ההשפעה על על טיב היכולת בכלל לאמוד את הקשר בין X לY. לדוגמא מחקר שיעסוק בניסיון עצמו לחזות ציון של סטודנט בקורס ויכמת את טיב האמידה של חיזוי כזה כפונקציה של עוד ועוד משתנים.</p>
<p>מכל מקום המטרה השניה של הסקה סטטיסטית היא לא נחלתם של חוקרים בלבד, והיא הרבה פעמים קלה יותר במודלים פשוטים יותר שמעניינים למשל רופאים. כשרופאה רוצה לקבוע פרוגנוזה של חומרת מחלה או להעריך כמה זמן אשפוז יצטרך פציינט, היא בודאי היתה מעדיפה מודל שהיא מסוגלת להבין ולתמוך בו, מודל שעושה שכל. ולא בהכרח המודל שחוזה בצורה המדויקת ביותר את חומרת המחלה או זמן האשפוז.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-estimation-problem">The Estimation Problem</h3>
<p>How do we learn <span class="math inline">\(\hat{f}\)</span>?</p>
<div>
<ul>
<li class="fragment">Let <span class="math inline">\(T = \{(x_1, y_1 ) \dots (x_n, y_n)\}\)</span> be a training sample of size <span class="math inline">\(n\)</span>
<ul>
<li class="fragment">Note that <span class="math inline">\(x_i = \begin{pmatrix}x_{i1} \\ \vdots \\ x_{ip}\end{pmatrix}\)</span> is a vector of <span class="math inline">\(p\)</span> features (by notation a column vector)</li>
</ul></li>
<li class="fragment">We assume that there exists a joint distribution <span class="math inline">\(X \times Y\)</span>, and that <span class="math inline">\((x_i, y_i)\)</span> is sampled from it
<ul>
<li class="fragment">Usually we assume that the samples are iid</li>
</ul></li>
<li class="fragment">In general most learning methods can be divided into <span style="color:red;">parametric</span> methods and <span style="color:red;">non-parametric</span> methods</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בכל מקרה בדרך כלל נקבל מדגם למידה שנסמן כT, עם n זוגות של X ו-Y. נשים לב שX יכול להיות בעצמו וקטור, בדרך כלל נסמן כוקטור עמודה באורך P פיצ’רים.</p>
<p>אנחנו מניחים שקיימת התפלגות משותפת של X וY על המרחב המשותף הזה שמתוכה אנחנו רואים דגימות, ובנוסף שהדגימות האלה בלתי תלויות.</p>
<p>ואת המודל שנבחר לתאר את f כפונקציה של מדגם הלמידה ניתן בדרך כלל לחלק לאחד משני סוגים: מודל פרמטרי ומודל א-פרמטרי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="parametric-models">Parametric Models</h3>
<ul>
<li>To learn a parametric model we first assume a known parametric form for <span class="math inline">\(f(x)\)</span>, and then learn the parameters of <span class="math inline">\(f\)</span></li>
<li>Example:</li>
</ul>
<div>
<ul>
<li class="fragment">Assume <span class="math inline">\(f(x) = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p\)</span>, namely <span class="math inline">\(f\)</span> is a linear function of the inputs with unknown fixed coefficients</li>
<li class="fragment">Estimate the values of <span class="math inline">\(\beta = \beta_0, \dots, \beta_p\)</span></li>
<li class="fragment">How? For example using least squares (OLS): <span class="math inline">\(\min_\beta \sum_i {(y_i - \beta^{T}x_i)^2}\)</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מודל פרמטרי הוא בעל הנחות חזקות מאוד. אנחנו מניחים מודל עם פרמטרים מסוימים עבור f וכל שנותר הוא לאמוד את הפרמטרים האלה.</p>
<p>הדוגמא הקלאסית בסטינג של רגרסיה היא רגרסיה ליניארית. אנחנו מניחים שf הוא פונקציה ליניארית של המשתנים, כל משתנה מוכפל בפרמטר בטא וf הוא סכום המכפלות הללו.</p>
<p>כל שנשאר הוא לשערך את הפרמטרים בטא, זה מה שכל אלגוריתם תחת הכותרת רגרסיה ליניארית ינסה לעשות. הקריטריון הקלאסי הוא עקרון הריבועים הפחותים ונרחיב על איך להביא אותו למינימום בהמשך, אבל ההנחה ברורה והמטרה ברורה, לאמוד את הבטא. נשים לב שהמודל הליניארי כפי שהוא מנוסח כרגע ועקרון הריבועים הפחותים, לא תלוי בשום היבט סטטיסטי, אין כאן שום אלמנט הסתברותי. כשנדבר על רגרסיה ליניארית נראה מה מוסיף ההיבט הסטטיסטי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="non-parametric-models">Non-Parametric Models</h3>
<ul>
<li>Don’t assume the form of <span class="math inline">\(f(x)\)</span>, the model just wants <span class="math inline">\(f(x)\)</span> to be close to the data
<ul>
<li>Under the assumption that <span class="math inline">\(f(x)\)</span> belongs to a wide family of smooth functions</li>
</ul></li>
<li>Example:</li>
</ul>
<div>
<ul>
<li class="fragment">Splines – smooth piecewise polynomials.</li>
<li class="fragment">Why do we need the smoothness for?</li>
<li class="fragment">If we remove the smoothness assumption then what is the <em>best</em> <span class="math inline">\(f(x)\)</span>? Is it a good choice?</li>
<li class="fragment">We will get back to them later in the course</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מודל א-פרמטרי, לא מניח שום הנחה מבנית לגבי האופי של f.&nbsp;ובדרך כלל, פשוט נרצה שf יהיה קרוב לנתונים, יתאר אותם היטב. גם כאן הרבה פעמים תסתתר הנחה שאת f אפשר לתאר כצירוף של פונקציות כן ספציפיות חלקות ממשפחה רחבה, אבל לא בצורה פשטנית כל כך כמו במודל הליניארי.</p>
<p>דוגמא אחת של מודל א-פרמטרי אפשר לראות בשיטת השכנים הקרובים, KNN שנדבר עליה היום בהקשר של קלסיפיקציה בכלל.</p>
<p>דוגמא פחות מוכרת, שמכלילה במובן מסוים את המודל הליניארי, היא ספליינים.</p>
<p>(להסביר על הלוח)</p>
<p>בספליינים אנחנו נרצה לתאר את f כפונקציה שמחברת בין נקודות Y על-ידי פונקציות חלקות כמו פולינומים, שנפגשות זו עם זו, כמה שקרוב יותר לנתונים. נרצה פונקציות כמה שיותר חלקות, וכדי להשיג את החלקות הזאת נוסיף “עונש” למדא על הנגזרת השניה של הפונקציות האלה, כי הנגזרת השניה היא מדד טוב לכמה הפונקציה חלקה. ככל שהנגזרת השניה קטנה יותר, קצב ההשתנות של הפונקציה קטן יותר והיא חלקה יותר. אז עונש למדא גדול יביא אותנו לפונקציות מאוד חלקות אולי אפילו חלקות מדי ומוגבלות שאין להן הרבה חופש לזוז, ולהיפך עונש למדא קטן.</p>
<p>למה אנחנו רוצים חלקות? כי אנחנו מניחים שf היא פונקציה שמשתנה לאט, שהנוף הזה של f בהינתן הפיצ’רים בX הוא לא חד ומקוטע אלא נוף יפה של “גבעות ועמקים”. ומה יקרה אם נוותר על האילוץ של החלקות או אפילו נקטין את הפרמטר של למדא מאוד? שום דבר לא יגביל את המודל שלנו למצוא את הפולינומים שפשוט עוברים דרך הנקודות! אם נניח שכן נדרוש איזו רציפות זה ממש יכול לראות קו מקוטע שיעבור בין כל זוג נקודות. וזה כנראה לא רעיון טוב, ותיכף נראה את זה פורמלית ובפועל.</p>
<p>מכל מקום ספליינים הם דוגמא מעולה למודל א-פרמטרי שנקבע אך ורק מהדאטא, גם אם בפועל הוא מורכב מקבוצה של פולינומים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="from-linear-regression-to-splines">From Linear Regression to Splines</h3>
<div id="f6dac570" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c01_intro_to_intro_files/figure-revealjs/cell-3-output-1.png" width="999" height="290"></p>
</figure>
</div>
</div>
</div>
<div class="fragment">
<p>The rough spline seems to be <span style="color:red;">overfitting</span> the data.</p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כאן יש לנו בעיה של מידול הכנסה, כפונקציה של השכלה וותק, כל נקודה כאן היא תצפית עם השכלה וותק והגובה הוא ההכנסה באלפי דולרים. בקיצון האחד יש לנו מודל פרמטרי למהדרין, המודל הלינארי. בדו-מימד מה שהוא מתאים כאן זה בעצם מישור, והוא עושה את זה על-ידי אמידת 3 פרמטרים בלבד: המקדם להשכלה, המקדם לותק וחותך.</p>
<p>באמצע יש לנו ספליין שמסוגל לתאר איזשהו משטח או יריעה מורכבים יותר. קשה לתאר מה רואים כאן אבל זה נראה סביר.</p>
<p>בקיצון השני יש לנו ספליין עם פרמטר למדא כמעט אפסי, שמאפשר ליריעה להיות מקוטעת ומקומטת כרצונה עד שהיא פשוט עוברת דרך כל הנקודות.</p>
<p>זה כבר פחות סביר, זה נראה מאולץ מדי ונראה שאם יגיעו אנשים חדשים שנרצה לחזות את ההכנסה שלהם המודל הזה מושפע מדי מהנתונים שראה ולא יעשה עבודה טובה. מה שמביא אותנו למושג החשוב: אוברפיטינג.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="overfitting" class="slide level2 title-slide center">
<h2>Overfitting</h2>
</section>
<section class="slide level2">

<h3 id="train-and-test">Train and Test</h3>
<ul>
<li>The goal of a model is its predictive power, namely how well will it fit on a new/unseen/out-of-sample set of observations.</li>
<li>The new set of observations is called <span style="color:red;">test set</span> (as opposed to the <span style="color:red;">train set</span>).</li>
</ul>
<div>
<ul>
<li class="fragment">Usually:
<ul>
<li class="fragment">Dividing the data to a train and test sets</li>
<li class="fragment">Learn models on train set, select models based on their performance on the test set</li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כדי לדבר על אוברפיטינג צריך להזכיר שוב שבפועל אנחנו עובדים עם מדגם אימון ומדגם מבחן שהמודל לא ראה, או טריינינג סט וטסט סט.</p>
<p>המטרה שלנו היא להיות טובים על הטסט סט, על תצפיות שלא ראינו. לכן אנחנו מקפידים לחלק את הנתונים לטריין ולטסט, ללמוד את המודלים שלנו על הטריין ולבדוק את הביצועים שלהם על הטסט.</p>
<p>ברור שיש חלוקות אחרות כמו חלוקה לשלושה חלקים או קרוס ולידיישן, אבל בכל מקרה תמיד יש השוואה בין ביצועי המודל על דאטא ששימש ללמידה, וביצועיו על דאטא שהוא לא ראה, שהם בדרך כלל אינדיקציה הרבה יותר טובה לטיב המודל, ולא משנה אגב אם המטרה הסופית היא חיזוי או הסקה. גם מודל שמשמש להסקה, שבו אנחנו רוצים להבין כיצד X משפיע על Y, הוא מודל שנרצה שיראה ביצועים טובים על X וY חדשים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="prediction-error-first-decomposition">Prediction Error: first decomposition</h3>
<ul>
<li>Assume an observation <span class="math inline">\((x, y)\)</span>, <span class="math inline">\(\hat{y} = \hat{f}(x)\)</span> is <em>given</em>, <span class="math inline">\(L(y, \hat{y})\)</span> is the MSE</li>
<li>What is the expected loss?</li>
</ul>
<div class="fragment">
<p><span class="math inline">\(E_y[(y - \hat{f}(x))^2] = E[(f(x) + \varepsilon - \hat{f}(x))^2] = [f(x) - \hat{f}(x)]^2 + Var(\varepsilon)\)</span></p>
</div>
<div class="fragment">
<ul>
<li>First term: <span style="color:red;">reducible error</span></li>
<li>Second term: <span style="color:red;">irreducible error</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Which part should we minimize?</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נניח שקיבלנו חיזוי, כלומר החיזוי הוא לא משתנה מקרי הוא קבוע, אנחנו מתנים עליו. איך אפשר לפרק לוס ריבועי?</p>
<p>(להדגים)</p>
<p>התוחלת כאן היא על המשתנה המקרי היחידי, שהוא Y, אבל בעצם כשאני מחליף את Y במודל אני מקבל שאפסילון הוא המשתנה המקרי. אם אני פותח את הריבוע, אני מקבל תוחלת של f פחות f האט בריבוע, תוחלת של אפסילון בריבוע ועוד פעמיים התוחלת של אפסילון כפול f פחות f האט. אבל כאמור f האט כרגע הוא לא משתנה מקרי הוא נתון, לכן יש כאן תוחלת רק על אפסילון מוכפלת פי איזשהו קבוע, והתוחלת של אפסילון היא אפס לכן כל הביטוי מתאפס. נשארנו עם תוחלת של f פחות f האט בריבוע ועוד התוחלת של אפסילון בריבוע, אבל זה בדיוק שווה לשונות של אפסילון.</p>
<p>אנחנו רואים שניתן לפרק את שגיאת החיזוי הריבועית לשני ביטויים. לביטוי הראשון הראשון נהוג לקרוא הרידוסיבל ארור, ולשני שהוא השונות של אפסילון האירדוסיבל ארור.</p>
<p>איזו טעות נרצה שתהיה קטנה? שתיהן, אבל את האירדוסיבל ארור אנחנו לא יכולים באמת להפחית, היא נובעת מהרעש הטבעי שבנתונים, והמיקוד שלנו יהיה להפחית את הטעות הרידוסיבל, לדאוג שהחיזוי שלנו יהיה כמה שקרוב לפונקציה f.&nbsp;אבל מה עלול לקרות בפועל?</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="a-taste-of-overfitting">A taste of overfitting</h3>
<ul>
<li>But given an observation <span class="math inline">\((x, y, \hat{y}(x))\)</span>, we never know which part of its error is irreducible!</li>
</ul>
<div>
<ul>
<li class="fragment">Informally, being too close to the data is called overfitting</li>
<li class="fragment">So the best prediction for a given <span class="math inline">\(x\)</span> is not necessarily <span class="math inline">\(y\)</span>!</li>
<li class="fragment">Overfitting may get worse for non-parametric models (highly flexible)</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בפועל, אם נסתכל על שלישייה כזאת של X, Y וY האט החיזוי, אנחנו לא יודעים בפער בין Y לY האט מה החלק הרידוסיבל ומה האירדוסיבל!</p>
<p>ואוברפיטינג, קורה במצב הקיצוני כשמודל פשוט לומד לשנן את מדגם הלמידה. עבור כל X שראה הוא יודע לחזות רק את הY שראה. וזה לא חיזוי טוב בהכרח, בפועל חיזוי די רע. אוברפיטינג קורה כשהמודל מנסה להקטין את האירדוסיבל ארור, ולא מאפשר מרווח טעות טבעי לנתונים.</p>
<p>זה קורה הרבה פעמים דווקא במודלים א-פרמטריים שמאפשרים לפונקציה f להיות יותר ויותר גמישה, כמו שראינו ונראה עוד דוגמא.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="overfitting-with-splines">Overfitting with Splines</h3>

<img data-src="images/spline_train_test_demo.png" class="r-stretch"><aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כאן בשחור אנחנו רואים פונקציה f אמיתית של Y כנגד X, שממנה נדגמו הנתונים שמופיעים כנקודות שחורות.</p>
<p>מודל רגרסיה ליניארית לא גמיש מספיק, הוא מתאים קו ישר לנקודות ואפשר לראות שגם הטריין וגם הטסט ארורז שלו גבוהות. ציר האיקס בגרף מצד ימין מתייחס לדרגות החופש של המודל, שהוא פרמטר שלא דיברנו עליו אבל אינטואיטיבית הוא מבטא את גמישות המודל. רגרסיה הוא מודל מאוד לא גמיש, הוא מאפשר רק לשני פרמטרים להיות “חופשיים”, השיפוע והחותך, ולכן דרגות החופש שלו הן 2.</p>
<p>לעומת המודל הליניארי, ספליין עם 5 דרגות חופש בקו הכחול נראה לא רע, הטסט ארור שלו הנמוכה ביותר כאן. וספליין עם 15 דרגות חופש בקו האדום הוא גמיש מדי. אפשר לראות שהוא כל כך גמיש שהוא מנסה ממש לעבור דרך הנקודות, פורמלית אמרנו שהוא מנסה להפחית כבר את האירדוסיבל ארור, הוא ממש מתחיל לשנן את הדאטא. זה אולי מאפשר לו להשיג טריין ארור נמוך כל כך, אבל הטסט ארור שלו על נתונים שהוא לא ראה גבוהה מאוד. סימן ברור לאובר-פיטינג.</p>
<p>ושוב נשים לב לקו השחור המקווקוו כאן שמסמל את השונות של סיגמא בריבוע, שכאן היא ידועה והיא 1 כי כך נבנתה הסימולציה. אפשר לראות שכל הקו הצהוב, שגיאת החיזוי של הטסט נמצא מעליה, כלומר היא בייסליין כמו שהיינו מצפים לשגיאת החיזוי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="why-do-we-prefer-simple-models">Why do we prefer simple models?</h3>
<ul>
<li>The simpler the more interpretable (try to explain Splines)</li>
<li>Simpler for inference</li>
<li>Reduces overfitting</li>
<li>But <span style="color:red;">may increase underfitting</span></li>
</ul>
<div class="fragment">
<ul>
<li>So: It all depends on your goal!</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נסכם מדוע אנחנו נזהרים כל הזמן, מדוע אנחנו שואפים למודלים פשוטים יותר, עם מעט “דרגות חופש”.</p>
<p>מודלים פשוטים יותר הם יותר אינטרפרטביליים, בדוגמת המודל הליניארי הפירוש של כל מקדם רגרסיה ברור, למשל אם הוא חיובי אז למשתנה המתאים יש השפעה “חיובית” על Y.</p>
<p>הם פשוטים יותר להסקה סטטיסטית, כי הם באים לרוב עם הנחות סטטיסטיות בדיוק לצורך מטרה זו.</p>
<p>וחשוב ביותר, הם בדרך כלל מקטינים אוברפיטינג, הם לא גמישים עד כדי כך שהם מתחילים כבר לשנן את הדאטא ולהפחית מהאירדוסיבל ארור, למצוא דפוס איפה שהוא לא קיים.</p>
<p>מנגד הם עלולים להגביר אנדרפיטינג, הם עלולים להיות נוקשים מדי. ולכן יש צורך למצוא את האיזון הנכון, והכל תלוי בסוף במטרת הפרויקט. הרבה פעמים אגב פשרה נחמדה היא שימוש בשני מודלים, מודל שתפקידו להסביר החוצה כיצד Y משתנה בהתאם לפיצ’רים בידיעה שהחיזוי שלו לא אופטימלי, ומודל שהוא קרוב יותר לקופסה שחורה שקשה להסביר, שתפקידו אך ורק חיזוי מיטבי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="why-do-we-prefer-simple-models-1">Why do we prefer simple models?</h3>
<div id="b81b7f85" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c01_intro_to_intro_files/figure-revealjs/cell-4-output-1.png" width="833" height="511"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בגדול יש איזשהו מתאם שלילי בין הגמישות שמאפשר המודל לבין היכולת לפרש אותו. בקצה באחד יש לנו סוגים שונים של רגרסיה כמו המודל הליניארי או רגרסיית לאסו שמבצעת פיצ’ר סלקשן בילט אין, כלומר המודל הסופי ייטה להיות מודל ליניארי עם אפילו פחות משתנים. ובקצה השני יש את למידה עמוקה, שאמורה להיות מסוגלת באמצעות רשת נוירונים עמוקה מספיק, להיות מסוגלת למדל f גמישה ככל שנרצה, עם סכנה חמורה של אוברפיטינג, ומודל סופי שהוא קופסה שחורה עד כדי כך שיש תחום מחקר שלם שנועד לנסות להסביר מודל מורכב כל כך.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-bias-variance-tradeoff" class="slide level2 title-slide center">
<h2>The Bias-Variance Tradeoff</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כשפירקנו את הטעות הריבועית הנחנו לרגע שY האט הוא נתון, קיבלנו אותו מאיפשהו. בפועל ברור שזה לא פרקטי, Y האט הוא גם כן משתנה מקרי, הוא גם תוצר של תהליך שיש שיגידו שכולל אפילו את מי מידל את הנתונים, ואיך הוא בחר את המודל וכולי. לכל הפחות נרצה להוסיף את האקראיות של מדגם הלמידה, הרי ראינו מדגם אחד ויכולנו לראות מדגם אחר. נראה כעת פירוק מפורט יותר של הטעות הריבועית, שמוביל לטריידאוף חשוב, הביאס-וריאנס טריידאוף.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="squared-error-decomposition">Squared error decomposition</h3>
<ul>
<li><p>For regression, take the standard model: <span class="math inline">\(y = f(x) + \epsilon\;,\;\epsilon \sim (0,\sigma^2)\)</span></p></li>
<li><p>Modeling approach (e.g.&nbsp;OLS), given training data <span class="math inline">\(T\)</span>, gives model <span class="math inline">\(\hat{f}(x)\)</span></p></li>
</ul>
<div class="fragment">
<ul>
<li>Assume we want to predict at new point <span class="math inline">\(x_0\)</span>, and understand our expected (squared) prediction error:</li>
</ul>
<p><span class="math inline">\(\mathbb{E}_{y_0, T}(y_0 - \hat{f}(x_0))^2 = \mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2\)</span></p>
<ul>
<li>Note we treat both the training data <span class="math inline">\(T\)</span> (and hence <span class="math inline">\(\hat{f}\)</span>) and the response <span class="math inline">\(y_0\)</span> as random variables in our expectations</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נתמקד ברגרסיה, שם אנחנו מניחים שY הוא איזושהי פונקציה f של X ועוד רעש עם תוחלת אפס ושונות סיגמא בריבוע, לאו דוקא מהתפלגות נורמלית. מהו f אם ככה, אם התוחלת של אפסילון היא אפס? f הוא התוחלת המותנית של Y בהינתן הנתונים בX. הערך הנמדד של Y הוא לא בדיוק הערך הצפוי שלו, התוחלת, אלא התוחלת ועוד איזשהו רעש.</p>
<p>כעת אנחנו לוקחים טריינינג דאטא T וממנו לומדים את f.&nbsp;אנחנו יכולים עכשיו לבדוק את הביצועים של המודל על מדגם הטסט, אבל אני רוצה שנחשוב על כל התהליך הזה של דגימת נתונים, בניית מודל f, למידה שלו מהנתונים - כעל תהליך אקראי. ולהסתכל על התוחלת של הלוס שלנו, לדוגמא ברגרסיה השגיאה הריבועית, על פני ביצוע התהליך הזה הרבה פעמים, אם היה לי טריינינג דאטא קצת שונה כל פעם. רוצה לומר גם f_hat המודל שבנינו הוא משתנה מקרי שמבוסס על מקריות מדגם הלמידה שלנו!</p>
<p>אז תגיע תצפית חדשה X0 וכמו שאמרנו נסתכל על תוחלת הלוס שלה, כאן השגיאה בין Y0 האמיתי שלה, למודל שבנינו f_hat של X0. בהקשר לקו הירוק של פרדיקשן ארור שראינו בשקף הקודם אנחנו בעצם מסתכלים על נקודה בו ועושים לה תוחלת, או מסתכלים על ההתנהגות האופיינית שלה, על פני הרבה מדגמי למידה.</p>
<p>כעת אני רושם את ההפרש כסכום של כמה אלמנטים. כל מה שאני עושה זה לחסר ולהוסיף את f של X0, שהוא התלות האמיתית של Y0 בX0, התוחלת המותנית. וגם מחסר ומוסיף את התוחלת של f_hat. שוב, למה זה דבר שקיים לנו? כי f_hat הוא בעצמו מעין משתנה מקרי שכאילו נדגם מתוך הרבה מודלים סופיים שנובעים ממדגמי למידה שונים.</p>
<p>כעת אנחנו פשוט שמים סוגריים ומסמנים אלמנט A, אלמנט B ואלמנט C. נרצה לפתח את הריבוע ולראות אכן ששגיאת החיזוי שלנו בתוחלת מתפרקת לביטויים מעניינים ולתת להם את השם המתמטי שלהם.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<p>Which factors are random variables, dependent on <span class="math inline">\(T\)</span>?</p>
<p><span class="math inline">\(\mathbb{E}(y_0 - \hat{f}(x_0))^2 = \mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2\)</span></p>
<p><span class="math display">\[A = y_0 - f(x_0)\]</span></p>
<p><span class="math display">\[B = f(x_0) - \mathbb{E} (\hat{f}(x_0))\]</span></p>
<p><span class="math display">\[C = \mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\]</span></p>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אילו מהגורמים תלויים באקראיות מדגם הלמידה?</p>
<p>הגורם A - לא תלוי כלל במדגם הלמידה. הוא מבטא איזושהי אמת, רעש טבעי שקיים ולא נוכל להקטין, זהו בעצם האפסילון אפס כלומר כן משתנה מקרי אבל לא תלוי במדגם הלמידה.</p>
<p>הגורם B - נראה בהתחלה שתלוי במדגם הלמידה כי יש בו את המודל הנאמד f_hat, אבל יש כאן תוחלת, כלומר זה קבוע. כך שגורם זה הוא מספר, לא משתנה מקרי, ומה היינו מצפים שיהיה המספר הזה? אפס. אנחנו מקווים שהמודל שלנו עשיר מספיק שבסופו של דבר בתוחלת הוא קירוב טוב ליחס האמיתי f.</p>
<p>הגורם C - כאן בעצם יש את הגורם שתלוי במדגם הלמידה - המרחק של f_hat המודל שלמדנו מהתוחלת שלו. אם אני מעלה את זה בריבוע ולוקח תוחלת מה זה? שונות! שונות המודל f_hat.</p>
<p>כך שעוד לפני שהוכחנו, אתם כבר יכולים להבין שגורם A הוא איזושהי טעות שאין לי הרבה מה לעשות לגביה. גורם B הוא בעצם האפרוקסימיישן ארור, כמה המודל שלנו בתוחלת עשיר מספיק כדי לבטא את הקשר האמיתי בין X לY. וגורם C הוא האסטימיישן ארור, האם יש לי מספיק דאטא כדי שאם אחזור על התהליך הזה עם דאטא קצת אחר אקבל מודל דומה, כלומר כמה קטנה השונות של המודל עצמו.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-bias-variance-decomposition">The bias-variance decomposition</h3>
<p><span class="math inline">\(\mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2 =\)</span><br><br> <span class="math inline">\(\;\;\;\;\;\;\;\;\;\;\;= \mathbb{E} A^2 + B^2 + \mathbb{E} C^2 + 2 B \cdot \mathbb{E} A + 2 \mathbb{E} (AC) + 2B \cdot\mathbb{E} C\)</span><br><br></p>
<div class="fragment">
<p><span class="math inline">\(\mathbb{E}(A^2) = \sigma^2\)</span> the <strong>Irreducible error</strong> of a perfect model which knows the true <span class="math inline">\(f\)</span></p>
<p><span class="math inline">\(B^2 = \left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)^2\)</span> is the <strong>squared bias</strong> — a measure of approximation error (note <span class="math inline">\(B\)</span> is not a random variable)</p>
<p><span class="math inline">\(\mathbb{E}(C^2) = \mathbb{E} \left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0) \right)^2\)</span> is the <strong>variance</strong> of the prediction — a measure of estimation error</p>
<p><span class="math inline">\(B \cdot\mathbb{E} A = \mathbb{E} (AC) = B \cdot \mathbb{E} C = 0\)</span> due to independence and mean-0 relations</p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כעת כשאני מעלה את סכום שלושת הגורמים בריבוע אני מקבל את כל אחד מהם בריבוע ועוד 2 כפול מכפלה של כל זוג. כשאני לוקח תוחלת, אני טוען שאנחנו נשארים עם הביטוי שלפנינו. מדוע? נסתכל שוב על הגורמים שקיבלנו, בריבוע:</p>
<p>התוחלת של A בריבוע היא התוחלת של אפסילון בריבוע, כלומר היא השונות של אפסילון, שהיא סיגמא בריבוע. אנחנו קוראים לזה irreducible error, זה הרעש הקיים בטבע, שגם תחת מודל מדויק לא נצליח להפחית. אפסילון לא תלוי בדאטא שלנו, בהגדרה אין לנו דאטא לחזות אותו.</p>
<p>הגורם השני – B בריבוע – הוא כאמור קבוע, לא משתנה מקרי, והוא מבטא את האפרוקסימיישן ארור, עד כמה המודל שלנו בתוחלת עשיר מספיק וקרוב לf האמיתית. היינו רוצים שזה יהיה אפס, שלא תהיה הטיה בילט אין במודל שלנו. אבל אולי למשל לא הכנסנו את כל המשתנים שצריך, ויש הטיה, אז זאת מעין הטיה בריבוע, ואכן אנחנו קוראים לזה squared bias.</p>
<p>הגורם השלישי – C בריבוע – הוא כמו שאמרנו הvariance של החיזוי בנקודה X0, וזה מדד לאסטימיישן ארור. מודל עם שונות נמוכה, גם אם אקח דאטא קצת אחר אקבל חיזוי מאוד דומה, האמידה תהיה יציבה. מודל עם שונות גבוהה – זוכרים את עצי ההחלטה? – אם אקח דאטא קצת אחר אקבל חיזוי שונה, האמידה לא יציבה. ואמרנו שמתכון לטיפול בטעות כזאת יכול להיות למשל להגדיל את מדגם הלמידה, כאן הוא בא לידי ביטוי. איפה ראינו את זה כבר, מתמטית? כשדיברנו על ממוצע המדגם המקרי, שהשונות שלו היא סיגמא בריבוע חלקי n, וככל שn גדול יותר ככה היא תקטן. הרי גם בממוצע המדגם אפשר לראות עם קצת מאמץ מודל חיזוי פשוט.</p>
<p>מה נשאר לנו? להראות שכל שאר הביטויים של מכפלות הם אפס.</p>
<p>התוחלת של הגורם A, היא התוחלת של אפסילון, היא אפס. אז כל הביטוי של B כפול התוחלת של A הוא אפס.</p>
<p>התוחלת של C, היא התוחלת של f_hat פחות התוחלת של f_hat, כלומר גם היא 0 וכל הביטוי של B כפול התוחלת של C הוא אפס.</p>
<p>התוחלת של מכפלת A ו-C היא גם כן אפס. כי A ו-C הם משתנים מקריים בלתי תלויים לכן התוחלת של המכפלה שלהם היא מכפלת התוחלות והתוחלת של כל אחד מהם היא אפס.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-bias-variance-decomposition-1">The bias-variance decomposition</h3>
<p><span class="math display">\[\mathbb{E}(y_0 - \hat{f}(x_0))^2 = \text{irreducible error} + \text{squared bias} + \text{variance}\]</span></p>
<ul>
<li><p>Our general intuition: as model flexibility increases, bias (approximation error) decreases and variance (estimation error) increases</p></li>
<li><p>For many models we can calculate and show these effects on the bias and variance of the model</p></li>
</ul>

<img data-src="images/bias_variance_tradeoff_demo.png" class="r-stretch"><aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כך שאנחנו רואים ששגיאת החיזוי שלנו בשורה התחתונה, על דאטא שהמודל לא ראה, היא סכום הגורמים הריבועיים: שגיאת רעש טבעי שאינה תלויה בדאטא או במודל. שגיאת הטייה ריבועית, שתלויה במודל אבל לא בדאטא. ושגיאת שונות המודל שתלויה גם במודל וגם בדאטא.</p>
<p>באופן כללי אנחנו רואים שהמתמטיקה מסתדרת עם האינטואיציה שלנו, ואפשר ממש לחשב את הטעויות האלה ולהראות שככל שהמודל מורכב יותר הביאס או אפרוקסימיישן יורד, והוריאנס או האסטימיישן עולה.</p>
<p>לדוגמא ברגרסיה ליניארית, ככל שנוסיף עוד משתנים המודל יהיה מורכב ומדויק יותר, ההטיה תרד ותרד. ומצד שני השונות תגדל ותגדל, האומדים שלנו ייהפכו יותר ויותר לא מדויקים ונצטרך יותר ויותר דאטא כדי שזה לא יתדרדר.</p>
<p>ניזכר בהדגמה מהקורס הקודם: כאן היתה לנו סימולציה שבה בין X לY יש פונקציה f ליניארית פשוטה עם 20 משתנים, אבל יש לנו רק 50 תצפיות במדגם הלמידה. באמצעות סימולציה יכולנו לדעת מראש מהו האירדוסיבל ארור, השונות הטבעית של אפסילון ולצייר אותה בקו שחור. הכנסנו עוד ועוד משתנים למודל, ויכולנו ממש לחשב את ההטיה בריבוע כאן בקו האדום, ולראות איך היא קטנה עד 0 כשכל המשתנים נמצאים במודל. מצד שני יכולנו לחשב גם את שונות המודל כי דגמנו את 50 התצפיות שלנו שוב ושוב, וכאן מסומנת השונות בקו הכחול, שככל שמוסיפים עוד משתנים הוא עולה, כי 50 תצפיות כבר לא מספיקות למדל בצורה טובה ויציבה 20 משתנים. בסופו של דבר סרטטנו גם את שגיאת החיזוי על נתונים שהמודל לא ראה בקו הירוק, וניתן לראות בצורה יפה איך היא מהווה סכום של שאר שלושת הקווים. עבור 50 תצפיות, הנקודה האופטימלית בסימולציה הזאת היא כעשרה משתנים למודל, לא יותר, זה מביא לסכום ההטייה הריבועית והשונות והאירדוסיבל ארור הקטן ביותר, או לשגיאת החיזוי הריבועית הקטנה ביותר.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-classification-setting" class="slide level2 title-slide center">
<h2>The Classification Setting</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נראה כעת כמה מהמושגים שדיברנו עליהם בסטינג של קלסיפיקציה, כשY הנחזה הוא בדיד, קטגוריאלי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="misclassification-rate">Misclassification rate</h3>
<div>
<ul>
<li class="fragment">Let <span class="math inline">\(y \in \{0, 1, \dots, J - 1\}\)</span></li>
<li class="fragment">For a classifier <span class="math inline">\(\hat{f}(x)\)</span>, define the indicator <span class="math inline">\(I(y \neq \hat{y})\)</span></li>
<li class="fragment">For a sample <span class="math inline">\(T = \{(x_1, y_1 ) \dots (x_n, y_n)\}\)</span> the <span style="color:red;">training error</span> (misclassification rate) is: <span class="math inline">\(\frac{1}{n}\sum_i I(y_i \neq \hat{y}_i)\)</span></li>
<li class="fragment">For unknown observations <span class="math inline">\((x_0, y_0)\)</span> we are interested in the <em>expected</em> error rate, but we look at <span style="color:red;">test error</span>: <span class="math inline">\(\frac{1}{m}\sum_i I(y_{i,0} \neq \hat{y}_{i,0})\)</span></li>
<li class="fragment">This error rate can be decomposed into three terms as well!</li>
</ul>
</div>
<div class="fragment">
<p>We want a predictor which drives the error rate to minimum. What is that predictor?</p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כשY הוא אחת מJ קטגוריות, הטעות האינטואיטיבית ביותר להסתכל עליה היא המיסקלסיפיקיישן רייט. עבור כל תצפית נספור “אחת” אם טעינו בחיזוי, ואפס אחרת, נסמן את זה באמצעות משתנה אינדיקטור.</p>
<p>עבור מדגם למידה T סך טעות החיזוי יהיה סכום של משתני האינדיקטורים האלה על n תצפיות.</p>
<p>כעת ברור שהיינו רוצים את תוחלת הטעות הזאת על נתונים שהמודל לא ראה, בפועל יש לנו מדגם טסט של m תצפיות נאמר, ואנחנו ננסה להביא למינימום את הטסט ארור, היא סכום האינדיקטורים על m התצפיות.</p>
<p>מסתבר שגם את הטעות הזאת ניתן לפרק לשלושה גורמים אם כי לא נעשה את זה כאן.</p>
<p>נשאל מהו החיזוי שיביא למינימום את הטסט ארור הזה?</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="bayes-decision-boundary">Bayes decision boundary</h3>
<p>Assume the conditional probability <span class="math inline">\(P(Y = 1 | X)\)</span> is a nice, slowly changing function of <span class="math inline">\(X\)</span>:</p>
<div id="a9c072ae" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c01_intro_to_intro_files/figure-revealjs/cell-5-output-1.png" width="642" height="434"></p>
</figure>
</div>
</div>
</div>
<div class="fragment">
<p>The <span style="color:red;">Bayes decision boundary</span> is where <span class="math inline">\(P(Y = 1 | X) = 0.5\)</span></p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נתמקד רגע במצב שבו Y הוא בינארי, אפס או אחת. נניח שY בהינתן X מתפלג ברנולי עם הסתברות מותנית להיות 1, P. ונניח שההסתברות הזאת היא פונקציה חלקה יחסית כמו כאן של מרחב X. כמו שניתן לראות כאן הפונקציה היא לא ליניארית באיקס אבל היא כאן משתנה לאט, יש לה גבעות ועמקים.</p>
<p>קו תיאורטי שיעניין אותנו מאוד יהיה הקו שמסמל את גובה חצי, כלומר איפה ההסתברות להיות כל אחד משני הקלאסים היא חצי בדיוק. הבאונדרי הזה נקרא הבייז דסיז’ן באונדרי. למה דסיז’ן? כי נראה נכון מצד אחד של הגבול לסווג תצפיות כ1 ומהצד השני כ0.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-bayes-classifier">The Bayes classifier</h3>
<ul>
<li>For a <span class="math inline">\(J\)</span>-class classification problem, assume we know the probabilities <span class="math inline">\(P(Y = j | X = x) \space \forall j, x\)</span></li>
<li>The <em>best</em> classifier is the <span style="color:red;">Bayes classifier</span>: <span class="math inline">\(\hat{f}(x) = \arg\max_j P(Y = j | X = x)\)</span></li>
<li>For a <span class="math inline">\(2\)</span>-class problem we can use the Bayes decision boundary and reach a simpler notation: <span class="math inline">\(\hat{f}(x) = 1 \space \forall x \space s.t. \space P(Y = 1 | X = x) &gt; 0.5 \text{ otherwise } 0\)</span></li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>זה בדיוק מה שיעשה הקלסיפייר התיאורטי של בייז, הבייז קלסיפייר.</p>
<p>באופן כללי עבור J קלאסים בייז יבחר בקלאס J שיביא למקסימום את ההסתברות שY שווה לJ. במצב של שני קלאסים זה פשוט אומר להסתכל על הבאונדרי שראינו, מצד אחד שלו עבור הסתברות גדולה מחצי לחזות 1 ומהצד השני 0.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="how-is-bayes-classifier-best">How is Bayes classifier “best”?</h3>
<p><span class="math inline">\(E_Y(I(y \neq \hat{y})| X) = P(Y = 1 | X) \cdot I(\hat{y} = 0) + P(Y = 0 | X) \cdot I(\hat{y} = 1)\)</span></p>
<div>
<ul>
<li class="fragment">Suppose <span class="math inline">\(P(Y = 1 | X) = 0.7\)</span>, between <span class="math inline">\(\hat{y} \in \{0, 1\}\)</span> –&gt; choose <span class="math inline">\(\hat{y} = 1\)</span> to minimize <span class="math inline">\(E_Y(I(y \neq \hat{y})| X)\)</span></li>
<li class="fragment">Suppose <span class="math inline">\(P(Y = 1 | X) = 0.3\)</span>, between <span class="math inline">\(\hat{y} \in \{0, 1\}\)</span> –&gt;choose <span class="math inline">\(\hat{y} = 0\)</span> to minimize <span class="math inline">\(E_Y(I(y \neq \hat{y})| X)\)</span></li>
<li class="fragment">Put differently the Bayes classifier <span class="math inline">\(\hat{f}(x) = \arg\max_j P(Y = j | X = x)\)</span> minimizes expected error rate!</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אנחנו טוענים שהבייס קלסיפייר, אם מפת ההסתברות הזאת ידועה, הוא הקלסיפייר האופטימלי. אופטימלי באיזה מובן? במובן שהוא מביא למינימום את תוחלת המיסקלסיפיקיישן רייט. הרי מהי אותה תוחלת?</p>
<p>בהסתברות שY שווה לאחת נקבל טעות אם ננבא אפס, ובהסתברות שY שווה לאפס, נקבל טעות אם ננבא אחת. ושוב, אנחנו מניחים שההסתברות ידועה!</p>
<p>אז אם עבור נקודה מסוימת ההסתברות להיות 1 שווה למשל 0.7, איך נביא את הביטוי הזה למינימום? נבחר לחזות 1 והטעות תהיה 0.3.</p>
<p>ואם עבור נקודה מסוימת ההסתברות להיות 1 שווה ל0.3, איך נביא את הביטוי הזה למינימום? נבחר לחזות 0 והטעות תהיה 0.3</p>
<p>אבל מה שתיארנו כאן זה בדיוק הבייס קלסיפייר עבור שני קלאסים, כלומר לבחור את הקלאס הJ שמביא למקסימום את ההסתברות שY שווה לJ, מביא למינימום את תוחלת המיסקלסיפיקיישן רייט, עבור נקודה ספציפית.</p>
<p>ומה הבעיה שוב?</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-bayes-classifier-is-purely-theoretical">The Bayes classifier is purely theoretical!</h3>
<div id="3f2adb29" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c01_intro_to_intro_files/figure-revealjs/cell-6-output-1.png" width="642" height="434"></p>
</figure>
</div>
</div>
</div>
<div class="fragment">
<p>Even for this training data, the Bayes classifier will have an <em>irreducible</em> error rate! (classes overlap)</p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>הבייס קלסיפייר הוא אורקל, מונח תיאורטי בלבד, שלו היינו יודעים את מפת ההסתברות הוא היה אופטימלי. ונשים לב שבפועל נקודות נדגמות על-פי הסתברות, המתחמים של הקלאסים הם לא דטרמיניסטיים עבור דאטא טיפוסי ויש ביניהם חפיפה. וגם עבור דאטה שהמודל רואה ויודע את מפת ההסתברות ובוחר בבייס קלסיפייר - יש טעויות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="the-bayes-error-rate">The Bayes error rate</h3>
<div>
<ul>
<li class="fragment">What is the Bayes classifier error?</li>
<li class="fragment">For a given <span class="math inline">\(x\)</span>: <span class="math inline">\(BE = 0 \cdot P(correct) + 1 \cdot P(error) = 1 - \max_j P(Y = j | X = x)\)</span></li>
<li class="fragment">Therefore for a test set the expected error rate is given by: <span class="math inline">\(1 - E\left(\max_j P(Y = j | X = x)\right)\)</span></li>
<li class="fragment">This bound is the <em>lowest</em> error rate achievable (under our assumptions), similar to the irreducible error in regression</li>
<li class="fragment">Sampling additional test observations in the example we reach test error rate of 0.205</li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>But in practice the conditional probability is unknown. How can we approximate it?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מהי טעות החיזוי של בייס קלסיפייר?</p>
<p>אם אני צודק, בקריטריון שלנו אני “משלם” 0, ואם אני טועה אני משלם 1. לפיכך הטעות היא הסיכוי שלי לטעות שהוא 1 פחות הסיכוי שהביא אותי מלכתחילה לבחור בקלאס, כלומר 1 פחות הסיכוי המקסימלי.</p>
<p>אם ניקח על זה תוחלת נראה ביטוי די פשוט לתוחלת הטעות של הבייס קליספייר. עבור תצפית שבה הסיכוי המקסימלי הזה הוא 0.6, כלומר תצפיות שקרובות לגבול ההחלטה, הטעות הזאת בתוחלת תהיה די גדולה, 0.4. אבל עבור תצפית שבה הסיכוי המקסימלי הזה הוא 0.9, כלומר תצפיות רחוקות מהגבול, הטעות הזאת בתוחלת תהיה די קטנה, 0.1.</p>
<p>אבל בכל מקרה, גם בתוחלת, התצפיות אינן דטרמניסטיות ותהיה לנו טעות, אפילו עבור הבייס קלסיפייר! הדבר מאוד דומה לטעות האירדוסיבל שראינו ברגרסיה.</p>
<p>למעשה, בדוגמא שראינו אם נחשב את טעות החיזוי של הבייס קלסיפייר, נראה שהטעות האמפירית עליהן היא בערך 20 אחוז.</p>
<p>אבל כל הדיון הוא דיון תיאורטי הרי, כי הבייס קלסיפייר הוא אורקל, ובפועל אנחנו צריכים לאמוד את מפת ההסתברות הזאת, בהנחה שהיא קיימת. איך נאמוד אותה? בחלק האחרון של השיעור נזכיר את שיטת הKNN, שיטת השכנים הקרובים ונראה שתחת ההנחה שהמפה הזו, פונקצית ההסתברות הזאת משתנה לאט, היא עושה עבודה לא רעה באמידה, וגבול ההחלטה שלה מתקרב מאוד לבייס קלסיפייר האופטימלי.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-knn-classifier" class="slide level2 title-slide center">
<h2>The KNN Classifier</h2>
</section>
<section class="slide level2">

<h3 id="k-nearest-neighbors">K-Nearest Neighbors</h3>
<ul>
<li>Let <span class="math inline">\(K\)</span> be a positive integer, and let <span class="math inline">\(T\)</span> be the training set</li>
<li>Classification rule for new observation <span class="math inline">\(x_0\)</span>:
<ul>
<li>Let the <span class="math inline">\(K\)</span>-neighborhood <span class="math inline">\(\mathcal{N}(x_0)\)</span> the <span class="math inline">\(K\)</span> points closest to <span class="math inline">\(x_0\)</span> in <span class="math inline">\(T\)</span></li>
<li><span class="math inline">\(\hat{f}(x_0) = \arg\max_j \left[\sum I(y(x) = j) | x' \in \mathcal{N}(x_0)\right]\)</span></li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li>Under what conditions is it an approximation to the Bayes classifier?</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נזכיר כעת כיצד מתבצעת קלסיפיקציה בשיטת KNN, שיטה א-פרמטרית ומאוד אינטואיטיבית. אנחנו רוצים לשערך את פונקצית ההסתברות של Y בכל שכונה של X? אז בואו נגדיר לכל X “שכונה”, כמספר שכנים K הכי קרובים אליו במדגם הלמידה.</p>
<p>החיזוי הסופי של הקלאס J של הנקודה X יהיה הJ שיקבל מהשכנים בשכונה את מירב הקולות, או יביא למקסימום את הביטוי שרשום כאן.</p>
<p>KNN זו אחת השיטות האינטואיטיביות ביותר לקלסיפיקציה, והיא למעשה “נטולת למידה”, אין אלגוריתם איטרטיבי או לא שצריך להפעיל על הנתונים, רק לשמור בזיכרון את מדגם הלמידה וגמרנו. אבל נזכיר שוב תחת איזו הנחה KNN תעבוד טוב ותקרב את הבייס קלסיפייר? תחת ההנחה שבכלל קיימת פונקציה או מפת הסתברות כזאת שמשתנה לאט, וניתן לאמוד אותה באמצעות שיטת השכנים. אם לדוגמא הקלאסים משתנים באמצעות כלל משוגע אחר, שכל נקודה “מחליטה לעצמה” ואין בכלל חלקות בנוף ההסתברות הזאת, KNN לא יעשה עבודה טובה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="knn-vs.-bayes-classifier">KNN vs.&nbsp;Bayes classifier</h3>
<div id="5fdab545" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c01_intro_to_intro_files/figure-revealjs/cell-7-output-1.png" width="642" height="434"></p>
</figure>
</div>
</div>
</div>
<p>(For <span class="math inline">\(K = 10\)</span> test error rate 0.270, only slightly worse than Bayes error rate!)</p>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>נניח שנבחר בK = 10, כלומר עבור כל נקודה חדשה המודל חוזה עבורה את הקלאס שזוכה לרוב הקולות מבין 10 השכנים הקרובים אליה ביותר במקרה הזה במרחק אוקלידי, אפשר לחשוב על מרחקים אחרים.</p>
<p>בבעיה שלנו עם 2 קלאסים אפשר לחזות עבור כל נקודה גם את שיעור השכנים שלה שחוזים את הקלאס 1, ואז אפשר לסרטט את גבול ההחלטה של KNN, כלומר הקו שעבורו היא חוזה הסתברות חצי, ולראות שהוא מאוד מזכיר את הבאונדרי של הבייס קליספייר, ולא במקרה, ההנחות שלנו נכונות. למעשה, עבור מדגם טסט של עוד כמה מאות תצפיות, טעות החיזוי כאן היא כ-27 אחוז, קצת מעל טעות החיזוי של בייס, כפי שהיינו מצפים.</p>
<p>אבל איך הגענו לK = 10?</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="knn-and-bias-variance-tradeoff">KNN and bias-variance tradeoff</h3>
<div id="f53eb653" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c01_intro_to_intro_files/figure-revealjs/cell-8-output-1.png" width="815" height="302"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>How can we efficiently find a neighborhood of an observation?
<ul>
<li>For small <span class="math inline">\(K\)</span>: classifier is flexible but has high variance</li>
<li>For large <span class="math inline">\(K\)</span>: classifier is stable but suffers from high bias</li>
</ul></li>
<li>There is no way to escape this bias-variance tradeoff</li>
</ul>
<div class="fragment">
<ul>
<li>So how to choose <span class="math inline">\(K\)</span>?</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אנחנו רואים כאן את אותו גבול החלטה עבור K = 1 ועבור K = 200. ולמעשה עוד המחשה של הביאס-וריאנס טריידאוף שקיים גם בקלסיפיקציה.</p>
<p>כשK קטן, הקלאסיפייר גמיש מאוד ומסוגל לתאר קוי החלטה שונים ומשונים. אבל הם מושפעים מאוד, כנראה יותר מדי, ממדגם הלמידה ומביאים לשונות גבוהה, במובן שאם יגיע מדגם למידה אחר הם יכולים להשתנות לחלוטין.</p>
<p>כשK גדול קו ההחלטה יציב יותר, הוא יחזה נקודה צהובה רק כשהוא ממש בטוח, אבל יש לו ביאס מאוד גבוה כי הפונקציה שהוא מסוגל לתאר היא מוגבלת מאוד ולא גמישה.</p>
<p>אז מה בכל זאת אנחנו עושים כדי לבחור את K?</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="how-to-choose-k">How to choose <span class="math inline">\(K\)</span>?</h3>
<div id="cb0568a8" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c01_intro_to_intro_files/figure-revealjs/cell-9-output-1.png" width="821" height="455"></p>
</figure>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li>Sometimes we have prior knowledge, can choose <span class="math inline">\(K\)</span> from theoretical considerations</li>
<li>In most real-life problems, choosing <span class="math inline">\(K\)</span> requires some additional data we use to validate our model</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>לפעמים יש לנו הבנה חזקה יותר בדומיין של הדאטה שבו אנחנו עוסקים ויכולות להיות לנו סיבות טובות לבחור בK ספציפי, עם משמעות אינהרנטית לבעיה, כמו למשל אם היינו צריכים לחזות תכונה גנטית של אדם והשכונה שהיינו בוחרים היתה המשפחה שלו.</p>
<p>ברוב המקרים אנחנו נשתמש באסטרטגיה של חלוקת הדאטה לטריין ולטסט כמו כאן, ונבחר את הK שמביא למינימום את טעות החיזוי על מדגם הטסט.</p>
<p>כאן אפשר לראות את טעות החיזוי כפונקציה של 1 חלקי K, כי אנחנו רגילים לראות את הגרף הזה שנע ממודל מאוד פשוט ולא מורכב, ועד מודל מאוד גמיש ומורכב מדי, כלומר אנחנו רוצים לראות את K יורד. שני דברים שאנחנו מצפים לראות בגרף החיזוי של KNN ומתקיימים כאן:</p>
<p>הארור של מדגם הלמידה, לא הטסט, עבור K = 1 יורד לאפס! כי עבור כל תצפית נחזה את הקלאס שלה עצמה. מה שממחיש את הבעייתיות בהסתכלות על מדגם הלמידה.</p>
<p>ועוד תופעה שצפינו, KNN במיטבו מתקרב לטעות החיזוי של הבייס קלסיפיר שמסומנת כאן בקו שחור מקווקוו, אבל לא מצליחה לרדת ממנו. זו טעות אירדוסיבל. בכל מקרה כאן נראה שהK הטוב ביותר הוא בסביבות 30, עבורו היינו מקבלים טעות חיזוי של כ-25 אחוזים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="quarto-auto-generated-content">
<p><img src="../Intro2SL_logo_white.jpg" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://intro2statlearn.github.io/mooc/" target="_blank">Intro to Statistical Learning</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../libs/revealjs/plugin/search/search.js"></script>
  <script src="../libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>