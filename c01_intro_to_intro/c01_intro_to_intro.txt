=== 1. מהי למידה סטטיסטית? ===

שלום. וברוכים הבאים לשיעור הראשון בקורס מבוא ללמידה סטטיסטית. בשיעור הזה נחזור על מושגים חשובים בלמידה, כמו טעות החיזוי, ואוברפיטינג. נראה איך המושגים האלה באים לידי ביטוי ברגרסיה ובקלסיפיקציה. אבל קודם כל ננסה להגדיר: מהי בכלל למידה סטטיסטית?

:::

מטרות העל של למידה סטטיסטית הן הבנה של הנתונים שאנחנו מביטים בהם, וחיזוי על סמך נתונים אלה.

נהוג לומר שלמידה סטטיסטית היא תת-תחום בעולם למידת המכונה, שהרי ההסתכלות הזאת של למידה מתוך נתונים היא לא האפשרות היחידה. מודלים של בינה מלאכותית קיימים גם מחוץ ללמידה סטטיסטית, כמו למשל למידת חיזוק, שהמודל הנלמד הוא תוצאה של ניסיון מתמשך מול העולם. ויש גם ניסיון ללמוד מודלים על-ידי הגדרת כללים ותרשימי זרימה, לאו דוקא על סמך נתונים.

אבל בלמידה סטטיסטית הדגש הוא על נתונים, וכשמה כן היא, הכלי העיקרי שבו אנחנו משתמשים לבניית מודלים הוא סטטיסטיקה והסתברות.

ועדיין, תת העולם שאנחנו מסתכלים עליו הוא עצום, וקיימים כלים רבים ומגוונים ללמידת מודלים על סמך הנתונים. בקורס זה נלמד את הכלים האלה בראייה סטטיסטית ולאו דוקא חישובית, ונדבר מעט יחסית על הסיבוכיות שלהם, ונשים דגש על איך בוחרים נכון את הכלי לטיפול בנתונים. הכלי הנכון לטיפול בנתונים נובע מהנתונים עצמם.

:::

בואו נחזור על כמה מושגי יסוד. נתייחס לדאטא שלנו כקבוצה של פיצ'רים, במקומות אחרים נראה גם שמות כמו משתנים מנבאים או בלתי תלויים. המשתנים האלה יכולים להיות רציפים כמו לדוגמא גובה, משתנים בדידים אורדינליים כמו למשל מספר הכוכבים של מלון. והם יכולים להיות גם בדידים קטגוריאליים ללא משמעות לסדר, כמו צבע עיניים.

בבעיות אמיתיות החלוקה הזאת לא תמיד ברורה, וגם הבחירה בסוג המשתנה שעומד מולי ואיך לייצג אותו, יכולה להיות תלויה במטרה הסופית של הפרויקט עליו אני עובד, לדוגמא להקטין טעות חיזוי. הרי אם מזינים גובה של אנשים בסנטימטרים שלמים, הוא לא באמת רציף, הוא בדיד ואורדינלי, אבל קשה לחשוב על יישום שבו יהיה יתרון להתייחס לגובה כמשתנה בדיד. מצד שני אם הזכרנו את מספר הכוכבים של בית מלון, יכול להיות שאפשר היה להתייחס אל משתנה כזה כרציף? פשוט להחליף את מספר הכוכבים ל1 עד 5? אולי, אבל כאן תסתתר הנחה שיש מרווחים קבועים בין הכוכבים, הנחה שהיא לא בטוח נכונה. כל מי שבדק פעם מלונות באינטרנט יודע שמלונות עד 3 כוכבים הם פחות טובים ושיש קפיצה משמעותית באיכות המלון כשמדברים על 4 ו-5 כוכבים. למעשה אולי נכון ביישומים מסוימים להתייחס לאיכות המלון כמשתנה בינארי - 3 כוכבים ומטה או 4 ומעלה.

ואפילו בדוגמא של צבע העיניים, צבע עצמו הוא לא בהכרח משתנה בדיד! הרי הצבע שאנחנו רואים הוא תוצר של הספקטרום האלקטרומגנטי, ותדרי גל אלקטרומגנטי שונים יוצרים צבעים שונים. סגול מתאים לגלים קצרים, ואדום לגלים ארוכים. זה משהו שכדאי לחשוב עליו בעבודה מעשית עם נתונים.

את כל הפיצ'רים האלה נבחן כדי לבנות מודל או לרנר בדרך כלל למטרת חיזוי. כשאם החיזוי הוא של משתנה רציף אנחנו קוראים לסטינג הזה רגרסיה, ואם החיזוי הוא של משתנה בדיד אנחנו קוראים לסטינג הזה קלסיפיקציה, ולמודל קלסיפייר.

שאלה חשובה: האם אפשר להשתמש במודלים לרגרסיה עבור בעיות קלסיפיקציה ולהיפך? התשובה המפתיעה היא שכן, וזה תלוי שוב במטרת הפרויקט, ומה עובד לנתונים שלכם. נחזור לדילמה הזאת מאוחר יותר בקורס.

:::

=== 2. למידה מפוקחת ובלתי מפוקחת ===

נהוג להפריד את הלמידה הסטטיסטית לשני סוגים עיקריים: למידה מפוקחת שבה יש משתנה מטרה ברור שאת האופי שלו אנחנו רוצים להבין או לחזות כתלות בנתונים, ולמידה בלתי מפוקחת, שבה אין משתנה מטרה, הנתונים עצמם הם לב העניין.

:::

מהי למידה מפוקחת?

אם איקס הוא וקטור באורך p של נתונים, אנחנו מניחים שוואי, סקלר אחר שאנחנו רואים הוא תוצאה של איזושהי פונקציה f ועוד משתנה רעש אפסילון. במודל הכי פשוט לרעש הזה יש תוחלת אפס ואיזושהי שונות סיגמא בריבוע, והוא לא תלוי בתצפיות, כלומר הוא קבוע. תיכף נגיד על זה הערה חשובה.

מה לומדים בלמידה מפוקחת? את f, כיצד X משפיע על Y, כאשר נהוג לחשוב על המשתנה התלוי Y כ"המפקח", כי ההשתנות שלו תקבע את הלמידה שלנו. כאן סימנתי סט של תצפיות X כמטריצה בגודל n על p, וy כוקטור, רק כדי לדייק, אבל לא תמיד נקפיד.

ברוב המקרים אין לנו דרך ללמוד את f האמיתית, אנחנו יודעים שאנחנו מקבלים רק אומד, f האט, וטיב האמידה שלנו ייקבע באמצעות פונקצית הפסד או לוס, שנסמן בL, מטריקה בין Y ל-Y הנחזה שנסמן כY האט, שנרצה להביא למינימום.

בסטינג של רגרסיה הלוס הנפוץ ביותר הוא השגיאה הריבועית או הממוצע שלה על פני התצפיות, הMSE, אבל לא רק. אפשר לחשוב למשל על ממוצע השגיאה האבסולוטית, בערך מוחלט, הMAE. לדוגמא ביישום שנוגע לחיזוי מחיר, כשY הוא כסף, למה שארצה להעניש בעונש ריבועי תצפיות שרחוקות מהאמת. כסף הוא כסף, ואולי אעדיף שגיאה בערך מוחלט.

בקלסיפיקציה הלוס הנפוץ הוא המיסקלסיפיקיישן רייט, שגיאת החיזוי שעוד נדבר עליה בשיעור זה. אבל יש הרבה בעיות עם הלוס הזה ביישומים אמיתיים, למשל בחיזוי מחלות נדירות. עבור מחלה שמופיעה בסיכוי נדיר של 1 אחוז, מהו קלסיפייר פשוט שיביא לשגיאת חיזוי נמוכה של 1 אחוז? לחזות שכולם בריאים. אז ביישומים כגון זה נעדיף אולי להסתכל על פרסיז'ן או ריקול. פרסיז'ן זה המדד שאומד את ההסתברות לחיזוי נכון של פציינט בהינתן שהמודל חוזה שהוא חולה. ריקול זה המדד שאומד את ההסתברות לחיזוי נכון של פציינט בהינתן שהוא אכן חולה. ובדוגמא שלנו, מודל שחוזה שכולם בריאים במחלה נדירה, יביא לריקול אפס, ולפרסיז'ן לא מוגדר.

:::

שתי הערות חשובות על הרעש, הנויז, המסתורי הזה. הרבה פעמים אנחנו מדמיינים איזה מכשיר מדידה עם איזושהי מחט שבאופן מסתורי זזה קצת בכל מדידה, וזה רעש בלתי מוסבר. או אם המשתנה התלוי הוא מספר החטיפים בשקית, זה אינטואיטיבי לנו שבשקית אחת יהיו 101 חטיפים, באחרת 99, אנחנו מניחים שיש שם מכונה שלוקחת איזו כמות ממוצעת ויש שם איזשהו "רעש" שלא באמת מאפשר לה להכניס בדיוק 100 חטיפים בשקית.

אבל מה עם המשתנה התלוי הוא משכורת? שום מכונה לא קובעת משכורת, וגם אם כן זה מחשב, ואף אחד לא הוסיף הרי בתכנית של מחשב איזשהו רעש אקראי. ובכל זאת - אם ניתקל בשני אנשים באותו תפקיד, עם אותו ניסיון, מאותו מגדר, אולי אפילו אותה חברה -- האם יהיה להם שכר זהה? זה אפילו סביר לצפות שלא, אז איך זה קורה? התשובה היא שכנראה בכל זאת יש כמה משתנים שלא לקחנו בחשבון, גם במקרה כזה שאין "טעות מדידה". וככה צריך להתייחס לרעש.

רעש הוא כל מה שאנחנו לא יכולים לקחת בחשבון, כלומר למדוד, כל מה שיכול להיות מוסבר אם היינו לוקחים עוד משתנים, שמשום מה לא לקחנו. וזה נכון אפילו בדוגמא פיסיקלית כמו מספר חטיפים בשקית, יכול להיות שאם מביאים את טמפרטורת החדר במפעל, את כיוון הרוח באותו יום, את זהות המפעיל - הרעש יקטן, וחיזוי מספר החטיפים בשקית יהיה מדויק יותר ויותר. אבל כל עוד אנחנו מכירים בזה שלא ניתן לקחת בחשבון את כל משתני החיזוי האפשריים בעולם, אנחנו חייבים להניח רעש.

הערה נוספת שעוד נחזור אליה - זה שרעש ועוד איך יכול להיות תלוי בתצפיות, באיקס. למשל אפשר לחשוב שהשונות של אפסילון בהינתן איקס, היא לא פרמטר שצריך לאמוד, אלא פונקציה. אם כבר דיברנו על Y כשכר, נניח שX הוא מספר שנות הניסיון. סביר להניח ששונות הרעש בשכר קטנה לאנשים עם מעט שנות ניסיון, כשלוקחים בחשבון תפקיד, חברה, שנה. אבל היא גדלה ככל ששנות הניסיון מתרבות, ושוב בגלל משתנים נוספים שמתווספים ואנחנו לא יכולים לקחת בחשבון. אז למה אנחנו מניחים שהרעש לא תלוי בתצפיות? כי זה מודל פשוט להתחיל ממנו. אפשר גם להתאים מודלים שבהם הרעש תלוי בתצפיות.

:::

נחזור ללמידה מפוקחת. דוגמאות ללמידה מפוקחת יש הרבה:

Y יהיה סך המכירות של מוצר ביום נתון, המשתנים בX יהיו מחיר המוצר, המיקום הגאוגרפי של המכירות, האם יש יום חופש.

Y יהיה האם לקוח של חברת סלולר יעזוב את החברה, מונח שנקרא צ'רן. המשתנים בX יהיו כמות השימוש בסלולרי שלו, גיל, סוג הפלאפון ועוד.

יישום מודרני יותר יכול להיות ברפואה ממוקדת. Y יהיה המינון המתאים לתרופה של חולה ספציפי, והמשתנים יהיו הפרופיל הגנטי שלו, האפקטיביות של טיפול כפי שנמדדה על רקמות תאים, ואם נהיה ציניים נוכל לכלול אפילו את מחיר התרופה עבור החולה ועבור קופת החולים. אבל אם המחיר הכלכלי כל כך מהותי לאפליקציה הזאת, אפשר גם לנסח את פונקצית ההפסד באופן שיכלול את המינימיזציה שלו, או כאילוץ.

כך שאנחנו רואים שהמושג של למידה מפוקחת כולל בתוכו שלל אתגרים מכל תחומי המחקר.

:::

 גם פונקצית ה-f שאנחנו אומדים, יכולים להיות לה פרופילים רבים. כאן אנחנו רואים נתונים אמיתיים של שכר שנתי של כ3000 גברים בארה"ב, כל פעם כפונקציה של משתנה אחר.

 שכר שנתי כפונקציה של גיל מתאר דפוס עולה עד לשיא בגילאי 40-50 ואז יורד קצת. אנחנו רואים כאן גם את הדפוס של הרעש שתלוי במשתנה, החל מפיזור קטן לגילאים צעירים ועד פיזור גדול מאוד לגילאים מבוגרים יותר.

 שכר כפונקציה של שנה, מתאר דפוס של עלייה ליניארית עם שיפוע ממש קטן, כמעט בלתי נראה. כאן למרות שקשה להבחין בזה השכר החציוני עולה בכאלף דולר כל שנה!

בגרף השלישי מתוארת ההשתנות של Y כפונקציה של רמת ההשכלה. רמת ההשכלה המתוארת כאן היא מאנשים שלא סיימו תיכון, ועד אנשים עם תואר שני ומעלה. זאת דוגמא מצוינת למשתנה שאפשר היה לחשוב עליו כמשתנה רציף, כמו שנה, אבל לא ברור אם היחס בין כל שתי רמות נשמר ולכן טבעי יותר להתיחס אליו כמשתנה בדיד אורדינלי, ולצייר בוקספלוטים במקום תרשים פיזור. כך או כך, ברור שהשכר החציוני עולה ככל שההשכלה עולה.

:::

מה עם אין לנו מפקח? אין משתנה תלוי. יש לנו רק דאטא X, ואנחנו רוצים ללמוד תכונות שלו.

היינו רוצים לדעת על קלאסטרים בתוך הדאטא, לדוגמא למטרות שיווקיות. אנחנו אתר מכירות עם הרבה לקוחות והיינו רוצים להקצות איש מכירות שיתמחה בפלח אחר של הלקוחות שלנו. אבל לא היינו רוצים לנסות להגדיר לבד את הקבוצות השונות של לקוחות, אלא היינו רוצים שהדאטא יראה לנו קלאסטרים טבעיים בנתוני הלקוחות. איזה דאטא? כל דבר החל ממין וגיל, ועד דפוסי גלישה ורכישה.

תחום אחר שנחשב ללמידה בלתי מפוקחת ינסה למצוא מתאמים בין הפיצ'רים, ותלויות. 

אנחנו יודעים למצוא קורלציה בין שני משתנים אבל הרי לא נעבור על כל הזוגות האפשריים של פיצ'רים ונחשב קורלציה, נרצה גם לדעת על קורלציות בין שילובי משתנים, כמו קבוצה של 3 שאלות במבחן כלשהו עם 4 שאלות אחרות. יחסים כאלה נקראים פקטורים והכלי הכי מוכר למצוא אותם הוא PCA.

כל הבעיות האלה שבהן אנחנו ממדלים את השתנות הפיצ'רים של הדאטא ואין לנו Y מפקח הן בעיות אנסופרוויזד. נעסוק בהן בקורס אבל פחות, רוב הקורס יתמקד בלמידה סופרוויזד.

מילה אחרונה לגבי סוג למידה נוסף או ביניים, שפופולרי במיוחד בעידן הביג-דאטא. בהרבה יישומים יש לי תרחיש של הרבה מאוד נתונים בלתי-מפוקחים, כלומר אין לי לגביהם את המשתנה התלוי הY או הלייבל, וסט נתונים קטן יותר שלגביהם יש לי את Y, הלייבל. זה קורה הרבה פעמים מסיבות של תקציב, אפשר לחשוב על אתר כמו אינסטגרם עם מיליוני תמונות של משתמשים. היינו רוצים לתת כותרת למה מתארת התמונה, ומסיבות של תקציב אנחנו יכולים לעשות את זה בצורה ידנית רק לכ10 אלף תמונות. אבל יש עוד מיליוני תמונות! למידה סמי-סופרוויזד או "חצי-מפוקחת", מאפשרת לממדל להשתמש בדאטא האנלייבלד על מנת לשפר את המודל שנבנה עם הדאטא הלייבלד. כנראה שנוכל למדל טוב יותר את ההסתברות של Y בהינתן הנתונים X אם נדע טוב יותר את ההתפלגות של הנתונים עצמם X. כשמדברים על רשתות נוירונים ההבנה הזאת מפיקה ארכיטקטורה טבעית של רשת לבעיה, אבל רשתות נוירונים הם לא בסקופ של הקורס שלנו ונעצור את זה כאן.

:::

=== 3. למידה פרמטרית וא-פרמטרית ===


נבחין כעת בין מודלים פרמטרים למודלים א-פרמטרים.

:::

סוג המודל שנבחר קשור קשר הדוק למהי המטרה שלנו. אז... מהי המטרה שלנו? למה לאמוד את f?

בגדול יש שתי מטרות. המטרה שנראית לעיתים כחזות הכל היא פרדיקציה, בהינתן תצפית חדשה איקס, נרצה לאמוד את הוואי המתאים לה, הוא המודל הנאמד f האט, כי תוחלת הרעש היא אפס.

אבל פרדיקציה היא לא חזות הכל! בעולם המחקר פעמים רבות המטרה היא לא בהכרח לחזות בצורה הטובה ביותר עבור תצפית חדשה, אלא מה שקרוי בסטטיסטיקה הסקה - להבין כיצד הנתונים בX משפיעים על Y. בדוגמא שראינו למשל, אולי התובנה שההשפעה של גיל על שכר היא לא רק שלא ליניארית אלא שהיא אפילו לא מונוטונית - אולי התובנה הזאת היא מטרת המחקר.

שאלה אחרת יכולה להיות ההשפעה של משתנים מסוימים על Y בהינתן משתנים אחרים. כלומר אם אני יודע מצב סוציו-אקונומי על סטודנט, האם המוצא שלו משפיע על הציון? רק התשובה לשאלה הזאת יכולה להיות שווה פרסום.

ואולי השאלה היא בכלל לא על אופן ההשפעה על טיב היכולת בכלל לאמוד את הקשר בין X לY. לדוגמא מחקר שיעסוק בניסיון עצמו לחזות ציון של סטודנט בקורס ויכמת את טיב האמידה של חיזוי כזה כפונקציה של עוד ועוד משתנים.

מכל מקום המטרה השניה של הסקה סטטיסטית היא לא נחלתם של חוקרים בלבד, והיא הרבה פעמים קלה יותר במודלים פשוטים יותר שמעניינים למשל רופאים. כשרופאה רוצה לקבוע פרוגנוזה של חומרת מחלה או להעריך כמה זמן אשפוז יצטרך פציינט, היא בודאי היתה מעדיפה מודל שהיא מסוגלת להבין ולתמוך בו, מודל שעושה שכל. ולא בהכרח המודל שחוזה בצורה המדויקת ביותר את חומרת המחלה או זמן האשפוז.

:::

בכל מקרה בדרך כלל נקבל מדגם למידה שנסמן כT, עם n זוגות של X ו-Y. נשים לב שX יכול להיות בעצמו וקטור, בדרך כלל נסמן כוקטור עמודה באורך P פיצ'רים.

אנחנו מניחים שקיימת התפלגות משותפת של X וY על המרחב המשותף הזה שמתוכה אנחנו רואים דגימות, ובנוסף שהדגימות האלה בלתי תלויות.

ואת המודל שנבחר לתאר את f כפונקציה של מדגם הלמידה ניתן בדרך כלל לחלק לאחד משני סוגים: מודל פרמטרי ומודל א-פרמטרי.

:::

מודל פרמטרי הוא בעל הנחות חזקות מאוד. אנחנו מניחים מודל עם פרמטרים מסוימים עבור f וכל שנותר הוא לאמוד את הפרמטרים האלה.

הדוגמא הקלאסית בסטינג של רגרסיה היא רגרסיה ליניארית. אנחנו מניחים שf הוא פונקציה ליניארית של המשתנים, כל משתנה מוכפל בפרמטר בטא וf הוא סכום המכפלות הללו.

כל שנשאר הוא לשערך את הפרמטרים בטא, זה מה שכל אלגוריתם תחת הכותרת רגרסיה ליניארית ינסה לעשות. הקריטריון הקלאסי הוא עקרון הריבועים הפחותים ונרחיב על איך להביא אותו למינימום בהמשך, אבל ההנחה ברורה והמטרה ברורה, לאמוד את הבטא. נשים לב שהמודל הליניארי כפי שהוא מנוסח כרגע ועקרון הריבועים הפחותים, לא תלוי בשום היבט סטטיסטי, אין כאן שום אלמנט הסתברותי. כשנדבר על רגרסיה ליניארית נראה מה מוסיף ההיבט הסטטיסטי.

:::

מודל א-פרמטרי, לא מניח שום הנחה מבנית לגבי האופי של f. ובדרך כלל, פשוט נרצה שf יהיה קרוב לנתונים, יתאר אותם היטב. גם כאן הרבה פעמים תסתתר הנחה שאת f אפשר לתאר כצירוף של פונקציות כן ספציפיות חלקות ממשפחה רחבה, אבל לא בצורה פשטנית כל כך כמו במודל הליניארי.

דוגמא אחת של מודל א-פרמטרי אפשר לראות בשיטת השכנים הקרובים, KNN שנדבר עליה בשיעור הבא בהקשר של קלסיפיקציה בכלל.

דוגמא פחות מוכרת, שמכלילה במובן מסוים את המודל הליניארי, היא ספליינים.

(להסביר על הלוח)

בספליינים אנחנו נרצה לתאר את f כפונקציה שמחברת בין נקודות Y על-ידי פונקציות חלקות כמו פולינומים, שנפגשות זו עם זו, כמה שקרוב יותר לנתונים. נרצה פונקציות כמה שיותר חלקות, וכדי להשיג את החלקות הזאת נוסיף "עונש" למדא על הנגזרת השניה של הפונקציות האלה, כי הנגזרת השניה היא מדד טוב לכמה הפונקציה חלקה. ככל שהנגזרת השניה קטנה יותר, קצב ההשתנות של הפונקציה קטן יותר והיא חלקה יותר. אז עונש למדא גדול יביא אותנו לפונקציות מאוד חלקות אולי אפילו חלקות מדי ומוגבלות שאין להן הרבה חופש לזוז, ולהיפך עונש למדא קטן.

למה אנחנו רוצים חלקות? כי אנחנו מניחים שf היא פונקציה שמשתנה לאט, שהנוף הזה של f בהינתן הפיצ'רים בX הוא לא חד ומקוטע אלא נוף יפה של "גבעות ועמקים". ומה יקרה אם נוותר על האילוץ של החלקות או אפילו נקטין את הפרמטר של למדא מאוד? שום דבר לא יגביל את המודל שלנו למצוא את הפולינומים שפשוט עוברים דרך הנקודות! אם נניח שכן נדרוש איזו רציפות זה ממש יכול להיראות קו מקוטע שיעבור בין כל זוג נקודות. וזה כנראה לא רעיון טוב, ותיכף נראה את זה בפועל.

מכל מקום ספליינים הם דוגמא מעולה למודל א-פרמטרי שנקבע אך ורק מהדאטא, גם אם בפועל הוא מורכב מקבוצה של פולינומים.

:::

כאן יש לנו בעיה של מידול הכנסה, כפונקציה של השכלה וותק, כל נקודה כאן היא תצפית עם השכלה וותק והגובה הוא ההכנסה באלפי דולרים. בקיצון האחד יש לנו מודל פרמטרי למהדרין, המודל הלינארי. בדו-מימד מה שהוא מתאים כאן זה בעצם מישור, והוא עושה את זה על-ידי אמידת 3 פרמטרים בלבד: המקדם להשכלה, המקדם לותק וחותך.

באמצע יש לנו ספליין שמסוגל לתאר איזשהו משטח או יריעה מורכבים יותר. קשה לתאר מה רואים כאן אבל זה נראה סביר.

בקיצון השני יש לנו ספליין עם פרמטר למדא כמעט אפסי, שמאפשר ליריעה להיות מקוטעת ומקומטת כרצונה עד שהיא פשוט עוברת דרך כל הנקודות.

זה כבר פחות סביר, זה נראה מאולץ מדי ונראה שאם יגיעו אנשים חדשים שנרצה לחזות את ההכנסה שלהם המודל הזה מושפע מדי מהנתונים שראה ולא יעשה עבודה טובה. מה שמביא אותנו למושג החשוב: אוברפיטינג.

:::

=== 4. אוברפיטינג ===

כדי לדבר על אוברפיטינג צריך להזכיר שוב שבפועל אנחנו עובדים עם מדגם אימון ומדגם מבחן שהמודל לא ראה, או טריינינג סט וטסט סט.

המטרה שלנו היא להיות טובים על הטסט סט, על תצפיות שלא ראינו. לכן אנחנו מקפידים לחלק את הנתונים לטריין ולטסט, ללמוד את המודלים שלנו על הטריין ולבדוק את הביצועים שלהם על הטסט.

ברור שיש חלוקות אחרות כמו חלוקה לשלושה חלקים או קרוס ולידיישן, אבל בכל מקרה תמיד יש השוואה בין ביצועי המודל על דאטא ששימש ללמידה, וביצועיו על דאטא שהוא לא ראה, שהם בדרך כלל אינדיקציה הרבה יותר טובה לטיב המודל, ולא משנה אגב אם המטרה הסופית היא חיזוי או הסקה. גם מודל שמשמש להסקה, שבו אנחנו רוצים להבין כיצד X משפיע על Y, הוא מודל שנרצה שיראה ביצועים טובים על X וY חדשים.

:::

נניח שקיבלנו חיזוי, כלומר החיזוי הוא לא משתנה מקרי הוא קבוע, אנחנו מתנים עליו. איך אפשר לפרק לוס ריבועי?

התוחלת כאן היא על המשתנה המקרי היחידי, שהוא Y, אבל בעצם כשאני מחליף את Y במודל אני מקבל שאפסילון הוא המשתנה המקרי.

בשלב הזה, כדאי להזכיר שהביטוי שלפנינו מוגדר היטב. במקרה של משתנה רציף מדובר באינטגרל של פונקציה של אפסילון, כפול הצפיפות של אפסילון, מסומנת כאן בh. לדוגמא אם נניח שאפסילון מתפלג נורמלית עם תוחלת אפס ושונות סיגמה בריבוע הצפיפות היא הצפיפות הנורמלית המוכרת, ומכאן אפשר להמשיך לפתח את האינטגרל וכולי. אבל בשביל השורה התחתונה שלנו לא צריך את האינטגרל, אפשר להישאר ברישום הסימבולי של תוחלת.

(להדגים)

 אם אני פותח את הריבוע, אני מקבל תוחלת של f פחות f האט בריבוע, תוחלת של אפסילון בריבוע ועוד פעמיים התוחלת של אפסילון כפול f פחות f האט. אבל כאמור f האט כרגע הוא לא משתנה מקרי הוא נתון, לכן יש כאן תוחלת רק על אפסילון מוכפלת פי איזשהו קבוע, והתוחלת של אפסילון היא אפס לכן כל הביטוי מתאפס. נשארנו עם תוחלת של f פחות f האט בריבוע ועוד התוחלת של אפסילון בריבוע, אבל זה בדיוק שווה לשונות של אפסילון.

אנחנו רואים שניתן לפרק את שגיאת החיזוי הריבועית לשני ביטויים. לביטוי הראשון הראשון נהוג לקרוא הרידוסיבל ארור, ולשני שהוא השונות של אפסילון האירדוסיבל ארור.

:::

הערות חשובות לגבי התוצאה שקיבלנו.

הערה ראשונה: איזו טעות נרצה שתהיה קטנה? שתיהן, אבל את האירדוסיבל ארור אנחנו לא יכולים באמת להפחית. היא נובעת מהרעש הטבעי במדידה אם יש כזה, ומהמשתנים שלקחנו בחשבון. זכרו את הדוגמא עם שקית החטיפים, יכול להיות שיש עוד משתנים שאנחנו יכולים לקחת כדי להקטין את הרעש בחיזוי מספר החטיפים בשקית או להפחית את האירדוסיבל ארור -- אנחנו פשוט מניחים שזה לא מעשי. והמיקוד שלנו יהיה להפחית את הטעות הרידוסיבל, לדאוג שהחיזוי שלנו יהיה כמה שקרוב לפונקציה f.

הערה שניה - כשכותבים את הטעות הריבועית בצורה כזאת מבינים מתמטית גם למה f האמיתי הוא החיזוי הטוב ביותר עבור y. זה אינטואיטיבי, ברור, אבל כאן ממש אפשר לראות שf האמיתי הוא היחיד שיכול להביא את השגיאה הרידוסיבל, לאפס. ולא רק זה, מהו f האמיתי? הוא התוחלת המותנית של Y בהינתן X. שימו לב, בכל מודל עם Y רציף כשהטעות היא טעות ריבועית, לא חשוב אם זה רגרסיה או רשת נוירונים, התוחלת המותנית של Y בהינתן X הוא האומד שיביא לאפס את הטעות הרדוסיבל, כלומר ה"ממוצע" של Y עבור כל X, המשטח הזה שנקווה שהוא חלק או שהוא משתנה לאט, שהוא ניתן לחיזוי.

אז אמרנו שהרידוסיבל ארור היא הטעות שהמודל שלנו יכול להפחית. אבל מה עלול לקרות בפועל?

:::

בפועל, אם נסתכל על שלישייה כזאת של X, Y וY האט החיזוי, אנחנו לא יודעים בפער בין Y לY האט מה החלק הרידוסיבל ומה האירדוסיבל!

ואוברפיטינג, קורה במצב הקיצוני כשמודל פשוט לומד לשנן את מדגם הלמידה. עבור כל X שראה הוא יודע לחזות רק את הY שראה. וזה לא חיזוי טוב בהכרח, בפועל חיזוי די רע. אוברפיטינג קורה כשהמודל מנסה להקטין את האירדוסיבל ארור, ולא מאפשר מרווח טעות טבעי לנתונים.

זה קורה הרבה פעמים דווקא במודלים א-פרמטריים שמאפשרים לפונקציה f להיות יותר ויותר גמישה, כמו שראינו ונראה עוד דוגמא.

:::

כאן בשחור אנחנו רואים פונקציה f אמיתית של Y כנגד X, שממנה נדגמו הנתונים שמופיעים כנקודות שחורות. בסימולציה הזאת, חוץ מהנקודות שאנחנו רואים בשחור, נדגמו גם נקודות טסט שאותן אנחנו לא רואים ועליהן בעצם נבחן המודל.

מודל רגרסיה ליניארית בירוק לא גמיש מספיק, הוא מתאים קו ישר לנקודות ואפשר לראות שגם הטריין וגם הטסט ארורז שלו גבוהות. ציר האיקס בגרף מצד ימין מתייחס לדרגות החופש של המודל, שהוא פרמטר שלא דיברנו עליו אבל אינטואיטיבית הוא מבטא את גמישות המודל. רגרסיה הוא מודל מאוד לא גמיש, הוא מאפשר רק לשני פרמטרים להיות "חופשיים", השיפוע והחותך, ולכן דרגות החופש שלו הן 2.

לעומת המודל הליניארי, ספליין עם 5 דרגות חופש בקו הכחול נראה לא רע, הטסט ארור שלו הנמוכה ביותר כאן. וספליין עם 15 דרגות חופש בקו האדום הוא גמיש מדי. אפשר לראות שהוא כל כך גמיש שהוא מנסה ממש לעבור דרך הנקודות, פורמלית אמרנו שהוא מנסה להפחית כבר את האירדוסיבל ארור, הוא ממש מתחיל לשנן את הדאטא. זה אולי מאפשר לו להשיג טריין ארור נמוך כל כך, אבל הטסט ארור שלו על נתונים שהוא לא ראה גבוהה מאוד. סימן ברור לאובר-פיטינג.

ושוב נשים לב לקו השחור המקווקוו כאן שמסמל את השונות של סיגמא בריבוע, שכאן היא ידועה והיא 1 כי כך נבנתה הסימולציה. אפשר לראות שכל הקו הצהוב, שגיאת החיזוי של הטסט נמצא מעליה, כלומר היא בייסליין כמו שהיינו מצפים לשגיאת החיזוי.

:::

נסכם מדוע אנחנו נזהרים כל הזמן, מדוע אנחנו שואפים למודלים פשוטים יותר, עם מעט "דרגות חופש".

מודלים פשוטים יותר הם יותר אינטרפרטביליים, בדוגמת המודל הליניארי הפירוש של כל מקדם רגרסיה ברור, למשל אם הוא חיובי אז למשתנה המתאים יש השפעה "חיובית" על Y.

הם פשוטים יותר להסקה סטטיסטית, כי הם באים לרוב עם הנחות סטטיסטיות בדיוק לצורך מטרה זו.

וחשוב ביותר, הם בדרך כלל מקטינים אוברפיטינג, הם לא גמישים עד כדי כך שהם מתחילים כבר לשנן את הדאטא ולהפחית מהאירדוסיבל ארור, למצוא דפוס איפה שהוא לא קיים.

מנגד הם עלולים להגביר אנדרפיטינג, הם עלולים להיות נוקשים מדי. ולכן יש צורך למצוא את האיזון הנכון, והכל תלוי בסוף במטרת הפרויקט. הרבה פעמים אגב פשרה נחמדה היא שימוש בשני מודלים, מודל שתפקידו להסביר החוצה כיצד Y משתנה בהתאם לפיצ'רים בידיעה שהחיזוי שלו לא אופטימלי, ומודל שהוא קרוב יותר לקופסה שחורה שקשה להסביר, שתפקידו אך ורק חיזוי מיטבי.

:::

נסיים במבט על על מספר מודלים שאנחנו לומדים בקורס מבחינת הגמישות והקלות שבה הם ניתנים לפירוש. בגדול יש איזשהו מתאם שלילי בין הגמישות שמאפשר המודל לבין היכולת לפרש אותו. בקצה באחד יש לנו סוגים שונים של רגרסיה כמו המודל הליניארי או רגרסיית לאסו שמבצעת פיצ'ר סלקשן בילט אין, כלומר המודל הסופי ייטה להיות מודל ליניארי עם אפילו פחות משתנים. ובקצה השני יש את למידה עמוקה, שאמורה להיות מסוגלת באמצעות רשת נוירונים עמוקה מספיק, להיות מסוגלת למדל f גמישה ככל שנרצה, עם סכנה חמורה של אוברפיטינג, ומודל סופי שהוא קופסה שחורה עד כדי כך שיש תחום מחקר שלם שנועד לנסות להסביר מודל מורכב כל כך.

:::

הערה אחרונה, שוב לגבי הרעש והאפשרות שהוא תלוי בנתונים. האם הפירוק שלנו נכון גם במקרה כזה? מסתבר שכן.

עכשיו אנחנו צריכים לעשות תוחלת גם לפי איקס.

נכתוב את האינטגרל המתאים עם איזושהי צפיפות משותפת g. וכעת נפרק את g למכפלת שתי צפיפויות: הצפיפות השולית של איקס שנסמן בk, כפול הצפיפות המותנית של אפסילון בהינתן איקס, שנסמן בh.

כעת יש לנו אינטגרל פנימי על המשתנה אפסילון בהינתן איקס. ואינטגרל חיצוני על איקס.

כשאנחנו פותחים את הריבוע יש גורמים שלא תלויים באפסילון ויכולים לצאת מחוץ לאינטגרל הפנימי. ההפרש f פחות f_hat לא תלוי באפסילון. כאן הוא יוצא בריבוע, וכאן ללא ריבוע. אבל כשהוא יוצא החוצה מה נותר? הביטוי הזה הוא התוחלת של אפסילון בהינתן איקס ואנחנו מניחים שהיא הרי אפס.

אנחנו נשארים עם שני ביטויים. הראשון חיובי, והשני הוא אינטגרל על השונות המותנית של אפסילון בהינתן איקס. כלומר פרמטר שלא תלוי באיקס, השונות השולית של אפסילון, שאותה אנחנו עדיין מסמנים בסיגמה בריבוע.

אז אם אנחנו מסכמים אנחנו עדיין מקבלים ביטוי חיובי שלא תלוי באיקס, ועוד רעש טבעי. אלה בדיוק הטעויות הרידוסיבל, והאירדוסיבל.
:::
