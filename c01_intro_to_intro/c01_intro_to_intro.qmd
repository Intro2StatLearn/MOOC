---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Intro. to Statistical Learning"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Intro to Statistical Learning - Class 1

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
שלום. וברוכים הבאים לשיעור הראשון בקורס מבוא ללמידה סטטיסטית.

:::
:::
---

## What is Statistical Learning? {.title-slide}

---

### What is Statistical Learning?

::: {.incremental}
- Statistical learning is the task of understanding data, and making predictions based on data
- It is a sub-domain of machine learning
- It uses statistics to build models that approximate the data
- There is a diverse set of tools for this task, where different problems calls for different tools
- We will focus on the statistical aspect of the task, and rarely mention computational aspects
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Key Concepts

::: {.incremental}
- The data is a set of features (covariates/ independent variables/ predictors):
  - Continuous
  - Ordered categorical (discrete)
  - Unordered categorical
- Using them we construct a model (learner) of the data
- Usually we use the model to predict a goal:
  - When the goal is continuous the model is a [regression model]{style="color:red;"}
  - When the goal is categorical the model is a [classification model]{style="color:red;"} (classifier)
:::

::: {.fragment}
::: {.callout-note}
Question: Can one use regression for classification and vice versa? We will get back to it later.
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

## Supervised and Unsupervised Learning {.title-slide}

---

### Supervised Learning

::: {.incremental}
- Definition: 
  - Input: Let $x$ be a vector of length $n$, and let $y = f(x) + \varepsilon$, where $E(\varepsilon) = 0$, $Var(\varepsilon) = \sigma^2$, $(\varepsilon, x)$ are independent
  - Goal: learn $f$ given a set of inputs $(X, y)$
- In most cases we can only find a function $\hat{f} \approx f$, an estimator
- The approximation is measured relative to some loss function $L(y, \hat{y})$
  - Regression example: MSE (This will be our common choice), MAE
  - Classification example: misclassification rate, recall and precision
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Supervised Learning: examples

- Goal: Predict the total sales of a product in a given day
  - Covariates: price, geographical region, day of week, holiday, ...
- Goal: Predict if a client of a cellular company will churn
  - Covariates: usage, age, socio-economical status, phone type, ...
- Goal: Personalized medicine (e.g. what is the effective medicine dosage for a *specific* patient)
  - Covariates: genes profile, effectiveness on cell tissues, ...
  - Drug cost is a part of the loss function $L$!

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Supervised Learning: wage prediction

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

wage = pd.read_csv('../datasets/Wage.csv')
wage['education'] = pd.Categorical(wage['education'], ordered=True)

fig, axes = plt.subplots(1, 3, figsize = (3 * 5, 3))
sns.regplot(data=wage, x='age', y='wage', color = 'grey', lowess=True, line_kws=dict(color="blue"), ax=axes[0])
sns.regplot(data=wage, x='year', y='wage', color = 'grey', order=1, line_kws=dict(color="blue"), ax=axes[1])
sns.boxplot(data=wage, x='education', y='wage', ax = axes[2])
axes[1].set_ylabel('')
axes[1].set_xlim((2003 - 0.5, 2009 + 0.5))
axes[2].set_ylabel('')
axes[2].set_xticklabels(range(1, 6))
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Unsupervised Learning

What if there is no $y$?

::: {.incremental}
- Given set of features $x$ find a model that describes properties of the data:
  - Clusters
  - Dependencies
  - Correlations
  - Common factors (PCA)
- Problems in which we model the *covariates data* are called [unsupervised]{style="color:red;"}
- Most of the course: we will focus on supervised learning
:::

::: {.fragment}
::: {.callout-note}
[Semi-supervised]{style="color:red;"}: there exists a goal but it is partially labeled
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

## Parametric and Non-Parametric Models {.title-slide}

---

### Why Estimate $f$?

::: {.fragment}
- For [prediction]{style="color:red;"}: Given a new instance vector $x$, predict: $\hat{y} = \hat{f}(x)$
  - This seems like a reasonable choice since the noise is zero-mean
:::
::: {.fragment}
- For [inference]{style="color:red;"}: Suppose that we learned an estimator $\hat{f}$, we can use it to learn properties of the input, such as:
  - Which variables of $x$ affects $y$?
  - Given a subset of variables $X^{'} \subset X$, is a variable $X^{''} \not\subset X^{'}$ informative for $f(y | X')$?
  - How well can we approximate $y$?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### The Estimation Problem

How do we learn $\hat{f}$?

::: {.incremental}
- Let $T = \{(x_1, y_1 ) \dots (x_n, y_n)\}$ be a training sample of size $n$
  - Note that $x_i = \begin{pmatrix}x_{i1} \\ \vdots \\ x_{ip}\end{pmatrix}$ is a vector of $p$ features (by notation a column vector)
- We assume that there exists a joint distribution $X \times Y$, and that $(x_i, y_i)$ is sampled from it
  - Usually we assume that the samples are iid
- In general most learning methods can be divided into [parametric]{style="color:red;"} methods and [non-parametric]{style="color:red;"} methods
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Parametric Models

- To learn a parametric model we first assume a known parametric form for $f(x)$, and then learn the parameters of $f$
- Example:

::: {.incremental}
  - Assume $f(x) = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p$, namely $f$ is a linear function of the inputs with unknown fixed coefficients
  - Estimate the values of $\beta = \beta_0, \dots, \beta_p$
  - How? For example using least squares: $\min_\beta \sum_i {(y_i - \beta^{T}x_i)^2}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Non-Parametric Models

- Don't assume the form of $f(x)$, the model just wants $f(x)$ to be close to the data
  - Under the assumption that $f(x)$ belongs to a wide family of smooth functions
- Example:

::: {.incremental}
  - Splines – smooth piecewise polynomials.
  - Why do we need the smoothness for?
  - If we remove the smoothness assumption then what is the *best* $f(x)$? Is it a good choice?
  - We will get back to them later in the course
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### From Linear Regression to Splines

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from matplotlib.patches import Patch
from matplotlib.lines import Line2D

from pygam import LinearGAM, s

df = pd.read_csv('../datasets/ISLP_income_data.csv')
X1 = df['Education']
X2 = df['Seniority']
X = np.column_stack((X1, X2))
Y = df['Income']

# Plotting
fig, axes = plt.subplots(1, 3, figsize=(3 * 4.5, 3), subplot_kw={'projection': '3d'})

# Create a grid of values for X1 and X2
X1_grid, X2_grid = np.meshgrid(np.linspace(min(X1), max(X1), 200),
                               np.linspace(min(X2), max(X2), 200))

# Flatten the grid to pass into the model
X_grid = np.column_stack((X1_grid.ravel(), X2_grid.ravel()))

# Define models
models = [LinearRegression(),
          LinearGAM(s(0, n_splines=10, lam=0.1) + s(1, n_splines=10, lam=0.1)),
          LinearGAM(s(0, n_splines=10, lam=0.0001) + s(1, n_splines=10, lam=0.0001))]
titles = ['Linear Regression', 'Smooth Spline', 'Rough Spline']

# Iterating through models:
for model, ax, title in zip(models, axes, titles):
  model.fit(X, Y)

  # Predict Y values
  Y_pred = model.predict(X_grid).reshape(X1_grid.shape)

  # Plot the fitted plane
  ax.scatter(X1, X2, Y, color='red', label='Data')
  ax.plot_surface(X1_grid, X2_grid, Y_pred, color='blue', alpha=0.5)

  # Add vertical lines from data points to the surface
  Y_surface = model.predict(X).reshape(Y.shape)  # Predicted Y values for the actual data points
  for x1, x2, y, y_surface in zip(X1, X2, Y, Y_surface):
      ax.plot([x1, x1], [x2, x2], [y, y_surface], color='black')

  # Labels
  ax.set_xlabel('Education')
  ax.set_ylabel('Seniority')
  ax.set_zlabel('Income')
  ax.set_title(title)

  # Adjust the viewing angle
  ax.view_init(elev=30, azim=-60)

  # Custom legend for the regression plane
  # fit_patch = Patch(color='blue', alpha=0.5, label='Fit')
  # Custom legend for the data points
  # data_points_legend = Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Data points')
  # Adding the legend with custom patches
  # ax.legend(handles=[data_points_legend, fit_patch], loc='upper left')

plt.show()
```

::: {.fragment}
The rough spline seems to be [overfitting]{style="color:red;"} the data.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

## Overfitting {.title-slide}

---

### Train and Test

::: {.incremental}
- The goal of a model is its predictive power, namely how well will it fit on a new/unseen/out-of-sample set of observations.
- The new set of observations is called [test set]{style="color:red;"} (as opposed to the [train set]{style="color:red;"}). 
- Usually:
  - Dividing the data to a train and test sets
  - Learn models on train set, select models based on their performance on the test set
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Prediction Error: first decomposition

- Assume an observation $(x, y)$, $\hat{y} = \hat{f}(x)$ is *given*, $L(y, \hat{y})$ is the MSE
- What is the expected loss?

::: {.fragment}
$E_y[(y - \hat{f}(x))^2] = E[(f(x) + \varepsilon - \hat{f}(x))^2] = [f(x) - \hat{f}(x)]^2 + Var(\varepsilon)$
:::

::: {.fragment}
- First term: [reducible error]{style="color:red;"}
- Second term: [irreducible error]{style="color:red;"}
:::
::: {.fragment}
- Which part should we minimize?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### A taste of overfitting

- But given an observation $(x, y, \hat{y})$, we never know which part of its error is irreducible!

::: {.incremental}
- So the best prediction for a given $x$ is not necessarily $y$!
- Informally, being too close to the data is called overfitting
- Overfitting may get worse for non-parametric models (highly flexible)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Overfitting with Splines

![](images/spline_train_test_demo.png)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Why do we prefer simple models?

- The simpler the more interpretable (Try to explain Splines)
- Simpler for inference
- Reduces overfitting
- But [may increase underfitting]{style="color:red;"}

::: {.fragment}
- So: It all depends on your goal!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Why do we prefer simple models?

```{python}
#| echo: false

import matplotlib.pyplot as plt

# Data for the scatter plot
models = ['Subset Selection', 'Lasso', 'Least Squares', 'GAM, Splines', 'Trees', 'Bagging, Boosting', 'SVM', 'Deep Learning']
x = [1, 1, 4, 7, 7, 10, 8.5, 12]  # Flexibility
y = [10, 9.5, 7, 5, 4.5, 3, 1.5, 1]  # Interpretability

# Create the scatter plot
plt.figure(figsize=(10, 6))

# Annotate each point with the corresponding letter
for i, model in enumerate(models):
    plt.text(x[i], y[i], model, fontsize=12, ha='center', va='center', color='black')#, bbox=dict(facecolor='blue', edgecolor='blue', boxstyle='circle'))

# Set axis labels
plt.xlabel('Flexibility', fontsize=14)
plt.ylabel('Interpretability', fontsize=14)

# Set axis limits and ticks to create space for labels at ends
plt.xlim(-1, 14)
plt.ylim(-1, 11)

# Customizing the x and y axis labels at the ends
plt.xticks([1, 12], ['Low', 'High'], fontsize=12)
plt.yticks([1, 10], ['Low', 'High'], fontsize=12)

# Show the plot
plt.grid(False)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

## The Bias-Variance Tradeoff {.title-slide}

---

### Squared error decomposition

- For regression, take the standard model: $y = f(x) + \epsilon\;,\;\epsilon \sim (0,\sigma^2)$

- Modeling approach (e.g. OLS), given training data $T$, gives model $\hat{f}(x)$

::: {.fragment}
- Assume we want to predict at new point $x_0$, and understand our expected (squared) prediction error: 

$\mathbb{E}_{y_0, T}(y_0 - \hat{f}(x_0))^2 = \mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2$

- Note we treat both the training data $T$ (and hence $\hat{f}$) and the response $y_0$ as random variables in our expectations
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתמקד ברגרסיה, שם אנחנו מניחים שY הוא איזושהי פונקציה f של X ועוד רעש עם תוחלת אפס ושונות סיגמא בריבוע, לאו דוקא מהתפלגות נורמלית. מהו f אם ככה, אם התוחלת של אפסילון היא אפס? f הוא התוחלת המותנית של Y בהינתן הנתונים בX. הערך הנמדד של Y הוא לא בדיוק הערך הצפוי שלו, התוחלת, אלא התוחלת ועוד איזשהו רעש.

כעת אנחנו לוקחים טריינינג דאטא TR וממנו לומדים את f. אנחנו יכולים עכשיו לבדוק את הביצועים של המודל על מדגם הטסט, אבל אני רוצה שנחשוב על כל התהליך הזה של דגימת נתונים, בניית מודל f, למידה שלו מהנתונים - כעל תהליך אקראי. ולהסתכל על התוחלת של הלוס שלנו, לדוגמא ברגרסיה השגיאה הריבועית, על פני ביצוע התהליך הזה הרבה פעמים, אם היה לי טריינינג דאטא קצת שונה כל פעם. רוצה לומר גם f_hat המודל שבנינו הוא משתנה מקרי שמבוסס על מקריות מדגם הלמידה שלנו!

אז תגיע תצפית חדשה X0 וכמו שאמרנו נסתכל על תוחלת הלוס שלה, כאן השגיאה בין Y0 האמיתי שלה, למודל שבנינו f_hat של X0. בהקשר לקו הירוק של פרדיקשן ארור שראינו בשקף הקודם אנחנו בעצם מסתכלים על נקודה בו ועושים לה תוחלת, או מסתכלים על ההתנהגות האופיינית שלה, על פני הרבה מדגמי למידה.

כעת אני רושם את ההפרש כסכום של כמה אלמנטים. כל מה שאני עושה זה לחסר ולהוסיף את f של X0, שהוא התלות האמיתית של Y0 בX0, התוחלת המותנית. וגם מחסר ומוסיף את התוחלת של f_hat. שוב, למה זה דבר שקיים לנו? כי f_hat הוא בעצמו מעין משתנה מקרי שכאילו נדגם מתוך הרבה מודלים סופיים שנובעים ממדגמי למידה שונים.

כעת אנחנו פשוט שמים סוגריים ומסמנים אלמנט A, אלמנט B ואלמנט C. נרצה לפתח את הריבוע ולראות אכן ששגיאת החיזוי שלנו בתוחלת מתפרקת לביטויים מעניינים ולתת להם את השם המתמטי שלהם.
:::
:::

---

Which factors are random variables, dependent on $T$?

$\mathbb{E}(y_0 - \hat{f}(x_0))^2 = \mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2$

$$A = y_0 - f(x_0)$$

$$B = f(x_0) - \mathbb{E} (\hat{f}(x_0))$$

$$C = \mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אילו מהגורמים תלויים באקראיות מדגם הלמידה?

הגורם A - לא תלוי כלל במדגם הלמידה. הוא מבטא איזושהי אמת, רעש טבעי שקיים ולא נוכל להקטין, זהו בעצם האפסילון אפס כלומר כן משתנה מקרי אבל לא תלוי במדגם הלמידה.

הגורם B - נראה בהתחלה שתלוי במדגם הלמידה כי יש בו את המודל הנאמד f_hat, אבל יש כאן תוחלת, כלומר זה קבוע. כך שגורם זה הוא מספר, לא משתנה מקרי, ומה היינו מצפים שיהיה המספר הזה? אפס. אנחנו מקווים שהמודל שלנו עשיר מספיק שבסופו של דבר בתוחלת הוא קירוב טוב ליחס האמיתי f.

הגורם C - כאן בעצם יש את הגורם שתלוי במדגם הלמידה - המרחק של f_hat המודל שלמדנו מהתוחלת שלו. אם אני מעלה את זה בריבוע ולוקח תוחלת מה זה? שונות! שונות המודל f_hat.

כך שעוד לפני שהוכחנו, אתם כבר יכולים להבין שגורם A הוא איזושהי טעות שאין לי הרבה מה לעשות לגביה. גורם B הוא בעצם האפרוקסימיישן ארור, כמה המודל שלנו בתוחלת עשיר מספיק כדי לבטא את הקשר האמיתי בין X לY. וגורם C הוא האסטימיישן ארור, האם יש לי מספיק דאטא כדי שאם אחזור על התהליך הזה עם דאטא קצת אחר אקבל מודל דומה, כלומר כמה קטנה השונות של המודל עצמו.
:::
:::

---

### The bias-variance decomposition

$\mathbb{E} \left( \underbrace{\left(y_0 - f(x_0)\right)}_{A} + \underbrace{\left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)}_{B} + \underbrace{\left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0)\right)}_{C}\right)^2 =$<br><br>
$\;\;\;\;\;\;\;\;\;\;\;= \mathbb{E} A^2 + B^2 + \mathbb{E} C^2 + 2 B \cdot \mathbb{E} A + 2 \mathbb{E} (AC) + 2B \cdot\mathbb{E} C$<br><br>

::: {.fragment}
$\mathbb{E}(A^2) = \sigma^2$ the **Irreducible error** of a perfect model which knows the true $f$ 

$B^2 = \left(f(x_0) - \mathbb{E} (\hat{f}(x_0))\right)^2$ is the **squared bias** --- a measure of approximation error (note $B$ is not a random variable)

$\mathbb{E}(C^2) = \mathbb{E} \left(\mathbb{E} (\hat{f}(x_0)) - \hat{f}(x_0) \right)^2$ is the **variance** of the prediction --- a measure of estimation error

$B \cdot\mathbb{E} A = \mathbb{E} (AC) = B \cdot \mathbb{E} C = 0$ due to independence and mean-0 relations
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת כשאני מעלה את סכום שלושת הגורמים בריבוע אני מקבל את כל אחד מהם בריבוע ועוד 2 כפול מכפלה של כל זוג. כשאני לוקח תוחלת, אני טוען שאנחנו נשארים עם הביטוי שלפנינו. מדוע? נסתכל שוב על הגורמים שקיבלנו, בריבוע:

התוחלת של A בריבוע היא התוחלת של אפסילון בריבוע, כלומר היא השונות של אפסילון, שהיא סיגמא בריבוע. אנחנו קוראים לזה irreducible error, זה הרעש הקיים בטבע, שגם תחת מודל מדויק לא נצליח להפחית. אפסילון לא תלוי בדאטא שלנו, בהגדרה אין לנו דאטא לחזות אותו.

הגורם השני -- B בריבוע -- הוא כאמור קבוע, לא משתנה מקרי, והוא מבטא את האפרוקסימיישן ארור, עד כמה המודל שלנו בתוחלת עשיר מספיק וקרוב לf האמיתית. היינו רוצים שזה יהיה אפס, שלא תהיה הטיה בילט אין במודל שלנו. אבל אולי למשל לא הכנסנו את כל המשתנים שצריך, ויש הטיה, אז זאת מעין הטיה בריבוע, ואכן אנחנו קוראים לזה squared bias.

הגורם השלישי -- C בריבוע -- הוא כמו שאמרנו הvariance של החיזוי בנקודה X0, וזה מדד לאסטימיישן ארור. מודל עם שונות נמוכה, גם אם אקח דאטא קצת אחר אקבל חיזוי מאוד דומה, האמידה תהיה יציבה. מודל עם שונות גבוהה -- זוכרים את עצי ההחלטה? -- אם אקח דאטא קצת אחר אקבל חיזוי שונה, האמידה לא יציבה. ואמרנו שמתכון לטיפול בטעות כזאת יכול להיות למשל להגדיל את מדגם הלמידה, כאן הוא בא לידי ביטוי. איפה ראינו את זה כבר, מתמטית? כשדיברנו על ממוצע המדגם המקרי, שהשונות שלו היא סיגמא בריבוע חלקי n, וככל שn גדול יותר ככה היא תקטן. הרי גם בממוצע המדגם אפשר לראות עם קצת מאמץ מודל חיזוי פשוט.

מה נשאר לנו? להראות שכל שאר הביטויים של מכפלות הם אפס.

התוחלת של הגורם A, היא התוחלת של אפסילון, היא אפס. אז כל הביטוי של B כפול התוחלת של A הוא אפס.

התוחלת של C, היא התוחלת של f_hat פחות התוחלת של f_hat, כלומר גם היא 0 וכל הביטוי של B כפול התוחלת של C הוא אפס.

התוחלת של מכפלת A ו-C היא גם כן אפס. כי A ו-C הם משתנים מקריים בלתי תלויים לכן התוחלת של המכפלה שלהם היא מכפלת התוחלות והתוחלת של כל אחד מהם היא אפס.

:::
:::

---

### The bias-variance decomposition

$$\mathbb{E}(y_0 - \hat{f}(x_0))^2 = \text{irreducible error} + \text{squared bias} + \text{variance}$$

- Our general intuition: as model flexibility increases, bias (approximation error) decreases and variance (estimation error) increases 

- For many models we can calculate and show these effects on the bias and variance of the model

![](images/bias_variance_tradeoff_demo.png)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כך שאנחנו רואים ששגיאת החיזוי שלנו בשורה התחתונה, על דאטא שהמודל לא ראה, היא סכום הגורמים הריבועיים: שגיאת רעש טבעי שאינה תלויה בדאטא או במודל. שגיאת הטייה ריבועית, שתלויה במודל אבל לא בדאטא. ושגיאת שונות המודל שתלויה גם במודל וגם בדאטא.

באופן כללי אנחנו רואים שהמתמטיקה מסתדרת עם האינטואיציה שלנו, ואפשר ממש לחשב את הטעויות האלה ולהראות שככל שהמודל מורכב יותר הביאס או אפרוקסימיישן יורד, והוריאנס או האסטימיישן עולה.

לדוגמא ברגרסיה ליניארית, ככל שנוסיף עוד משתנים המודל יהיה מורכב ומדויק יותר, ההטיה תרד ותרד. ומצד שני השונות תגדל ותגדל, האומדים שלנו ייהפכו יותר ויותר לא מדויקים ונצטרך יותר ויותר דאטא כדי שזה לא יתדרדר.

ניזכר בהדגמה מהקורס הקודם...
:::
:::

---

## The Classification Setting {.title-slide}

---

### Misclassification rate

::: {.incremental}
- Let $y \in \{0, 1\}$
- For a classifier $\hat{f}(x)$, define the indicator $I(y \neq \hat{y})$
- For a sample $T = \{(x_1, y_1 ) \dots (x_n, y_n)\}$ the [training error]{style="color:red;"} (misclassification rate) is: $\frac{1}{n}\sum_i I(y_i \neq \hat{y}_i)$
- For unknown observations $(x_0, y_0)$ we are interested in the *expected* error rate, but we look at [test error]{style="color:red;"}: $\frac{1}{m}\sum_i I(y_{i,0} \neq \hat{y}_{i,0})$
- This error rate can be decomposed into three terms as well!
:::

::: {.fragment}
We want a predictor which drives the error rate to minimum. What is that predictor?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Bayes decision boundary

Assume the conditional probability $P(Y = 1 | X)$ is a nice, slowly changing function of $X$:

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Define a more complex logistic function for P(Y = 1 | X1, X2) to create a wavy decision boundary
def logistic_function(x1, x2):
    return 1 / (1 + np.exp(- (np.sin(x1) + np.cos(x2) + 0.5 * x1 * x2 - 1)))

# Create a grid of X1 and X2 values
x1 = np.linspace(-3, 3, 300)
x2 = np.linspace(-3, 3, 300)
X1, X2 = np.meshgrid(x1, x2)

# Compute P(Y = 1 | X1, X2) over the grid
P = logistic_function(X1, X2)

# Plot the heatmap
plt.figure(figsize=(8, 5))
plt.contourf(X1, X2, P, levels=50, cmap='viridis')
plt.colorbar(label='$P(Y = 1 | X1, X2)$')

# Plot the decision boundary where P(Y = 1 | X1, X2) = 0.5
contour = plt.contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2)
plt.clabel(contour, fmt = '%.1f', colors = 'red')

# Labels and title
plt.xlabel('$X1$')
plt.ylabel('$X2$')
plt.show()
```

::: {.fragment}
The [Bayes decision boundary]{style="color:red;"} is where $P(Y = 1 | X) = 0.5$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The Bayes classifier

- For a $J$-class classification problem, assume we know the probabilities $P(Y = j | X = x) \space \forall j, x$
- The *best* classifier is the [Bayes classifier]{style="color:red;"}: $\hat{f}(x) = \arg\max_j P(Y = j | X = x)$
- For a $2$-class problem we can use the Bayes decision boundary and reach a simpler notation: $\hat{f}(x) = 1 \space \forall x \space s.t. \space P(Y = 1 | X = x) > 0.5 \text{ otherwise } 0$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### How is Bayes classifier "best"?

$E_Y(I(y \neq \hat{y})| X) = P(Y = 1 | X) \cdot I(\hat{y} = 0) + P(Y = 0 | X) \cdot I(\hat{y} = 1)$

::: {.incremental}
- Suppose $P(Y = 1 | X) = 0.7$, between $\hat{y} \in \{0, 1\}$ choose $\hat{y} = 1$ to minimize $E_Y(I(y \neq \hat{y})| X)$
- Suppose $P(Y = 1 | X) = 0.3$, between $\hat{y} \in \{0, 1\}$ choose $\hat{y} = 0$ to minimize $E_Y(I(y \neq \hat{y})| X)$
- Put differently the Bayes classifier $\hat{f}(x) = \arg\max_j P(Y = j | X = x)$ minimizes expected error rate!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The Bayes classifier is purely theoretical!

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Define a more complex logistic function for P(Y = 1 | X1, X2) to create a wavy decision boundary
def logistic_function(x1, x2):
    return 1 / (1 + np.exp(- (np.sin(x1) + np.cos(x2) + 0.5 * x1 * x2 - 1)))

# Create a grid of X1 and X2 values
x1 = np.linspace(-3, 3, 300)
x2 = np.linspace(-3, 3, 300)
X1, X2 = np.meshgrid(x1, x2)

# Compute P(Y = 1 | X1, X2) over the grid
P = logistic_function(X1, X2)

# Simulate random points within the grid for the training set
np.random.seed(42)  # For reproducibility
X1_train = np.random.uniform(-3, 3, 500)
X2_train = np.random.uniform(-3, 3, 500)

# Calculate the probability for each training sample point
P_train = logistic_function(X1_train, X2_train)

# Generate Y values based on the probabilities for the training set
Y_train = np.random.binomial(1, P_train)

# Plot the heatmap
plt.figure(figsize=(8, 5))
heatmap = plt.contourf(X1, X2, P, levels=50, cmap='viridis')
plt.colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

# Plot the decision boundary where P(Y = 1 | X1, X2) = 0.5
decision_boundary = plt.contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
plt.clabel(decision_boundary, fmt = '%.1f', colors = 'red')

# Plot the simulated training points
plt.scatter(X1_train[Y_train == 1], X2_train[Y_train == 1], color='yellow', label='Y = 1 (Train)', edgecolor='black')
plt.scatter(X1_train[Y_train == 0], X2_train[Y_train == 0], color='blue', label='Y = 0 (Train)', edgecolor='black')

# Labels and title
plt.xlabel('$X1$')
plt.ylabel('$X2$')
plt.show()
```

::: {.fragment}
Even for this training data, the Bayes classifier will have an *irreducible* error rate! (classes overlap)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### The Bayes error rate

::: {.incremental}
- What is the Bayes classifier error?
- For a given $x$: $1 - \max_j P(Y = j | X = x)$
- Therefore for a test set the expected error rate is given by: $1 - E\left(\max_j P(Y = j | X = x)\right)$
- This bound is the *lowest* error rate achievable (under our assumptions), similar to the irreducible error in regression
- Sampling additional test observations in example we reach test error rate of 0.205
:::

::: {.fragment}
::: {.callout-note}
But in practice the conditional probability is unknown. How can we approximate it?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## The KNN Classifier {.title-slide}

---

### K-Nearest Neighbors

- Let $K$ be a positive integer, and let $T$ be the training set
- Classification rule for new observation $x_0$:
  - Let the $K$-neighborhood $\mathcal{N}(x_0)$ the $K$ points closest to $x_0$ in $T$
  - $\hat{f}(x_0) = \arg\max_j \left[\sum I(y(x) = j) | x' \in \mathcal{N}(x_0)\right]$

::: {.fragment}
- Under what conditions is it an approximation to the Bayes classifier?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### KNN vs. Bayes classifier


```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Define a logistic function with a single wavy decision boundary
def logistic_function(x1, x2):
    return 1 / (1 + np.exp(- (np.sin(x1) + np.cos(x2) + 0.5 * x1 * x2 - 1)))

# Create a grid of X1 and X2 values
x1 = np.linspace(-3, 3, 300)
x2 = np.linspace(-3, 3, 300)
X1, X2 = np.meshgrid(x1, x2)

# Compute P(Y = 1 | X1, X2) over the grid
P = logistic_function(X1, X2)

# Simulate 300 random points within the grid for the training set
np.random.seed(42)  # For reproducibility
X1_train = np.random.uniform(-3, 3, 500)
X2_train = np.random.uniform(-3, 3, 500)

# Calculate the probability for each training sample point
P_train = logistic_function(X1_train, X2_train)

# Generate Y values based on the probabilities for the training set
Y_train = np.random.binomial(1, P_train)

# Plot the heatmap
plt.figure(figsize=(8, 5))
heatmap = plt.contourf(X1, X2, P, levels=50, cmap='viridis')
plt.colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

# Plot the decision boundary where P(Y = 1 | X1, X2) = 0.5
decision_boundary = plt.contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
plt.clabel(decision_boundary, fmt = '%.1f', colors = 'red')

# Plot the simulated training points
plt.scatter(X1_train[Y_train == 1], X2_train[Y_train == 1], color='yellow', label='Y = 1 (Train)', edgecolor='black')
plt.scatter(X1_train[Y_train == 0], X2_train[Y_train == 0], color='blue', label='Y = 0 (Train)', edgecolor='black')

# Labels and title
plt.xlabel('$X1$')
plt.ylabel('$X2$')

# Generate 100 random points within the grid for the testing set
X1_test = np.random.uniform(-3, 3, 200)
X2_test = np.random.uniform(-3, 3, 200)

# Calculate the probability for each testing sample point
P_test = logistic_function(X1_test, X2_test)

# Generate Y values based on the probabilities for the testing set
Y_test = np.random.binomial(1, P_test)

# Predict Y values for the testing set based on the decision rule
Y_pred = (P_test >= 0.5).astype(int)

# Prepare the training and testing data for KNN
X_train = np.vstack((X1_train, X2_train)).T
X_test = np.vstack((X1_test, X2_test)).T

# Fit a KNN classifier with K = 10
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(np.column_stack((X1_train, X2_train)), Y_train)

# Predict probabilities for each point on the grid
Z = knn.predict_proba(np.column_stack((X1.ravel(), X2.ravel())))[:, 1]
Z = Z.reshape(X1.shape)

# Plot the decision boundary created by the KNN classifier
plt.contour(X1, X2, Z, levels=[0.5], colors='orange', linewidths=2)

plt.show()
```

(For $K = 10$ test error rate 0.270, only slightly worse than Bayes error rate!)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### KNN and bias-variance tradeoff

```{python}
#| echo: false

knn1 = KNeighborsClassifier(n_neighbors=1)
knn1.fit(np.column_stack((X1_train, X2_train)), Y_train)

knn2 = KNeighborsClassifier(n_neighbors=200)
knn2.fit(np.column_stack((X1_train, X2_train)), Y_train)

Z1 = knn1.predict_proba(np.column_stack((X1.ravel(), X2.ravel())))[:, 1]
Z1 = Z1.reshape(X1.shape)

Z2 = knn2.predict_proba(np.column_stack((X1.ravel(), X2.ravel())))[:, 1]
Z2 = Z2.reshape(X1.shape)

fig, axes = plt.subplots(1, 2, figsize = (2 * 5, 3))

heatmap = axes[0].contourf(X1, X2, P, levels=50, cmap='viridis')
# axes[0].colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

decision_boundary = axes[0].contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
axes[0].clabel(decision_boundary, fmt = '%.1f', colors = 'red')

axes[0].scatter(X1_train[Y_train == 1], X2_train[Y_train == 1], color='yellow', label='Y = 1 (Train)', edgecolor='black')
axes[0].scatter(X1_train[Y_train == 0], X2_train[Y_train == 0], color='blue', label='Y = 0 (Train)', edgecolor='black')

axes[0].set_xlabel('$X1$')
axes[0].set_ylabel('$X2$')
axes[0].set_title('K = 1')
axes[0].contour(X1, X2, Z1, levels=[0.5], colors='orange', linewidths=2)

heatmap = axes[1].contourf(X1, X2, P, levels=50, cmap='viridis')
# axes[1].colorbar(heatmap, label='$P(Y = 1 | X1, X2)$')

decision_boundary = axes[1].contour(X1, X2, P, levels=[0.5], colors='red', linewidths=2.5)
axes[1].clabel(decision_boundary, fmt = '%.1f', colors = 'red')

axes[1].scatter(X1_train[Y_train == 1], X2_train[Y_train == 1], color='yellow', label='Y = 1 (Train)', edgecolor='black')
axes[1].scatter(X1_train[Y_train == 0], X2_train[Y_train == 0], color='blue', label='Y = 0 (Train)', edgecolor='black')

axes[1].set_xlabel('$X1$')
axes[1].set_ylabel('$X2$')
axes[1].set_title('K = 200')
axes[1].contour(X1, X2, Z2, levels=[0.5], colors='orange', linewidths=2)

plt.show()
```
- How can we efficiently find a neighborhood of an observation?
  - For small $K$: classifier is flexible but has high variance
  - For large $K$: classifier is stable but suffers from high bias
- There is no way to escape this bias-variance tradeoff

::: {.fragment}
- So how to choose $K$?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### How to choose $K$?

```{python}
#| echo: false

# Function to simulate data
def simulate_data(n_samples):
    X1 = np.random.uniform(-3, 3, n_samples)
    X2 = np.random.uniform(-3, 3, n_samples)
    P = logistic_function(X1, X2)
    Y = np.random.binomial(1, P)
    return np.column_stack((X1, X2)), Y

# Number of repetitions
n_repeats = 10

# Values of K to evaluate
K_values = [1, 2, 3, 5, 10, 30, 300]

# Initialize lists to store error rates
train_errors = {K: [] for K in K_values}
test_errors = {K: [] for K in K_values}

# Loop over repetitions
for _ in range(n_repeats):
    # Generate training and testing data
    X_train, y_train = simulate_data(500)
    X_test, y_test = simulate_data(200)
    
    # Loop over different values of K
    for K in K_values:
        knn = KNeighborsClassifier(n_neighbors=K)
        knn.fit(X_train, y_train)
        
        # Predict on training and testing data
        y_train_pred = knn.predict(X_train)
        y_test_pred = knn.predict(X_test)
        
        # Compute error rates
        train_error = 1 - accuracy_score(y_train, y_train_pred)
        test_error = 1 - accuracy_score(y_test, y_test_pred)
        
        # Store error rates
        train_errors[K].append(train_error)
        test_errors[K].append(test_error)

# Compute mean and standard error of the mean for each K
train_error_means = [np.mean(train_errors[K]) for K in K_values]
test_error_means = [np.mean(test_errors[K]) for K in K_values]
train_error_stds = [np.std(train_errors[K]) / np.sqrt(n_repeats) for K in K_values]
test_error_stds = [np.std(test_errors[K]) / np.sqrt(n_repeats) for K in K_values]

# Plotting the results
plt.figure(figsize=(10, 5))
plt.errorbar(1/np.array(K_values), train_error_means, yerr=train_error_stds, label='Train Error', marker='o', capsize=5)
plt.errorbar(1/np.array(K_values), test_error_means, yerr=test_error_stds, label='Test Error', marker='o', capsize=5)
plt.axhline(y=0.205, color='k', linestyle='--')
plt.xticks(1/np.array(K_values), ['1/' + str(K) for K in K_values], rotation='vertical')
plt.xlabel('1/K')
plt.ylabel('Error Rate')
plt.legend()
plt.grid(True)
plt.show()
```

::: {.fragment}
- Sometimes we have prior knowledge, can choose $K$ from theoretical considerations
- In most real-life problems, choosing $K$ requires some additional data we use to validate our model
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
