---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Intro. to Statistical Learning"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Intro to Statistical Learning - Class 1

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
שלום. וברוכים הבאים לשיעור הראשון בקורס מבוא ללמידה סטטיסטית.

:::
:::
---

## What is Statistical Learning? {.title-slide}

---

### What is Statistical Learning?

::: {.incremental}
- Statistical learning is the task of understanding data, and making predictions based on data
- It is a sub-domain of machine learning
- It uses statistics to build models that approximate the data
- There is a diverse set of tools for this task, where different problems calls for different tools
- We will focus on the statistical aspect of the task, and rarely mention computational aspects
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Key Concepts

::: {.incremental}
- The data is a set of features (covariates/ independent variables/ predictors):
  - Continuous
  - Ordered categorical (discrete)
  - Unordered categorical
- Using them we construct a model (learner) of the data
- Usually we use the model to predict a goal:
  - When the goal is continuous the model is a [regression model]{style="color:red;"}
  - When the goal is categorical the model is a [classification model]{style="color:red;"} (classifier)
:::

::: {.fragment}
::: {.callout-note}
Question: Can one use regression for classification and vice versa? We will get back to it later.
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

## Supervised and Unsupervised Learning {.title-slide}

---

### Supervised Learning

::: {.incremental}
- Definition: 
  - Input: Let $x$ be a vector of length $n$, and let $y = f(x) + \varepsilon$, where $E(\varepsilon) = 0$, $Var(\varepsilon) = \sigma^2$, $(\varepsilon, x)$ are independent
  - Goal: learn $f$ given a set of inputs $(X, y)$
- In most cases we can only find a function $\hat{f} \approx f$, an estimator
- The approximation is measured relative to some loss function $L(y, \hat{y})$
  - Regression example: MSE (This will be our common choice), MAE
  - Classification example: misclassification rate, recall and precision
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Supervised Learning: examples

- Goal: Predict the total sales of a product in a given day
  - Covariates: price, geographical region, day of week, holiday, ...
- Goal: Predict if a client of a cellular company will churn
  - Covariates: usage, age, socio-economical status, phone type, ...
- Goal: Personalized medicine (e.g. what is the effective medicine dosage for a *specific* patient)
  - Covariates: genes profile, effectiveness on cell tissues, ...
  - Drug cost is a part of the loss function $L$!

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Supervised Learning: wage prediction

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

wage = pd.read_csv('../datasets/Wage.csv')
wage['education'] = pd.Categorical(wage['education'], ordered=True)

fig, axes = plt.subplots(1, 3, figsize = (3 * 5, 3))
sns.regplot(data=wage, x='age', y='wage', color = 'grey', lowess=True, line_kws=dict(color="blue"), ax=axes[0])
sns.regplot(data=wage, x='year', y='wage', color = 'grey', order=1, line_kws=dict(color="blue"), ax=axes[1])
sns.boxplot(data=wage, x='education', y='wage', ax = axes[2])
axes[1].set_ylabel('')
axes[1].set_xlim((2003 - 0.5, 2009 + 0.5))
axes[2].set_ylabel('')
axes[2].set_xticklabels(range(1, 6))
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Unsupervised Learning

What if there is no $y$?

::: {.incremental}
- Given set of features $x$ find a model that describes properties of the data:
  - Clusters
  - Dependencies
  - Correlations
  - Common factors (PCA)
- Problems in which we model the *covariates data* are called [unsupervised]{style="color:red;"}
- Most of the course: we will focus on supervised learning
:::

::: {.fragment}
::: {.callout-note}
[Semi-supervised]{style="color:red;"}: there exists a goal but it is partially labeled
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

## Parametric and Non-Parametric Models {.title-slide}

---

### Why Estimate $f$?

::: {.fragment}
- For [prediction]{style="color:red;"}: Given a new instance vector $x$, predict: $\hat{y} = \hat{f}(x)$
  - This seems like a reasonable choice since the noise is zero-mean
:::
::: {.fragment}
- For [inference]{style="color:red;"}: Suppose that we learned an estimator $\hat{f}$, we can use it to learn properties of the input, such as:
  - Which variables of $x$ affects $y$?
  - Given a subset of variables $X^{'} \subset X$, is a variable $X^{''} \not\subset X^{'}$ informative for $f(y | X')$?
  - How well can we approximate $y$?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### The Estimation Problem

How do we learn $\hat{f}$?

::: {.incremental}
- Let $T = \{(x_1, y_1 ) \dots (x_n, y_n)\}$ be a training sample of size $n$
  - Note that $x_i = \begin{pmatrix}x_{i1} \\ \vdots \\ x_{ip}\end{pmatrix}$ is a vector of $p$ features (by notation a column vector)
- We assume that there exists a joint distribution $X \times Y$, and that $(x_i, y_i)$ is sampled from it
  - Usually we assume that the samples are iid
- In general most learning methods can be divided into [parametric]{style="color:red;"} methods and [non-parametric]{style="color:red;"} methods
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Parametric Models

- To learn a parametric model we first assume a known parametric form for $f(x)$, and then learn the parameters of $f$
- Example:

::: {.incremental}
  - Assume $f(x) = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p$, namely $f$ is a linear function of the inputs with unknown fixed coefficients
  - Estimate the values of $\beta = \beta_0, \dots, \beta_p$
  - How? For example using least squares: $\min_\beta \sum_i {(y_i - \beta^{T}x_i)^2}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Non-Parametric Models

- Don't assume the form of $f(x)$, the model just wants $f(x)$ to be close to the data
  - Under the assumption that $f(x)$ belongs to a wide family of smooth functions
- Example:

::: {.incremental}
  - Splines – smooth piecewise polynomials.
  - Why do we need the smoothness for?
  - If we remove the smoothness assumption then what is the *best* $f(x)$? Is it a good choice?
  - We will get back to them later in the course
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### From Linear Regression to Splines

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from matplotlib.patches import Patch
from matplotlib.lines import Line2D

from pygam import LinearGAM, s

df = pd.read_csv('../datasets/ISLP_income_data.csv')
X1 = df['Education']
X2 = df['Seniority']
X = np.column_stack((X1, X2))
Y = df['Income']

# Plotting
fig, axes = plt.subplots(1, 3, figsize=(3 * 4.5, 3), subplot_kw={'projection': '3d'})

# Create a grid of values for X1 and X2
X1_grid, X2_grid = np.meshgrid(np.linspace(min(X1), max(X1), 200),
                               np.linspace(min(X2), max(X2), 200))

# Flatten the grid to pass into the model
X_grid = np.column_stack((X1_grid.ravel(), X2_grid.ravel()))

# Define models
models = [LinearRegression(),
          LinearGAM(s(0, n_splines=10, lam=0.1) + s(1, n_splines=10, lam=0.1)),
          LinearGAM(s(0, n_splines=10, lam=0.0001) + s(1, n_splines=10, lam=0.0001))]
titles = ['Linear Regression', 'Smooth Spline', 'Rough Spline']

# Iterating through models:
for model, ax, title in zip(models, axes, titles):
  model.fit(X, Y)

  # Predict Y values
  Y_pred = model.predict(X_grid).reshape(X1_grid.shape)

  # Plot the fitted plane
  ax.scatter(X1, X2, Y, color='red', label='Data')
  ax.plot_surface(X1_grid, X2_grid, Y_pred, color='blue', alpha=0.5)

  # Add vertical lines from data points to the surface
  Y_surface = model.predict(X).reshape(Y.shape)  # Predicted Y values for the actual data points
  for x1, x2, y, y_surface in zip(X1, X2, Y, Y_surface):
      ax.plot([x1, x1], [x2, x2], [y, y_surface], color='black')

  # Labels
  ax.set_xlabel('Education')
  ax.set_ylabel('Seniority')
  ax.set_zlabel('Income')
  ax.set_title(title)

  # Adjust the viewing angle
  ax.view_init(elev=30, azim=-60)

  # Custom legend for the regression plane
  # fit_patch = Patch(color='blue', alpha=0.5, label='Fit')
  # Custom legend for the data points
  # data_points_legend = Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Data points')
  # Adding the legend with custom patches
  # ax.legend(handles=[data_points_legend, fit_patch], loc='upper left')

plt.show()
```

::: {.fragment}
The rough spline seems to be [overfitting]{style="color:red;"} the data.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

## Overfitting {.title-slide}

---

### Train and Test

::: {.incremental}
- The goal of a model is its predictive power, namely how well will it fit on a new/unseen/out-of-sample set of observations.
- The new set of observations is called [test set]{style="color:red;"} (as opposed to the [train set]{style="color:red;"}). 
- Usually:
  - Dividing the data to a train and test sets
  - Learn models on train set, select models based on their performance on the test set
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Prediction Error First Decomposition

- Assume an observation $(x, y)$, $\hat{y} = \hat{f}(x)$ is *given*, $L(y, \hat{y})$ is the MSE
- What is the expected loss?

::: {.fragment}
$E[(y - \hat{y})^2] = E[(f(x) + \varepsilon - \hat{f}(x))^2] = [f(x) - \hat{f}(x)]^2 + Var(\varepsilon)$
:::

::: {.fragment}
- First term: [reducible error]{style="color:red;"}
- Second term: [irreducible error]{style="color:red;"}

- Which part should we minimize?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### A taste of Overfitting

- But given an observation $(x, y, \hat{y})$, we never know which part of its error is irreducible!
- So the best prediction for a given x is not necessarily $y$!
- Informally, being too close to the data is called overfitting
- Overfitting may get worse for non-parametric models (highly flexible).

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Overfitting with Splines

![](images/spline_train_test_demo.png)

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Why do we prefer simple models?

- The simpler the more interpretable (Try to explain Splines)
- Simpler for inference
- Reduces overfitting
- But: [May increase underfitting]{style="color:red;"}

::: {.fragment}
- So: It all depends on your goal!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

### Why do we prefer simple models?

```{python}
#| echo: false

import matplotlib.pyplot as plt

# Data for the scatter plot
models = ['Subset Selection', 'Lasso', 'Least Squares', 'GAM, Splines', 'Trees', 'Bagging, Boosting', 'SVM', 'Deep Learning']
x = [1, 1, 4, 7, 7, 10, 8.5, 12]  # Flexibility
y = [10, 9.5, 7, 5, 4.5, 3, 1.5, 1]  # Interpretability

# Create the scatter plot
plt.figure(figsize=(10, 6))

# Annotate each point with the corresponding letter
for i, model in enumerate(models):
    plt.text(x[i], y[i], model, fontsize=12, ha='center', va='center', color='black')#, bbox=dict(facecolor='blue', edgecolor='blue', boxstyle='circle'))

# Set axis labels
plt.xlabel('Flexibility', fontsize=14)
plt.ylabel('Interpretability', fontsize=14)

# Set axis limits and ticks to create space for labels at ends
plt.xlim(-1, 14)
plt.ylim(-1, 11)

# Customizing the x and y axis labels at the ends
plt.xticks([1, 12], ['Low', 'High'], fontsize=12)
plt.yticks([1, 10], ['Low', 'High'], fontsize=12)

# Show the plot
plt.grid(False)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::

---

## Assessing Model Prediction {.title-slide}

---

### The Bias-Variance Tradeoff

::: {.notes}
::: {style="direction:rtl; font-size:16px"}

:::
:::
