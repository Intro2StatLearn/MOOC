---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Linear Models - Part B"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Linear Models - Part B - Class 4

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Extending the linear model {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשיעור הקודם הצגנו את מודל הרגרסיה הליניארית, אחד המודלים הותיקים והנחקרים במאה השנים האחרונות. בשיעור הזה, ננסה לראות כיצד ניתן להתאים את המודל הלינארי כאשר Y הוא משתנה קטגוריאלי, ונלמד על מודל הרגרסיה הלוגיסטית. לפני זה, נדון בכמה דרכים להגמיש את המודל הליניארי. יש הרבה נושאים כאן, אז נתמקד בהוספת משתנים קטגוריאליים, באינטראקציות וברגרסיה פולינומיאלית.
:::
:::

---

### Categorical features with $k$ levels

::: {.fragment}
When $k = 2$:
:::

::: {.incremental}
- If $x$ indicates whether person "has tattoos" or "no tattoos"
- Define: $x_i =
\begin{cases} 
1 & \text{if person } i \text{ has tattoos,} \\
0 & \text{otherwise.}
\end{cases}$
- Model: $y_i = \beta_0 + \beta_1x_i + \varepsilon_i$
- Meaning: $\beta_0 = E(y|\text{no tattoos}) \quad \beta_1 = E(y|\text{has tattoos}) - E(y|\text{no tattoos})$
:::

::: {.fragment}
::: {.callout-note}
Question: how would you test if "tattoo" has effect on $y$?
:::
:::

::: {.fragment}
::: {.callout-note}
Question: what if $x_i =
\begin{cases} 
1 & \text{if person } i \text{ has tattoos} \\
-1 & \text{otherwise.}
\end{cases}$
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתחיל במשתנים קטגוריאליים בעלי K קטגוריות, כאשר הנפוצים ביותר הם משתנים בינאריים, או משתני אינדיקטור. לדוגמא אנחנו ממדלים זמן שהות של עצורים במעצר ואחת העמודות שלנו כוללת חיווי האם לעצור יש קעקוע או לא.

הדרך להתמודד עם משתנה כזה היא בדרך כלל להגדיר משתנה אינדיקטור, X יקבל את הערך 1 אם לעצור יש קעקוע, ו0 אחרת.

עכשיו אפשר להכניס את X למודל הליניארי בלי בעיה. ומה המשמעות של בטא-אפס ובטא-אחת?

כש-X הוא 1 Y יהיה בטא אפס ועוד בטא אחת ועוד רעש. כשX הוא 0 Y יהיה רק בטא-אפס. כלומר בטא-אפס הוא החיזוי לברירת המחדל, אנשים בלי קעקוע. בטא-אחת יבטא את ההפרש בין זמן המעצר של אנשים עם ובלי קעקוע. באופן מדויק יותר הוא הפרש התוחלת המותנית של Y בהינתן קעקוע לעומת בלי קעקוע.

אז אם אני רוצה לבדוק האם יש השפעה למשתנה קעקוע על זמן המעצר איך אני בוחן הזה? באמצעות בדיקת השערות על בטא-אחת, אם הוא לא שונה במובהק מאפס לא צריך פרמטר נוסף בשביל קעקוע.

אבל זו לא הדרך היחידה לקודד משתנה כזה, למעשה יש הרבה. אחת האפשרויות היא לקודד אותו כ1 לבעלי קעקוע ומינוס 1 לאלה שאין להם קעקוע. במקרה כזה עצרו וחשבו מה המשמעות של בטא-אפס ובטא-אחת. בטא-אפס יהיה החותך כרגיל, זמן המעצר הממוצע על פני כל העצורים, ובטא-אחת יבטא עד כמה נבדלים עצורים עם קעקוע ובלי קעקוע, מעל ומתחת לממוצע הזה. זה לא בדיוק ממוצע, ברור, כי הקבוצות לא חייבות להיות שוות בגודלן, זה מדגיש כמה הקידוד יכול לשנות את פירוש המקדמים.
:::
:::

---

#### Categorical features with $k$ levels: $k > 2$

::: {.incremental}
- If $x$ is a person's profession (carpenter, gardener, or teacher)

::: {.fragment}
::: {.callout-note}
Question: what if we just had $x_{i} =
\begin{cases} 
0 & \text{if person } i \text{ is a teacher} \\
1 & \text{if person } i \text{ is a carpenter} \\
2 & \text{if person } i \text{ is a gardener}
\end{cases}$
:::
:::

- Define $k - 1$ indicators:

::: {.fragment}
$x_{i1} =
\begin{cases} 
1 & \text{if person } i \text{ is a carpenter,} \\
0 & \text{otherwise.}
\end{cases}$, $x_{i2} =
\begin{cases} 
1 & \text{if person } i \text{ is a gardener,} \\
0 & \text{otherwise.}
\end{cases}$
:::

- Why $k - 1$ indicators?
- Model: $y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \varepsilon_i$
- Meaning: $\beta_1 = E(y|\text{carpenter}) - E(y|\text{teacher}); \quad \beta_2 = E(y|\text{gardener}) - E(y|\text{teacher})$
:::

::: {.fragment}
::: {.callout-note}
Question: how would you test if "profession" has effect on $y$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מה קורה כאשר יש יותר מ2 קטגוריות?

לדוגמא משתנה ברגרסיה עם מקצוע של נבדק, ויש לנו 3 רמות: נגר, גנן ומורה.

אז קודם כל נשאלת השאלה למה שלא נמשיך בקו הקודם ונקודד מורה למשל להיות קוד 0, נגר להיות 1 וגנן להיות 2? זה לא לא-נכון, אם אנחנו מאמינים שיש קודם כל איזשהו סדר כזה, שלהיות מורה זה "פחות" מלהיות נגר, ונגר זה "פחות" מלהיות גנן. כאן זה נשמע שרירותי לחלוטין, ולא רק זה, מסתתרת כאן הנחה שמשתנה כזה יכול להימדד בסולם מנה או סולם רווח, שהאינטרוולים בין מורה לנגר, ובין נגר לגנן הם זהים, קבועים, בעלי איזושהי משמעות. וזה כבר ממש לא סביר, אבל ישנם מקרים שאפשר היה לחשוב על קידוד כזה, כשהמשתנה קטגוריאלי אבל עדיין בעל סדר ברור, לדוגמא: חומרת מחלה, או קבוצת גיל.

מכל מקום מה שמקובל לעשות עבור משתנים שאין היררכיה באמת בין הרמות שלהם, נקרא one hot encoding. נגדיר לK הקטגוריות K מינוס 1 משתני אינדיקטור. רמה אחת נשאיר כבייסליין, כאן זה מורה. וכל רמה אחרת תוגדר כמשתנה אינדיקטור שמקבל 1 אם X הוא ברמה הזאת או 0 אחרת.

למה K פחות 1? למה לא להוסיף אינדיקטור כאן ל"מורה"? כי אז תהיה לנו תלות ליניארית מושלמת בין המשתנים שלנו. המשתנה של מורה יהיה בדיוק 1 פחות סכום המשתנים של גנן ונגר. מה שיהווה בעיה מתמטית לפתרון הריבועים הפחותים ועוד נדבר על זה. אנחנו לא באמת צריכים 3 משתנים לתיאור 3 קטגוריות כי הרי איך נדע שנבדק הוא מורה? אם יש לו אפס גם במשתנה נגר וגם במשתנה גנן.

כעת אנחנו מכניסים את הK מינוס 1 משתנים למודל, ושוב נחשוב מה משמעות הבטאות הנאמדות?

עבור מורים נחזה בטא-אפס. עבור נגרים נחזה בטא-אפס ועוד בטא-אחת. עבור גננים נחזה בטא-אפס ועוד בטא-שתיים. כלומר בטא-אפס הוא התוחלת המותנית של מורים, הבייסליין. ובטא-אחת ובטא-שתיים מודדים את ההפרש, התוספת של נגר ממורה וגנן ממורה בהתאמה. 

ושוב נשאל, בסיטואציה כזאת: איך נבדוק את ההשערה האם למשתנה מקצוע יש אפקט על המשתנה התלוי Y? כאן נהיה חייבים להשתמש בהשערה בו-זמנית של שני הפרמטרים בטא-אחת ובטא-שתיים, ולבדוק האם הם שווים אפס או שלפחות אחד לא, כלומר כאן נצטרך לערוך 2 רגרסיות ולהשתמש במבחן F להשוואה בין שני מודלים מקוננים.

ובכל זאת עוד שאלה: מה עם K הוא ממש גדול? הנבדקים שלנו יש להם אלף מקצועות? אפשר לעבוד באותה צורה בדיוק, אבל כאן אני מקווה שאתם חשים בסכנה, שאפשר לכנות פשוט כאוברפיטינג. לעשות OHE על משתנה קטגוריאלי עם כל כך הרבה רמות, סביר להניח שגודל הקבוצה של חלק מהרמות הוא קטן מאוד, והמקדם שנקבל הוא רועש מאוד, הוא אוברפיטד לקבוצה קטנה ולא מייצגת. אכן בעיה, אבל היא לא בסקופ של הקורס שלנו.
:::
:::

---

### Interactions

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression

# Generate the dataset
np.random.seed(42)
n_samples = 100
X = np.random.normal(size=(n_samples, 2))  # Random values in 2D
X[:, 0] *= 3  # Scale the first feature
X[:, 1] *= 3  # Scale the second feature
noise = np.random.normal(size=(n_samples,)) * 0.1 # Noise to add to the output

# Non-linear function with an upward trend
y = 0.5*X[:, 0] + 0.2*X[:, 1] + 0.3*X[:, 0] * X[:, 1] + noise

# Fit a linear regression model
reg = LinearRegression()
reg.fit(X, y)
y_pred = reg.predict(X)

# Plot the data and the fitted plane
fig = plt.figure(figsize=(14, 7))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot of the original data
ax.scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')

# Create a grid for plotting the plane
x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
y_grid = reg.intercept_ + reg.coef_[0] * x1_grid + reg.coef_[1] * x2_grid

# Plot the linear regression plane
ax.plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')

# Draw vertical lines from the plane to the data points
for i in range(n_samples):
    ax.plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

# Set labels and title
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('y')

plt.show()
```

::: {.fragment}
A [multiplicative]{style="color:red;"} effect?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
דרך נוספת להגמיש את המודל הליניארי, היא להתחשב בתופעה שנקראת אינטראקציה בין משתנים. אנחנו רואים כאן איזשהו מישור שהותאם לאיזשהו Y כנגד שני משתנים X1 וX2, וכבר נראה שיש בעיה. אף על פי שהמישור בסך הכל מזהה טרנד נכון, נראה כאילו יכולנו לקבל משטח הרבה יותר טוב. בפינה אחת הוא לא חוזה מספיק גבוה עבור התצפיות, אז היינו רוצים שהמשטח "יעלה" קצת כלפי מעלה, ובפינה אחרת הוא לא חוזה מספיק נמוך עבור התצפיות, אז היינו רוצים שהמשטח "ירד" קצת כלפי מטה.

במילים אחרות נראה שY משתנה בX1 וגם בX2, אבל גם באיזשהו שילוב שלהם, ויש כאן אפקט כפלי, או אפקט מולטיפליקטיבי. שימו לב אני עדיין לא אומר שX1 וX2 תלויים זה בזה, אני פשוט אומר שאי אפשר לנתק את ההשפעה של X2 כשמסתכלים על ההשפעה של X1 על Y ולהיפך. X2 משנה את ההשפעה של X1 על Y, מגביר או מחליש אותה, ולהיפך.
:::
:::

---

### Interactions: residuals plots

```{python}
#| echo: false

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].scatter(X[:, 0], y_pred - y)
axes[0].axhline(y=0, color='r', linestyle='--')
axes[0].set_xlabel('X1')
axes[0].set_ylabel('Residuals')
axes[1].scatter(X[:, 1], y_pred - y)
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('X2')
plt.show()
```

::: {.fragment}
Consider adding an interaction term to the model: $x_1 \cdot x_2$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אחת הדרכים לראות שהמודל האדיטיבי כפי שהוא לא מתאים היא באמצעות תרשימי שאריות. כאן גם עבור X1 וגם עבור X2 מתקבלת תמונה בעייתית מאוד, ככל שהמודל מתרחק ממרכז המשתנים, מאפס, הפיזור הולך וגדל.

אז במקרה כזה, שווה להוסיף למודל אפקט כפלי, וזה נקרא אפקט של אינטראקציה. כשההשפעה של משתנה אחד, מושפעת מההשתנות של משתנה אחר.
:::
:::

---

### Interactions: adding an interaction term

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression

# Generate the dataset
np.random.seed(42)
n_samples = 100
X = np.random.normal(size=(n_samples, 2))  # Random values in 2D
X[:, 0] *= 3  # Scale the first feature
X[:, 1] *= 3  # Scale the second feature
X = np.hstack((X, (X[:, 0] * X[:, 1]).reshape(-1, 1)))  # Add a new interaction feature
noise = np.random.normal(size=(n_samples,)) * 0.1 # Noise to add to the output

# Non-linear function with an upward trend
y = 0.5*X[:, 0] + 0.2*X[:, 1] + 0.3*X[:, 0] * X[:, 1] + noise

# Fit a linear regression model
reg = LinearRegression()
reg.fit(X, y)
y_pred = reg.predict(X)

# Plot the data and the fitted plane
fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot of the original data
ax.scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')

# Create a grid for plotting the plane
x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
y_grid = reg.intercept_ + reg.coef_[0] * x1_grid + reg.coef_[1] * x2_grid + reg.coef_[2] * x1_grid * x2_grid

# Plot the linear regression plane
ax.plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')

# Draw vertical lines from the plane to the data points
for i in range(n_samples):
    ax.plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

# Set labels and title
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('y')

plt.show()
```

$y_i = \beta_0 + \beta_1x_{i1} + \beta_2 x_{i1} + \beta_3 x_{i1} \cdot x_{i2}+ \varepsilon_i$

::: {.fragment}
::: {.callout-note}
Note: in multiplicative models we usually add the features without interactions to the model ("main effects"), even if their P-value is not small.
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשאנחנו מוסיפים את גורם האינטראקציה המשטח שלנו נראה הרבה יותר מתאים לנתונים, וניתן לראות איך למרות שאנחנו עדיין נשארים בפריימוורק של המודל הליניארי, אנחנו כבר לא מתאימים רק מישור, אנחנו כבר משיגים משטח מעניין.

יש כל מיני אתגרים עם אינטראקציות, לא בטוח שכדאי למהר אם ככה להכניס את כל האינטראקציות הזוגיות כמועמדות לרגרסיה. אם יש לכם P משתנים זה אומר עוד P מעל 2 משתנים שנוספו, סדר גודל של P בריבוע.

בעיה אחרת היא מה לעשות אם מקדם האינטראקציה עצמו יוצא מובהק סטטיסטית, אבל המקדמים של האפקטים ה"ראשיים" עצמם לא יוצאים מובהקים. במקרה כזה נהוג בכל זאת להשאיר אותם במודל.

ועוד לא דיברנו על אינטראקציות מסדר גבוה יותר, סדר שלישי ומעלה. נסו לחשוב מה המשמעות של אינטראקציה מסדר שלישי.
:::
:::

---

### Non-linear relations

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression

df = pd.read_csv('../datasets/mtcars.csv')

model = LinearRegression().fit(df[['hp']], df['mpg'])
y_pred = model.predict(df[['hp']])

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.scatterplot(data=df, x='hp', y='mpg', ax=axes[0])
axes[0].plot(df['hp'], y_pred, color='red')
axes[0].set_xlabel('Horsepower')
axes[0].set_ylabel('Miles per Gallon')

sns.scatterplot(x=df['hp'], y=y_pred - df['mpg'], ax=axes[1])
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('Horsepower')
axes[1].set_ylabel('Residuals')

plt.show()
```

::: {.fragment}
Consider adding polynomial terms $x^2_1, x^3_1$, ...
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אינטראקציה היא לא הגורם היחיד שניתן להוסיף על מנת להגמיש את המודל הליניארי. כאן אנחנו רואים נתונים של כ30 מכוניות, ואנחנו מנסים למדל את צריכת הדלק שלהן, מיילים לגלון, כנגד כוח הסוס של המנוע שלהן. מייד ברור שיש יחס יורד, אבל האם הוא ליניארי? הוא נראה יותר עקום. ובאמת תרשים שאריות מראה את זה מייד, נוצרת איזושהי פרסה, שמעידה שהנחות המודל לא מתקיימות.

יש הרבה גורמים שאפשר לנסות להוסיף ועדיין להישאר במודל הליניארי, סינוס, קוסינוס. אבל בואו נתחיל מהוספת גורמים פולינומיאלים, כמו X בחזקת 2, 3...
:::
:::

---

### Plynomial regression: Beware 1

```{python}
#| echo: false

x_hp = df[['hp']].values
X2 = np.hstack((x_hp, x_hp**2))
X5 = np.hstack((x_hp, x_hp**2, x_hp**3, x_hp**4, x_hp**5))
model1 = LinearRegression().fit(x_hp, df['mpg'])
model2 = LinearRegression().fit(X2, df['mpg'])
model5 = LinearRegression().fit(X5, df['mpg'])

X1_range = np.linspace(df['hp'].min(), df['hp'].max(), 100).reshape(-1, 1)
X2_range = np.hstack((X1_range, X1_range**2))
X5_range = np.hstack((X1_range, X1_range**2, X1_range**3, X1_range**4, X1_range**5))
y_pred1 = model1.predict(X1_range)
y_pred2 = model2.predict(X2_range)
y_pred5 = model5.predict(X5_range)

fig = plt.figure(figsize=(6, 5))
sns.scatterplot(data=df, x='hp', y='mpg')
plt.plot(X1_range, y_pred1, color='red', label = 'Linear')
plt.plot(X1_range, y_pred2, color='orange', label = 'Degree 2')
plt.plot(X1_range, y_pred5, color='green', label = 'Degree 5')
plt.xlabel('Horsepower')
plt.ylabel('Miles per Gallon')

plt.legend()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשאנחנו מוסיפים גורמים פולינומיאליים יש שמכנים זאת רגרסיה פולינומיאלית. אבל כאן צריך לנקוט משנה זהירות. כמו שניתן לראות בתרשים, כשאנחנו עוברים מקו ליניארי שהוא מוגבל לפולינום, אנחנו לאט לאט עושים אוברפיטינג חריג לנתונים. פולינום מדרגה חמישית למשל כבר ממש מנסה לגעת בכל הנקודות האפשריות של הנתונים.
:::
:::

---

### Plynomial regression: Beware 1

Quadratic:

```{python}
#| echo: false

import statsmodels.api as sm

X2 = sm.add_constant(X2)
model = sm.OLS(df['mpg'], X2).fit()
model.summary().tables[1]
```

Cubic:

```{python}
#| echo: false

X3 = np.hstack((x_hp, x_hp**2, x_hp**3))
X3 = sm.add_constant(X3)
model = sm.OLS(df['mpg'], X3).fit()
model.summary().tables[1]
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הדרך כמובן להגביל את עצמנו ברגרסיה פולינומיאלית היא להוסיף גורמים בצורה הדרגתית ולעצור אם הם לא מובהקים ונראה שאין בהם צורך, בהסתכלות על מבחנים סטטיסטיים ותרשימי שאריות. כאן אני מוסיף לנתונים של המכוניות X ואז X בריבוע, ואז X בשלישית. ניתן לראות שפולינום ריבועי מתאים היטב לנתונים של המכוניות, שני הגורמים X וX בריבוע מובהקים. כשאנחנו מוסיפים X בשלישית הוא כבר גורם לא חשוב ברגרסיה והוא ממסך גם את ההשפעה של הגורם הריבועי, אז כאן כדאי לעצור ולהישאר עם המודל הריבועי.
:::
:::

---

### Plynomial regression: Beware 2

```{python}
#| echo: false

X1_range = np.linspace(df['hp'].min() - 100, df['hp'].max() + 100, 1000).reshape(-1, 1)
X2_range = np.hstack((X1_range, X1_range**2))
X5_range = np.hstack((X1_range, X1_range**2, X1_range**3, X1_range**4, X1_range**5))
y_pred1 = model1.predict(X1_range)
y_pred2 = model2.predict(X2_range)
y_pred5 = model5.predict(X5_range)

fig = plt.figure(figsize=(6, 5))
sns.scatterplot(data=df, x='hp', y='mpg')
plt.plot(X1_range, y_pred1, color='red', label = 'Linear')
plt.plot(X1_range, y_pred2, color='orange', label = 'Degree 2')
plt.plot(X1_range, y_pred5, color='green', label = 'Degree 5')
plt.xlabel('Horsepower')
plt.ylabel('Miles per Gallon')

plt.legend()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
סכנה נוספת במעבר מקו ליניארי לפולינום: מה קורה כשמתרחקים, או שחוזים על נקודה חדשה מעבר לסקאלה שראינו?

הבעיה הזאת חמורה גם לקו ליניארי, אבל קו ליניארי הוא צפוי ואינטואיטיבי. אם נקודה חדשה תהיה מאוד רחוקה מנקודות שראינו עדיין נחזה עבורה ערך צפוי, אבל בפולינום הרבה פעמים מקבלים ערך לא הגיוני בכלל. כאן כבר בפולינום ריבועי המשמעות היא שעבור ערכים גבוהים של כוח סוס החיזוי יעלה. ועבור פולינום ממעלה 5 החיזוי של צריכת דלק ירד בצורה חדה מתחת לאפס!

אז ראינו כמה דרכים להגמיש את המודל הליניארי. בכל אחת מהדרכים שדיברנו עליהן אבל יש מחיר לשלם, והמחיר הוא פוטנציאל גבוה יותר לאוברפיטינג, ולמודל מורכב מדי שלא יהיו לו ביצועים טובים על נתונים שהמודל לא ראה.
:::
:::

---

## Other issues with linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כבר אמרנו שרגרסיה ליניארית הוא אחד המודלים הנחקרים ביותר בספרות, זה אומר שעל כל ניואנס בה נכתבו תלי תלים של מאמרים. לסיכום הנושא נזכיר שתי בעיות שצריך להיות מודעים אליהן ולקרוא עוד קצת על הטיפול בהן: קוליניאריות ותצפיות חריגות.
:::
:::

---

### Why "not significant"?

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from matplotlib.patches import Patch
from matplotlib.lines import Line2D

# Seed for reproducibility
np.random.seed(42)

# Generate synthetic data
n_samples = 100
X1 = np.random.normal(0, 1, n_samples)
X2 = 0.5 * X1 + np.random.normal(0, 0.1, n_samples)  # X2 is correlated with X1
Y = 0.5 * X1 + 1 * X2 + np.random.normal(0, 1, n_samples)

# Create a DataFrame
data = pd.DataFrame({'X1': X1, 'X2': X2, 'Y': Y})
X = data[['X1', 'X2']].values

# Plotting
fig, ax = plt.subplots(1, 1, figsize=(12, 5), subplot_kw={'projection': '3d'})

# Create a grid of values for X1 and X2
X1_grid, X2_grid = np.meshgrid(np.linspace(min(X1), max(X1), 200),
                               np.linspace(min(X2), max(X2), 200))

# Flatten the grid to pass into the model
X_grid = np.column_stack((X1_grid.ravel(), X2_grid.ravel()))

model = LinearRegression()

model.fit(X, Y)

# Predict Y values
Y_pred = model.predict(X_grid).reshape(X1_grid.shape)

# Plot the fitted plane
ax.scatter(X1, X2, Y, color='red', label='Data')
ax.plot_surface(X1_grid, X2_grid, Y_pred, color='blue', alpha=0.5)

# Add vertical lines from data points to the surface
Y_surface = model.predict(X).reshape(Y.shape)  # Predicted Y values for the actual data points
for x1, x2, y, y_surface in zip(X1, X2, Y, Y_surface):
    ax.plot([x1, x1], [x2, x2], [y, y_surface], color='black')

# Labels
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('y')

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

plt.show()
```

```{python}
#| echo: false

import statsmodels.api as sm

# Perform linear regression with both features
X_both = sm.add_constant(data[['X1', 'X2']])
model_both = sm.OLS(data['Y'], X_both).fit()
print(model_both.summary().tables[1])
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כדי להציג את הבעיה הראשונה בואו נסתכל על הנתונים האלה. Y משתנה בבירור כפונקציה של X1 ושל X2. המישור שאנחנו מתאימים ברגרסיה ליניארית אפילו נראה סביר. אבל אם נבצע הסקה סטטיסטית על מקדמי הרגרסיה שקיבלנו, אף אחד לא מובהק סטטיסטית, נראה כאילו היה מתאים פה המישור השוכב של הממוצע של Y. למה זה קורה? אם נביט היטב נראה שבכל זאת יש משהו חריג בנתונים האלה. אם נשרטט את X2 כנגד X1 נראה בעיה: יש ביניהם קורלציה גבוהה מאוד.
:::
:::

---

### Collinearity

::: {.incremental}
- [Collinearity]{style="color:red;"} is when a feature is in the span of other features
- Usually it is not exactly in the span but near the span
- In the case of exact collinearity: the matrix $X^TX$ is singular, no unique solution
- In the case of approximate collinearity:
  - If $X_1 \approx X_2$, then: $y = X_1; \quad y = X_2; \quad y = \frac{1}{2}X_1 + \frac{1}{2}X_2;\quad y = 1000X_1 - 999X_2$ are the same models!
  - the solution is not numerically stable
  - $SE(\hat{\beta})$ grows, $t_{obs}$ value decreases, feature is "masked" in inference
- Explore your data with a correlation matrix, many other metrics
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
קוליניאריות היא מצב שבו פיצ'ר מסוים, או יותר, הם בספאן של פיצ'רים אחרים, כלומר צירוף ליניארי של משתנים אחרים. ראינו פוטנציאל לזה כשדיברנו על ייצוג של משתני אינדיקטור למשתנים קטגוריאליים עם OHE. דוגמאות שכיחות יותר זה כשמישהו החליט למשל לתת את הטמפרטורה בצלזיוס וגם בפרנהייט. בשני המקרים מדובר במשתנה מיותר שכדאי היה פשוט להוריד.

אבל זאת דוגמא קיצונית, נגיד שקוליניאריות מתרחשת גם כשפיצ'ר אחד הוא בקירוב צירוף ליניארי של פיצ'רים אחרים, כמו למשל שערכתם מחקר על ילדים והחלטתם להכניס למודל גם את הגיל של הילדים וגם את מידת הנעליים שלהם. ובין שני אלה הרי יש מתאם לא מבוטל. או אולי יש לכם פשוט הרבה פיצ'רים, ומה לעשות במקרה יש שם פיצ'רים עם מתאם גדול ביניהם.

מה יכול להיות בעייתי בזה? שימו-לב, לא הנחנו שהעמודות של X הן בלתי תלויות, אבל אם עמודה אחת היא צירוף ליניארי של האחרות אז הדרגה של X תהיה לא מלאה, והמטריצה X'X הזאת שמופיעה באומדן הריבועים הפחותים תהיה סינגולרית, אין לה הופכית ולכן אין גם פתרון.

במצב הפחות קיצוני אבל המדאיג יותר, שבו יש קוליניאריות בקירוב, לדוגמא כשמשתנה X1 דומה מאוד למשתנה X2 כמו בדוגמא שלנו. המודל Y שווה X1 או המודל Y שווה X2. או המודל Y שווה אלף X1 פחות 999 X2 - הם נורא דומים. בפועל, הופכי של X'X יהיה לא יציב נומרית, הוא יכול להיות פתאום נורא גדול. אם ניקח את ערך הטי לפיצ'ר ספציפי, נראה שאם האיבר על אלכסון הX'X נעשה גדול יותר ויותר, זה אומר שטעות התקן או המכנה של הסטטיסטי נעשה גדול יותר ויותר, ובהתאמה ערך הטי קטן יותר, והאיבר הזה לא ייראה כמובהק סטטיסטי. הוא עבר מיסוך בהסקה סטטיסטית.

אז לא ניכנס לכל פתרון אפשרי לטיפול בקוליניאריות, אבל זו בעיה שצריכים להיות מודעים אליה. לכל הפחות הביטו במטריצת הקורלציה בין המשתנים שלכם, וקראו עוד על מטריקות שאמורות להציף את הבעיה.
:::
:::

---

### High-leverage and Outlier observations

::: {.incremental}
- An observation with high [leverage]{style="color:red;"} has an extreme $x$ value

- For simple linear regression: $h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum (x_{i'} - \bar{x})^2}$

- Observations with large $h_i$ $\Rightarrow$ large effect on $SE(\hat{\beta})$

- If such an observation also has an unusual $y$ -- an [outlier]{style="color:red;"} $\Rightarrow$ the model changes a lot if removed
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הנושא השני שנצביע עליו הוא תצפיות חריגות.

באופו ספציפי מעניינות אותנו תצפיות עם מנוף או לברג' גבוה, והכוונה באופן כללי לתצפיות עם ערך X חריג.

ברגרסיה עם משתנה אחד יש אינדקס H שנהוג לחשב לכל תצפית כדי להגדיר את הלברג' שלה. אפשר לראות כאן בעצם יחס בין המרחק הריבועי של התצפית מממוצע הX לעומת סך המרחקים הריבועיים של התצפיות X מהממוצע שלהן. ברגרסיה מרובה אגב יש לנו הכללה לזה אבל לא נרשום אותה כאן.

חשבו מה יכולה להיות הבעיה עם תצפיות עם לברג' גבוה. קודם כל מתמטית כפי שראינו ברגרסיה ליניארית פשוטה, יש להן אפקט גדול נורא על טעות התקן של האומד. אם המכנה הזה של הסטטיסטי טי מתנפח בגלל תצפית אחת, האומד לא יכול להימצא מובהק סטטיסטית כי השונות שלו נורא גבוהה. נקודה שכזאת יכולה להשפיע כל כך על קו הרגרסיה, לכן היא נקראת מנוף, לברג'.

אם בנוסף ערך הY של נקודה כזאת גם חריג, מה שקרוי אאוטלייר, זה אומר שיש לנו תצפית חריגה עם מנוף גבוה - היא יכולה להטות את המודל לגמרי. בואו נראה דוגמא.
:::
:::

---

### High-leverage and Outlier observations

```{python}
#| echo: false

model = LinearRegression().fit(df[['hp']], df['mpg'])
y_pred = model.predict(df[['hp']])

SSx = np.sum((x_hp - np.mean(x_hp))**2)
h = 1 / x_hp.shape[0] + (x_hp - np.mean(x_hp))**2 / SSx

highlight_point1 = {'X1': df['hp'].values[-2], 'Y': df['mpg'].values[-2]}
highlight_point2 = {'X1': h[-2], 'Y': (y_pred - df['mpg'].values)[-2]}

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.scatterplot(data=df, x='hp', y='mpg', ax=axes[0])
axes[0].scatter(highlight_point1['X1'], highlight_point1['Y'], color='red', s=100)
axes[0].plot(df['hp'], y_pred, color='red')
axes[0].set_xlabel('Horsepower')
axes[0].set_ylabel('Miles per Gallon')

sns.scatterplot(x=h.flatten(), y=y_pred - df['mpg'], ax=axes[1])
axes[1].scatter(highlight_point2['X1'], highlight_point2['Y'], color='red', s=100)
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('Leverage')
axes[1].set_ylabel('Residuals')

plt.show()
```

::: {.fragment}
Consider removing (careful!).
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
האמת היא שכבר יש לנו דוגמא מוכנה, ברגרסיה של צריכת דלק של מכוניות כפונקציה של כוח סוס. אם תחשבו את אינדקס הH, תראו שהלברג' של התצפית האחרונה מצד ימין עם הכי הרבה כוח סוס, הוא ממש חריג. רואים את זה בדרך כלל בגרף של שאריות מול לברג'. אנחנו רואים תצפית עם לברג' גבוה שיש לה גם ערך Y קטן מאוד, כדאי לחשוד בה, ומעניין לראות איך היה נראה המודל בלעדיה.

מאוד חשוב להדגיש שאני לא אומר מייד להוריד אותה, ואנחנו בטח לא עושים את זה כדי "לשפר תוצאות". יש גאיידליינז ומטריקות מתי ראוי להוריד תצפית מהנתונים ואנחנו שואפים לא לעשות את זה. אם המדגם קטן אז באמת מעניין לראות איך מתנהג המודל כשמסירים אותה ואז להחליט. ובכל מקרה יש תחום שלם בסטטיסטיקה ששואף לא להוריד תצפיות מהמודל אלא פשוט לאפשר מודל גמיש יותר וחסין להשפעה של תצפיות כאלה, זה נקרא סטטיסטיקה רובסטית או חסינה.
:::
:::

---

#### Excluding high-leverage + outlier

```{python}
#| echo: false

df2 = df.drop(x_hp.shape[0] - 2, axis=0)
model = LinearRegression().fit(df2[['hp']], df2['mpg'])
y_pred2 = model.predict(df[['hp']])

x_hp2 = np.delete(x_hp, -2)
SSx = np.sum((x_hp2 - np.mean(x_hp2))**2)
h = 1 / x_hp2.shape[0] + (x_hp2 - np.mean(x_hp2))**2 / SSx

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.scatterplot(data=df2, x='hp', y='mpg', ax=axes[0])
axes[0].plot(df['hp'], y_pred, color='red', label = 'All obs')
axes[0].plot(df['hp'], y_pred2, color='green', label = 'Excluding outlier')
axes[0].set_xlabel('Horsepower')
axes[0].set_ylabel('Miles per Gallon')
axes[0].legend()

sns.scatterplot(x=h.flatten(), y=y_pred2[:-1] - df2['mpg'], ax=axes[1])
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('Leverage')
axes[1].set_ylabel('Residuals')

plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אם אנחנו מסירים את התצפית המודל אכן משתנה בצורה ניכרת לעין למרות שמדובר בתצפית אחת מתוך יותר מ30, וגרף השאריות מול לברג' נראה הרבה יותר טוב. מטריקה חשובה לקרוא בהקשר זה היא הקוקס דיסטנס.

נסיים כאן את הדיון ברגרסיה ליניארית. נזכיר שאפשר לפתח כל נושא כאן לשיעור שלם, ומי שרוצים להעמיק במודל הכל כך מפורסם הזה, יש להם אינספור ספרים בכל מיני רמות לקרוא עליו.
:::
:::

---

## Logistic Regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
התחלנו ברגרסיה, כשY הוא ממשי, ונעבור עכשיו לקלאסיפיקציה, כשY קטגוריאלי.
:::
:::

---

### What about classification?

- Let's start with the simplest (and most important) case of two-class classification: 
    - Sick vs. healthy
    - Buy vs. won't buy
    - Dog vs. cat

- As before, we have $T = (\mathbf{X}, \mathbf{y})$ a sample of size $n$

- For now, keep assuming $x \in \mathbb{R}^p$ is numeric

::: {.fragment}
- Can we use the OLS mechanism we have built to build a classification model? 
:::

::: {.fragment}
- For sure we can, if we encode $y=\text{sick} \Rightarrow y=1,\;\;y=\text{healthy} \Rightarrow y=0$, we have numeric $y$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נתמקד בבעיה הפשוטה שY הוא אחת משתי קטגוריות אפשריות, כלומר הוא בינארי, ובתרגול תדברו קצת יותר על Y שיש לו יותר משתי קטגוריות.

דוגמאות לY בינארי יכולות להיות למדל האם האדם נשא של נגיף או לא, האם צרכנית תקנה או לא תקנה מוצר כלשהו, האם התמונה שלפנינו היא של כלב או של חתול.

אנחנו עדיין ממדלים באמצעות הטריין סט, מדגם הלמידה בגודל n שמורכב מזוגות תצפיות של X ושל Y. ונניח גם שהאיקסים שלנו ממשיים.

ואז נשאלת השאלה: האם ניתן להשתמש בכל התוצאות שלנו מרגרסיה ליניארית עבור Y שהוא קטגוריאלי עם שתי קטגוריות?

התשובה היא שברור שכן, פשוט צריך להפוך אותו לנומרי, נגדיר שחולה זה 1, ובריא זה 0, והנה יש לנו Y ממשי וכל הפונקציות הרלוונטיות יעבדו. לא נקבל שגיאה.
:::
:::

---

### What is wrong with using OLS for classification? 

- If we encode $y$ as above what is $E(y|x)$? It is $P(y=\text{sick}|\;\text{data})$ --- a clearly interesting quantity

::: {.incremental}
- Problem: as a probability, $0\leq P(y=\text{sick}|\;\text{data}) \leq 1.$ But model predictions $x^T\hat{\beta}$ can fall outside the legal range!

- Another problem: can we make the model assumptions of normal $\varepsilon$? No --- because $y$ can only be $0$ or $1$
:::

::: {.fragment}
- The idea: try to create an approach that is similar to OLS, but more fitting for classification, taking into account the limited range of values and the need for a sensible statistical model
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איפה פה בכל זאת השגיאה, למה זה לא פתרון טבעי לבעיה?

נזכור שמה אנחנו ממדלים בעצם ברגרסיה ליניארית? את התוחלת המותנית של Y בהינתן X, את מה שלמדנו על Y בממוצע בעקבות המידע על המשתנים האחרים. ומה זו התוחלת של משתנה שיש לו שתי תוצאות, אפס או 1? כמו משתנה ברנולי, זו ההסתברות שהוא יקבל 1 או ההסתברות שהאדם חולה. בדוגמא שלנו, בהינתן שראינו נאמר את נתוני הבדיקה עליו, כמו לחץ דם, נתוני ביופסיה, נתונים גנטיים וכולי.

זה מצוין. אבל הסתברות היא כמות בין אפס לאחת! ואין שום אילוץ ברגרסיה ליניארית לקבל תחזיות בין אפס לאחת. נוכל לקבל חיזויים קטנים מאפס, גדולים מאחת, ונוכל לקבל אפילו סט של תחזיות שכולן מתחת לאפס או כולן מעל 1. ואז מה נעשה?

ניתקל בעוד בעיה כשנרצה לבצע הסקה סטטיסטית: האם סביר להניח שY הוא צירוף ליניארי של משתנים ועוד רעש נורמלי, אפסילון, בלתי תלוי? לא, כי וואי מקבל ערכים אפס או אחת.

אז אנחנו צריכים גישה אחרת. גישה שכן תתחשב בערכים האפשריים שY יכול לקבל ושתתאים מודל סטטיסטי הולם לבעיה.

:::
:::

---

### Logistic regression

- Deals with the two problems above

- We start from assuming a model: 
$$\log\frac{P(y=1|x)}{P(y=0|x)} = \log\frac{P(y=1|x)}{1 - P(y=1|x)} = \text{logit}(P(y=1|x)) = x^T\beta$$

- Notice that now all values are legal: 
$$ 0\leq P(y=1|x) \leq 1 \;\; \Leftrightarrow\;\; -\infty \leq \log\frac{P(y=1|x)}{P(y=0|x)} \leq \infty.$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
גישה כזאת היא רגרסיה לוגיסטית.

אנחנו לא ממדלים את Y עצמו כמודל ליניארי. אלא את הכמות הבאה: לוג, של ההסתברות שY שווה 1, חלקי ההסתברות שY שווה אפס. הכמות הזאת היא תהיה צירוף ליניארי של המשתנים באיקס. ניתן גם לרשום את זה כלוג של ההסתברות p חלקי 1 פחות p, כך שאנחנו רואים שבמקום למדל את התוחלת של Y בהינתן X אנחנו ממדלים איזושהי פונקציה G שלה. הפונקציה הזאת די מפורסמת וקוראים לה גם לוג'יט.

מכל מקום כעת הכמות שאנחנו ממדלים היא בין מינוס אינסוף לאינסוף ולכן כל חיזוי שלנו יהיה כמות לגיטימית.

:::
:::

---

### The Sigmoid function

::: {.fragment}
Another way of writing this:

$P(y=1|x) = \text{sigmoid}(x^T\beta) = \frac{\exp(x^T\beta)}{1+\exp(x^T\beta)}$

$P(y=0|x) = 1- P(y=1|x) = \frac{1}{1+\exp(x^T\beta)}$
:::

::: {.fragment}
```{python}
#| echo: false

import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-10, 10, 101)
plt.plot(x, 1/(1 + np.exp(-x)))
plt.xlabel('z')
plt.ylabel('sigmoid(z)')
plt.show()
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אם יש לנו את המקדמים ואנחנו רוצים לקבל בחזרה את ההסתברות החזויה, את הכמות בין אפס לאחת, אפשר לראות שהפונקציה ההופכית נראית כך: e בחזקת הצירוף הליניארי, חלקי 1 ועוד e בחזקת הצירוף הליניארי. או ההסתברות המשלימה עם נוסחה קצת יותר נעימה שמופיעה כאן.

הפונקציה הזאת נקראת פונקצית הזיגמויד, וחשוב להכיר אותה כי היא מקיימת תכונות שטובות לנו, ואחר-כך אנחנו רואים אותה כשאנחנו לומדים על רשתות נוירונים. עבור ערכים שליליים מאוד היא שואפת לאפס, עבור ערכים חיוביים מאוד היא שואפת לאחת. ובאמצע - היא מונוטונית עולה. מה שמתאים לנו, כי יחס ליניארי הוא בהכרח גם מונוטוני. אם אנחנו ממדלים למשל את הסיכוי שלקוח יחזיר הלוואה, ואחד המשתנים המסבירים הוא הכנסה. ונאמר שהתקבל המקדם "פלוס 10". זה אומר שנצפה שככל שההכנסה של לקוח עולה, כך עולה הסיכוי שיחזיר הלוואה. ופונקצית הזיגמויד היא לא האפשרות היחידה שלנו להשיג את האפקט הזה אבל היא בהחלט שימושית.
:::
:::

---

### Fitting a logistic regression

- Given training data $T$, we want to find the best coefficients $\hat{\beta}$

- This is done by maximum likelihood, finding $\beta$ to maximize:
$$L(\beta|X, y) = \prod_{i = 1}^n{P(y_i|x_i;\beta)} = \prod_{i = 1}^n{P(y_i = 1|x_i;\beta)^{y_i}P(y_i = 0|x_i;\beta)^{1-y_i}}$$

::: {.fragment}
$$\max_\beta \prod_{i=1}^n  \left(\frac{\exp(x_i^T\beta)}{1+\exp(x_i^T\beta)}\right)^{y_i} \left(\frac{1}{1+\exp(x_i^T\beta)}\right)^{1-y_i}$$
:::
::: {.fragment}
- The solution is $\hat{\beta}$, the logistic regression coefficients estimates
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז איך אנחנו מוצאים את המקדמים במקרה של רגרסיה לוגיסטית?

כאן אנחנו חייבים לעבור דרך פונקצית הנראות, הlikelihood ולבצע אמידת נראות מקסימלית. פונקצית הנראות שלנו היא פונקציה של בטא בהינתן הדאטא בטריינינג ומסומנת בדרך כלל בL. במקרה הבדיד כמו לפנינו שY הוא בעצם משתנה ברנולי, מדובר במכפלת ההסתברויות במדגם תחת המודל.

מאחר שY מקבל ערכים אפס או אחת, ניתן לרשום את הביטוי בצורה כזאת. נציב את הביטויים של מודל הרגרסיה הלוגיסטית עבור ההסתברות שוואי שווה אחת ועבור ההסתברות שוואי שווה אפס. ונגיע לביטוי לא סימפטי אבל מפורש, שצריך למקסם כדי לקבל את בטא-האט.

שימו לב: זה לא סתם שאנחנו לא רושמים כאן ביטוי מפורש לבטא-האט, הסיבה שאין כזה. פונקצית הנראות אמנם מפורשת וקמורה אבל אין לה פתרון סגור, לכן משתמשים בשיטות אופטימיזציה כמו ניוטון רפסון למי שמכיר, כדי למצוא את המקדמים בצורה איטרטיבית.
:::
:::

---

### Logistic Regression Likelihood

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from scipy.special import expit  # Sigmoid function

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
X = np.random.randn(n_samples)
true_beta0 = 0.5
true_beta1 = 2.0
y = np.random.binomial(1, expit(true_beta0 + true_beta1 * X))

# Define the likelihood function
def likelihood(beta0, beta1, X, y):
    linear_combination = beta0 + beta1 * X
    likelihood_vals = y * np.log(expit(linear_combination)) + (1 - y) * np.log(1 - expit(linear_combination))
    return np.sum(likelihood_vals)

# Create a grid of beta0 and beta1 values
beta0_vals = np.linspace(-1, 2, 100)
beta1_vals = np.linspace(0, 4, 100)
beta0_grid, beta1_grid = np.meshgrid(beta0_vals, beta1_vals)
likelihood_grid = np.array([[likelihood(b0, b1, X, y) for b0, b1 in zip(beta0_row, beta1_row)]
                            for beta0_row, beta1_row in zip(beta0_grid, beta1_grid)])

# Plotting
fig = plt.figure(figsize=(12, 5))

# 3D Surface Plot
ax1 = fig.add_subplot(121, projection='3d')
ax1.plot_surface(beta0_grid, beta1_grid, likelihood_grid, cmap='viridis', alpha=0.8, zorder=-1)
ax1.set_xlabel(r'$\beta_0$')
ax1.set_ylabel(r'$\beta_1$')
ax1.set_zlabel('Log-Likelihood')
ax1.set_title('3D Likelihood Function')
# ax1.scatter(true_beta0, true_beta1, likelihood(true_beta0, true_beta1, X, y), color='r', s=100, zorder=10)  # True parameters

# Contour Plot
ax2 = fig.add_subplot(122)
contour = ax2.contour(beta0_grid, beta1_grid, likelihood_grid, levels=50, cmap='viridis')
ax2.set_xlabel(r'$\beta_0$')
ax2.set_ylabel(r'$\beta_1$')
ax2.set_title('Contour Plot of Likelihood Function')
ax2.scatter(true_beta0, true_beta1, color='r', s=100)  # True parameters
ax2.axhline(true_beta1, color='r', linestyle='--')
ax2.axvline(true_beta0, color='r', linestyle='--')
plt.colorbar(contour, ax=ax2)

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אנחנו רואים הדגמה של הפונקציה הזאת עם נתונים עבור חותך בטא-אפס ושיפוע של משתנה נוסף, בטא-אחת. ממש ניתן לראות כיצד הפונקציה קעורה או קמורה כלפי מעלה. נהפוך אותה אם ניקח פשוט מינוס והיא תהיה קמורה, מה שאומר שמובטח לנו שבכמה צעדי גרדיאנט נגיע לנקודת המינימום של הבטאות ושהיא מינימום גלובלי, למינוס הנראות.
:::
:::

---

### Prediction and interpretation of coefficients

::: {.incremental}
- Predicting on $x \in Te$:
$$\widehat{P(y=1|x)} = \frac{\exp(x^T\hat{\beta})}{1+\exp(x^T\hat{\beta})}\;\; \Rightarrow\;\; \hat{y} = \begin{cases} 1 & \mbox{if } \widehat{P(y=1|x)}> 0.5 \\
0 & \mbox{otherwise}\end{cases}$$

- We can write our model as: 
$$\log\frac{P(y=1|x)}{P(y=0|x)} = x^T\beta$$

- The expression on the left is called the *log odds*: log of the ratio of "positive" vs "negative" probability

- Interpretation: ${\beta}_j$ is the change in the log odds from a change of 1 unit in $x_j$. 

- For example, if ${\beta}_j=1$ then when $x_j=1$ vs $x_j=0$ the log odds increase by $1$, and the odds increase **times** $e=2.72$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כשנקבל את בטא-האט, ותגיע תצפית חדשה ממדגם הטסט, נוכל לחזות את ההסתברות שהיא אחת באמצעות הצבה בנוסחה של הפונקציה ההופכית. ואם אנחנו רוצים חיזוי סופי, האם Y הוא 1 או 0, אפשר להשוות את ההסתברות הנחזית לאיזשהו קאטאוף, לדוגמא חצי. אם היא גדולה מחצי נחזה 1, ואם לא נחזה 0.

ועוד מילה על המודל שלנו. יחס הסיכויים שאנחנו ממדלים הוא בעצם הodds. בשפה יומיומית, אם מאורע יכול לקרות בסיכוי שליש, או לא לקרות בסיכוי שני שליש, אנחנו אומרים "הוא יקרה ביחס של 1 ל-2". כלומר מה שאנחנו ממדלים כצירוף ליניארי הוא הלוג-אודז, לוג יחס הסיכויים.

הפרשנות של מקדמי הבטא ברגרסיה לוגיסטית הרבה פחות פשוטה לעומת רגרסיה ליניארית, ועלולה לבלבל, אז נשים לב:

מה המשמעות של מקדם בטא-ג'יי? עלייה של יחידה אחת בXj, פירושה עלייה של בטא-ג'יי בלוג אודז.

ואם זה לא אומר הרבה, קחו את האקספוננט של בטא-ג'יי ותראו מה הוא עושה לאודז עצמו. עלייה של בטא-ג'יי בגודל 1 היא עלייה פי e של האודז עצמו, בערך פי 2.72.
:::
:::

---

## Example: SAHeart {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
נראה כעת דוגמא של רגרסיה לוגיסטית על דאטא קטן שמתאים לבעיה.
:::
:::

---

### Example: South African Hearth Disease Data

```{python}
#| echo: false

saheart = pd.read_table("../datasets/SAheart.data", header = 0, sep=',', index_col=0)

print(saheart.head())

saheart_X=pd.get_dummies(saheart.iloc[:, :9]).iloc[:, :9]
saheart_y=saheart.iloc[:, 9]

from sklearn.model_selection import train_test_split

Xtr, Xte, Ytr, Yte = train_test_split(saheart_X, saheart_y, test_size=0.2, random_state=42)

print('')
print('')
print(f'No. of train rows: {Xtr.shape[0]}, no. train of cols: {Xtr.shape[1]}')
print(f'No. of test rows: {Xte.shape[0]}, no. test of cols: {Xte.shape[1]}')
print(f'no. of obs in train y: {Ytr.shape[0]}')
print(f'no. of obs in test y: {Yte.shape[0]}')
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בנתונים שלפנינו יש 462 גברים מדרום אפריקה. יש לנו מידע רפואי עליהם כמו לחץ דם, מידת העישון, צריכת אלכוהול וגיל, והמשתנה שיעניין אותנו הוא הchd, coronary heart disease, האם הם לקו במחלת לב או לא.
:::
:::

---

### SAHeart: LR with `statsomdels`

```{python}
#| echo: false

import statsmodels.api as sm

model = sm.Logit(Ytr, sm.add_constant(Xtr))
model = model.fit()

print(model.summary())
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ברגע שיודעים לבצע רגרסיה ליניארית בstatsmodels או בsklearn, רגרסיה לוגיסטית זה קל. נקבל פלט סטטיסטי עשיר ומסודר של המקדמים ומבחנים סטטיסטיים עליהם.

נשים לב שהמבחנים כאן הם מבחני Z אבל לא ניכנס בקורס הזה לסיבה לכך. מכל מקום לכל מקדם יש טעות תקן שונה, אז כדי להשוות ביניהם נסתכל על הציוני תקן, על ערכי הZ. משתנים עם ערכים גבוהים הם age למשל, הגיל. לכל שנת חיים נוספת, המודל מוסיף איזושהי כמות ללוג-אודז שהפציינט יחלה במחלת לב. מקדם שלילי גדול הוא הfamhist_Absent, שמשמעותו האם אין לך היסטוריה משפחתית של מחלת לב. אם אין, יורדת לך כמות של 0.8 מהלוג-אודז.
:::
:::


---

### SAHeart: LR Test Performance

```{python}
#| echo: false

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver='lbfgs', max_iter=10000)
model.fit(Xtr, Ytr)

from sklearn.metrics import confusion_matrix

p_hat_te = model.predict_proba(Xte)[:, 1]
y_hat_te = p_hat_te > 0.5
conf = confusion_matrix(Yte, y_hat_te)

pd.DataFrame(
  confusion_matrix(Yte, y_hat_te),
  index=['true:no', 'true:yes'], 
  columns=['pred:no', 'pred:yes']
)
```
::: {.fragment}
```{python}
#| echo: false

acc = np.mean(Yte == y_hat_te)
err = np.mean(Yte != y_hat_te)
print(f'Accuracy: {acc: .2f}, Misclassification loss: {err: .2f}')
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אבל איך אנחנו מכמתים את הביצועים של המודל על מדגם טסט שלא ראינו, על זה לא דיברנו. אפשר לדווח את הנראות שאותה מיקסמנו, אבל זה לא יגיד הרבה.

מקובל יותר למשל לחזות את ההסתברויות p_hat על מדגם הטסט, באמצעות הנוסחה שראינו.

ואז להשוות את ההסתברויות החזויות לאיזשהו קאטאוף דיפולטיבי של חצי. ואם אתם מודאגים מזה אתם צודקים, ותיכף נדבר על זה. זה יביא אותנו לחיזוי סופי של Y, האם הוא 0 או 1.

את החיזוי הזה אפשר להכניס לתוך קונפיוז'ן מטריקס או "מטריצת בלבול", מטריצה 2 על 2 שתראה לנו, מתוך הפציינטים שהם לא חולים, כמה המודל חזה שהם כן חולים, וכמה לא. ומתוך הפציינטים שהם כן חולים, כמה המודל חזה שהם חולים וכמה לא.

אפשר גם לחשב מדדים אינטואיטיביים של אחוז דיוק, accuracy, ואחוז שגיאה, error. מדובר באופן כללי באחוז התצפיות מהטסט סט שהמודל צדק לגביהן, והאחוז שהוא טעה. כאן על נתונים שהמודל לא ראה הוא צודק ב76 אחוז וטועה ב24 אחוז.

בחלק האחרון של השיעור נרחיב על אווליואציה של מודלים לקלסיפיקציה, מסתבר שהמדדים שהסתכלנו עליהם כרגע יכולים להיות בעייתיים.

לדוגמא (להדגים), נניח ש99 אחוז מהפציינטים היו בריאים ורק אחוז אחד היו חולים. מה אם אתן לכם מודל שחוזה שכל הפציינטים הם בריאים? זה מודל נהדר, הוא יקבל אקיורסי של 99 אחוז! חייב להיות אם כן מדד טוב יותר שיעיד על כך שזה לא מודל נהדר ואפילו די גרוע.
:::
:::

---

## Classification Model Evaluation {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אני טוען שאחוז הaccuracy לא תמיד משקף נכון את הביצועים של המודל, וראינו שהתופעה חמורה במיוחד כשהנתונים הם מה שקרוי imbalanced, אין באוכלוסיה מספר שווה של דוגמאות חיוביות ושליליות. ואנחנו זקוקים למדדים טובים יותר.
:::
:::

---

### Measuring Classification Performance

- Different errors have different costs/value. 

- Summarize performance in different ways that capture different types of errors:

::: {.fragment}
|        |   Pred   |          |     |
|--------|----------|----------|-----|
|**Real**| Pos      | Neg      |Total|
|Pos     | $TP$     | $FN$     | $P$ |
|Neg     | $FP$     | $TN$     | $N$ |
|Total   | $\hat{P}$| $\hat{N}$|     |
:::

::: {.fragment}
$P = \sum_{i=n+1}^{n+m} y_i$ number of positive examples, similarly $N$.

$\hat{P} = \sum_{i=n+1}^{n+m} \hat{y}_i$ number of positive predictions, similarly $\hat{N}$.

$TP = \sum_{i=n+1}^{n+m} y_i \hat{y}_i$ number of true positives, $FP = \hat{P}-TP$

$TN = \sum_{i=n+1}^{n+m} (1-y_i) (1-\hat{y}_i)$ number of true negatives, $FN = \hat{N}-TN$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
רמזנו בתחילת היחידה, שלטעויות שונות יכול להיות משקל שונה. לפספס חולה ולהגיד לו שהוא בריא, יכולה להיות לזה משמעות אחרת לגמרי מלהגיד לאדם בריא שהוא חולה.

נסמן את הכמויות בטבלת הקונפיוז'ן מטריקס בצורה כזו:

מספר הדוגמאות החיוביות, או חולים, באופן שולי נסמן כP. מספר הדוגמאות השליליות נסמן כN. באופן דומה, מספר החיזויים החיוביים נסמן כP_hat, ומספר החיזויים השליליים נסמן כN_hat.

אם המציאות היא חולה והמודל חזה חולה, זה true positive או TP. אם המציאות היא חולה והמודל חזה לא-חולה, כלומר בריא, זה false negative או FN.

באופן דומה מגדירים false positive וtrue negative.

ומי שצריך להגדיר את זה עם נוסחאות יותר פורמליות, אפשר להגדיר את כל הכמויות האלה עם y וy_hat בצורה כזאת.

זה זמן טוב אגב גם לעצור ולהגיד שלא תמיד ברור בבעיות שלא קשורות במחלות מה זה פוזיטיב ומה זה נגטיב. הרבה מהטרמינולוגיה באמת הגיעה ממחקר קליני על תרופות ומחלות שבו פוזיטיב זה מי שחולה במחלה או נושא איזשהו גן למחלה, הרבה פעמים הקלאס הנדיר יותר, ונגטיב זה מי שבריא. אבל מה אם אתם מנסים לבנות מודל שיבדיל בין תמונות של חתולים לכלבים? אז המושגים של פוזיטיב ונגטיב הם קצת יותר שרירותיים ומה שחשוב זה לבחור באחד ולהיות עקביים.
:::
:::

---

|        |   Pred   |          |     |
|--------|----------|----------|-----|
|**Real**| Pos      | Neg      |Total|
|Pos     | $TP$     | $FN$     | $P$ |
|Neg     | $FP$     | $TN$     | $N$ |
|Total   | $\hat{P}$| $\hat{N}$| $m$ |

<hr>

::: {.fragment}
Accuracy: $P(Correct) = \;(TN+TP)/m$

Prediction error: $P(Error) = \;(FN+FP)/m$

Precision+ (positive predictive value): $P(True + | Pred +) = \;TP/\hat{P}$

Recall+ (sensitivity, true positive rate):  $P(Pred + | True +) = \;TP/P$

False positive rate: $P(Pred + | True -) = \;FP/N$

Harmonic mean of precision and recall: $\;F_1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הטבלה הזאת מאפשרת לנו להסתכל על מדדים הרבה יותר ספציפיים למה שמעניין אותנו בבעיה הנתונה. נפרט אותם כעת:

האקיורסי כעת היא סכום התאים על האלכסון של הקונפיוז'ן מטריקס, הTP והTN, חלקי m גודל מדגם הטסט.

טעות הניבוי היא בדיוק סכום התאים האחרים לא על האלכסון חלקי m.

הפרסיז'ן הוא הסיכוי להיות חולה בהינתן שחזיתי חולה. נקרא גם positive predictive value או PPV. באופן דומה אפשר להגדיר את הפרסיז'ן לקלאס האחר, הסיכוי להיות לא-חולה אם חזיתי לא-חולה, או הnegative predictive value. בכל מקרה נרצה שהפרסיז'ן יהיה כמה שיותר גדול.

מדד אחר הוא הריקול, או sensitivity או true positive rate, TPR. זה הסיכוי שבהינתן חולה המודל אכן חוזה חולה, או שיעור החולים שהמודל אכן מחלץ. גם כאן ניתן לחשוב על הריקול של הקלאס האחר, אחוז הבריאים שהמודל מחלץ נכון. וגם כאן ברור שנרצה שהמדד הזה יהיה גדול ככל שניתן.

מדד שנרצה שיהיה קטן ככל האפשר הוא הfalse positive rate, הFPR, בהינתן לא-חולה הסיכוי לחזות חולה.

ומאחר שהמדדים פרסיז'ן וריקול מודדים דברים שונים והיינו רוצים ששניהם יהיו גבוהים מסתכלים לפעמים על הממוצע ההרמוני שלהם, 2 כפול המכפלה שלהם חלקי הסכום שלהם. קוראים לזה אף-וואן סקור.

למה צריך את כל זה? בדיוק בגלל מה שאמרנו, יכול להיות לקוח של המודל שלנו, שפשוט לא מסוגל לפספס אף חולה, לקוח כזה ירצה ריקול כמה שיותר גבוה. ויכולה להיות לקוחה שאין לה בעיה לפספס חולים, אבל כשהמודל מסמן לה חולים הוא חייב להיות צודק, לדוגמא היא רופאה שלא יכולה להעניק טיפול קשה כזה לבריאים -- בשביל לקוחה כזאת נרצה אולי למקסם את הפרסיז'ן. וברור שיהיה כאן טריידאוף בין השניים במודל לא מושלם.
:::
:::

---

```{python}
#| echo: false

pd.DataFrame(
  confusion_matrix(Yte, y_hat_te),
  index=['true:no', 'true:yes'], 
  columns=['pred:no', 'pred:yes']
)
```

::: {.fragment}
```{python}
#| echo: false

from sklearn.metrics import classification_report

print(classification_report(Yte, y_hat_te))
```
:::

::: {.fragment}
All is still based on that cutoff, 0.5!
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בנתונים שלנו כך נראית הקונפיוז'ן מטריקס:

כדי לקבל את כל המדדים שדיברנו עליהם אפשר לבקש classification_report מsklearn. נדגים לראות שהבנו.

למשל, הפרסיז'ן של החולים: אם חזיתי חולה או 1, מה הסיכוי שהפציינט באמת חולה: מתוך 26 חזויים כחולים, 19 אכן חולים, דיוק של 73 אחוז.

או, הריקול של החולים, אם אתה חולה מה הסיכוי שהמודל יחזה נכון. מתוך 34 חולים המודל צדק רק לגבי 19, שזה 56 אחוז, לא גבוה מאוד.

נשים לב אגב שהכל מבוסס על ערך הסף הזה שהשווינו אליו את ההסתברויות החזויות כדי לקבל את y_hat_te, הקאטאוף של חצי -- יכול להיות שעם קאטאוף אחר היינו מקבלים מדדים טובים יותר?
:::
:::

---

### Classification evaluation: different goals

We can think of several different prediction goals, all potentially important: 

1. Classify correctly --- make few (weighted) errors on test set or new prediction points
2. Predict probabilities well: $\widehat{P(y=1|x)} \approx P(y=1|x)$ for new points
3. Rank well: given multiple prediction points, predict which one is *more likely* to have $y = 1$.

::: {.fragment}
These different tasks can reflect in the loss function / model evaluation task:

1. Correct classification: misclassification loss as above, also precision, recall etc.
2. Good probability prediction: using Bernoulli loss / likelihood: 
$$L(y,\hat{p}) = \hat{p}^y (1-\hat{p})^{(1-y)}$$
3. How do we measure ranking perofrmance of a model on a test set?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
באופן כללי יכולות להיות לדאטא סיינטיסט מטרות שונות במודל קלסיפיקציה:

האם אני רוצה פשוט לחזות נכון.

האם מעניינות אותי דווקא ההסתברויות הנחזות ולדאוג שהן יהיו מכוילות כמו שיותר עם ההסתברויות המקוריות.

ואולי, כל מה שמעניין אותי זה הדירוג של תצפיות, מבחינת הסיכוי שהן 1 או שהפציינט חולה. לא חשוב לי אפילו אם הכמויות שאני חוזה הן הסתברויות, אלא אני מתייחס אליהן כאיזשהו סקור כללי שאני רוצה שידרג נכון את התצפיות שלי.

המטרה שלי משפיעה על המדד שלי לטיב המודל:

אם המטרה שלי היא פשוט לחזות נכון אני מסתכל על מדדים כמו אלה שראינו: אחוז דיוק, פרסיז'ן, ריקול.

אם המטרה שלי היא הסתברויות מכוילות כמה שיותר, אני באמת אדווח אולי על הלוס שהשתמשנו בו, הנראות.

אבל אם, כמו שקורה פעמים רבות, המדד שמעניין אותי זה הראנקינג, רק הדירוג של התצפיות, איך כדאי להסתכל על הביצועים של המודל?
:::
:::

---

### The ROC Curve

The idea: to evaluate ranking performance, do not set the threshold $0.5$ but check what happens at all possible thresholds: 

1. True positive rate: what % of the positive observations pass the threshold?
2. False positive rate: what % of the negative observations pass the threshold?

::: {.fragment}
- The ROC curve plots TPR vs FPR for all possible threholds: if the model ranks well, for high thresholds we will have $FPR\approx 0$, while for low thresholds we will have $TPR \approx 1$
:::

::: {.fragment}
- Note that even if $\widehat{P(y=1|x)}$ predicts probabilities badly, or even if the predictions are not in the range $[0,1]$, the ranking can still be good
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אם מה שמעניין אותנו הוא ראנקינג, נהוג להסתכל בעקומה שנקראת receiver operating characteristic או ROC בקיצור.

הרעיון הוא לא להסתפק בקאטאוף דיפולטיבי של חצי כפי שעשינו, כי יכול להיות שהסקור שהוצאנו איננו בדיוק הסתברות. נשנה את הקאטאוף בצעדים קבועים, ובכל צעד נמדוד את ה: טרו פוזיטיב רייט, זה בעצם הריקול, ואת הפולס פוזיטיב רייט, אחוז הדוגמאות השליליות שעוברות את הקאטאוף הנוכחי ונחזות כחיוביות.

עקומת הROC תצייר את הTPR לעומת הFPR לכל קאטאוף אפשרי. מודל מושלם ימצא קאטאוף שעבורו הFPR קרוב ל0 והTPR קרוב ל1.

ושוב נדגיש שגם אם ההסתברות הנחזית מגיעה ממודל שבכלל לא אמור להוציא הסתברויות והיא לא מכוילת או אפילו היא לא הסתברות, היא איזשהו סקור, הדירוג עצמו עדיין יכול להיות מצוין. וזה היתרון הגדול של גישה כזאת. התחשבות בקאטאופים שונים והעובדה שהיא פרקטית לכל סקור.
:::
:::

---


```{python}
#| echo: false

from sklearn.metrics import roc_curve, auc

fpr, tpr, thresholds = roc_curve(Yte, p_hat_te)
auc1 = auc(fpr, tpr)

plt.plot(fpr, tpr, color='darkorange',
         lw=2, label='ROC curve (area = %0.2f)' % auc1)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve for our logistic model')
plt.legend(loc="lower right")
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כך נראית עקומת הROC על מדגם הטסט שלנו, עבור מודל הרגרסיה הלוגיסטית שמנסה לחזות אם פציינט יחלה במחלת לב או לא.

אנחנו לא רואים את הספים עצמם שהקוד משנה, אבל כן נוצרת עקומה יפה של TPR מול FPR, ואנחנו רואים שעבור ספים נמוכים מתקבל TPR גבוה, ועבור ספים גבוהים מתקבל FPR נמוך. אפשר גם לראות את הטריידאוף בין שני המדדים בצורה יפה.

ואם רוצים לחזור לניבוי סופי, אפשר לבחור את הקאטאוף שיביא אותנו לנקודה האופטימלית מבחינתנו. אם אין לנו העדפה מסוימת, כמו שהדגמנו קודם, הנקודה האופטימלית שומרת על TPR כמו שיותר גבוה וFPR כמה שיותר נמוך אז זו תהיה הנקודה הכי קרובה לקצה השמאלי העליון של הגרף, נאמר זאת.

וברור שבמודל מושלם יימצא סף שבאמת מגיע עד הנקודה בקצה השמאלי עליון של הגרף.

עכשיו, היינו רוצים לתמצת את העקומה הזאת לאיזשהו מדד יחיד, שיבטא את טיב המודל שלנו. עקומה זה מרשים אבל צריך לסכם אותה איכשהו. נראה שבמודל מושלם השטח תחת העקומה יהיה השטח של כל הריבוע, 1 כפול 1, זאת אומרת 1, וכאן כפי שמודפס השטח הוא רק 0.8, או 80 אחוז.
:::
:::

---

### The Area Under the Curve (AUC)

- *For a random ranking:* $FPR \approx TPR$ at every threshold, so we are around the diagonal $x=y$: $$AUC\approx 0.5$$

- *For a perfect ranking model:* at high thresholds, $FPR=0$, at low thresholds $TPR=1$, hence: $$AUC=1.$$

::: {.fragment}
- Very nice interpretation of AUC: Assume the test set has $m_1$ ones ($y=1$) and $m_0$ zeros, then AUC is the % of correctly ranked pairs with different response: 
$$AUC = \frac{ \#\left\{(i,j): y_i = 0, y_j=1 \mbox{ and } \hat{p}_i < \hat{p}_j\right\}}{m_1\times m_0}$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
השטח תחת עקומת הROC , הarea under curve או AUC, הוא מדד מקובל מאוד, והוא מצוין כי הוא לא תלוי בקאטאוף ספציפי, הוא מתחשב בכולם, מעין ממוצע או אינטגרציה של טיב המודל על פני ספים שונים.

מודל אקראי לחלוטין, שככל שאנחנו מעלים את הסף כך יורד הTPR ועולה הFPR באופן שווה, העקומה תהיה בעצם הקו האלכסוני בתוך הריבוע, והשטח תחתיה, הAUC יהיה חצי.

מודל מושלם כאמור, יגיע לAUC של 1, כל שטח הריבוע.

פרשנות יפה של AUC היא, מתוך כל זוגות התצפיות האפשריות כך שאחת חיובית ואחת שלילית, כמה מדורגות "נכון". ומה זה נכון? אם הסקור הנאמד לתצפית החיובית בזוג, גדול מהסקור לתצפית השלילית. אז אחוז הזוגות שמדורגים נכון, במודל אקראי יהיה -- חמישים אחוז, חצי. כמו שאמרנו שAUC למודל אקראי יהיה. ואחוז הזוגות שמדורגים נכון במודל מושלם יהיה -- מאה אחוז.

בצורה פורמלית אפשר לסמן זאת כך, אם יש לנו m0 תצפיות שליליות וm1 תצפיות חיוביות, אז הAUC שווה לאחוז הזוגות שעבורם p_hat לתצפית החיובית גבוה מp_hat לתצפית השלילית. ונשים לב שהמדד הזה גם לא סובל במרכאות אם יש לנו הרבה יותר תצפיות חיוביות משליליות במדגם, או להיפך, כמו שראינו עבור מדד הaccuracy. זו פרופורציה שבכל מקרה נרצה שתהיה כמה שיותר קרובה לאחת.

עד כאן לגבי מודלים ליניאריים. בניגוד למה שאולי תשמעו מאנשים מסוימים, מודלים ליניאריים עדיין בשימוש נרחב בתעשייה ולו בתור בייסליין שניתן לסמוך עליו, בייסליין שבאופן מפתיע הרבה פעמים קשה לנצח.
:::
:::
