<!DOCTYPE html>
<html lang="en"><head>
<script src="../libs/clipboard/clipboard.min.js"></script>
<script src="../libs/quarto-html/tabby.min.js"></script>
<script src="../libs/quarto-html/popper.min.js"></script>
<script src="../libs/quarto-html/tippy.umd.min.js"></script>
<link href="../libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.554">

  <title>Boosting</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="../slides_quarto.css">
  <link href="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  

    <link rel="icon" href="../Intro2SL_logo.jpg" type="image/jpg"> 

    <link rel="shortcut icon" href="../Intro2SL_logo.jpg" type="image/jpg">

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css">

  </head>

<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section id="section" class="slide level2 logo-slide">
<h2></h2>
</section>
<section id="introduction-to-statistical-learning" class="slide level2 title-slide center">
<h2>Introduction to Statistical Learning</h2>
<h3 id="boosting---class-10">Boosting - Class 10</h3>
<h3 id="giora-simchoni">Giora Simchoni</h3>
<h4 id="gsimchonigmail.com-and-add-intro2sl-in-subject"><code>gsimchoni@gmail.com</code> and add <code>#intro2sl</code> in subject</h4>
<h3 id="stat.-and-or-department-tau">Stat. and OR Department, TAU</h3>
</section>
<section id="adaboost" class="slide level2 title-slide center">
<h2>AdaBoost</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בשיעור הזה נלמד על אחד האלגוריתמים שנמצאים הכי הרבה בשימוש במאשין לרנינג על נתונים גדולים ובמיוחד על נתונים טבלאיים. יש לו הרבה גירסאות והרבה שימושים מאוד פופולריים, אבל כולם בסופו של דבר נקראים: בוסטינג.</p>
<p>הגירסה הראשונה של האלגוריתם פותחה בתחילת שנות התשעים על-ידי צמד חוקרים בשם פרויד ושפייר שיקבלו על הפיתוח שלהם את פרס גדל. הגירסה הזאת נקראת: אדאבוסט.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="instead-of-averaging">Instead of averaging</h3>
<div>
<ul>
<li class="fragment">Suppose <span class="math inline">\(y \in \{-1, 1\}\)</span></li>
<li class="fragment">Recall the final Random Forests model: <span class="math inline">\(\hat{f}(x_0) = \text{sign}\left[\frac{1}{M}\sum_{m = 1}^M T_m(x_0)\right]\)</span></li>
<li class="fragment">Why not <span class="math inline">\(\hat{f}(x_0) = \text{sign}\left[\sum_{m = 1}^M \alpha_m T_m(x_0)\right]\)</span>?
<ul>
<li class="fragment">If <span class="math inline">\(M\)</span> is the number of all possible trees <span class="math inline">\(T_m\)</span>, then finding weights <span class="math inline">\(\alpha_m\)</span> is intractable!</li>
</ul></li>
<li class="fragment"><span style="color:red;">Boosting</span> finds <span class="math inline">\(\alpha_m\)</span> sequentially, <em>adaptively</em>:
<ul>
<li class="fragment">Take a <span style="color:red;">weak classifier</span> <span class="math inline">\(G(x)\)</span> with accuracy slightly better than random
<ul>
<li class="fragment">for any dist. <span class="math inline">\(P_{T}\)</span> of the training data: <span class="math inline">\(Err_{P_{T}} = \mathbb{E}_{P_{T}}\left(\mathbb{I}\left[y_i \neq G(x_i)\right]\right) \leq \frac{1}{2} - \gamma\)</span></li>
</ul></li>
<li class="fragment">Apply it sequentially over modified (weighted) versions of the training data</li>
<li class="fragment">Each time selecting the best <span class="math inline">\(\alpha_m\)</span> to get: <span class="math inline">\(\hat{f}(x_0) = \text{sign}\left[\sum_{m = 1}^M \alpha_m G_m(x_0)\right]\)</span></li>
</ul></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אדאבוסט עוסק ספציפית בקלסיפיקציה לשני קלאסים, 1 ומינוס 1. מאוחר יותר נרחיב אותו לבעיות אחרות.</p>
<p>בסיטואציה הזאת, מה אמרנו שיעשה אלגוריתם כמו רנדום פורסט? הוא יגדל M עצים שנקרא להם T, כל פעם על גירסה קצת אחרת של הנתונים, וכשתגיע תצפית חדשה הוא ימצע את הסיווג שלהם, ואם אנחנו רוצים סיווג סופי של 1 או מינוס 1, ניקח את פונקציה sign.</p>
<p>מהי ביקורת סבירה על רנדום פורסט? למה למצע. אולי יש עצים טובים יותר, ועצים טובים פחות, ואנחנו צריכים לא ממוצע פשוט שלהם אלא ממוצע משוקלל, עם משקולת אלפא-אם לכל עץ.</p>
<p>אבל איך נמצא את המשקולות האופטימליות? הרי אם M הוא מספר כל העצים האפשריים, אולי אפילו נגביל את העומק שלהם, לכל סט נתונים סביר, זה מספר ענק. זה לא שאנחנו יכולים לגדל את כולם ואז להתחיל לבדוק מה סט המשקולות הכי טוב למצע אותם.</p>
<p>אבל בוסטינג יעשה בדיוק את זה, הוא ימצא את המשקולות והעצים האלה בצורה סדרתית אחד אחרי השני, בצורה אדפטיבית.</p>
<p>זה לא חייב להיות עץ, זה יכול להיות כל קלסיפייר חלש, או וויק קלסיפייר G, והכוונה בחלש שיש לו דיוק קצת יותר טוב מאקראי.</p>
<p>אם אנחנו רוצים להיות קצת יותר רשמיים, נניח שסיכוי לסיווג שגוי הוא חצי, אז הקלסיפייר שלנו יכול להשיג על המדגם הנתון שגיאה של קצת פחות מחצי, חצי פחות איזשהו פרמטר גאמא קטן, וזה לכל משקול של הנתונים. אנחנו מרדדים קצת את ההגדרה הרשמית אבל זה מספיק טוב בשבילנו.</p>
<p>ואת הקלסיפייר הזה נתאים לנתונים שלנו, כל פעם על גירסה קצת אחרת כשכל תצפית מקבלת משקולת מתעדכנת,</p>
<p>נתאים את הקלסיפייר וגם נבחר במשקולת הכי טובה עבורו לפי איזושהי פונקצית הפסד, וזה באמת יהיה החיזוי הסופי, הסיין של סכום הקלסיפיירים הממושקלים שאימנו.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="adaboost-1">AdaBoost</h3>
<ol type="1">
<li>Initialize observations weights <span class="math inline">\(w_i = 1/n\)</span> for <span class="math inline">\(i = 1, \dots, n\)</span></li>
<li>For <span class="math inline">\(m  = 1\)</span> to <span class="math inline">\(M\)</span>:
<ol type="a">
<li>Fit classifier <span class="math inline">\(G_m(x)\)</span> to the training data using <strong>weights</strong> <span class="math inline">\(w_i\)</span></li>
<li>Compute weighted classification error: <span class="math display">\[err_m = \frac{\sum_{i = 1}^n w_i\mathbb{I}\left[y_i \neq G_m(x_i)\right]}{\sum_{i = 1}^n w_i}\]</span></li>
<li>Compute coefficient <span class="math inline">\(\alpha_m = \ln\left[\frac{1-err_m}{err_m}\right]\)</span></li>
<li>Update weights: <span class="math inline">\(w_i \leftarrow w_i \cdot \exp\left[\alpha_m \cdot \mathbb{I}\left[y_i \neq G_m(x_i)\right]\right]\)</span></li>
</ol></li>
<li>Output: <span class="math inline">\(\hat{f}(x) = \text{sign}\left[\sum_{m = 1}^M \alpha_m G_m(x)\right]\)</span></li>
</ol>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>What does it mean training a classification tree with weighted data?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ככה נראה אלגוריתם האדאבוסט הרשמי, לא להיבהל, הוא דוקא די פשוט.</p>
<p>אנחנו מתחילים עם משקולות שוות לכל התצפיות, אם יש n תצפיות זה יוצר 1 חלקי n.</p>
<p>ועכשיו בכל איטרציה, אנחנו מגדלים עץ על התצפיות שלנו במשקול הנוכחי w_i. תיכף נדבר על מה זה אומר, לגדל עץ עם משקולות על התצפיות, לא נתקלנו בזה קודם.</p>
<p>מכל מקום, אנחנו מחשבים את השגיאה הממושקלת הנוכחית, כלומר סוכמים את כל השגיאות וכופלים כל אחת במשקל שלה, ומחלקים בסך המשקולות.</p>
<p>המשקולת לכל הקלסיפייר אלפא-אם מחושבת עכשיו והיא שווה לביטוי שרשום כאן שמבוסס על שגיא הממושקלת ארר.</p>
<p>והשלב האחרון באיטרציה, עדכון המשקולות לאיטרציה הבאה: המשקולת של כל תצפית תוכפל באקספוננט בחזקת אלפא-אם אם הקלסיפייר הנוכחי עשה טעות על התצפית הזאת, וכפול אקספוננט בחזקת אפס אחרת, כלומר כפול 1 – אם צדקנו המשקולת תישאר כפי שהיא.</p>
<p>נשים לב למה זה הגיוני – אם טעינו, הכפלה פי אקספוננט בחזקת אלפא, תגדיל את המשקולת, וככה באיטרציה הבאה הקלסיפייר שלנו יצטרך לעבוד קשה יותר כדי לסווג אותה נכון, בניגוד לתצפיות עם משקולות קטנות. עוד דבר שנשים לב שעצים עם שגיאה ארר מאוד קטנה יקבלו משקולות אלפא-אם מאוד גדולות, ועצים עם שגיאה מאוד גדולה יקבלו משקולת אלפא-אם מאוד נמוכה.</p>
<p>בכל אופן האאוטפוט של של אדאבוסט הוא מה שרצינו, ממוצע משוקלל של וויק קלסיפיירים, שאנחנו מפעילים עליו פונקצית סיין כדי לקבל חיזוי סופי של 1 ומינוס 1.</p>
<p>נחזור רגע לעניין המשקול - הקלסיפייר שלנו חייב לדעת לטפל נכון בתצפיות ממושקלות. למשל איך מגדלים עץ עם משקולות על התצפיות? אז אם זה עץ רגרסיה, הלוס שאנחנו עושים עליו מינימום בכל פיצול הוא הRSS, אבל אנחנו לוקחים RSS ממושקל.</p>
<p>ואם זה עץ קלסיפיקציה, אז הלוס שלנו הוא למשל מדד הג’יני, ואנחנו צריכים לחשב בכל צומת את אחוז התצפיות שהן פלוס או מינוס אחת, אז במקום לחשב אותן בצורה הרגילה נחשב את סכום משקולות התצפיות שהן אחת חלקי סכום המשקולות של התצפיות בצומת.</p>
<p>מכל מקום, כעת ברור למה קוראים לאלגוריתם אדאבוסט. הוא לוקח קלסיפייר חלש יחסית, ועושה לו בוסט, בצורה אדאפטיבית. בחלק הבא נראה שבצורה הזאת יש לנו הבטחה, שאנחנו יכולים להוריד את השגיאה על מדגם הלמידה להיות קטנה כרצוננו, כתלות במספר האיטרציות ובפרמטר השגיאה גאמא שאנחנו יכולים להבטיח לכל וויק קלסיפייר. קטנה כרצוננו זה אומר אפילו אפס.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="adaboost-example" class="slide level2 title-slide center">
<h2>AdaBoost Example</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בואו נראה דוגמה פשוטה של אדאבוסט, בדו-מימד.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="adaboost-with-tree-stumps">AdaBoost with Tree Stumps</h3>
<div id="d30c412a" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-2-output-1.png" width="495" height="411"></p>
</figure>
</div>
</div>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Can a tree stump get to <span class="math inline">\(\overline{err} = 0\)</span>? Can a deep decision tree?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בדוגמה שלנו יש 10 תצפיות בשני משתנים, ונניח שהאדומות הן פלוס 1 והכחולות הן מינוס 1.</p>
<p>הוויק קלסיפייר שלנו יהיה גזעים של עצים או טרי סטאמפס, כלומר אנחנו מסוגלים בכל איטרציה לעשות רק פיצול אחד על משתנה אחד.</p>
<p>זה קלסיפייר ממש חלש, נכון? נניח האם אפשר עם הקלסיפייר הזה להגיע על הנתונים האלה לשגיאת חיזוי אפס? אי אפשר. המשמעות של טרי-סטאמפ היא שאנחנו יכולים רק לחלק את המרחב של שני המשתנים חלוקה אחת מקבילה לאחת הצירים.</p>
<p>אם היינו מאפשרים עץ עמוק יותר האם היינו צריכים בוסטינג? כנראה שכן (להדגים). אבל זאת רק דוגמה פשוטה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="adaboost-iteration-1">AdaBoost: iteration 1</h3>
<div id="9431c08f" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-3-output-1.png" width="495" height="449"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז אם אנחנו מתחילים עם כל התצפיות ממושקלות אותו הדבר, כלומר עשירית, הטרי סטאמפ הראשון מוצא לנכון את החלוקה על פי המשתנה השני, מתחת יחזה כחול ומעל יחזה אדום.</p>
<p>השגיאה הממושקלת שלנו היא שני סיווגים שגויים עם משקולות עשירית חלקי סך משקולות של עשר עשיריות או 0.2. והאלפא שלנו תהיה לוג של 0.8 חלקי 0.2 כלומר לוג 4 או 1.386.</p>
<p>אנחנו ממשקלים מחדש את התצפיות, כאשר המשקולות של התצפיות עם סיווג נכון נשארות עשירית, והמשקולות של התצפיות שסיווגנו לא נכון מוכפלות פי אקספוננט בלוג-4 כלומר פי 4, הן יהיו 0.4.</p>
<p>אנחנו שומרים את המשקולות, את אלפא ואת העץ בצד וממשיכים לעץ הבא.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="adaboost-iteration-2">AdaBoost: iteration 2</h3>
<div id="6918643c" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-4-output-1.png" width="495" height="449"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>באיטרציה הבאה אנחנו באמת רואים את המשקולות שחישבנו, ומגדלים סטאמפ חדש. הסטאמפ החדש רואה את הנתונים אחרת, הפעם הוא שם הרבה יותר דגש על לסווג נכון את התצפיות שלא סווגו נכון בעבר ויש להן משקולת של 0.4. ואכן הוא מחלק את המרחב בצורה שונה לחלוטין.</p>
<p>אבל זה יוצר שגיאה ממושקלת חדשה, יש לנו 3 טעויות במשקל כולל 0.3 חלקי סכום המשקלים שהוא 1.6 כלומר שגיאה של 0.188.</p>
<p>האלפא החדשה היא לוג של 0.812 חלקי 0.188, יוצא 1.46.</p>
<p>ושוב נגדיל את המשקל של התצפיות שחזינו לא נכון, נכפיל עשירית פי אקספוננט בחזקת 1.466, צריך לתת 0.43.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="adaboost-iteration-3">AdaBoost: iteration 3</h3>
<div id="374bd31a" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-5-output-1.png" width="495" height="449"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>וככה אנחנו ממשיכים לסטאמפ הבא.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="adaboost-iteration-4">AdaBoost: iteration 4</h3>
<div id="f2606d98" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-6-output-1.png" width="495" height="449"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ולסטאמפ הבא.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="adaboost-iteration-5">AdaBoost: iteration 5</h3>
<div id="d9f62d7d" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-7-output-1.png" width="495" height="449"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ולסטאמפ הבא.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="adaboost-aggregate">AdaBoost: aggregate</h3>
<div class="fragment">
<p><span class="math inline">\(\hat{f}(x) = \text{sign}\left[\sum_{m = 1}^M \alpha_m G_m(x)\right] =\)</span></p>
<p><span class="math inline">\(= \text{sign}\left[1.386G_1(x) + 1.466G_2(x) + 0.934G_3(x) + 1.299G_4(x) + 1.126G_5(x)\right]\)</span></p>
</div>
<div class="fragment">
<div id="2b23c5cd" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-8-output-1.png" width="495" height="449"></p>
</figure>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ואם נניח שאנחנו עושים את זה 5 איטרציות, כלומר 5 סטאמפים, נעצור.</p>
<p>החיזוי הסופי שלנו לכל נקודה במרחב הזה יהיה פונקציית סיין, על הממוצע המשוקלל של הטרי סטאמפס, G1 עד G5.</p>
<p>אם נצייר את תחום ההחלטה הזה נראה שהוא כבר אחרי 5 איטרציות הגיע לשגיאה אפס, הוא מסווג נכון את כל התצפיות, ובאופן מרשים אדאבוסט לקח את הקלסיפייר החלש הזה שמסוגל למעט מאוד, ועשה לו בוסטינג בצורה אדאפטיבית ככה שהוא יכול לתאר גבול החלטה הרבה יותר מורכב.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="adaboost-guarantee">AdaBoost Guarantee</h3>
<div class="fragment">
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Theorem:</p>
<p><span class="math display">\[\overline{err} = \sum_{i = 1}^n \mathbb{I}\left[y_i \neq \hat{f}(x_i)\right] \le \left(1-4\gamma^2\right)^{M/2} \le e^{-2M\gamma^2}\]</span></p>
</div>
</div>
</div>
</div>
<div>
<ul>
<li class="fragment"><p>This means for a large enough <span class="math inline">\(M\)</span> we can get <span class="math inline">\(\overline{err}\)</span> as low as we want (on training data)!</p></li>
<li class="fragment"><p>Specifically, to make the upper bound <span class="math inline">\(e^{-2M\gamma^2} &lt; \frac{1}{n}\)</span> (i.e.&nbsp;<span class="math inline">\(\overline{err} = 0\)</span>) <span class="math inline">\(\Rightarrow M &gt; \frac{\log(n)}{2\gamma^2}\)</span></p></li>
<li class="fragment"><p>A weak classifier which can only guarantee up to 40% error at each iteration (<span class="math inline">\(\gamma = 0.1\)</span>):</p>
<ul>
<li class="fragment"><p>after <span class="math inline">\(M = 100\)</span> iterations will be boosted to give <span class="math inline">\(\overline{err} = (1 - 4 \cdot 0.1^2)^{(100/2)} = 0.96^{50} \approx 0.13\)</span> error</p></li>
<li class="fragment"><p>if <span class="math inline">\(n = 1000\)</span>, need <span class="math inline">\(M &gt; \log(1000) / 2 \cdot 0.1^2 \approx 345\)</span> iterations to get <span class="math inline">\(\overline{err} = 0\)</span></p></li>
</ul></li>
<li class="fragment"><p>Really nice proof in Freund &amp; Schapire (1997)</p></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כמובטח, אנחנו יכולים להביא את השגיאה על מדגם הלמידה להיות קטנה כרצוננו. ספציפית אנחנו יכולים לחסום אותה על-ידי הביטוי שמופיע כאן, 1 פחות ארבעה גאמא בריבוע בחזקת M חלקי 2. ואפשר להראות בקלות שחסם פשוט יותר הוא סדר גודל של אי בחזקת מינוס פעמיים מספר האיטרציות כפול גאמא בריבוע.</p>
<p>למשל, אם רוצים לחסום את הטעות שלא תהיה גדולה יותר מ1 חלקי n, מה שבפועל אומר שהיא חייבת להיות אפס, אנחנו לא מוכנים לעשות אפילו שגיאה אחת, אז אחרי קצת אלגברה אפשר לקבל חסם תחתון על מספר האיטרציות שאנחנו חייבים לבצע, שהוא סדר גודל של לוג-מספר התצפיות מוחלק בפרמטר גאמא בריבוע. שוב כל זה בהנחה שאנחנו מבטיחים שבכל איטרציה הקלסיפייר שלנו טועה עד כדי מרחק גאמא קטן מחצי, השגיאה האקראית.</p>
<p>אז אם לדוגמא יש לכם קלסיפייר די בינוני, הוא יכול לעשות שגיאה של עד 40 אחוז או גאמא שווה 0.1, אחרי 100 איטרציות, הוא הופך להיות קלסיפייר לא רע בכלל עם שגיאה לא גדולה יותר מ13 אחוז, פשוט מציבים בחסם.</p>
<p>ואם לדוגמא אנחנו יודעים שיש אלף תצפיות במדגם הלמידה, כדי להגיע לשגיאה אפס אנחנו יודעים שצריך לפחות 345 איטרציות.</p>
<p>לא נוכיח את זה כאן אבל מי שרוצה יכול לקרוא את אחד המאמרים המקוריים של פרוינד ושפייר, זה פשוט הרבה אלגברה מאוד מאוד נחמדה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="adaboost-as-additive-stagewise-modeling" class="slide level2 title-slide center">
<h2>AdaBoost as Additive Stagewise Modeling</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">

</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="questions-from-adaboost">Questions from AdaBoost</h3>
<ul>
<li><p>Does it optimize a specific loss? Can we plug-in a different loss?</p></li>
<li><p>What about regression?</p></li>
<li><p>What about <span class="math inline">\(K\)</span>-class classification?</p></li>
<li><p>Is there a probabilistic justification? A likelihood-based approach?</p></li>
</ul>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>הרבה שאלות צריכות לעלות לנו כשאנחנו מסתכלים על אדאבוסט.</p>
<p>בחשיבה שלנו קודם מגדירים פונקצית הפסד ואז עושים לה מינימיזציה - האם אדאבוסט עושה מינימיזציה לאיזשהו הפסד, ואם כן האם אפשר לייצר אלגוריתמים דומים עם פונקציות הפסד אחרות?</p>
<p>למשל האם האלגוריתם טוב רק לקלסיפיקציה בינארית, או שאנחנו יכולים ליצור משהו דומה לרגרסיה או לקלסיפיקציה ליותר מ2 קלאסים?</p>
<p>ואם אמרנו שההוכחה שהאלגוריתם עובד היא פשוט הרבה אלגברה - האם יש לאלגוריתם הצדקה הסתברותית? זה מזכיר את העובדה שראינו כיצד רגרסיה ליניארית היא קודם כל פתרון סגור באלגברה, שאפשר להצדיק אותו גם מנקודת מבט סטטיסטית, עם הנחה של רעש שמתפלג נורמלית וכולי. יכול להיות שגם כאן נמצא נקודת מבט סטטיסטית?</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="adaboost-sure-sounds-familiar">AdaBoost sure sounds familiar</h3>
<div class="fragment">
<p>Recall: <strong>Forward stagewise regression</strong></p>
</div>
<div class="fragment">
<ol start="0" type="1">
<li>Standardize all features, input some <span class="math inline">\(\tau_{thresh} \in (0, 1)\)</span> and <span class="math inline">\(\varepsilon &gt; 0\)</span> step size</li>
<li>Residual <span class="math inline">\(\mathbf{r} = \mathbf{y} - \bar{y}\)</span>, <span class="math inline">\(\beta_1, \dots, \beta_p = 0\)</span></li>
<li>Find the predictor <span class="math inline">\(\mathbf{x}_j\)</span> most correlated with <span class="math inline">\(\mathbf{r}\)</span>, and let <span class="math inline">\(\tau = Corr(\mathbf{r}, \mathbf{x}_j)\)</span></li>
<li>While <span class="math inline">\(|\tau| &gt; \tau_{thresh}\)</span>:
<ol type="i">
<li>Update <span class="math inline">\(\beta_j \leftarrow \beta_j + \delta_j\)</span>, where <span class="math inline">\(\delta_j = \varepsilon \cdot \text{sign}(\tau)\)</span></li>
<li>Update <span class="math inline">\(\mathbf{r} \leftarrow \mathbf{r} - \delta_j\mathbf{x}_j\)</span></li>
<li>Find the predictor <span class="math inline">\(\mathbf{x}_j\)</span> most correlated with <span class="math inline">\(\mathbf{r}\)</span>, and let <span class="math inline">\(\tau = Corr(\mathbf{r}, \mathbf{x}_j)\)</span></li>
</ol></li>
</ol>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>מסתבר שבאמת אפשר להכליל את אדאבוסט, למשפחה של אלגוריתמים, שאחד מהם ראינו והבטחתי שעוד נראה שוב.</p>
<p>כשדיברנו על שיטות לבחירת משתנים ברגרסיה ליניארית, ראינו שיטה בשם פורוורד סטייג’וויז רגרשן, שלאט לאט מכניסה משתנים למודל, במשקל קטן. משהו בה מאוד מזכיר את אדאבוסט.</p>
<p>ניזכר בפורוורד סטייג’וויז: אנחנו עושים סטנדרטיזציה למשתנים, ומחליטים על גודל צעד קטן אפסילון.</p>
<p>אנחנו בודקים מה השארית בין התצפיות Y לממוצע, מסמנים אותן בR. מאתחלים את המקדמים של כל המשתנים לאפס.</p>
<p>כעת מוצאים את המשתנה האחד שנמצא בקורלציה הגבוהה ביותר עם השארית, והוא המועמד להיכנס למודל. אם אכן הקורלציה גדולה מאיזשהו סף אנחנו מכניסים עוד קצת מהמשתנה למודל אבל רק אפסילון קטן מתוך הקורלציה הזאת.</p>
<p>מחשבים מחדש את השאריות, וחוזרים למצוא את המשתנה הJ הבא עם הכי הרבה קורלציה לשאריות, וכך הלאה, עד שאין משתנה יחיד עם קורלציה מספיק גדולה לשאריות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="a-general-forward-stagewise-additive-model-fsam">A general forward stagewise additive model (FSAM)</h3>
<div class="fragment">
<ol type="1">
<li>Initialize <span class="math inline">\(f_0(x) = 0\)</span></li>
<li>For <span class="math inline">\(m = 1\)</span> to <span class="math inline">\(M\)</span>:
<ol type="a">
<li>Compute: <span class="math display">\[(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + \beta b(x_i, \gamma)\right)\]</span></li>
<li>Set <span class="math inline">\(f_m(x) = f_{m - 1}(x) + \beta_m b(x; \gamma_m)\)</span></li>
</ol></li>
</ol>
</div>
<div class="fragment">
<ol start="3" type="1">
<li>Output: <span class="math inline">\(\hat{f}(x) = \sum_{m = 1}^M \beta_m b(x; \gamma_m)\)</span></li>
</ol>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>If <span class="math inline">\(L\)</span> is the squared loss, and finding <span class="math inline">\(\gamma_m\)</span> is finding the <span class="math inline">\(j\)</span>-th predictor <span class="math inline">\(\mathbf{x}_j\)</span> most correlated with with the residual</p>
<p><span class="math inline">\(\Rightarrow\)</span> this is forward stagewise regression!</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>את האלגוריתם שכרגע ראינו קל לכתוב הרבה יותר בתור מודל פורוורד אדיטיבי בשלבים, או פורוורד סטייג’וויז אדיטיב מודל:</p>
<p>נתחיל בחיזוי אפס לכל התצפיות.</p>
<p>בכל איטרציה M ניקח איזשהו פונקציה פשוטה של הנתונים שנקרא לה B, שיש לה פרמטרים גאמא. ננסה לעדכן את המודל שיש לנו עד כאן עם הליכה של צעד בטא בכיוון הפונקציה הזאת, באופן שיקטין לנו הכי הרבה איזשהו לוס. או כמו שרשום כאן, נמצא את הפרמטרים בטא וגאמא שיביאו למינימום את הלוס של עדכון פשוט כזה.</p>
<p>נעדכן את המודל עם הפרמטרים שמצאנו ונמשיך לאיטרציה הבאה.</p>
<p>המודל הסופי הוא אכן מודל אדיטיבי, הוא סכום ממושקל של הרבה מודלים קטנים ופשוטים. בביטוי הזה אנחנו גם מייד רואים את הרמז לאדאבוסט אבל זה עוד לא לגמרי ברור.</p>
<p>מכל מקום, אם L הוא שגיאה ריבועית, וגאמא בכל איטרציה פירושה פשוט למצוא את המשתנה האחד עם הקורלציה הכי גדולה לשארית, מה שיש כאן זה בדיוק פורוורד סטייג’וויז רגרשן, מלבד הבדלים זניחים. שם אתחלנו את כל התצפיות בממוצע Y וכאן באפס, שם עשינו את זה עד שלא נמצא יותר משתנה עם מספיק קורלציה לשאריות וכאן אנחנו עושים את זה מספר קבוע של צעדים מראש.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="claim-adaboost-is-also-a-fsam">Claim: AdaBoost is also a FSAM</h3>
<div class="fragment">
<ul>
<li>If <span class="math inline">\(L\)</span> is the exponential loss: <span class="math inline">\(L(y, f(x)) = \exp(-yf(x))\)</span></li>
<li>And, <span class="math inline">\(b(x; \gamma_m) = Gm(x)\)</span></li>
<li>We get AdaBoost!</li>
</ul>
</div>
<div class="fragment">
<div id="b8eed4e3" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-9-output-1.png" width="353" height="302"></p>
</figure>
</div>
</div>
</div>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Fast forward: plug-in <em>other</em> loss functions!</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז אני טוען, שגם אדאבוסט הוא מודל פורווד סטייג’וויז אדיטיב.</p>
<p>אני טוען שהלוס שהוא מביא למינימום בכל איטרציה הוא לוס אקספוננציאלי שלא ראינו עדיין, אקספוננט בחזקת מינוס Y שאני מזכיר שהוא מינוס או פלוס 1, כפול החיזוי f(X).</p>
<p>ואם הפונקציות הפשוטות שקראנו להן B הן הקלסיפיירים החלשים שלנו Gm בכל איטרציה – נקבל בדיוק את אדאבוסט.</p>
<p>אז לפני שנראה את זה, נכיר את הלוס האקספוננציאלי ולמה הוא סביר לבעיה שלנו: נניח שY הוא פלוס 1. אם אנחנו חוזים f(X) בדיד, שהוא גם פלוס ומינוס אחת, אז אם אנחנו חוזים מינוס 1 וטועים אנחנו מקבלים שגיאה של אקספוננט בחזקת 1 כלומר 2.7 בערך, ואם אנחנו חוזים 1 וצודקים אנחנו מקבלים שגיאה של אקספוננט בחזקת מינוס 1, כלומר 0.3 בערך.</p>
<p>אפשר אבל גם להרציף את הטעות הזאת, לחזות f(X) רציף, ואז ככל שהוא שלילי הטעות גדולה יותר, וככל שהוא חיובי הטעות שואפת לאפס.</p>
<p>ועוד דבר קטן, שוב למה אנחנו טורחים כל כך, רק כי זה מעניין? לא, המטרה שלנו היא להכליל לעוד פונקציות הפסד, ולהיות מסוגלים לעשות אדאבוסט לרגרסיה ולבעיות אחרות. אז בואו ניגש למשימה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="showing-adaboost-is-fsam">Showing AdaBoost is FSAM</h3>
<div>
<ul>
<li class="fragment"><p>Recall the goal in FSAM iteration <span class="math inline">\(m\)</span>: <span class="math display">\[(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + \beta b(x_i, \gamma_)\right)\]</span></p></li>
<li class="fragment"><p>For exponential loss: <span class="math display">\[(\beta_m, G_m) = \arg\min_{\beta, G} \sum_{i = 1}^n \exp\left[-y_i(f_{m-1}(x_i) + \beta G(x_i))\right]\]</span></p></li>
</ul>
</div>
<div class="fragment">
<p><span class="math inline">\(= \arg\min_{\beta, G} \sum_{i = 1}^n \exp\left[-y_if_{m-1}(x_i)\right]\exp\left[\beta G(x_i)\right]\)</span></p>
</div>
<div class="fragment">
<p><span class="math inline">\(= \arg\min_{\beta, G} \sum_{i = 1}^n w_i^{(m)}\exp\left[-\beta y_i G(x_i)\right]\)</span></p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>האלגוריתם של פורוורד סטייג’וויז אומר לנו בכל איטרציה למצוא את הצעד בטא והגאמא הכי טובים כדי לעשות מינימום ללוס.</p>
<p>נציב את הלוס האקספוננציאלי ונחליף את בי בקלסיפייר החדש שלנו Gm.</p>
<p>עכשיו יש פה אקספוננט בחזקת סכום שאני מפרק למכפלת האקספוננטים.</p>
<p>ואנחנו מסמנים את כל הביטוי הזה כמשקולת w_i שבכל צעד m מבטאת את הלוס באיטרציה האחרונה, אם היה חיזוי טוב היא תהיה קטנה ואם היה חיזוי גרוע, היא תהיה גדולה. נשים לב שהיא לא משתתפת במינימיזציה בגלל זה אני יכול לעשות את זה. הביטוי שקיבלנו כבר מתחיל להזכיר את השגיאה הממושקלת שאנחנו מורידים למינימום באדאבוסט אבל נצטרך עוד קצת אלגברה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="getting-g_m">Getting <span class="math inline">\(G_m\)</span></h3>
<div class="fragment">
<ul>
<li>Assuming <span class="math inline">\(\beta &gt; 0\)</span>, separate the criterion to “correct” + “incorrect” sums:</li>
</ul>
</div>
<div class="fragment">
<p><span class="math inline">\(\sum_{i = 1}^n w_i^{(m)}\exp\left[-\beta y_i G(x_i)\right] =\)</span> <span class="math inline">\(= e^{-\beta}\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i = G(x_i)\right] + e^{\beta}\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right]\)</span></p>
</div>
<div class="fragment">
<ul>
<li>Can also write as:</li>
</ul>
<p><span class="math inline">\(= (e^{\beta} - e^{-\beta})\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right] + e^{-\beta}\sum_{i = 1}^n w_i^{(m)}\)</span></p>
</div>
<div class="fragment">
<ul>
<li>AdaBoost stage 2a: <span class="math inline">\(G_m\)</span> minimizing the criterion is minimizing weighted error rate: <span class="math display">\[G_m = \arg\min_G \sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right]\]</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>עכשיו בואו נניח לרגע שהפרמטר בי שמתקבל מבעיה המינימיזציה הזאת הוא חיובי, ונראה שהקלסיפייר של אדאבוסט G_m הוא בדיוק הקלסיפייר שנקבל כאן:</p>
<p>יש כאן סכום ממושקל. כשהחיזוי נכון, y כפול G נותן 1 והאלמנט בסכום יהיה e בחזקת מינוס בטא. כשהחיזוי לא נכון, y כפול G נותן מינוס 1, והאלמנט בסכום יהיה e בחזקת בטא. אז אני יכול להפריד את הסכום הזה לשני סכומים, סכום החיזויים הנכונים שנותנים אי במינוס בטא ועוד סכום החיזויים הלא נכונים שנותנים אי בבטא.</p>
<p>אבל גם את זה אני יכול לרשום בצורה יותר קומפקטית שבה G שאני מנסה לעשות עליו מינימום, נמצא רק בביטוי אחד. הדרך לעשות את זה היא לתת לכל התצפיות את האלמנט אי במינוס בטא, ורק לחיזויים הנכונים להוסיף אי בבטא פחות אי במינוס בטא.</p>
<p>וזה מראה לי שכדי לקבל את קלסיפייר G האופטימלי בכל שלב m אני צריך לעשות בדיוק את מה שאדאבוסט שאף לעשות בשלב 2a. להגיע לקלסיפייר שיביא למינימום את שגיאת החיזוי הממושקלת. נשאר לי רק להראות שמדובר בדיוק באותן המשקולות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="getting-alpha_m">Getting <span class="math inline">\(\alpha_m\)</span></h3>
<div class="fragment">
<p><span class="math inline">\(\text{criterion} = (e^{\beta} - e^{-\beta})\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right] + e^{-\beta}\sum_{i = 1}^n w_i^{(m)}\)</span></p>
</div>
<div class="fragment">
<p><span class="math inline">\(\frac{\partial \text{criterion}}{\partial \beta} = (e^{\beta} + e^{-\beta})\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right] - e^{-\beta}\sum_{i = 1}^n w_i^{(m)} \to =0\)</span></p>
</div>
<div class="fragment">
<ul>
<li>Multiply by <span class="math inline">\(e^\beta\)</span>:</li>
</ul>
<p><span class="math inline">\((1 + e^{2\beta}) = \frac{\sum_{i = 1}^n w_i^{(m)}}{\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right]} = \frac{1}{err_m}\)</span></p>
</div>
<div class="fragment">
<ul>
<li>Take <span class="math inline">\(ln\)</span>, we get AdaBoost stages 2b and 2c:</li>
</ul>
<p><span class="math inline">\(\beta_m = \frac{1}{2}\ln\left[\frac{1-err_m}{err_m}\right] = \frac{1}{2}\alpha_m\)</span></p>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז בואו נראה שאנחנו מקבלים בדיוק את אותן המשקולות, ואיך מהאלגוריתם הכללי של פורוורד סטייג’וויז אנחנו מקבלים את הביטוי המוזר לאלפא-אם בכל איטרציה. נזכיר שבכל איטרציה מחשבים לוג של 1 פחות הטעות הממושקלת חלקי הטעות הממושקלת, והמשקולת הנוכחית מוכפלת פי אקספוננט בחזקת אלפא-אם אם היה חיזוי לא נכון.</p>
<p>אז הגענו לקריטריון שאני פשוט מעתיק שוב כאן, הבנו מה הG שיביא למינימום, נמצא עכשיו מה הבטא שיביא למינימום.</p>
<p>אנחנו גוזרים את הקריטריון לפי בטא, ומשווים לאפס. תשימו לב שאתם מבינים את הגזירה היא די פשוטה.</p>
<p>יש לנו עכשיו משוואה, נכפיל את שני האגפים באותו ביטוי חיובי, והנה אנחנו מוצאים שבטא שיביא למינימום את הקריטריון שלנו, קשור לביטוי שסימנו באלגוריתם אדאבוסט כerr, שגיאת החיזוי הממושקלת חלקי סך המשקולות.</p>
<p>אחרי עוד קצת אלגברה נגיע לביטוי סופי עבור הצעד בטא בכל איטרציה m, ששווה בדיוק לחצי הפרמטר אלפא-אם שאדאבוסט אמר לנו לחשב.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="getting-w_i">Getting <span class="math inline">\(w_i\)</span></h3>
<div class="fragment">
<ul>
<li>FSAM says:</li>
</ul>
<p><span class="math inline">\(f_m(x) = f_{m - 1}(x) + \beta_m b(x; \gamma_m) = f_{m - 1}(x) + \frac{1}{2}\alpha_m G_m(x)\)</span></p>
</div>
<div class="fragment">
<ul>
<li>Which means next iteration weights are:</li>
</ul>
<p><span class="math inline">\(w_i^{(m + 1)} = \exp\left[-y_if_{m+1}(x_i)\right] = \exp\left[-y_i(f_{m}(x) + \frac{1}{2}\alpha_m G_m(x))\right] =\)</span> <span class="math inline">\(= w_i^{(m)}\cdot\exp\left[-\frac{1}{2}\alpha_m y_i G_m(x)\right]\)</span></p>
</div>
<div class="fragment">
<ul>
<li>Given that <span class="math inline">\(-y_i G_m(x_i) = 2\cdot\mathbb{I}\left[y_i \neq G_m(x_i)\right] - 1\)</span>, we can write:</li>
</ul>
<p><span class="math inline">\(w_i^{(m + 1)} = w_i^{(m)} \cdot \exp\left[\alpha_m \cdot \mathbb{I}\left[y_i \neq G_m(x_i)\right]\right] \cdot Const\)</span></p>
</div>
<div class="fragment">
<ul>
<li>Equivalent to AdaBoost stage 2d.</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ולמה העובדה הזאת אומרת שהגענו בדיוק לאותו משקול?</p>
<p>כעת האלגוריתם פורוורד סטייג’וויז אומר לעדכן את החיזוי להיות החיזוי באיטרציה הקודמת f אמ-מינוס-1, ועוד, צעד בטא כפול הפונקציה בי עם הפרמטר גאמא הטוב ביותר שנמצא. אצלינו זה פשוט אומר החיזוי עד עכשיו ועוד חצי אלפא-אם כפול הקלסיפייר G הכי טוב שנמצא.</p>
<p>אבל זה אומר שהמשקולות שלנו בכל איטרציה הן בדיוק מה שאדאבוסט אומר לנו לחשב: המשקולות שהגענו אליהן לפני שני שקפים הן הלוס האקספוננצילי מהאיטרציה הקודמת. מציבים את הביטוי שפורוורד סטייג’וויז אומר לנו בfm. ומקבלים שהעדכון שווה למשקולת הקודמת כפול ביטוי שכבר מתחיל להזכיר את הביטוי שאנחנו רוצים להגיע אליו, שמערב את אלפא-אם.</p>
<p>הוא מזכיר מאוד כי הוא שקול, נשים לב שאפשר להגיע בקלות מהמכפלה של y כפול G לביטוי שאנחנו רוצים, האינדיקטור הרם יש לנו כאן חיזוי נכון או לא באמצעות הקשר מינוס y כפול G שווה לפעמיים האינדיקטור פחות 1.</p>
<p>אנחנו מציבים את הקשר הזה ורואים שהמשקולת באיטרציה הבאה היא תמיד תהיה המשקולת הנוכחית כפול קבוע אקספוננט בחזקת אלפא-אם אם היה חיזוי שגוי, או תישאר כפי שהיא אם היה חיזוי נכון. כל זה כפול איזה קבוע שמכפילים בו את כל המשקולות ולכן אפשר להתעלם ממנו.</p>
<p>בכך קיבלנו את שלב 2d של אדאבוסט, ובעצם סיימנו להראות שהוא מקרה פרטי של אלגוריתם כללי יותר, פורוורד סטייג’וויז אדיטיב, שנותן הנחיה מה לעשות עם כל לוס שנרצה, וגם לסטינג של רגרסיה. זה אולי ידהים אתכם לדעת שהתובנה הזאת לא הגיעה אלא לפחות חמש שנים אחרי שאדאבוסט פותח.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="classification-losses">Classification losses</h3>
<div class="columns">
<div class="column">
<div id="a598870b" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-10-output-1.png" width="427" height="356"></p>
</figure>
</div>
</div>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>The <span class="math inline">\(f(x)\)</span> minimizing exponential loss is also minimizing Binomial negative log-likelihood!</p>
</div>
</div>
</div>
</div>
</div><div class="column">
<ul>
<li><p>Misclassification (0/1 loss):</p>
<p><span class="math inline">\(L_{\text{misclass}}(y, f) = \begin{cases}
0 &amp; \text{if } y \cdot f(x) &gt; 0 \\
1 &amp; \text{otherwise}
\end{cases}\)</span></p></li>
<li><p>Exponential loss:</p>
<p><span class="math inline">\(L_{\text{exp}}(y, f) = \exp(-y \cdot f(x))\)</span></p></li>
<li><p>Binomial negative log-likelihood (cross-entropy):</p>
<p><span class="math inline">\(L_{\text{bin}}(y, f) =\)</span> <span class="math inline">\(\quad \log(1 + \exp(-2 \cdot y \cdot f(x))) / \log(2)\)</span></p></li>
<li><p>Squared error loss:</p>
<p><span class="math inline">\(L_{\text{sq}}(y, f) = (1 - y \cdot f(x))^2\)</span></p></li>
<li><p>Support vector (hinge Loss):</p>
<p><span class="math inline">\(L_{\text{hinge}}(y, f) = \max(0, 1 - y \cdot f(x))\)</span></p></li>
</ul>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז אם נישאר רגע עדיין בקלסיפיקציה, לא חייבים להישאר עם הלוס האקספוננציאלי, יש כל מיני לוסים שאולי מתאימים לקלסיפיקציה לשני קלאסים.</p>
<p>נראה שהם לא מאוד שונים אגב, וכולם עושים איזושהי הרצפה ללוס הכי טבעי אבל הלא רציף של מיסקלסיפיקיישן, שהבעיה איתו שהוא לא רגיש, המעבר בו בין עונש ללא-עונש נורא חד, הוא לא מעניש נורא אם החיזוי מאוד מאוד שלילי.</p>
<p>לוס בעייתי לבעיה שלנו, בדיוק כמו שדיברנו ברגרסיה לוגיסטית הוא הלוס הריבועי. כי הוא מתחיל להעניש גם אם החיזוי f(x) חיובי מדי.</p>
<p>נשארנו עם הלוס שלנו האקספוננציאלי, והלוס שראינו למשל כשדיברנו על קרוס אנתרופי כלוס אפשרי לעץ קלסיפיקציה. וכאן אחרי קצת אלגברה אנחנו רושמים אותו כפונקציה של המכפלה y כפול f, כדי שנוכל לראות אותו בהקשר של כולם. באופן מפתיע, הלוס הזה הוא גם הלוס המתקבל תחת נקודת מבט סטטיסטית, שוואי מתפלג ברנולי או בינומי עם איזושהי הסתברות שאותה אנחנו ממדלים בדיוק כמו ברגרסיה לוגיסטית, ואז הלוס שלנו הוא מינוס לוג הנראות.</p>
<p>זה דורש קצת אלגברה להראות שכל הדברים האלה מתכנסים לאותו דבר, אבל התובנה המדהימה שרציתי שתראו כאן, זה ששתי הפונקציות האלה, הפונקציה שלנו והפונקציה שמגיעה מנקודת מבט סטטיסטית, הן מאוד דומות, אחת היא הלוג של השניה בקירוב!</p>
<p>וזה אומר, שפונקצית החיזוי שאנחנו נמצא בכל שלב של אדאבוסט, שמביאה למינימום לוס אקספונציאלי, צריכה להיות אותה פונקצית חיזוי שתביא למינימום את הלוס של מינוס לוג הנראות, תחת המודל הבינומי!</p>
<p>ואיך עושים בוסטינג לרגרסיה? את זה נראה בחלק הבא.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="boosting-for-regression" class="slide level2 title-slide center">
<h2>Boosting for Regression</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ננסה עכשיו להגיע לאלגוריתם לבוסטינג לרגרסיה באותה גישה של פורוורד סטייג’וויז אדיטיב מודלינג.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="recap">Recap</h3>
<p>What the FSAM framework got us:</p>
<p><span class="math display">\[(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + \beta b(x_i, \gamma)\right)\]</span></p>
<ul>
<li>Classification:
<ul>
<li><span class="math inline">\(y \in \{-1, 1\}\)</span>, exponential loss, <span class="math inline">\(b(x_i, \gamma)\)</span> are weak learners <span class="math inline">\(G(x)\)</span> <span class="math inline">\(\to\)</span> AdaBoost</li>
</ul></li>
<li>Regression:
<ul>
<li><span class="math inline">\(y \in \mathbb{R}\)</span>, squared error loss, <span class="math inline">\(b(x_i, \gamma)\)</span> are single features <span class="math inline">\(\mathbf{x}_j\)</span> <span class="math inline">\(\to\)</span> Forward stagewise regression</li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li>What if we want to boost weak learners <span class="math inline">\(G(x)\)</span> for regression?</li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ניזכר שוב בהיי-לבל מה ראינו עד עכשיו.</p>
<p>האלגוריתם פורוורד סטייג’ווויז אדיטיב אומר לנו בכל איטרציה למצוא את הצעד בטא ואת הפונקציה הפשוטה בי האופטימליים, כדי להביא למינימום כל לוס, עם מודל אדיטיבי.</p>
<p>הראינו שבקלסיפיקציה בינארית איפה שוואי הוא 1 או מינוס 1, הלוס הוא אקספוננציאלי והפונקציות הפשוטות שלנו הן וויק קלסיפיירים G, מקבלים את אלגוריתם אדאבוסט.</p>
<p>הראינו שברגרסיה איפה שוואי הוא רציף, הלוס ריבועי והפונקציות הפשוטות שלנו הן כל פעם משתנה אחד ויחיד - מקבלים את פורוורד סטייג’וויז רגרשן.</p>
<p>השאלה המתבקשת היא האם אפשר לקחת את הלומדים החלשים האלה G מהאחד, ולשים אותם בשני, ולקבל גרסה ולידית של בוסטינג לרגרסיה. והתשובה היא לגמרי כן. נשים לב רק שברגרסיה אכן נקרא לG באופן כללי לרנרז או רגרסורז, הם כבר לא יהיהו קלסיפיירים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="boosted-trees-for-regression">Boosted trees for regression</h3>
<div>
<ul>
<li class="fragment">Specifically, let us focus on boosting trees: <span class="math inline">\(f(x) = \sum_{m = 1}^M T(x, \Theta_m)\)</span></li>
<li class="fragment">Where <span class="math inline">\(T(x, \Theta) = \sum_{j = 1}^J \gamma_j\mathbb{I}\left(X \in R_j\right)\)</span></li>
<li class="fragment">With parameters <span class="math inline">\(\Theta = \{R_j, \gamma_j\}_1^J\)</span></li>
<li class="fragment">“Building a regression tree” is equivalent to: <span class="math display">\[\hat{\Theta} = \arg\min_{\Theta} \sum_{i = 1}^n\left(y_{i} - T(x_i, \Theta)\right)^2\]</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>באופן ספציפי, נתמקד בעצים ונרצה לעשות להם בוסטינג. נרצה שהחיזוי שלנו יהיה סכום חיזויים של עצים או סכום ממושקל שאני רגע משמיט רק בשביל הקיצור. ולמה דווקא עצים? בגלל כל היתרונות שלהם שראינו בשיעור הקודם, עצים מטפלים בלי בעיה בסוגי משתנים מגוונים בלי הכנה נוספת, הם יודעים לטפל בתצפיות חסרות, הם לא-ליניאריים מצד אחד אבל לא גמישים מדי מצד שני, ומצד שני החיזוי שלהם בשורה התחתונה הוא לא מאוד טוב, שזה בדיוק מה שאנחנו רוצים בלומד חלש.</p>
<p>נזכור מהו עץ רגרסיה בשורה התחתונה. עץ רגרסיה הוא בעצם חלוקה של הדאטא לJ שכונות Rj, שבכל שכונה אנחנו חוזים מספר יחיד גאמא J.</p>
<p>כלומר כל עץ בעומק קבוע ניתן לרשום בעצמו כמודל אדיטיבי שהחיזוי שלו הוא סכום על כל השכונות, שיחפש לאיזו שכונה התצפית שייכת ויחזה עבורה גאמא J. וכאן אנחנו מתייחסים למודל הזה כמודל פרמטרי לכל דבר, לכל עץ עם מספר שכונות קבוע J יש פרמטרים תטא שמה הם? השכונות עצמן, והמספר שמתאים לכל שכונה, גאמא.</p>
<p>זה חשוב להבין את זה כי תיכף נזריק את התהליך הזה של בניית עץ אל תוך האלגוריתם פורוורד סטייג’וויז שלנו. לבנות עץ רגרסיה עם עומק קבוע עם לוס ריבועי זה פשוט למצוא את הפרמטרים תטא הכי טובים, למצוא את השכונות והחיזויים הכי טובים כדי לעשות מינימום לשגיאה ריבועית.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="boosted-trees-for-regression-1">Boosted trees for regression</h3>
<div class="fragment">
<ul>
<li>So FSAM with squared loss, no problem:</li>
</ul>
<p><span class="math inline">\(\Theta_m = \arg\min_{\Theta} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + T(x_i, \Theta)\right) =\)</span> <span class="math inline">\(= \arg\min_{\Theta} \sum_{i = 1}^n\left(y_i - f_{m-1}(x_i) - T(x_i, \Theta)\right)^2\)</span></p>
</div>
<div class="fragment">
<p><span class="math inline">\(= \arg\min_{\Theta} \sum_{i = 1}^n\left(r_{im} - T(x_i, \Theta)\right)^2\)</span></p>
</div>
<div>
<ul>
<li class="fragment">Because we know how to build such trees!</li>
<li class="fragment">To sum up: at each iteration <span class="math inline">\(m\)</span> build standard regression tree <span class="math inline">\(T(x, \Theta)\)</span> which best fits the current residuals <span class="math inline">\(\mathbf{r}_m\)</span></li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>At high-level, similar to AdaBoost!</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>אז אם אנחנו ברגרסיה וההפסד הריבועי, אין בעיה. בכל שלב של האלגוריתם סטייג’וויז נמצא את הפרמטרים תטא שמביאים למינימום את ההפסד הריבועי של המודל האדיטיבי. אבל מה זה אומר? לבנות את העץ הכי שחוזה הכי טוב את השאריות!</p>
<p>ולמה אין בעיה, כי זה בדיוק עץ הרגרסיה שאנחנו יודעים לבנות.</p>
<p>אז סיכום ביניים, אם הלוס הוא ריבועי, לעשות בוסטינג לעצי רגרסיה אומר בכל איטרציה m לחשב את השאריות ולבנות עץ רגרסיה לחיזוי השאריות. לעדכן את השאריות ולעבור לאיטרציה הבאה וכולי.</p>
<p>נשים לב שהפעולה הזאת תואמת מאוד את הרציונל של אדאבוסט - בכל שלב תמצה מה שהמודל עד כה לא הצליח לחזות טוב, וברגרסיה זה שאריות, ותמדל את הפער הזה. זה ממש שקול למשקול מחדש של הנתונים.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="what-about-more-robust-losses">What about more <span style="color:red;">robust</span> losses?</h3>
<div class="columns">
<div class="column">
<div id="7c5d242e" class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-11-output-1.png" width="427" height="356"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<ul>
<li><p>Squared Error Loss:</p>
<p><span class="math inline">\(L_{\text{sq}}(y, f) = (y - f(x))^2\)</span></p></li>
<li><p>Absolute Error Loss:</p>
<p><span class="math inline">\(L_{\text{abs}}(y, f) = |y - f(x)|\)</span></p></li>
<li><p>Huber Loss:</p></li>
</ul>
<div style="font-size: 50%;">
<p><span class="math inline">\(L_{\text{H}}(y, f) = \begin{cases}
   (y - f(x))^2 &amp; |y - f(x)| \leq \delta \\
   2\delta |y - f(x)| - \delta^2 &amp; \text{otherwise.}
   \end{cases}\)</span></p>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>הבעיה היא מה קורה אם אנחנו רוצים להכליל את הרעיון הזה לכל פונקצית הפסד, גם פונקציות שאנחנו קוראים להן רובסטיות או חסינות יותר מההפסד הריבועי.</p>
<p>סטטיסטיקה רובסטית זה תחום שאפשר לבלות עליו סמסטר שלם. ובכל זאת למה הכוונה – הכוונה היא לפונקציות הפסד שמושפעות פחות מתצפיות חריגות. הרי הלוס הריבועי מעניש אותנו בשארית בריבוע כשהמודל טועה. שארית אחת שלגביה הוא טועה מאוד יכולה לנפח את הלוס מאוד.</p>
<p>אופציה אחרת היא ההפסד בערך מוחלט, שיעניש אותנו בצורה ליניארית לשארית.</p>
<p>אופציה אחרת היא הפסד הובר למשל, שעד איזשהו קבוע דלתא יעניש את השארית בריבוע, ומדלתא יעניש אותה בצורה ליניארית כמו הערך המוחלט. ויש עוד הרבה גרסאות לפונקציות הפסד חסינות. העניין הוא שאנחנו לא יודעים לבנות עץ רגרסיה לכל פונקציה כזאת, זאת בעיה די קשה.</p>
<p>אז בחלק הבא נכליל את הפרוצדורה של בוסטינג שראינו אפילו יותר, וזה יאפשר לנו לטפל בכל פונקצית הפסד. זה גם יהיה אלגוריתם הבוסטינג המוכר והסופי שנגיע אליו בשיעור הזה, שנמצא בשימוש נרחב כל כך בתעשייה היום.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="gradient-boosting" class="slide level2 title-slide center">
<h2>Gradient Boosting</h2>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>הגירסה שהתקבעה בסופו של דבר לבוסטינג, נקראת גרדיאנט בוסטינג. לפעמים גרדיאנט בוסטינג מאשינז, לפעמים גרדיאנט בוסטינג טריז. מה הקשר בין מה שראינו לגרדיאנט, או לנגזרת של פונקציה?</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="gradient-descent-algorithms">Gradient descent algorithms</h3>
<p>Minimize a function <span class="math inline">\(J(\theta)\)</span> by moving in the opposite direction of the gradient: <span class="math display">\[\hat\theta_{i+1} = \hat\theta_i - \varepsilon \frac{\partial J(\theta)}{\partial \theta}\]</span></p>
<div id="b878bbcb" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-12-output-1.png" width="444" height="362"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>ניזכר בשיטה כללית למצוא מינימום של פונקציה מסובכת, כשאין לנו פתרון סגור. אנחנו מתחילים מאיזשהו ניחוש התחלתי לפרמטר שלנו, כאן נגיד פרמטר דו-מימדי תטא. מחשבים את הנגזרת של הפונקציה שלנו ביחס לפרמטר, כלומר הגרדיאנט. הגרדיאנט הוא בעצם הכיוון של ההשתנות הכי מהירה של הפונקציה. כמו להיות על הר גבוה ולחשב איפה הוא יורד הכי מהר. ואז, אנחנו עושים צעד קטן בגודל אפסילון במורד הגרדיאנט. מה שבפועל אומר לקחת את הפרמטרים תטא שיש לנו עד עכשיו, ולהחסיר מהם אפסילון קטן כפול הגרדיאנט.</p>
<p>אנחנו חוזרים על הצעד הזה שוב ושוב, כל פעם מסתכלים בנקודה שבה אנחנו נמצאים מהו הגרדיאנט והולכים המורד הגרדיאנט צעד קטן, עד שנמצא את הפרמטר שמביא למינימום א הפונקציה. לפונקציה כללית נגיע למינימום לוקאלי, ואם הפונקציה קמורה והצעד אפסילון מספיק קטן מובטח לנו שנגיע למינימום גלובלי.</p>
<p>אז מה הקשר למה שאנחנו עושים כאן? מאוחר יותר הבינו חוקרים שהאלגוריתם בוסטינג הפשוט שתיארנו רק לפני רגע, הוא ממש חיקוי של גרדיאנט דיסנט.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="gradient-boosting-machines">Gradient boosting machines</h3>
<div>
<ul>
<li class="fragment">Consider any loss <span class="math inline">\(L(y, \mathbf{f})\)</span> a function of <span class="math inline">\(n\)</span> data points <span class="math inline">\(\mathbf{f} = (f(x_1), \dots, f(x_n))\)</span></li>
<li class="fragment">At each iteration <span class="math inline">\(m\)</span>, calculate the gradient of <span class="math inline">\(L\)</span> by <span class="math inline">\(\mathbf{f}\)</span>, evaluated at <span class="math inline">\(\mathbf{f}_{m - 1}\)</span>: <span class="math display">\[\mathbf{g}_m = \frac{\partial L(y, \mathbf{f})}{\partial \mathbf{f}}\Bigr|_{\mathbf{f} = \mathbf{f}_{m - 1}}\]</span></li>
<li class="fragment">Move a small step <span class="math inline">\(\varepsilon\)</span> down this gradient: <span class="math display">\[\mathbf{f}_m = \mathbf{f}_{m - 1} - \varepsilon\mathbf{g}_m\]</span></li>
<li class="fragment">Technically, <span class="math inline">\(\varepsilon\)</span> can also be optimized at each iteration <span class="math inline">\(m\)</span> to minimize <span class="math display">\[\varepsilon_m = \arg\min_\varepsilon L(y, \mathbf{f}_{m - 1} - \varepsilon\mathbf{g}_m)\]</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>במקרה שלנו הפונקציה שאנחנו רוצים לעשות לה מינימום היא פונקצית הפסד, של הרבה תצפיות וחיזויים של התצפיות. הרעיון היפה הוא להתייחס לוקטור התחזיות שלנו כוקטור פרמטרים של הפונקציה. נכון שאלה לא ממש פרמטרים אלה תחזיות שמבוססות על מודל כמו עץ רגרסיה, אבל הרישום הזה מאפשר לנו לפתור את הבעיה שלנו באמצעות גרדיאנט דיסנט.</p>
<p>בכל איטרציה m, נרשום את הגרדיאנט של הפונקצית הפסד שלנו לפי התחזיות שהן עכשיו פרמטרים. ונציב בגרדיאנט את החיזויים האחרונים שיש בידינו.</p>
<p>כעת כדי לקבל חיזויים חדשים, ניקח את החיזויים הקודמים ונלך צעד קטן בכיוון הגרדיאנט, כלומר נחסר מהוקטור הזה צעד אפסילון קטן כפול וקטור הגרדיאנט. ואת זה נעשה שוב ושוב עד שנגיע לחיזויים שמביאים למינימום את פונקצית ההפסד שלנו.</p>
<p>אפשרות נוספת אגב היא לא להסתפק בצעד קטן אפסילון אלא בכל איטרציה לחשב את האפסילון שמתאים ספציפית לאיטרציה הזאת, אפסילון-אם. כלומר לחשב מהו גודל הצעד שיביא למינימום את ההפסד בנקודה הנוכחית לעומת התחזיות החדשות. בפועל לא נראה תמיד שיש יתרון לתוספת הזאת וצעד אפסילון קטן עובד יופי אבל נזכור את הגירסה הזאת גם כן.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="gradient-boosting-machines-1">Gradient boosting machines</h3>
<div>
<ul>
<li class="fragment">Eventually we would get: <span class="math display">\[\mathbf{f}_M = \sum_{i = 1}^M\varepsilon_m(-\mathbf{g}_m)\]</span></li>
<li class="fragment">But we are not interested in <span class="math inline">\(\mathbf{f}_M\)</span> for our training data, we wanted a model! What about <span class="math inline">\(x_0\)</span>?
<ul>
<li class="fragment">E.g. <span class="math inline">\(f(x) = \sum_{m = 1}^M \varepsilon T(x, \Theta_m)\)</span></li>
</ul></li>
<li class="fragment">Solution: at each iteartion <span class="math inline">\(m\)</span> approximate the negative gradient <span class="math inline">\(-\mathbf{g}_m\)</span> with a weak learner or regression tree! <span class="math display">\[\Theta_m = \arg\min_\Theta \sum_{i = 1}^n (-g_{im} - T(x_i, \Theta))^2\]</span></li>
</ul>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>בסופו של דבר נקבל שהחיזויים הסופיים שלנו אחרי M איטרציות הם מזכירים מודל אדיטיבי כמו שרצינו. החיזויים הסופיים הם סכום של צעד אפסילון כפול מינוס הגרדיאנט, ועוד צעד אפסילון כפול מינוס הגרדיאנט וכולי.</p>
<p>אבל כאן מופיע הקץ’ – זה לא המודל שרצינו! מה שמתואר כאן זה לא משין לרנינג. אם תתייחסו לn החיזויים כפרמטרים, במקרה הטוב פשוט תגיעו לחיזויים טובים ספציפית על מדגם הלמידה. כשתגיע תצפית חדשה X0 לא תדעו איך לחזות עליה. או במילים אחרות למדתם פרמטרים אבל לא למדתם מודל.</p>
<p>אנחנו רצינו מודל אדיטיבי, למשל סכום של עצים ממושקלים, או שכל אחד יופיע עם משקולת קטנה קבועה.</p>
<p>והנה הפתרון היפה לגשר בין שתי הגישות: בכל איטרציה, כן נלמד מודל, למשל עץ החלטה, והמודל הזה ינסה לקרב את הגרדיאנט!</p>
<p>שימו-לב, לא משנה מה הלוס, הוא יכול להיות הובר או ערך מוחלט, הוא יכול להיות אפילו לוס של קלסיפיקציה – העץ שאנחנו בונים תמיד יהיה עץ רגרסיה, עם הפסד ריבועי, שמנסה לקרב את הגרדיאנט. זה עץ שאמרנו שאנחנו יודעים לבנות די בקלות.</p>
<p>ואז, ברגע שיש לנו עץ כזה נוסיף אותו למודל, אולי עם איזו משקולת קטנה אפסילון, ובאיטרציה הבאה נחשב שוב את הגרדיאנט, נמדל אותו עם עץ, וכך הלא וכך הלאה.</p>
<p>לכן קוראים לכל הגישה גרדיאנט בוסטינג, מדובר על בוסטינג באמצעות גרדיאנט דיסנט. והיא מתאימה לכל פונקצית הפסד שניתן לחשב לה בקלות את הגרדיאנט, אף על פי שהלומדים שמרכיבים אותה הם עצי רגרסיה.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="gradients-of-common-loss-functions">Gradients of common loss functions</h3>
<table>
<colgroup>
<col style="width: 18%">
<col style="width: 32%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>setting</th>
<th>loss</th>
<th>gradient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regression</td>
<td><span class="math inline">\(\frac{1}{2}(y_i - f(x_i))^2\)</span></td>
<td><span class="math inline">\(y_i - f(x)\)</span></td>
</tr>
<tr class="even">
<td>Regression</td>
<td><span class="math inline">\(|y_i - f(x_i)|\)</span></td>
<td><span class="math inline">\(\text{sign}(y_i - f(x))\)</span></td>
</tr>
<tr class="odd">
<td>Regression</td>
<td>Huber</td>
<td><span class="math inline">\(\begin{cases} y_i - f(x) &amp; \text{if } |y - f(x)| \leq \delta \\ \delta\text{sign}(y_i - f(x)) &amp; \text{otherwise} \end{cases}\)</span></td>
</tr>
<tr class="even">
<td>Classification*</td>
<td>NLL</td>
<td><span class="math inline">\(\mathbb{I}\left[y_i = 1\right] - \hat{p}_i\)</span></td>
</tr>
</tbody>
</table>
<p>*Here <span class="math inline">\(y \in \{0, 1\}\)</span> and <span class="math inline">\(\hat{p}_i = \frac{\exp(\hat{y}_i)}{1 + \exp(\hat{y}_i)} = \frac{\exp(f_{m - 1}(x_i))}{1 + \exp(f_{m - 1}(x_i))}\)</span></p>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Notice for (half) squared loss the gradient is the residuals, as we got earlier.</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>תיכף נסכם את האלגוריתם הסופי אבל קודם נראה כמה קל עבור הלוסים שראינו עד כה למצוא את הגרדיאנט ביחס לחיזויים:</p>
<p>אם ההפסד ריבועי, ונכפיל פי חצי כדי לקבל תוצאה יפה, נקבל את השאריות. שימו-לב, זה אומר שהאלגוריתם בוסטינג הספציפי שראינו לפי רגע הוא מקרה פרטי של גרדיאנט בוסטינג. כי עם לוס ריבועי, וקטור הגרדיאנט הוא בעצם וקטור השאריות!</p>
<p>ואם הלוס הוא ערך מוחלט או הובר, יש לנו ביטויים סגורים ויפים לוקטור הגרדיאנט.</p>
<p>ואפילו אם הלוס הוא בכלל לוס של קלסיפיקציה בינארית, Y הוא אפס או אחת, ואנחנו משתמשים במינוס לוג הנראות הבינומית שראינו קודם בקלסיפיקציה. גם אז וקטור הגרדיאנט שנמדל הוא ביטוי פשוט וסגור, מינוס ההסתברות החזויה לתצפיות שהן 0 או 1 פחות ההסתברות החזויה לתצפיות שהן 1.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="gradient-boosting-trees-simplified">Gradient boosting trees (simplified)</h3>
<ol type="1">
<li>Initialize <span class="math inline">\(f_0(x) = \arg\min_\gamma \sum_{i = 1}^n L(y_i, \gamma)\)</span> (e.g.&nbsp;<span class="math inline">\(f_0(x) = \bar{y}\)</span>)</li>
<li>For <span class="math inline">\(m = 1\)</span> to <span class="math inline">\(M\)</span>:
<ol type="a">
<li>Compute pseudo-residuals: <span class="math display">\[\mathbf{r}_m = -\left[\frac{\partial L(y, \mathbf{f})}{\partial \mathbf{f}}\right]_{\mathbf{f} = \mathbf{f}_{m - 1}}\]</span></li>
<li>Fit a <strong>regression tree</strong> to data <span class="math inline">\((\mathbf{x}, \mathbf{r}_m)\)</span>, giving <span class="math inline">\(T(x, \Theta_m)\)</span></li>
<li>Update: <span class="math inline">\(f_m(x) = f_{m - 1}(x) + \varepsilon T(x, \Theta_m)\)</span> (or <span class="math inline">\(\varepsilon_m\)</span>)</li>
</ol></li>
<li>Output: <span class="math inline">\(\hat{f}(x) = \sum_{m = 1}^M \varepsilon T(x, \Theta_m)\)</span></li>
</ol>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>What parameters need tuning?</p>
</div>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>וככה נראה אלגוריתם גרדיאנט בוסטינג טריז בצור הכי פשוטה.</p>
<p>נתאחל את החיזויים להיות מה שמביא למינימום את פונקצית ההפסד שבחרנו. אם זה הפסד ריבועי נקבל את ממוצע Y, אם זה הפסד בערך מוחלט נקבל את חציון Y. אפשר להתחיל גם עם חיזוי אפס לכל התצפיות, זה גם יעבוד.</p>
<p>בכל איטרציה m:</p>
<p>נחשב את וקטור הגרדיאנט או הפסאודו-שאריות. למה זה נקרא פסאודו-שאריות, כי עם הפסד ריבועי מדובר ממש בשאריות (להדגים), ובאופן כללי וקטור שמבטא מה שהמודל לא הצליח לחזות עד כאן.</p>
<p>נתאים עץ רגרסיה לשאריות, כלומר על הגירסה הזאת של הדאטא, שאפשר לחשוב עליה כמו באדאבוסט כגירסה ממושקלת של הנתונים כך שכל תצפית מקבלת משקולת גדולה יותר אם לא הצלחנו לחזות אותה עד כה.</p>
<p>ונעדכן את המודל, נוסיף את העץ שלנו עם צעד אפסילון קטן שיכול להיות לרנינג רייט קבוע מראש או ספציפי לאיטרציה הנוכחית.</p>
<p>אחרי M איטרציות המודל שנקבל יהיה מודל אדיטיבי של אנסמבל של עצי החלטה, עם משקולת קטנה קבועה או משקולת ייחודית לכל אחד, שנבנו בצורה אדפטיבית, אחד אחרי השני, כל פעם על גרסה אחרת של הנתונים.</p>
<p>נשים לב שיש כאן כמה פרמטרים שצריך לעשות עליהם טיונינג: מספר העצים – בניגוד לרנדום פורסט שם מובטח לנו שלא נעשה אוברפיטינג עם עוד עצים, כאן אין הבטחה כזאת – אבל נגיד שעדיין בפועל על נתונים אמיתיים בוסטינג נוטה לתת תוצאות טובות עם הרבה עצים, במיוחד אם שולטים בעומק העצים. וזה בדיוק הפרמטר הבא - לאילו עצים בוסטינג יכול לסייע במיוחד, עצים עמוקים או עצים לא עמוקים? אנחנו רוצים וויק לרנרז, שלומדים תופעות פשוטות אבל יציבות, שלאט לאט כשנצרף את כולן נקבל מודל חזק, כלומר בפועל בבוסטינג אנחנו מעדיפים עצים לא עמוקים מדי, בדרך כלל עומק 2 עד 4, ואפילו גזעים של עצים ראינו שעובד.</p>
<p>והפרמטר האחרון הוא קצב הלמידה אפסילון, שהוא כמו גודל הצעד בגרדיאנט דיסנט, אם אנחנו אומרים שהוא קבוע לכל האיטרציות. אבל גם כאן בפועל לנתונים אמיתיים ערך דיפולטיבי כמו 0.01 או 0.001 עובד היטב.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="example-credit-data">Example: credit data</h3>
<div id="6e5ebdf0" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="c10_boosting_files/figure-revealjs/cell-13-output-1.png" width="536" height="434"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<div style="direction:rtl; font-size:16px">
<p>כאן אנחנו רואים את בוסטינג על הקרדיט דאטא, בהם אנחנו מנסים למדל עבור כל אדם את היתרה בחשבון שלה מכמה משתנים כמו אם הוא סטודנט או לא, או מה נתוני האשראי שלו. במקרה הזה אפשר לראות שבוסטינג משפר בהרבה את שגיאת החיזוי של עץ יחיד, הקו הכתום הוא ממוצע הMSE על תצפיות שהמודל לא ראה בפרוצדורת קרוס ולידיישן, ואנחנו משתמשים בערכים הדיפולטיביים לכל הפרמטרים שהתוכנה נותנת, כאן זה בוסטינג באמצעות sklearn בפייתון.</p>
<p>במקרה הזה לא נראה שיש נטייה למודל לעשות אוברפיטינג אם נוסיף עוד ועוד עצים, הוא גם מבצע קצת יותר טוב מרנדום פורסט שראינו בשיעור הקודם.</p>
<p>בשורה התחתונה בוסטינג שמבוסס על עצים הוא אחד האלגוריתמים הטובים ביותר אוף דה שלף להתאים לנתונים טבלאיים גדולים, יש לו הרבה מימושים מאוד מהירים כמו xgboost, ולפני מהפיכת הדיפ לרנינג הוא גם היה בשימוש רב בבעיות של קומפיוטר ויז’ן בהן הדאטא הוא תמונות.</p>
</div>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="quarto-auto-generated-content">
<p><img src="../Intro2SL_logo_white.jpg" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://intro2statlearn.github.io/mooc/" target="_blank">Intro to Statistical Learning</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../libs/revealjs/plugin/search/search.js"></script>
  <script src="../libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>