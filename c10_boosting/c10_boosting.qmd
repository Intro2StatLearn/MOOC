---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Boosting"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Boosting - Class 10

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## AdaBoost {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשיעור הזה נלמד על אחד האלגוריתמים שנמצאים הכי הרבה בשימוש במאשין לרנינג על נתונים גדולים ובמיוחד על נתונים טבלאיים. יש לו הרבה גירסאות והרבה מימושים מאוד פופולריים, אבל כולם בסופו של דבר נקראים: בוסטינג.

הגירסה הראשונה של האלגוריתם פותחה בתחילת שנות התשעים על-ידי צמד חוקרים בשם פרוינד ושפייר שקיבלו על הפיתוח שלהם את פרס גדל. הגירסה הזאת נקראת: אדאבוסט.
:::
:::

---

### Instead of averaging

::: {.incremental}
- Suppose $y \in \{-1, 1\}$
- Recall the final Random Forests model: $\hat{f}(x_0) = \text{sign}\left[\frac{1}{M}\sum_{m = 1}^M T_m(x_0)\right]$
- Why not $\hat{f}(x_0) = \text{sign}\left[\sum_{m = 1}^M \alpha_m T_m(x_0)\right]$?
  - If $M$ is the number of all possible trees $T_m$, then finding weights $\alpha_m$ is intractable!

- [Boosting]{style="color:red;"} finds $\alpha_m$ sequentially, *adaptively*:
  - Take a [weak classifier]{style="color:red;"} $G(x)$ with accuracy slightly better than random
    - for any dist. $P_{T}$ of the training data:
  $Err_{P_{T}} = \mathbb{E}_{P_{T}}\left(\mathbb{I}\left[y_i \neq G(x_i)\right]\right) \leq \frac{1}{2} - \gamma$
  - Apply it sequentially over modified (weighted) versions of the training data
  - Each time selecting the best $\alpha_m$ to get: $\hat{f}(x_0) = \text{sign}\left[\sum_{m = 1}^M \alpha_m G_m(x_0)\right]$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אדאבוסט עוסק ספציפית בקלסיפיקציה לשני קלאסים, 1 ומינוס 1. מאוחר יותר נרחיב אותו לבעיות אחרות.

בסיטואציה הזאת, מה אמרנו שיעשה אלגוריתם כמו רנדום פורסט? הוא יגדל M עצים שנקרא להם T, כל פעם על גירסה קצת אחרת של הנתונים, וכשתגיע תצפית חדשה הוא ימצע את הסיווג שלהם, ואם אנחנו רוצים סיווג סופי של 1 או מינוס 1, ניקח את פונקציה sign.

מהי ביקורת סבירה על רנדום פורסט? למה למצע. אולי יש עצים טובים יותר, ועצים טובים פחות, ואנחנו צריכים לא ממוצע פשוט שלהם אלא ממוצע משוקלל, עם משקולת אלפא-אם לכל עץ.

אבל איך נמצא את המשקולות האופטימליות? הרי אם M הוא מספר כל העצים האפשריים, אולי אפילו נגביל את העומק שלהם, לכל סט נתונים סביר, זה מספר ענק. זה לא שאנחנו יכולים לגדל את כולם ואז להתחיל לבדוק מה סט המשקולות הכי טוב למצע אותם.

אבל בוסטינג יעשה בדיוק את זה, הוא ימצא את המשקולות והעצים האלה בצורה סדרתית אחד אחרי השני, בצורה אדפטיבית.

זה לא חייב להיות עץ, זה יכול להיות כל קלסיפייר חלש, או וויק קלסיפייר G, והכוונה בחלש שיש לו דיוק קצת יותר טוב מאקראי.

אם אנחנו רוצים להיות קצת יותר רשמיים, נניח שסיכוי לסיווג שגוי הוא חצי, אז הקלסיפייר שלנו יכול להשיג על המדגם הנתון שגיאה של קצת פחות מחצי, חצי פחות איזשהו פרמטר גאמא קטן, וזה לכל משקול של הנתונים. אנחנו מרדדים קצת את ההגדרה הרשמית אבל זה מספיק טוב בשבילנו.

ואת הקלסיפייר הזה נתאים לנתונים שלנו, כל פעם על גירסה קצת אחרת כשכל תצפית מקבלת משקולת מתעדכנת,

נתאים את הקלסיפייר וגם נבחר במשקולת הכי טובה עבורו לפי איזושהי פונקצית הפסד, וזה באמת יהיה החיזוי הסופי, הסיין של סכום הקלסיפיירים הממושקלים שאימנו.
:::
:::

---

### AdaBoost

1. Initialize observations weights $w_i = 1/n$ for $i = 1, \dots, n$
2. For $m  = 1$ to $M$:
    (a) Fit classifier $G_m(x)$ to the training data using **weights** $w_i$
    (b) Compute weighted classification error: $$err_m = \frac{\sum_{i = 1}^n w_i\mathbb{I}\left[y_i \neq G_m(x_i)\right]}{\sum_{i = 1}^n w_i}$$
    (c) Compute coefficient $\alpha_m = \ln\left[\frac{1-err_m}{err_m}\right]$
    (d) Update weights: $w_i \leftarrow w_i \cdot \exp\left[\alpha_m \cdot \mathbb{I}\left[y_i \neq G_m(x_i)\right]\right]$
3. Output: $\hat{f}(x) = \text{sign}\left[\sum_{m = 1}^M \alpha_m G_m(x)\right]$

::: {.fragment}
::: {.callout-note}
What does it mean training a classification tree with weighted data?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ככה נראה אלגוריתם האדאבוסט הרשמי, לא להיבהל, הוא דוקא די פשוט.

אנחנו מתחילים עם משקולות שוות לכל התצפיות, אם יש n תצפיות זה יוצא 1 חלקי n.

ועכשיו בכל איטרציה, אנחנו מגדלים עץ על התצפיות שלנו במשקול הנוכחי w_i. תיכף נדבר על מה זה אומר, לגדל עץ עם משקולות על התצפיות, לא נתקלנו בזה קודם.

מכל מקום, אנחנו מחשבים את השגיאה הממושקלת הנוכחית, כלומר סוכמים את כל השגיאות כאשר כל אחת נכפלת במשקל שלה, ומחלקים בסך המשקולות.

המשקולת לכל הקלסיפייר אלפא-אם מחושבת עכשיו והיא שווה לביטוי שרשום כאן שמבוסס על השגיאה הממושקלת ארר.

והשלב האחרון באיטרציה, עדכון המשקולות לאיטרציה הבאה: המשקולת של כל תצפית תוכפל באקספוננט בחזקת אלפא-אם אם הקלסיפייר הנוכחי עשה טעות על התצפית הזאת, וכפול אקספוננט בחזקת אפס אחרת, כלומר כפול 1 -- או, אם צדקנו המשקולת תישאר כפי שהיא.

נשים לב למה זה הגיוני -- אם טעינו, הכפלה פי אקספוננט בחזקת אלפא, תגדיל את המשקולת, וככה באיטרציה הבאה הקלסיפייר שלנו יצטרך לעבוד קשה יותר כדי לסווג אותה נכון, בניגוד לתצפיות עם משקולות קטנות. עוד דבר שנשים אליו לב, זה שעצים עם שגיאה ארר מאוד קטנה יקבלו משקולות אלפא-אם מאוד גדולות, ועצים עם שגיאה מאוד גדולה יקבלו משקולות אלפא-אם מאוד קטנות.

בכל אופן האאוטפוט של של אדאבוסט הוא מה שרצינו, ממוצע משוקלל של וויק קלסיפיירים, שאנחנו מפעילים עליו פונקצית סיין כדי לקבל חיזוי סופי של 1 ומינוס 1.

נחזור רגע לעניין המשקול - הקלסיפייר שלנו חייב לדעת לטפל נכון בתצפיות ממושקלות. למשל איך מגדלים עץ עם משקולות על התצפיות?
אז אם זה עץ רגרסיה, הלוס שאנחנו עושים עליו מינימום בכל פיצול הוא הRSS, אבל אנחנו לוקחים RSS ממושקל.

ואם זה עץ קלסיפיקציה, אז הלוס שלנו הוא למשל מדד הג'יני, ואנחנו צריכים לחשב בכל צומת את אחוז התצפיות שהן פלוס או מינוס אחת, אז במקום לחשב אותן בצורה הרגילה נחשב את סכום משקולות התצפיות שהן אחת חלקי סכום המשקולות של התצפיות בצומת.

מכל מקום, כעת ברור למה קוראים לאלגוריתם אדאבוסט. הוא לוקח קלסיפייר חלש יחסית, ועושה לו בוסט, בצורה אדאפטיבית. בחלק הבא נראה שבצורה הזאת יש לנו הבטחה, שאנחנו יכולים להוריד את השגיאה על מדגם הלמידה להיות קטנה כרצוננו, כתלות במספר האיטרציות ובפרמטר השגיאה גאמא שאנחנו יכולים להבטיח לכל וויק קלסיפייר. קטנה כרצוננו זה אומר אפילו אפס.
:::
:::

---

## AdaBoost Example{.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בואו נראה דוגמה פשוטה של אדאבוסט, בדו-מימד.
:::
:::

---

### AdaBoost with Tree Stumps

```{python}
#| echo: false

import numpy as np
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# Set seed for reproducibility
np.random.seed(42)

# Generate 10 random 2D points and binary labels {-1, 1}
n = 10
X = np.random.randn(n, 2)
y = np.random.choice([-1, 1], size=n)

# Initial weights, all equal to 1/n
w = np.ones(n) / n

x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1

for i in range(len(X)):
  if y[i] == -1:
    plt.scatter(X[i, 0], X[i, 1], color='blue', s=500*w[i], marker='o', edgecolor='black')
  else:
    plt.scatter(X[i, 0], X[i, 1], color='red', s=500*w[i], marker='s', edgecolor='black')
  plt.text(X[i, 0], X[i, 1], f'{w[i]:.2f}', color='black', fontsize=9,
    verticalalignment='bottom', horizontalalignment='right')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.show()
```

::: {.fragment}
::: {.callout-note}
Can a tree stump get to $\overline{err} = 0$? Can a deep decision tree?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בדוגמה שלנו יש 10 תצפיות בשני משתנים, ונניח שהאדומות הן פלוס 1 והכחולות הן מינוס 1.

הוויק קלסיפייר שלנו יהיה גזעים של עצים או טרי סטאמפס, כלומר אנחנו מסוגלים בכל איטרציה לעשות רק פיצול אחד על משתנה אחד.

זה קלסיפייר ממש חלש, נכון? נניח האם אפשר עם הקלסיפייר הזה להגיע על הנתונים האלה לשגיאת חיזוי אפס? אי אפשר. המשמעות של טרי-סטאמפ היא שאנחנו יכולים רק לחלק את המרחב של שני המשתנים חלוקה אחת מקבילה לאחת הצירים.

אם היינו מאפשרים עץ עמוק יותר האם היינו צריכים בוסטינג? כנראה שלא (להדגים). אבל זאת רק דוגמה פשוטה.
:::
:::

---

### AdaBoost: iteration 1

```{python}
#| echo: false

def plot_status(X, y, classifier, w, iteration, err, alpha):
    # Create mesh grid
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                         np.arange(y_min, y_max, 0.01))

    # Plot for the current weak classifier
    Z_weak = classifier.predict(np.c_[xx.ravel(), yy.ravel()])
    Z_weak = Z_weak.reshape(xx.shape)

    # Plot weak classifier decision boundary
    plt.contourf(xx, yy, Z_weak, alpha=0.3, cmap=ListedColormap(['blue', 'red']))
    for i in range(len(X)):
        if y[i] == -1:
            plt.scatter(X[i, 0], X[i, 1], color='blue', s=500*w[i], marker='o', edgecolor='black')
        else:
            plt.scatter(X[i, 0], X[i, 1], color='red', s=500*w[i], marker='s', edgecolor='black')
        plt.text(X[i, 0], X[i, 1], f'{w[i]:.2f}', color='black', fontsize=9,
                    verticalalignment='bottom', horizontalalignment='right')
    plt.title(f'Weak Classifier (Iteration {iteration})\nerr = {err:.3f}, alpha = {alpha:.3f}')
    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)

    plt.show()

# Reset stumps and alphas for clean state
stumps = []
alphas = []

iteration = 1

# Fit a tree stump (DecisionTreeClassifier with max_depth=1)
stump = DecisionTreeClassifier(max_depth=1)
stump.fit(X, y, sample_weight=w)

# Store the stump and alpha values
stumps.append(stump)

# Calculate weighted predictions and error
pred = stump.predict(X)
err = np.sum(w * (pred != y)) / np.sum(w)

# Calculate alpha
alpha = np.log((1 - err) / err)
alphas.append(alpha)

# Plot weak classifier
plot_status(X, y, stump, w, iteration, err, alpha)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אם אנחנו מתחילים עם כל התצפיות ממושקלות אותו הדבר, כלומר עשירית, הטרי סטאמפ הראשון מוצא לנכון לעשות את החלוקה על פי המשתנה השני, מתחת יחזה כחול ומעל יחזה אדום.

השגיאה הממושקלת שלנו היא שני סיווגים שגויים עם משקולות עשירית חלקי סך משקולות של עשר עשיריות או 0.2. והאלפא שלנו תהיה לוג של 0.8 חלקי 0.2 כלומר לוג 4 או 1.386.

אנחנו ממשקלים מחדש את התצפיות, כאשר המשקולות של התצפיות עם סיווג נכון נשארות עשירית, והמשקולות של התצפיות שסיווגנו לא נכון מוכפלות פי אקספוננט בלוג-4 כלומר פי 4, הן יהיו 0.4.

אנחנו שומרים את המשקולות, את אלפא ואת העץ בצד וממשיכים לעץ הבא.
:::
:::

---

### AdaBoost: iteration 2

```{python}
#| echo: false

# Update weights
w = w * np.exp(alpha * (pred != y))
# w /= np.sum(w)  # Normalize weights

iteration = 2

# Fit a tree stump (DecisionTreeClassifier with max_depth=1)
stump = DecisionTreeClassifier(max_depth=1)
stump.fit(X, y, sample_weight=w)

# Store the stump and alpha values
stumps.append(stump)

# Calculate weighted predictions and error
pred = stump.predict(X)
err = np.sum(w * (pred != y)) / np.sum(w)

# Calculate alpha
alpha = np.log((1 - err) / err)
alphas.append(alpha)

# Plot weak classifier
plot_status(X, y, stump, w, iteration, err, alpha)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
באיטרציה הבאה אנחנו באמת רואים את המשקולות שחישבנו, ומגדלים סטאמפ חדש. הסטאמפ החדש רואה את הנתונים אחרת, הפעם הוא שם הרבה יותר דגש על לסווג נכון את התצפיות שלא סווגו נכון בעבר ויש להן משקולת של 0.4. ואכן הוא מחלק את המרחב בצורה שונה לחלוטין.

אבל זה יוצר שגיאה ממושקלת חדשה, יש לנו 3 טעויות במשקל כולל 0.3 חלקי סכום המשקלים שהוא 1.6 כלומר שגיאה של 0.188.

האלפא החדשה היא לוג של 0.812 חלקי 0.188, יוצא 1.46.

ושוב נגדיל את המשקל של התצפיות שחזינו לא נכון, נכפיל עשירית פי אקספוננט בחזקת 1.466, צריך לתת 0.43.
:::
:::

---

### AdaBoost: iteration 3

```{python}
#| echo: false

# Update weights
w = w * np.exp(alpha * (pred != y))
# w /= np.sum(w)  # Normalize weights

iteration = 3

# Fit a tree stump (DecisionTreeClassifier with max_depth=1)
stump = DecisionTreeClassifier(max_depth=1)
stump.fit(X, y, sample_weight=w)

# Store the stump and alpha values
stumps.append(stump)

# Calculate weighted predictions and error
pred = stump.predict(X)
err = np.sum(w * (pred != y)) / np.sum(w)

# Calculate alpha
alpha = np.log((1 - err) / err)
alphas.append(alpha)

# Plot weak classifier
plot_status(X, y, stump, w, iteration, err, alpha)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
וככה אנחנו ממשיכים לסטאמפ הבא.
:::
:::

---

### AdaBoost: iteration 4

```{python}
#| echo: false

# Update weights
w = w * np.exp(alpha * (pred != y))
# w /= np.sum(w)  # Normalize weights

iteration = 4

# Fit a tree stump (DecisionTreeClassifier with max_depth=1)
stump = DecisionTreeClassifier(max_depth=1)
stump.fit(X, y, sample_weight=w)

# Store the stump and alpha values
stumps.append(stump)

# Calculate weighted predictions and error
pred = stump.predict(X)
err = np.sum(w * (pred != y)) / np.sum(w)

# Calculate alpha
alpha = np.log((1 - err) / err)
alphas.append(alpha)

# Plot weak classifier
plot_status(X, y, stump, w, iteration, err, alpha)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ולסטאמפ הבא.
:::
:::

---

### AdaBoost: iteration 5

```{python}
#| echo: false

# Update weights
w = w * np.exp(alpha * (pred != y))
# w /= np.sum(w)  # Normalize weights

iteration = 5

# Fit a tree stump (DecisionTreeClassifier with max_depth=1)
stump = DecisionTreeClassifier(max_depth=1)
stump.fit(X, y, sample_weight=w)

# Store the stump and alpha values
stumps.append(stump)

# Calculate weighted predictions and error
pred = stump.predict(X)
err = np.sum(w * (pred != y)) / np.sum(w)

# Calculate alpha
alpha = np.log((1 - err) / err)
alphas.append(alpha)

# Plot weak classifier
plot_status(X, y, stump, w, iteration, err, alpha)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ולסטאמפ הבא.
:::
:::

---

### AdaBoost: aggregate

::: {.fragment}
$\hat{f}(x) = \text{sign}\left[\sum_{m = 1}^M \alpha_m G_m(x)\right] =$

$= \text{sign}\left[1.386G_1(x) + 1.466G_2(x) + 0.934G_3(x) + 1.299G_4(x) + 1.126G_5(x)\right]$
:::

::: {.fragment}

```{python}
#| echo: false

# Plot for the aggregated classifier
# Create mesh grid
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                      np.arange(y_min, y_max, 0.01))

agg_pred = np.zeros_like(xx, dtype=float)
y_pred = np.zeros_like(y, dtype=float)
for a, stump in zip(alphas, stumps):
    agg_pred += a * stump.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    y_pred += a * stump.predict(X)
Z_agg = np.sign(agg_pred)
y_pred_agg = np.sign(y_pred)
overall_err = np.mean(y_pred_agg != y)

# Plot aggregated classifier decision boundary
plt.contourf(xx, yy, Z_agg, alpha=0.3, cmap=ListedColormap(['blue', 'red']))
for i in range(len(X)):
    if y[i] == -1:
        plt.scatter(X[i, 0], X[i, 1], color='blue', s=500*0.1, marker='o', edgecolor='black')
    else:
        plt.scatter(X[i, 0], X[i, 1], color='red', s=500*0.1, marker='s', edgecolor='black')
    # plt.text(X[i, 0], X[i, 1], f'{w[i]:.2f}', color='black', fontsize=9,
    #             verticalalignment='bottom', horizontalalignment='right')
plt.title(f'Aggregated Classifier (after {iteration} iterations)\noverall_err = {overall_err:.3f}')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)

plt.show()
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ואם נניח שאנחנו עושים את זה 5 איטרציות, כלומר 5 סטאמפים, נעצור.

החיזוי הסופי שלנו לכל נקודה במרחב הזה יהיה פונקציית סיין, על הממוצע המשוקלל של הטרי סטאמפס, G1 עד G5.

אם נצייר את תחום ההחלטה הזה נראה שהוא כבר אחרי 5 איטרציות הגיע לשגיאה אפס, הוא מסווג נכון את כל התצפיות, ובאופן מרשים אדאבוסט לקח את הקלסיפייר החלש הזה שמסוגל למעט מאוד, ועשה לו בוסטינג בצורה אדאפטיבית ככה שהוא יכול לתאר גבול החלטה הרבה יותר מורכב.
:::
:::

---

### AdaBoost Guarantee

::: {.fragment}
::: {.callout-tip}
Theorem:

$$\overline{err} = \sum_{i = 1}^n \mathbb{I}\left[y_i \neq \hat{f}(x_i)\right] \le \left(1-4\gamma^2\right)^{M/2} \le e^{-2M\gamma^2}$$
:::
:::

::: {.incremental}
- This means for a large enough $M$ we can get $\overline{err}$ as low as we want (on training data)!

- Specifically, to make the upper bound $e^{-2M\gamma^2} < \frac{1}{n}$ (i.e. $\overline{err} = 0$) $\Rightarrow M > \frac{\log(n)}{2\gamma^2}$

- A weak classifier which can only guarantee up to 40\% error at each iteration ($\gamma = 0.1$):
  - after $M = 100$ iterations will be boosted to give $\overline{err} \le (1 - 4 \cdot 0.1^2)^{(100/2)} = 0.96^{50} \approx 0.13$ error

  - if $n = 1000$, need $M > \log(1000) / 2 \cdot 0.1^2 \approx 345$ iterations to get $\overline{err} = 0$

- Really nice proof in Freund \& Schapire (1997)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כמובטח, אנחנו יכולים להביא את השגיאה על מדגם הלמידה להיות קטנה כרצוננו. ספציפית אנחנו יכולים לחסום אותה על-ידי הביטוי שמופיע כאן, 1 פחות ארבעה גאמא בריבוע בחזקת M חלקי 2. ואפשר להראות בקלות שחסם פשוט יותר הוא סדר גודל של אי בחזקת מינוס פעמיים מספר האיטרציות כפול גאמא בריבוע.

למשל, אם רוצים לחסום את הטעות שלא תהיה גדולה יותר מ1 חלקי n, מה שבפועל אומר שהיא חייבת להיות אפס, אנחנו לא מוכנים לעשות אפילו שגיאה אחת, אז אחרי קצת אלגברה אפשר לקבל חסם תחתון על מספר האיטרציות שאנחנו חייבים לבצע, שהוא סדר גודל של לוג-מספר התצפיות מחולק בפרמטר גאמא בריבוע. שוב כל זה בהנחה שאנחנו מבטיחים שבכל איטרציה הקלסיפייר שלנו טועה עד כדי מרחק גאמא קטן מחצי, השגיאה האקראית.

אז אם לדוגמא יש לכם קלסיפייר די בינוני, הוא יכול לעשות שגיאה של עד 40 אחוז או גאמא שווה 0.1, אחרי 100 איטרציות, הוא הופך להיות קלסיפייר לא רע בכלל עם שגיאה לא גדולה יותר מ13 אחוז, פשוט מציבים בחסם.

ואם לדוגמא אנחנו יודעים שיש אלף תצפיות במדגם הלמידה, כדי להגיע לשגיאה אפס אנחנו יודעים שצריך לפחות 345 איטרציות.

לא נוכיח את זה כאן אבל מי שרוצה יכול לקרוא את אחד המאמרים המקוריים של פרוינד ושפייר, זה פשוט הרבה אלגברה מאוד מאוד נחמדה.
:::
:::

---

## AdaBoost as Additive Stagewise Modeling {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Questions from AdaBoost

- Does it optimize a specific loss? Can we plug-in a different loss?

- What about regression?

- What about $K$-class classification?

- Is there a probabilistic justification? A likelihood-based approach?

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הרבה שאלות צריכות לעלות לנו כשאנחנו מסתכלים על אדאבוסט.

בחשיבה שלנו קודם מגדירים פונקצית הפסד ואז עושים לה מינימיזציה - האם אדאבוסט עושה מינימיזציה לאיזשהו הפסד, ואם כן האם אפשר לייצר אלגוריתמים דומים עם פונקציות הפסד אחרות?

למשל האם האלגוריתם טוב רק לקלסיפיקציה בינארית, או שאנחנו יכולים ליצור משהו דומה לרגרסיה או לקלסיפיקציה ליותר מ2 קלאסים?

ואם אמרנו שההוכחה שהאלגוריתם עובד היא פשוט הרבה אלגברה - האם יש לאלגוריתם הצדקה הסתברותית? זה מזכיר את העובדה שראינו כיצד רגרסיה ליניארית היא קודם כל פתרון סגור באלגברה, שאפשר להצדיק אותו גם מנקודת מבט סטטיסטית, עם הנחה של רעש שמתפלג נורמלית וכולי. יכול להיות שגם כאן נמצא נקודת מבט סטטיסטית?
:::
:::

---

### AdaBoost sure sounds familiar

::: {.fragment}
Recall: **Forward stagewise regression**
:::

::: {.fragment}
0. Standardize all features, input some $\tau_{thresh} \in (0, 1)$ and $\varepsilon > 0$ step size
1. Residual $\mathbf{r} = \mathbf{y} - \bar{y}$, $\beta_1, \dots, \beta_p = 0$
2. Find the predictor $\mathbf{x}_j$ most correlated with $\mathbf{r}$, and let $\tau = Corr(\mathbf{r}, \mathbf{x}_j)$
3. While $|\tau| > \tau_{thresh}$:
    i. Update $\beta_j \leftarrow \beta_j + \delta_j$, where $\delta_j = \varepsilon \cdot \text{sign}(\tau)$
    ii. Update $\mathbf{r} \leftarrow \mathbf{r} - \delta_j\mathbf{x}_j$
    iii. Find the predictor $\mathbf{x}_j$ most correlated with $\mathbf{r}$, and let $\tau = Corr(\mathbf{r}, \mathbf{x}_j)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מסתבר שבאמת אפשר להכליל את אדאבוסט, למשפחה של אלגוריתמים, שאחד מהם ראינו והבטחתי שעוד נראה שוב.

כשדיברנו על שיטות לבחירת משתנים ברגרסיה ליניארית, ראינו שיטה בשם פורוורד סטייג'וויז רגרשן, שלאט לאט מכניסה משתנים למודל, במשקל קטן. משהו בה מאוד מזכיר את אדאבוסט.

ניזכר בפורוורד סטייג'וויז: אנחנו עושים סטנדרטיזציה למשתנים, ומחליטים על גודל צעד קטן אפסילון.

אנחנו בודקים מה השארית בין התצפיות Y לממוצע, מסמנים אותן בR. מאתחלים את המקדמים של כל המשתנים לאפס.

כעת מוצאים את המשתנה האחד שנמצא בקורלציה הגבוהה ביותר עם השארית, והוא המועמד להיכנס למודל. אם אכן הקורלציה גדולה מאיזשהו סף אנחנו מכניסים עוד קצת מהמשתנה למודל אבל רק אפסילון קטן מתוך הקורלציה הזאת.

מחשבים מחדש את השאריות, וחוזרים למצוא את המשתנה הJ הבא עם הכי הרבה קורלציה לשאריות, וכך הלאה, עד שאין משתנה יחיד עם קורלציה מספיק גדולה לשאריות.
:::
:::

---

### A general forward stagewise additive model (FSAM)

::: {.fragment}
1. Initialize $f_0(x) = 0$
2. For $m = 1$ to $M$:
    (a) Compute:
    $$(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + \beta b(x_i, \gamma)\right)$$
    (b) Set $f_m(x) = f_{m - 1}(x) + \beta_m b(x; \gamma_m)$
:::

::: {.fragment}
3. Output: $\hat{f}(x) = \sum_{m = 1}^M \beta_m b(x; \gamma_m)$
:::

::: {.fragment}
::: {.callout-note}
If $L$ is the squared loss, and finding $\gamma_m$ is finding the $j$-th predictor $\mathbf{x}_j$ most correlated with the residual

$\Rightarrow$ this is forward stagewise regression!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
את האלגוריתם שכרגע ראינו קל לכתוב הרבה יותר בתור מודל פורוורד אדיטיבי בשלבים, או פורוורד סטייג'וויז אדיטיב מודל:

נתחיל בחיזוי אפס לכל התצפיות.

בכל איטרציה M ניקח איזושהי פונקציה פשוטה של הנתונים שנקרא לה B, שיש לה פרמטרים גאמא. ננסה לעדכן את המודל שיש לנו עד כאן עם הליכה של צעד בטא בכיוון הפונקציה הזאת, באופן שיקטין לנו הכי הרבה איזשהו לוס. או כמו שרשום כאן, נמצא את הפרמטרים בטא וגאמא שיביאו למינימום את הלוס של עדכון פשוט כזה.

נעדכן את המודל עם הפרמטרים שמצאנו ונמשיך לאיטרציה הבאה.

המודל הסופי הוא אכן מודל אדיטיבי, הוא סכום ממושקל של הרבה מודלים קטנים ופשוטים. בביטוי הזה אנחנו גם מייד רואים את הרמז לאדאבוסט אבל זה עוד לא לגמרי ברור.

מכל מקום, אם L הוא שגיאה ריבועית, וגאמא בכל איטרציה פירושה פשוט למצוא את המשתנה האחד עם הקורלציה הכי גדולה לשארית, מה שיש כאן זה בדיוק פורוורד סטייג'וויז רגרשן, מלבד הבדלים זניחים. שם אתחלנו את כל התצפיות בממוצע Y וכאן באפס, שם עשינו את זה עד שלא נמצא יותר משתנה עם מספיק קורלציה לשאריות וכאן אנחנו עושים את זה מספר קבוע של צעדים מראש. מומלץ לעצור רגע ולראות שאתם מבינים מדוע האלגוריתם הזה, הפורוורד סטייג'וויז אדיטיב, הוא צורה כללית הרבה יותר של הפורוורד סטייג'וויז רגרשן.
:::
:::

---

### Claim: AdaBoost is also a FSAM

::: {.fragment}
- If $L$ is the exponential loss: $L(y, f(x)) = \exp(-yf(x))$
- And, $b(x; \gamma_m) = Gm(x)$
- We get AdaBoost!
:::

::: {.fragment}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

f_x = np.linspace(-2, 2, 100)
L = np.exp(-f_x)

f_x_disc = np.array([-1, 1])
L_disc = np.exp(-f_x_disc)

plt.figure(figsize=(4, 3))
plt.plot(f_x, L, label = 'f(x) continuous')
plt.bar(f_x_disc, L_disc, label = 'f(x) discrete', width = 0.1)
plt.xlabel('f(x)')
plt.ylabel('L(f(x)) = exp(-f(x))')
plt.title('Exponential loss for y = +1')
plt.legend()
plt.show()
```

:::

::: {.fragment}
::: {.callout-note}
Fast forward: plug-in *other* loss functions!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כעת אני טוען, שגם אדאבוסט הוא מודל פורווד סטייג'וויז אדיטיב.

אני טוען שהלוס שהוא מביא למינימום בכל איטרציה הוא לוס אקספוננציאלי שלא ראינו עדיין, אקספוננט בחזקת מינוס Y שאני מזכיר שהוא מינוס או פלוס 1, כפול החיזוי f(X).

ואם הפונקציות הפשוטות שקראנו להן B הן הקלסיפיירים החלשים שלנו Gm בכל איטרציה -- נקבל בדיוק את אדאבוסט.

אז לפני שנראה את זה, נכיר את הלוס האקספוננציאלי ולמה הוא סביר לבעיה שלנו: נניח שY הוא פלוס 1. אם אנחנו חוזים f(X) בדיד, שהוא גם פלוס ומינוס אחת, אז אם אנחנו חוזים מינוס 1 וטועים אנחנו מקבלים שגיאה של אקספוננט בחזקת 1 כלומר 2.7 בערך, ואם אנחנו חוזים 1 וצודקים אנחנו מקבלים שגיאה של אקספוננט בחזקת מינוס 1, כלומר 0.3 בערך.

אפשר אבל גם להרציף את הטעות הזאת, לחזות f(X) רציף, ואז ככל שהוא שלילי הטעות גדולה יותר, וככל שהוא חיובי הטעות שואפת לאפס.

ועוד דבר קטן, שוב למה אנחנו טורחים כל כך, רק כי זה מעניין? לא, המטרה שלנו היא להכליל לעוד פונקציות הפסד, ולהיות מסוגלים לעשות אדאבוסט לרגרסיה ולבעיות אחרות. אז בואו ניגש למשימה.
:::
:::

---

### Showing AdaBoost is FSAM

::: {.incremental}
- Recall the goal in FSAM iteration $m$:
$$(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + \beta b(x_i, \gamma_)\right)$$

- For exponential loss:
$$(\beta_m, G_m) = \arg\min_{\beta, G} \sum_{i = 1}^n \exp\left[-y_i(f_{m-1}(x_i) + \beta G(x_i))\right]$$
:::
::: {.fragment}
$= \arg\min_{\beta, G} \sum_{i = 1}^n \exp\left[-y_if_{m-1}(x_i)\right]\exp\left[\beta G(x_i)\right]$
:::
::: {.fragment}
$= \arg\min_{\beta, G} \sum_{i = 1}^n w_i^{(m)}\exp\left[-\beta y_i G(x_i)\right]$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
האלגוריתם של פורוורד סטייג'וויז אומר לנו בכל איטרציה למצוא את הצעד בטא והגאמא הכי טובים כדי לעשות מינימום ללוס.

נציב את הלוס האקספוננציאלי ונחליף את בי בקלסיפייר החדש שלנו Gm.

עכשיו יש פה אקספוננט בחזקת סכום שאני מפרק למכפלת האקספוננטים.

ואנחנו מסמנים את כל הביטוי הזה כמשקולת w_i שבכל צעד m מבטאת את הלוס באיטרציה האחרונה, אם היה חיזוי טוב היא תהיה קטנה ואם היה חיזוי גרוע, היא תהיה גדולה. נשים לב שהיא לא משתתפת במינימיזציה בגלל זה אני יכול לעשות את זה. הביטוי שקיבלנו כבר מתחיל להזכיר את השגיאה הממושקלת שאנחנו מורידים למינימום באדאבוסט אבל נצטרך עוד קצת אלגברה.
:::
:::

---

### Getting $G_m$

::: {.fragment}
- Assuming $\beta > 0$, separate the criterion to "correct" + "incorrect" sums:
:::
::: {.fragment}
$\sum_{i = 1}^n w_i^{(m)}\exp\left[-\beta y_i G(x_i)\right] =$
$= e^{-\beta}\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i = G(x_i)\right] + e^{\beta}\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right]$
:::
::: {.fragment}
- Can also write as:

$= (e^{\beta} - e^{-\beta})\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right] + e^{-\beta}\sum_{i = 1}^n w_i^{(m)}$
:::
::: {.fragment}
- AdaBoost stage 2a: $G_m$ minimizing the criterion is minimizing weighted error rate:
$$G_m = \arg\min_G \sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right]$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
עכשיו בואו נניח לרגע שהפרמטר בטא שמתקבל מבעית המינימיזציה הזאת הוא חיובי, ונראה שהקלסיפייר של אדאבוסט G_m הוא בדיוק הקלסיפייר שנקבל כאן:

יש כאן סכום ממושקל. כשהחיזוי נכון, y כפול G נותן 1 והאלמנט בסכום יהיה e בחזקת מינוס בטא. כשהחיזוי לא נכון, y כפול G נותן מינוס 1, והאלמנט בסכום יהיה e בחזקת בטא. אז אני יכול להפריד את הסכום הזה לשני סכומים, סכום החיזויים הנכונים שנותנים אי במינוס בטא ועוד סכום החיזויים הלא נכונים שנותנים אי בבטא.

אבל גם את זה אני יכול לרשום בצורה יותר קומפקטית שבה G שאני מנסה לעשות עליו מינימום, נמצא רק בביטוי אחד. הדרך לעשות את זה היא לתת לכל התצפיות את האלמנט אי במינוס בטא, ורק לחיזויים הלא נכונים להוסיף אי בבטא פחות אי במינוס בטא.

וזה מראה לי שכדי לקבל את קלסיפייר G האופטימלי בכל שלב m אני צריך לעשות בדיוק את מה שאדאבוסט שאף לעשות בשלב 2a. להגיע לקלסיפייר שיביא למינימום את שגיאת החיזוי הממושקלת. מומלץ לחזור אחורה או לשים את האלגוריתם המקורי לידכם כדי לראות את זה! 

נשאר לי רק להראות שמדובר בדיוק באותן המשקולות.
:::
:::

---

### Getting $\alpha_m$

::: {.fragment}
$\text{criterion} = (e^{\beta} - e^{-\beta})\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right] + e^{-\beta}\sum_{i = 1}^n w_i^{(m)}$
:::
::: {.fragment}
$\frac{\partial \text{criterion}}{\partial \beta} = (e^{\beta} + e^{-\beta})\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right] - e^{-\beta}\sum_{i = 1}^n w_i^{(m)} \to =0$
:::
::: {.fragment}
- Multiply by $e^\beta$:

$(1 + e^{2\beta}) = \frac{\sum_{i = 1}^n w_i^{(m)}}{\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right]} = \frac{1}{err_m}$
:::
::: {.fragment}
- Take $ln$, we get AdaBoost stages 2b and 2c:

$\beta_m = \frac{1}{2}\ln\left[\frac{1-err_m}{err_m}\right] = \frac{1}{2}\alpha_m$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז בואו נראה שאנחנו מקבלים בדיוק את אותן המשקולות, ואיך מהאלגוריתם הכללי של פורוורד סטייג'וויז אנחנו מקבלים את הביטוי המוזר לאלפא-אם בכל איטרציה. נזכיר שבכל איטרציה מחשבים לוג של 1 פחות הטעות הממושקלת חלקי הטעות הממושקלת, והמשקולת הנוכחית מוכפלת פי אקספוננט בחזקת אלפא-אם אם היה חיזוי לא נכון.

ואנחנו הגענו לקריטריון שאני פשוט מעתיק שוב כאן, הבנו מה הG שיביא למינימום, נמצא עכשיו מה הבטא שיביא למינימום.

אנחנו גוזרים את הקריטריון לפי בטא, ומשווים לאפס. תשימו לב שאתם מבינים את הגזירה היא די פשוטה.

יש לנו עכשיו משוואה, נכפיל את שני האגפים באותו ביטוי חיובי, והנה אנחנו מוצאים שבטא שיביא למינימום את הקריטריון שלנו, קשור לביטוי שסימנו באלגוריתם אדאבוסט כerr, שגיאת החיזוי הממושקלת חלקי סך המשקולות.

אחרי עוד קצת אלגברה נגיע לביטוי סופי עבור הצעד בטא בכל איטרציה m, ששווה בדיוק לחצי הפרמטר אלפא-אם שאדאבוסט אמר לנו לחשב.
:::
:::

---

### Getting $w_i$

::: {.fragment}
- FSAM says:

$f_m(x) = f_{m - 1}(x) + \beta_m b(x; \gamma_m) = f_{m - 1}(x) + \frac{1}{2}\alpha_m G_m(x)$
:::
::: {.fragment}
- Which means next iteration weights are:

$w_i^{(m)} = \exp\left[-y_if_{m}(x_i)\right] = \exp\left[-y_i(f_{m-1}(x) + \frac{1}{2}\alpha_m G_m(x))\right] =$
$= w_i^{(m-1)}\cdot\exp\left[-\frac{1}{2}\alpha_m y_i G_m(x)\right]$
:::
::: {.fragment}
- Given that $-y_i G_m(x_i) = 2\cdot\mathbb{I}\left[y_i \neq G_m(x_i)\right] - 1$, we can write:

$w_i^{(m)} = w_i^{(m-1)} \cdot \exp\left[\alpha_m \cdot \mathbb{I}\left[y_i \neq G_m(x_i)\right]\right] \cdot Const$
:::
::: {.fragment}
- Equivalent to AdaBoost stage 2d.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ולמה העובדה הזאת אומרת שהגענו בדיוק לאותו משקול?

כעת האלגוריתם פורוורד סטייג'וויז אומר לעדכן את החיזוי להיות החיזוי באיטרציה הקודמת f אמ-מינוס-1, ועוד, צעד בטא כפול הפונקציה בי עם הפרמטר גאמא הטוב ביותר שנמצא. אצלינו זה פשוט אומר החיזוי עד עכשיו ועוד חצי אלפא-אם כפול הקלסיפייר G הכי טוב שנמצא.

אבל זה אומר שהמשקולות שלנו בכל איטרציה הן בדיוק מה שאדאבוסט אומר לנו לחשב: המשקולות שהגענו אליהן לפני שני שקפים הן הלוס האקספוננצילי מהאיטרציה הקודמת. מציבים את הביטוי שפורוורד סטייג'וויז אומר לנו בfm. ומקבלים שהעדכון שווה למשקולת הקודמת כפול ביטוי שכבר מתחיל להזכיר את הביטוי שאנחנו רוצים להגיע אליו, שמערב את אלפא-אם.

הוא מזכיר מאוד כי הוא שקול! נשים לב שאפשר להגיע בקלות מהמכפלה של y כפול G לביטוי שאנחנו רוצים, האינדיקטור שבודק האם יש לנו כאן חיזוי נכון או לא. באמצעות הקשר מינוס y כפול G שווה לפעמיים האינדיקטור פחות 1.

אנחנו מציבים את הקשר הזה ורואים שהמשקולת באיטרציה הבאה היא תמיד תהיה המשקולת הנוכחית כפול קבוע אקספוננט בחזקת אלפא-אם אם היה חיזוי שגוי, או תישאר כפי שהיא אם היה חיזוי נכון. כל זה כפול איזה קבוע שמכפילים בו את כל המשקולות ולכן אפשר להתעלם ממנו.

בכך קיבלנו את שלב 2d של אדאבוסט, ושוב מומלץ לראות אותו מול העיניים שלכם כדי לראות את זה. ובעצם סיימנו להראות שאדאבוסט מקרה פרטי של אלגוריתם כללי יותר, פורוורד סטייג'וויז אדיטיב, שנותן הנחיה מה לעשות עם כל לוס שנרצה, וגם לסטינג של רגרסיה. זה אולי ידהים אתכם לדעת שהתובנה הזאת לא הגיעה אלא לפחות חמש שנים אחרי שאדאבוסט פותח.
:::
:::

---

### Classification losses

:::: {.columns}
::: {.column}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Define the range for y * f(x)
yf = np.linspace(-2, 2, 400)

# Define the loss functions
misclass_loss = np.where(yf > 0, 0, 1)
exp_loss = np.exp(-yf)
binomial_deviance = np.log(1 + np.exp(-2 * yf)) / np.log(2)
squared_error = (1 - yf) ** 2
hinge_loss = np.maximum(0, 1 - yf)

# Plot the losses
plt.figure(figsize=(5, 4))

# Misclassification Loss
plt.plot(yf, misclass_loss, label="Misclassification", color="black")

# Exponential Loss
plt.plot(yf, exp_loss, label="Exponential", color="cyan")

# Binomial Deviance
plt.plot(yf, binomial_deviance, label="Binomial NLL", color="orange")

# Squared Error
plt.plot(yf, squared_error, label="Squared Error", color="red")

# Support Vector (Hinge Loss)
plt.plot(yf, hinge_loss, label="Support Vector (Hinge)", color="green")

# Add labels and legend
plt.xlabel("y · f")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()
```

::: {.fragment}
::: {.callout-note}
The $f(x)$ minimizing exponential loss is also minimizing Binomial negative log-likelihood!
:::
:::

:::
::: {.column}
- Misclassification (0/1 loss):

   $L_{\text{misclass}}(y, f) = \begin{cases} 
   0 & \text{if } y \cdot f(x) > 0 \\
   1 & \text{otherwise}
   \end{cases}$

- Exponential loss:

   $L_{\text{exp}}(y, f) = \exp(-y \cdot f(x))$

- Binomial negative log-likelihood (cross-entropy):

   $L_{\text{bin}}(y, f) =$
   $\quad \log(1 + \exp(-2 \cdot y \cdot f(x))) / \log(2)$

- Squared error loss:

   $L_{\text{sq}}(y, f) = (1 - y \cdot f(x))^2$

- Support vector (hinge Loss):

   $L_{\text{hinge}}(y, f) = \max(0, 1 - y \cdot f(x))$
:::
::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אם נישאר רגע עדיין בקלסיפיקציה, לא חייבים להישאר עם הלוס האקספוננציאלי, יש כל מיני לוסים שאולי מתאימים לקלסיפיקציה לשני קלאסים.

נראה שהם לא מאוד שונים אגב, וכולם עושים איזושהי הרצפה ללוס הכי טבעי אבל הלא רציף של מיסקלסיפיקיישן, שהבעיה איתו שהוא לא רגיש, המעבר בו בין עונש ללא-עונש נורא חד, הוא לא מעניש נורא אם החיזוי מאוד מאוד שלילי.

לוס בעייתי לבעיה שלנו, בדיוק כמו שדיברנו ברגרסיה לוגיסטית הוא הלוס הריבועי. כי הוא מתחיל להעניש גם אם החיזוי f(x) חיובי מדי.

נשארנו עם הלוס שלנו האקספוננציאלי, והלוס שראינו למשל כשדיברנו על קרוס אנתרופי כלוס אפשרי לעץ קלסיפיקציה. וכאן אחרי קצת אלגברה אנחנו רושמים אותו כפונקציה של המכפלה y כפול f, כדי שנוכל לראות אותו בהקשר של כולם. באופן מפתיע, הלוס הזה הוא גם הלוס המתקבל תחת נקודת מבט סטטיסטית, שוואי מתפלג ברנולי או בינומי עם איזושהי הסתברות שאותה אנחנו ממדלים בדיוק כמו ברגרסיה לוגיסטית, ואז הלוס שלנו הוא מינוס לוג הנראות. 

זה דורש קצת אלגברה להראות שכל הדברים האלה מתכנסים לאותו דבר, אבל התובנה המדהימה שרציתי שתראו כאן, זה ששתי הפונקציות האלה, הפונקציה שלנו והפונקציה שמגיעה מנקודת מבט סטטיסטית, הן מאוד דומות, אחת היא הלוג של השניה בקירוב!

וזה אומר, שפונקצית החיזוי שאנחנו נמצא בכל שלב של אדאבוסט, שמביאה למינימום לוס אקספונציאלי, צריכה להיות אותה פונקצית חיזוי שתביא למינימום את הלוס של מינוס לוג הנראות, תחת המודל הבינומי, אם נניח שוואי מתפלג ברנולי! מצאנו את נקודת המבט הסטטיסטית לאדאבוסט.

ואיך עושים בוסטינג לרגרסיה? את זה נראה בחלק הבא.
:::
:::

---

## Boosting for Regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ננסה עכשיו להגיע לאלגוריתם לבוסטינג לרגרסיה באותה גישה של פורוורד סטייג'וויז אדיטיב מודלינג.
:::
:::

---

### Recap

What the FSAM framework got us:

$$(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + \beta b(x_i, \gamma)\right)$$

- Classification:
  - $y \in \{-1, 1\}$, exponential loss, $b(x_i, \gamma)$ are weak learners $G(x)$ $\Rightarrow$ AdaBoost
- Regression:
  - $y \in \mathbb{R}$, squared error loss, $b(x_i, \gamma)$ are single features $\mathbf{x}_j$ $\Rightarrow$ Forward stagewise regression

::: {.fragment}
- What if we want to boost weak learners $G(x)$ for regression?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניזכר שוב בהיי-לבל מה ראינו עד עכשיו.

האלגוריתם פורוורד סטייג'ווויז אדיטיב אומר לנו בכל איטרציה למצוא את הצעד בטא ואת הפונקציה הפשוטה בי האופטימליים, כדי להביא למינימום כל לוס, עם מודל אדיטיבי.

הראינו שבקלסיפיקציה בינארית איפה שוואי הוא 1 או מינוס 1, הלוס הוא אקספוננציאלי והפונקציות הפשוטות שלנו הן וויק קלסיפיירים G, מקבלים את אלגוריתם אדאבוסט.

הראינו שברגרסיה איפה שוואי הוא רציף, הלוס ריבועי והפונקציות הפשוטות שלנו הן כל פעם משתנה אחד ויחיד - מקבלים את פורוורד סטייג'וויז רגרשן.

השאלה המתבקשת היא האם אפשר לקחת את הלומדים החלשים האלה G מהאחד, ולשים אותם בשני, ולקבל גרסה ולידית של בוסטינג לרגרסיה. והתשובה היא לגמרי כן. נשים לב רק שברגרסיה אכן נקרא לG באופן כללי לרנרז או רגרסורז, הם כבר לא יהיו קלסיפיירים.
:::
:::

---

### Boosted trees for regression (I)

::: {.incremental}
- Specifically, let us focus on boosting trees: $f(x) = \sum_{m = 1}^M T(x, \Theta_m)$
- Where $T(x, \Theta) = \sum_{j = 1}^J \gamma_j\mathbb{I}\left(X \in R_j\right)$
- With parameters $\Theta = \{R_j, \gamma_j\}_1^J$
- "Building a regression tree" is equivalent to:
$$\hat{\Theta} = \arg\min_{\Theta} \sum_{i = 1}^n\left(y_{i} - T(x_i, \Theta)\right)^2$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
באופן ספציפי, נתמקד בעצים ונרצה לעשות להם בוסטינג. נרצה שהחיזוי שלנו יהיה סכום חיזויים של עצים או סכום ממושקל שאני רגע משמיט רק בשביל הקיצור. ולמה דווקא עצים? בגלל כל היתרונות שלהם שראינו בשיעור הקודם, עצים מטפלים בלי בעיה בסוגי משתנים מגוונים בלי הכנה נוספת, הם יודעים לטפל בתצפיות חסרות, הם לא-ליניאריים מצד אחד אבל לא גמישים מדי מצד שני, ומצד שני החיזוי שלהם בשורה התחתונה הוא לא מאוד טוב, שזה בדיוק מה שאנחנו רוצים בלומד חלש.

נזכור מהו עץ רגרסיה בשורה התחתונה. עץ רגרסיה הוא בעצם חלוקה של הדאטא לJ שכונות Rj, שבכל שכונה אנחנו חוזים מספר יחיד גאמא J.

כלומר כל עץ בעומק קבוע ניתן לרשום בעצמו כמודל אדיטיבי שהחיזוי שלו הוא סכום על כל השכונות, שיחפש לאיזו שכונה התצפית שייכת ויחזה עבורה גאמא J. וכאן אנחנו מתייחסים למודל הזה כמודל פרמטרי לכל דבר, לכל עץ עם מספר שכונות קבוע J יש פרמטרים תטא שמה הם? השכונות עצמן, והמספר שמתאים לכל שכונה, גאמא. 

זה חשוב להבין את זה כי תיכף נזריק את התהליך הזה של בניית עץ אל תוך האלגוריתם פורוורד סטייג'וויז שלנו. לבנות עץ רגרסיה עם עומק קבוע עם לוס ריבועי זה פשוט למצוא את הפרמטרים תטא הכי טובים, למצוא את השכונות והחיזויים הכי טובים כדי לעשות מינימום לשגיאה ריבועית.
:::
:::

---

### Boosted trees for regression (II)

::: {.fragment}
- So FSAM with squared loss, no problem:

$\Theta_m = \arg\min_{\Theta} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + T(x_i, \Theta)\right) =$
$= \arg\min_{\Theta} \sum_{i = 1}^n\left(y_i - f_{m-1}(x_i) - T(x_i, \Theta)\right)^2$
:::
::: {.fragment}
$= \arg\min_{\Theta} \sum_{i = 1}^n\left(r_{im} - T(x_i, \Theta)\right)^2$
:::
::: {.incremental}
- Because we know how to build such trees!
- To sum up: at each iteration $m$ build standard regression tree $T(x, \Theta)$ which best fits the current residuals $\mathbf{r}_m$
:::
::: {.fragment}
::: {.callout-note}
At high-level, similar to AdaBoost!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אם אנחנו ברגרסיה וההפסד הריבועי, אין בעיה. בכל שלב של האלגוריתם סטייג'וויז נמצא את הפרמטרים תטא שמביאים למינימום את ההפסד הריבועי של המודל האדיטיבי. אבל מה זה אומר? לבנות את העץ הכי שחוזה הכי טוב את השאריות!

ולמה אין בעיה, כי זה בדיוק עץ הרגרסיה שאנחנו יודעים לבנות.

אז סיכום ביניים, אם הלוס הוא ריבועי, לעשות בוסטינג לעצי רגרסיה אומר בכל איטרציה m לחשב את השאריות ולבנות עץ רגרסיה לחיזוי השאריות. לעדכן את השאריות ולעבור לאיטרציה הבאה וכולי.

נשים לב שהפעולה הזאת תואמת מאוד את הרציונל של אדאבוסט - בכל שלב תמצה מה שהמודל עד כה לא הצליח לחזות טוב, וברגרסיה זה שאריות, ותמדל את הפער הזה. זה ממש דומה למשקול מחדש של הנתונים.
:::
:::

---

### What about more [robust]{style="color:red;"} losses?

:::: {.columns}
::: {.column}
```{python}
#| echo: false

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt

# Define the range for y - f(x)
residual = np.linspace(-3, 3, 400)

# Define the loss functions
squared_error = residual ** 2
absolute_error = np.abs(residual)

# Define Huber loss with delta = 1
delta = 1
huber_loss = np.where(np.abs(residual) <= delta,
                      residual ** 2,
                      2 * delta * np.abs(residual) - delta**2)

# Plot the losses
plt.figure(figsize=(5, 4))

# Squared Error Loss
plt.plot(residual, squared_error, label="Squared Error", color="orange")

# Absolute Error Loss
plt.plot(residual, absolute_error, label="Absolute Error", color="cyan")

# Huber Loss
plt.plot(residual, huber_loss, label="Huber", color="green")

# Add vertical lines at -delta and delta for Huber loss
plt.axvline(x=-delta, color='black', linestyle='dotted')
plt.axvline(x=delta, color='black', linestyle='dotted')

# Add labels and legend
plt.xlabel("y - f")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()
```

:::
::: {.column}
- Squared Error Loss:
   
  $L_{\text{sq}}(y, f) = (y - f(x))^2$

- Absolute Error Loss:
  
  $L_{\text{abs}}(y, f) = |y - f(x)|$

- Huber Loss:

::: {style="font-size: 50%;"}  
  $L_{\text{H}}(y, f) = \begin{cases} 
   (y - f(x))^2 & |y - f(x)| \leq \delta \\
   2\delta |y - f(x)| - \delta^2 & \text{otherwise.}
   \end{cases}$
:::

:::
::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הבעיה היא מה קורה אם אנחנו רוצים להכליל את הרעיון הזה לכל פונקצית הפסד, גם פונקציות שאנחנו קוראים להן רובסטיות או חסינות יותר מההפסד הריבועי.

סטטיסטיקה רובסטית זה תחום שאפשר לבלות עליו סמסטר שלם. ובכל זאת למה הכוונה -- הכוונה היא לפונקציות הפסד שמושפעות פחות מתצפיות חריגות. הרי הלוס הריבועי מעניש אותנו בשארית בריבוע כשהמודל טועה. שארית אחת שלגביה הוא טועה מאוד יכולה לנפח את הלוס מאוד.

אופציה אחרת היא ההפסד בערך מוחלט, שיעניש אותנו בצורה ליניארית לשארית.

אופציה אחרת היא הפסד הובר למשל, שעד איזשהו קבוע דלתא יעניש את השארית בריבוע, ומדלתא יעניש אותה בצורה ליניארית כמו הערך המוחלט. ויש עוד הרבה גרסאות לפונקציות הפסד חסינות. העניין הוא שאנחנו לא יודעים לבנות עץ רגרסיה לכל פונקציה כזאת, זאת בעיה די קשה.

אז בחלק הבא נכליל את הפרוצדורה של בוסטינג שראינו אפילו יותר, וזה יאפשר לנו לטפל בכל פונקצית הפסד. זה גם יהיה אלגוריתם הבוסטינג המוכר והסופי שנגיע אליו בשיעור הזה, שנמצא בשימוש נרחב כל כך בתעשייה היום.
:::
:::

---

## Gradient Boosting {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הגירסה שהתקבעה בסופו של דבר לבוסטינג, נקראת גרדיאנט בוסטינג. לפעמים גרדיאנט בוסטינג מאשינז, לפעמים גרדיאנט בוסטינג טריז. מה הקשר בין מה שראינו לגרדיאנט, או לנגזרת של פונקציה?
:::
:::

---

### Gradient descent algorithms

Minimize a function $J(\theta)$ by moving in the opposite direction of the gradient:
$$\hat\theta_{i+1} = \hat\theta_i - \varepsilon \frac{\partial J(\theta)}{\partial \theta}$$

```{python}
#| echo: false

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt

# Define the function J(theta) = theta_1^2 + 0.5 * theta_2^2 (quadratic function)
def J(theta):
    return theta[0]**2 + 0.5 * theta[1]**2

# Define the gradient of J(theta)
def grad_J(theta):
    return np.array([2 * theta[0], 1 * theta[1]])

# Perform gradient descent
def gradient_descent(initial_theta, learning_rate, num_iterations):
    theta = initial_theta
    path = [theta]
    for i in range(num_iterations):
        theta = theta - learning_rate * grad_J(theta)
        path.append(theta)
    return np.array(path)

# Set parameters for gradient descent
initial_theta = np.array([-3.0, -3.0])
learning_rate = 0.3
num_iterations = 5

# Run gradient descent and get the path
path = gradient_descent(initial_theta, learning_rate, num_iterations)

# Create a grid of points for the contour plot
theta1_vals = np.linspace(-4, 4, 400)
theta2_vals = np.linspace(-4, 4, 400)
theta1, theta2 = np.meshgrid(theta1_vals, theta2_vals)
Z = J([theta1, theta2])

# Plot the contour map
plt.figure(figsize=(5, 4))
plt.contour(theta1, theta2, Z, levels=20, colors='blue')

# Plot the path of gradient descent
plt.plot(path[:, 0], path[:, 1], marker='x', color='red', markersize=10, label="Gradient Descent Path")
plt.quiver(path[:-1, 0], path[:-1, 1], path[1:, 0] - path[:-1, 0], path[1:, 1] - path[:-1, 1], 
           scale_units='xy', angles='xy', scale=1, color='red')

# Annotate the points
for i in range(len(path)):
    plt.text(path[i, 0], path[i, 1], r"$\hat\theta_{}$".format(i), color="black", fontsize=12)

# Set labels and title
plt.xlabel(r'$\theta_1$')
plt.ylabel(r'$\theta_2$')
plt.grid(True)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ניזכר בשיטה כללית למצוא מינימום של פונקציה מסובכת, כשאין לנו פתרון סגור. אנחנו מתחילים מאיזשהו ניחוש התחלתי לפרמטר שלנו, כאן נגיד פרמטר דו-מימדי תטא. מחשבים את הנגזרת של הפונקציה שלנו ביחס לפרמטר, כלומר הגרדיאנט. הגרדיאנט הוא בעצם הכיוון של ההשתנות הכי מהירה של הפונקציה. כמו להיות על הר גבוה ולחשב איפה הוא יורד הכי מהר. ואז, אנחנו עושים צעד קטן בגודל אפסילון במורד הגרדיאנט. מה שבפועל אומר לקחת את הפרמטרים תטא שיש לנו עד עכשיו, ולהחסיר מהם אפסילון קטן כפול הגרדיאנט.

אנחנו חוזרים על הצעד הזה שוב ושוב, כל פעם מסתכלים בנקודה שבה אנחנו נמצאים מהו הגרדיאנט והולכים במורד הגרדיאנט צעד קטן, עד שנמצא את הפרמטר שמביא למינימום את הפונקציה. לפונקציה כללית נגיע למינימום לוקאלי, ואם הפונקציה קמורה והצעד אפסילון מספיק קטן מובטח לנו שנגיע למינימום גלובלי.

אז מה הקשר למה שאנחנו עושים כאן? מאוחר יותר הבינו חוקרים שהאלגוריתם בוסטינג הפשוט שתיארנו רק לפני רגע, הוא ממש חיקוי של גרדיאנט דיסנט.
:::
:::

---

### Gradient boosting machines

::: {.incremental}
- Consider any loss $L(\mathbf{y}, \mathbf{f})$ a function of $n$ data points $\mathbf{f} = (f(x_1), \dots, f(x_n))$
- At each iteration $m$, calculate the gradient of $L$ by $\mathbf{f}$, evaluated at $\mathbf{f}_{m - 1}$:
$$\mathbf{g}_m = \frac{\partial L(\mathbf{y}, \mathbf{f})}{\partial \mathbf{f}}\Bigr|_{\mathbf{f} = \mathbf{f}_{m - 1}}$$
- Move a small step $\varepsilon$ down this gradient:
$$\mathbf{f}_m = \mathbf{f}_{m - 1} - \varepsilon\mathbf{g}_m$$
- Technically, $\varepsilon$ can also be optimized at each iteration $m$ to minimize
$$\varepsilon_m = \arg\min_\varepsilon L(\mathbf{y}, \mathbf{f}_{m - 1} - \varepsilon\mathbf{g}_m)$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
במקרה שלנו הפונקציה שאנחנו רוצים לעשות לה מינימום היא פונקצית הפסד, של הרבה תצפיות וחיזויים של התצפיות. הרעיון היפה הוא להתייחס לוקטור התחזיות שלנו כוקטור פרמטרים של הפונקציה. נכון שאלה לא ממש פרמטרים אלה תחזיות שמבוססות על מודל כמו עץ רגרסיה, אבל הרישום הזה מאפשר לנו לפתור את הבעיה שלנו באמצעות גרדיאנט דיסנט.

בכל איטרציה m, נרשום את הגרדיאנט של הפונקצית הפסד שלנו לפי התחזיות שהן עכשיו פרמטרים. ונציב בגרדיאנט את החיזויים האחרונים שיש בידינו.

כעת כדי לקבל חיזויים חדשים, ניקח את החיזויים הקודמים ונלך צעד קטן בכיוון הגרדיאנט, כלומר נחסר מהוקטור הזה צעד אפסילון קטן כפול וקטור הגרדיאנט. ואת זה נעשה שוב ושוב עד שנגיע לחיזויים שמביאים למינימום את פונקצית ההפסד שלנו.

אפשרות נוספת אגב היא לא להסתפק בצעד קטן אפסילון אלא בכל איטרציה לחשב את האפסילון שמתאים ספציפית לאיטרציה הזאת, אפסילון-אם. כלומר לחשב מהו גודל הצעד שיביא למינימום את ההפסד בנקודה הנוכחית לעומת התחזיות החדשות. בפועל לא נראה תמיד שיש יתרון לתוספת הזאת וצעד אפסילון קטן עובד יופי אבל נזכור את הגירסה הזאת גם כן.
:::
:::

---

### Gradient boosting machines

::: {.incremental}
- Eventually we would get:
$$\mathbf{f}_M = \sum_{i = 1}^M\varepsilon_m(-\mathbf{g}_m)$$
- But we are not interested in $\mathbf{f}_M$ for our training data, we wanted a model! What about $x_0$?
  - E.g. $f(x) = \sum_{m = 1}^M \varepsilon T(x, \Theta_m)$

- Solution: at each iteartion $m$ approximate the negative gradient $-\mathbf{g}_m$ with a weak learner or regression tree!
$$\Theta_m = \arg\min_\Theta \sum_{i = 1}^n (-g_{im}(L) - T(x_i, \Theta))^2$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בסופו של דבר נקבל שהחיזויים הסופיים שלנו אחרי M איטרציות הם מזכירים מודל אדיטיבי כמו שרצינו. החיזויים הסופיים הם סכום של צעד אפסילון כפול מינוס הגרדיאנט, ועוד צעד אפסילון כפול מינוס הגרדיאנט וכולי.

אבל כאן מופיע הקץ' -- זה לא המודל שרצינו! מה שמתואר כאן זה לא משין לרנינג. אם תתייחסו לn החיזויים כפרמטרים, במקרה הטוב פשוט תגיעו לחיזויים טובים ספציפית על מדגם הלמידה. כשתגיע תצפית חדשה X0 לא תדעו איך לחזות עליה. או במילים אחרות למדתם פרמטרים אבל לא למדתם מודל.

אנחנו רצינו מודל אדיטיבי, למשל סכום של עצים ממושקלים, או שכל אחד יופיע עם משקולת קטנה קבועה.

והנה הפתרון היפה לגשר בין שתי הגישות: בכל איטרציה, כן נלמד מודל, למשל עץ החלטה, והמודל הזה ינסה לקרב את הגרדיאנט!

שימו-לב, לא משנה מה הלוס, הוא יכול להיות הובר או ערך מוחלט, הוא יכול להיות אפילו לוס של קלסיפיקציה -- העץ שאנחנו בונים תמיד יהיה עץ רגרסיה, עם הפסד ריבועי, שמנסה לקרב את הגרדיאנט, כאן אני רושם אותו כפונקציה של L כדי שנבין שאפשר להשתמש בכל פונקצית הפסד. זה עץ שאמרנו שאנחנו יודעים לבנות די בקלות.

ואז, ברגע שיש לנו עץ כזה נוסיף אותו למודל, אולי עם איזו משקולת קטנה אפסילון, ובאיטרציה הבאה נחשב שוב את הגרדיאנט, נמדל אותו עם עץ, וכך הלאה וכך הלאה.

לכן קוראים לכל הגישה גרדיאנט בוסטינג, מדובר על בוסטינג באמצעות גרדיאנט דיסנט. והיא מתאימה לכל פונקצית הפסד שניתן לחשב לה בקלות את הגרדיאנט, אף על פי שהלומדים שמרכיבים אותה הם עצי רגרסיה.
:::
:::

---

### Gradients of common loss functions

| setting         | loss  : $L(y_i, f(x_i))$      | gradient: $-g(L(y_i, f(x_i)))$                   |
|-----------------|-------------------------------|----------------------------------------------|
| Regression      | $\frac{1}{2}(y_i - f(x_i))^2$ | $y_i - f(x_i)$                                 |
| Regression      | $|y_i - f(x_i)|$              | $\text{sign}(y_i - f(x_i))$                    |
| Regression      | Huber                         | $\begin{cases} y_i - f(x_i) & \text{if } |y_i - f(x_i)| \leq \delta \\ \delta\text{sign}(y_i - f(x_i)) & \text{otherwise} \end{cases}$ |
| Classification* | NLL                           | $\mathbb{I}\left[y_i = 1\right] - \hat{p}_i$ |

*Here $y \in \{0, 1\}$ and $\hat{p}_i = \frac{\exp(\hat{y}_i)}{1 + \exp(\hat{y}_i)} = \frac{\exp(f(x_i))}{1 + \exp(f(x_i))}$

::: {.fragment}
::: {.callout-note}
Notice for (half) squared loss the gradient is the residuals, as we got earlier.
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
תיכף נסכם את האלגוריתם הסופי אבל קודם נראה כמה קל עבור הלוסים שראינו עד כה למצוא את הגרדיאנט ביחס לחיזויים:

אם ההפסד ריבועי, ונכפיל פי חצי כדי לקבל תוצאה יפה, נקבל את השאריות. שימו-לב, זה אומר שהאלגוריתם בוסטינג הספציפי שראינו לפי רגע הוא מקרה פרטי של גרדיאנט בוסטינג. כי עם לוס ריבועי, וקטור הגרדיאנט הוא בעצם וקטור השאריות!

ואם הלוס הוא ערך מוחלט או הובר, יש לנו ביטויים סגורים ויפים לוקטור הגרדיאנט.

ואפילו אם הלוס הוא בכלל לוס של קלסיפיקציה בינארית, Y הוא אפס או אחת, ואנחנו משתמשים במינוס לוג הנראות הבינומית שראינו קודם בקלסיפיקציה. גם אז וקטור הגרדיאנט שנמדל הוא ביטוי פשוט וסגור, מינוס ההסתברות החזויה לתצפיות שהן 0 או 1 פחות ההסתברות החזויה לתצפיות שהן 1. כשההסתברות החזויה נתונה בביטוי שלפנינו.

מכל מקום, לאיזו פונקצית הפסד שנרצה אפשר להציב את הגרדיאנט באלגוריתם הכללי של גרדיאנט בוסטינג והוא יהיה ולידי.
:::
:::

---

### Gradient boosting trees (simplified)

1. Initialize $f_0(x) = \arg\min_\gamma \sum_{i = 1}^n L(y_i, \gamma)$ (e.g. $f_0(x) = \bar{y}$)
2. For $m = 1$ to $M$:
    (a) Compute pseudo-residuals:
    $$\mathbf{r}_m = -\mathbf{g}_m(L) = -\left[\frac{\partial L(\mathbf{y}, \mathbf{f})}{\partial \mathbf{f}}\right]_{\mathbf{f} = \mathbf{f}_{m - 1}}$$
    (b) Fit a **regression tree** to data $(\mathbf{x}, \mathbf{r}_m)$, giving $T(x, \Theta_m)$
    (c) Update: $f_m(x) = f_{m - 1}(x) + \varepsilon T(x, \Theta_m)$ (or $\varepsilon_m$)
3. Output: $\hat{f}(x) = \sum_{m = 1}^M \varepsilon T(x, \Theta_m)$

::: {.fragment}
::: {.callout-note}
What parameters need tuning?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
וככה נראה אלגוריתם גרדיאנט בוסטינג טריז בצורה הכי פשוטה.

נתאחל את החיזויים להיות מה שמביא למינימום את פונקצית ההפסד שבחרנו. אם זה הפסד ריבועי נקבל את ממוצע Y, אם זה הפסד בערך מוחלט נקבל את חציון Y. אפשר להתחיל גם עם חיזוי אפס לכל התצפיות, זה גם יעבוד.

בכל איטרציה m:

נחשב את וקטור הגרדיאנט או הפסאודו-שאריות. למה זה נקרא פסאודו-שאריות, כי עם הפסד ריבועי מדובר ממש בשאריות (להדגים), ובאופן כללי וקטור שמבטא מה שהמודל לא הצליח לחזות עד כאן.

נתאים עץ רגרסיה לשאריות, כלומר על הגירסה הזאת של הדאטא, שאפשר לחשוב עליה כמו באדאבוסט כגירסה ממושקלת של הנתונים כך שכל תצפית מקבלת משקולת גדולה יותר אם לא הצלחנו לחזות אותה עד כה.

ונעדכן את המודל, נוסיף את העץ שלנו עם צעד אפסילון קטן שיכול להיות לרנינג רייט קבוע מראש או ספציפי לאיטרציה הנוכחית.

אחרי M איטרציות המודל שנקבל יהיה מודל אדיטיבי של אנסמבל של עצי החלטה, עם משקולת קטנה קבועה או משקולת ייחודית לכל אחד, שנבנו בצורה אדפטיבית, אחד אחרי השני, כל פעם על גרסה אחרת של הנתונים.

נשים לב שיש כאן כמה פרמטרים שצריך לעשות עליהם טיונינג:
מספר העצים -- בניגוד לרנדום פורסט שם מובטח לנו שלא נעשה אוברפיטינג עם עוד עצים, כאן אין הבטחה כזאת -- אבל נגיד שעדיין בפועל על נתונים אמיתיים בוסטינג נוטה לתת תוצאות טובות עם הרבה עצים, במיוחד אם שולטים בעומק העצים.
וזה בדיוק הפרמטר הבא - לאילו עצים בוסטינג יכול לסייע במיוחד, עצים עמוקים או עצים לא עמוקים? אנחנו רוצים וויק לרנרז, שלומדים תופעות פשוטות אבל יציבות, שלאט לאט כשנצרף את כולן נקבל מודל חזק, כלומר בפועל בבוסטינג אנחנו מעדיפים עצים לא עמוקים מדי, בדרך כלל עומק 2 עד 4, ואפילו גזעים של עצים ראינו שעובד.

והפרמטר האחרון הוא קצב הלמידה אפסילון, שהוא כמו גודל הצעד בגרדיאנט דיסנט, אם אנחנו אומרים שהוא קבוע לכל האיטרציות. אבל גם כאן בפועל לנתונים אמיתיים ערך דיפולטיבי כמו 0.01 או 0.001 עובד היטב.
:::
:::

---

### Example: credit data

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score, KFold

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])
# Assuming X, y are already defined
n_splits = 5  # Number of cross-validation folds
n_trees = range(1, 101, 10)  # Range of trees to evaluate in GBT

train_errors_gb = []
test_errors_gb = []
train_errors_gb_std = []
test_errors_gb_std = []

# Cross-validation setup
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

for n_tree in n_trees:
    train_scores = []
    test_scores = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        # Initialize the GBT Regressor
        gb_regressor = GradientBoostingRegressor(n_estimators=n_tree, random_state=42)
        gb_regressor.fit(X_train, y_train)
        
        # Evaluate train and test MSE
        train_scores.append(np.mean((gb_regressor.predict(X_train) - y_train) ** 2))
        test_scores.append(np.mean((gb_regressor.predict(X_test) - y_test) ** 2))
    
    # Store mean and standard deviation of MSE across folds
    train_errors_gb.append(np.mean(train_scores))
    test_errors_gb.append(np.mean(test_scores))
    train_errors_gb_std.append(np.std(train_scores)/np.sqrt(5))
    test_errors_gb_std.append(np.std(test_scores)/np.sqrt(5))

# Compute train/test MSE for Linear Regression and Single Decision Tree
linear_regressor = LinearRegression()
tree_regressor = DecisionTreeRegressor(max_depth=9)

# Single Decision Tree MSE
train_mse_tree = -cross_val_score(tree_regressor, X, y, cv=kf, scoring='neg_mean_squared_error').mean()
test_mse_tree = train_mse_tree  # Same for train and test in cross-validation

# Plotting the results
# plt.figure(figsize=(12, 8))

# Plot GBT MSE vs. Number of Trees
plt.errorbar(n_trees, train_errors_gb, yerr=train_errors_gb_std, label='GBT Train MSE')
plt.errorbar(n_trees, test_errors_gb, yerr=test_errors_gb_std, label='GBT Test MSE')

# Plot horizontal lines for a Single Decision Tree
plt.axhline(y=train_mse_tree, color='black', linestyle='-.', label='Single Tree MSE')

plt.xlabel('Number of trees')
plt.ylabel('MSE')
plt.ylim(0, 40000)
plt.legend()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כאן אנחנו רואים את בוסטינג על הקרדיט דאטא, בהם אנחנו מנסים למדל עבור כל אדם את היתרה בחשבון שלה מכמה משתנים כמו אם הוא סטודנט או לא, או מה נתוני האשראי שלו. במקרה הזה אפשר לראות שבוסטינג משפר בהרבה את שגיאת החיזוי של עץ יחיד, הקו הכתום הוא ממוצע הMSE על תצפיות שהמודל לא ראה בפרוצדורת קרוס ולידיישן, ואנחנו משתמשים בערכים הדיפולטיביים לכל הפרמטרים שהתוכנה נותנת, כאן זה בוסטינג באמצעות sklearn בפייתון.

במקרה הזה לא נראה שיש נטייה למודל לעשות אוברפיטינג אם נוסיף עוד ועוד עצים, הוא גם מבצע קצת יותר טוב מרנדום פורסט שראינו בשיעור הקודם.

בשורה התחתונה בוסטינג שמבוסס על עצים הוא אחד האלגוריתמים הטובים ביותר אוף דה שלף להתאים לנתונים טבלאיים גדולים, יש לו הרבה מימושים מאוד מהירים כמו xgboost, ולפני מהפיכת הדיפ לרנינג הוא גם היה בשימוש רב בבעיות של קומפיוטר ויז'ן בהן הדאטא הוא תמונות. נתראה בשיעור הבא.
:::
:::
