---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Boosting"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Boosting - Class 10

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## AdaBoost {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בשיעור הזה נלמד על אחד האלגוריתמים שנמצאים הכי הרבה בשימוש במאשין לרנינג על נתונים גדולים ובמיוחד על נתונים טבלאיים. יש לו הרבה גירסאות והרבה שימושים מאוד פופולריים, אבל כולם בסופו של דבר נקראים: בוסטינג.

הגירסה הראשונה של האלגוריתם פותחה בתחילת שנות התשעים על-ידי צמד חוקרים בשם פרויד ושפייר שיקבלו על הפיתוח שלהם את פרס גדל. הגירסה הזאת נקראת: אדאבוסט.
:::
:::

---

### Instead of averaging

::: {.incremental}
- Suppose $y \in \{-1, 1\}$
- Recall the final Random Forests model: $\hat{f}(x_0) = \text{sign}\left[\frac{1}{M}\sum_{m = 1}^M T_m(x_0)\right]$
- Why not $\hat{f}(x_0) = \text{sign}\left[\sum_{m = 1}^M \alpha_m T_m(x_0)\right]$?
  - If $M$ is the number of all possible trees $T_m$, then finding weights $\alpha_m$ is intractable!

- [Boosting]{style="color:red;"} finds $\alpha_m$ sequentially, *adaptively*:
  - Take a [weak classifier]{style="color:red;"} $G(x)$ with accuracy slightly better than random
    - for any dist. $P_{T}$ of the training data:
  $Err_{P_{T}} = \mathbb{E}_{P_{T}}\left(\mathbb{I}\left[y_i \neq G(x_i)\right]\right) \leq \frac{1}{2} - \gamma$
  - Apply it sequentially over modified (weighted) versions of the training data
  - Each time selecting the best $\alpha_m$ to get: $\hat{f}(x_0) = \text{sign}\left[\sum_{m = 1}^M \alpha_m G_m(x_0)\right]$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אדאבוסט עוסק ספציפית בקלסיפיקציה לשני קלאסים, 1 ומינוס 1. מאוחר יותר נרחיב אותו לבעיות אחרות.

בסיטואציה הזאת, מה אמרנו שיעשה אלגוריתם כמו רנדום פורסט? הוא יגדל M עצים שנקרא להם T, כל פעם על גירסה קצת אחרת של הנתונים, וכשתגיע תצפית חדשה הוא ימצע את הסיווג שלהם, ואם אנחנו רוצים סיווג סופי של 1 או מינוס 1, ניקח את פונקציה sign.

מהי ביקורת סבירה על רנדום פורסט? למה למצע. אולי יש עצים טובים יותר, ועצים טובים פחות, ואנחנו צריכים לא ממוצע פשוט שלהם אלא ממוצע משוקלל, עם משקולת אלפא-אם לכל עץ.

אבל איך נמצא את המשקולות האופטימליות? הרי אם M הוא מספר כל העצים האפשריים, אולי אפילו נגביל את העומק שלהם, לכל סט נתונים סביר, זה מספר ענק. זה לא שאנחנו יכולים לגדל את כולם ואז להתחיל לבדוק מה סט המשקולות הכי טוב למצע אותם.

אבל בוסטינג יעשה בדיוק את זה, הוא ימצא את המשקולות והעצים האלה בצורה סדרתית אחד אחרי השני, בצורה אדפטיבית.

זה לא חייב להיות עץ, זה יכול להיות כל קלסיפייר חלש, או וויק קלסיפייר G, והכוונה בחלש שיש לו דיוק קצת יותר טוב מאקראי.

אם אנחנו רוצים להיות קצת יותר רשמיים, נניח שסיכוי לסיווג שגוי הוא חצי, אז הקלסיפייר שלנו יכול להשיג על המדגם הנתון שגיאה של קצת פחות מחצי, חצי פחות איזשהו פרמטר גאמא קטן, וזה לכל משקול של הנתונים. אנחנו מרדדים קצת את ההגדרה הרשמית אבל זה מספיק טוב בשבילנו.

ואת הקלסיפייר הזה נתאים לנתונים שלנו, כל פעם על גירסה קצת אחרת כשכל תצפית מקבלת משקולת מתעדכנת,

נתאים את הקלסיפייר וגם נבחר במשקולת הכי טובה עבורו לפי איזושהי פונקצית הפסד, וזה באמת יהיה החיזוי הסופי, הסיין של סכום הקלסיפיירים הממושקלים שאימנו.
:::
:::

---

### AdaBoost

1. Initialize observations weights $w_i = 1/n$ for $i = 1, \dots, n$
2. For $m  = 1$ to $M$:
    (a) Fit classifier $G_m(x)$ to the training data using weights* $w_i$
    (b) Compute weighted classification error: $$err_m = \frac{\sum_{i = 1}^n w_i\mathbb{I}\left[y_i \neq G_m(x_i)\right]}{\sum_{i = 1}^n w_i}$$
    (c) Compute coefficient $\alpha_m = \ln\left[\frac{1-err_m}{err_m}\right]$
    (d) Update weights: $w_i \leftarrow w_i \cdot \exp\left[\alpha_m \cdot \mathbb{I}\left[y_i \neq G_m(x_i)\right]\right]$
3. Output: $\hat{f}(x) = \text{sign}\left[\sum_{m = 1}^M \alpha_m G_m(x)\right]$

::: {.fragment}
::: {.callout-note}
*What does it mean training a classification tree with weighted data?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ככה נראה אלגוריתם האדאבוסט הרשמי, לא להיבהל, הוא דוקא די פשוט.

אנחנו מתחילים עם משקולות שוות לכל התצפיות, אם יש n תצפיות זה יוצר 1 חלקי n.

ועכשיו בכל איטרציה, אנחנו מגדלים עץ על התצפיות שלנו במשקול הנוכחי w_i.

אנחנו מחשבים את השגיאה הממושקלת הנוכחית, כלומר סוכמים את כל השגיאות וכופלים כל אחת במשקל שלה, ומחלקים בסך המשקולות.

המשקולת לכל הקלסיפייר אלפא-אם מחושבת עכשיו והיא שווה לביטוי שרשום כאן שמבוסס על שגיא הממושקלת ארר.

והשלב האחרון באיטרציה, עדכון המשקולות לאיטרציה הבאה: המשקולת של כל תצפית תוכפל באקספוננט בחזקת אלפא-אם אם הקלסיפייר הנוכחי עשה טעות על התצפית הזאת, וכפול אקספוננט בחזקת אפס אחרת, כלומר כפול 1 -- אם צדקנו המשקולת תישאר כפי שהיא.

נשים לב למה זה הגיוני -- אם טעינו, הכפלה פי אקספוננט בחזקת אלפא, תגדיל את המשקולת, וככה באיטרציה הבאה הקלסיפייר שלנו יצטרך לעבוד קשה יותר כדי לסווג אותה נכון, בניגוד לתצפיות עם משקולות קטנות. עוד דבר שנשים לב שעצים עם שגיאה ארר מאוד קטנה יקבלו משקולות אלפא-אם מאוד גדולות, ועצים עם שגיאה מאוד גדולה יקבלו משקולת אלפא-אם מאוד נמוכה.

בכל אופן האאוטפוט של של אדאבוסט הוא מה שרצינו, ממוצע משוקלל של וויק קלסיפיירים, שאנחנו מפעילים עליו פונקצית סיין כדי לקבל חיזוי סופי של 1 ומינוס 1.

רק נקודה קטנה שהרבה פעמים שוכחים - הקלסיפייר שלנו חייב לדעת לטפל נכון בתצפיות ממושקלות. למשל איך מגדלים עץ עם משקולות על התצפיות?
אז אם זה עץ רגרסיה, הלוס שאנחנו עושים עליו מינימום בכל פיצול הוא הRSS, אבל אנחנו לוקחים RSS ממושקל.

ואם זה עץ קלסיפיקציה, אז הלוס שלנו הוא למשל מדד הג'יני, ואנחנו צריכים לחשב בכל צומת את אחוז התצפיות שהן פלוס או מינוס אחת, אז במקום לחשב אותן בצורה הרגילה נחשב את סכום משקולות התצפיות שהן אחת חלקי סכום המשקולות של התצפיות בצומת.

מכל מקום, כעת ברור למה קוראים לאלגוריתם אדאבוסט. הוא לוקח קלסיפייר חלש יחסית, ועושה לו בוסט, בצורה אדאפטיבית. בחלק הבא נראה שבצורה הזאת יש לנו הבטחה, שאנחנו יכולים להוריד את השגיאה על מדגם הלמידה להיות קטנה כרצוננו, כתלות במספר האיטרציות ובפרמטר השגיאה גאמא שאנחנו יכולים להבטיח לכל וויק קלסיפייר. קטנה כרצוננו זה אומר אפילו אפס.
:::
:::

---

## AdaBoost Example{.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בואו נראה דוגמה פשוטה של אדאבוסט, בדו-מימד.
:::
:::

---

### AdaBoost with Tree Stumps

```{python}
#| echo: false

import numpy as np
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# Set seed for reproducibility
np.random.seed(42)

# Generate 10 random 2D points and binary labels {-1, 1}
n = 10
X = np.random.randn(n, 2)
y = np.random.choice([-1, 1], size=n)

# Initial weights, all equal to 1/n
w = np.ones(n) / n

x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1

for i in range(len(X)):
  if y[i] == -1:
    plt.scatter(X[i, 0], X[i, 1], color='blue', s=500*w[i], marker='o', edgecolor='black')
  else:
    plt.scatter(X[i, 0], X[i, 1], color='red', s=500*w[i], marker='s', edgecolor='black')
  plt.text(X[i, 0], X[i, 1], f'{w[i]:.2f}', color='black', fontsize=9,
    verticalalignment='bottom', horizontalalignment='right')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.show()
```

::: {.fragment}
::: {.callout-note}
Can a tree stump get to $\overline{err} = 0$? Can a deep decision tree?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
בדוגמה שלנו יש 10 תצפיות בשני משתנים, ונניח שהאדומות הן פלוס 1 והכחולות הן מינוס 1.

הוויק קלסיפייר שלנו יהיה גזעים של עצים או טרי סטאמפס, כלומר אנחנו מסוגלים בכל איטרציה לעשות רק פיצול אחד על משתנה אחד.

זה קלסיפייר ממש חלש, נכון? נניח האם אפשר עם הקלסיפייר הזה להגיע על הנתונים האלה לשגיאת חיזוי אפס? אי אפשר. המשמעות של טרי-סטאמפ היא שאנחנו יכולים רק לחלק את המרחב של שני המשתנים חלוקה אחת מקבילה לאחת הצירים.

אם היינו מאפשרים עץ עמוק יותר האם היינו צריכים בוסטינג? כנראה שכן (להדגים). אבל זאת רק דוגמה פשוטה.
:::
:::

---

### AdaBoost: iteration 1

```{python}
#| echo: false

def plot_status(X, y, classifier, w, iteration, err, alpha):
    # Create mesh grid
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                         np.arange(y_min, y_max, 0.01))

    # Plot for the current weak classifier
    Z_weak = classifier.predict(np.c_[xx.ravel(), yy.ravel()])
    Z_weak = Z_weak.reshape(xx.shape)

    # Plot weak classifier decision boundary
    plt.contourf(xx, yy, Z_weak, alpha=0.3, cmap=ListedColormap(['blue', 'red']))
    for i in range(len(X)):
        if y[i] == -1:
            plt.scatter(X[i, 0], X[i, 1], color='blue', s=500*w[i], marker='o', edgecolor='black')
        else:
            plt.scatter(X[i, 0], X[i, 1], color='red', s=500*w[i], marker='s', edgecolor='black')
        plt.text(X[i, 0], X[i, 1], f'{w[i]:.2f}', color='black', fontsize=9,
                    verticalalignment='bottom', horizontalalignment='right')
    plt.title(f'Weak Classifier (Iteration {iteration})\nerr = {err:.3f}, alpha = {alpha:.3f}')
    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)

    plt.show()

# Reset stumps and alphas for clean state
stumps = []
alphas = []

iteration = 1

# Fit a tree stump (DecisionTreeClassifier with max_depth=1)
stump = DecisionTreeClassifier(max_depth=1)
stump.fit(X, y, sample_weight=w)

# Store the stump and alpha values
stumps.append(stump)

# Calculate weighted predictions and error
pred = stump.predict(X)
err = np.sum(w * (pred != y)) / np.sum(w)

# Calculate alpha
alpha = np.log((1 - err) / err)
alphas.append(alpha)

# Plot weak classifier
plot_status(X, y, stump, w, iteration, err, alpha)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אם אנחנו מתחילים עם כל התצפיות ממושקלות אותו הדבר, כלומר עשירית, הטרי סטאמפ הראשון מוצא לנכון את החלוקה על פי המשתנה השני, מתחת יחזה כחול ומעל יחזה אדום.

השגיאה הממושקלת שלנו היא שני סיווגים שגויים עם משקולות עשירית חלקי סך משקולות של עשר עשיריות או 0.2. והאלפא שלנו תהיה לוג של 0.8 חלקי 0.2 כלומר לוג 4 או 1.386.

אנחנו ממשקלים מחדש את התצפיות, כאשר המשקולות של התצפיות עם סיווג נכון נשארות עשירית, והמשקולות של התצפיות שסיווגנו לא נכון מוכפלות פי אקספוננט בלוג-4 כלומר פי 4, הן יהיו 0.4.

אנחנו שומרים את המשקולות, את אלפא ואת העץ בצד וממשיכים לעץ הבא.
:::
:::

---

### AdaBoost: iteration 2

```{python}
#| echo: false

# Update weights
w = w * np.exp(alpha * (pred != y))
# w /= np.sum(w)  # Normalize weights

iteration = 2

# Fit a tree stump (DecisionTreeClassifier with max_depth=1)
stump = DecisionTreeClassifier(max_depth=1)
stump.fit(X, y, sample_weight=w)

# Store the stump and alpha values
stumps.append(stump)

# Calculate weighted predictions and error
pred = stump.predict(X)
err = np.sum(w * (pred != y)) / np.sum(w)

# Calculate alpha
alpha = np.log((1 - err) / err)
alphas.append(alpha)

# Plot weak classifier
plot_status(X, y, stump, w, iteration, err, alpha)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
באיטרציה הבאה אנחנו באמת רואים את המשקולות שחישבנו, ומגדלים סטאמפ חדש. הסטאמפ החדש רואה את הנתונים אחרת, הפעם הוא שם הרבה יותר דגש על לסווג נכון את התצפיות שלא סווגו נכון בעבר ויש להן משקולת של 0.4. ואכן הוא מחלק את המרחב בצורה שונה לחלוטין.

אבל זה יוצר שגיאה ממושקלת חדשה, יש לנו 3 טעויות במשקל כולל 0.3 חלקי סכום המשקלים שהוא 1.6 כלומר שגיאה של 0.188.

האלפא החדשה היא לוג של 0.812 חלקי 0.188, יוצא 1.46.

ושוב נגדיל את המשקל של התצפיות שחזינו לא נכון, נכפיל עשירית פי אקספוננט בחזקת 1.466, צריך לתת 0.43.
:::
:::

---

### AdaBoost: iteration 3

```{python}
#| echo: false

# Update weights
w = w * np.exp(alpha * (pred != y))
# w /= np.sum(w)  # Normalize weights

iteration = 3

# Fit a tree stump (DecisionTreeClassifier with max_depth=1)
stump = DecisionTreeClassifier(max_depth=1)
stump.fit(X, y, sample_weight=w)

# Store the stump and alpha values
stumps.append(stump)

# Calculate weighted predictions and error
pred = stump.predict(X)
err = np.sum(w * (pred != y)) / np.sum(w)

# Calculate alpha
alpha = np.log((1 - err) / err)
alphas.append(alpha)

# Plot weak classifier
plot_status(X, y, stump, w, iteration, err, alpha)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
וככה אנחנו ממשיכים לסטאמפ הבא.
:::
:::

---

### AdaBoost: iteration 4

```{python}
#| echo: false

# Update weights
w = w * np.exp(alpha * (pred != y))
# w /= np.sum(w)  # Normalize weights

iteration = 4

# Fit a tree stump (DecisionTreeClassifier with max_depth=1)
stump = DecisionTreeClassifier(max_depth=1)
stump.fit(X, y, sample_weight=w)

# Store the stump and alpha values
stumps.append(stump)

# Calculate weighted predictions and error
pred = stump.predict(X)
err = np.sum(w * (pred != y)) / np.sum(w)

# Calculate alpha
alpha = np.log((1 - err) / err)
alphas.append(alpha)

# Plot weak classifier
plot_status(X, y, stump, w, iteration, err, alpha)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ולסטאמפ הבא.
:::
:::

---

### AdaBoost: iteration 5

```{python}
#| echo: false

# Update weights
w = w * np.exp(alpha * (pred != y))
# w /= np.sum(w)  # Normalize weights

iteration = 5

# Fit a tree stump (DecisionTreeClassifier with max_depth=1)
stump = DecisionTreeClassifier(max_depth=1)
stump.fit(X, y, sample_weight=w)

# Store the stump and alpha values
stumps.append(stump)

# Calculate weighted predictions and error
pred = stump.predict(X)
err = np.sum(w * (pred != y)) / np.sum(w)

# Calculate alpha
alpha = np.log((1 - err) / err)
alphas.append(alpha)

# Plot weak classifier
plot_status(X, y, stump, w, iteration, err, alpha)
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ולסטאמפ הבא.
:::
:::

---

### AdaBoost: aggregate

::: {.fragment}
$\hat{f}(x) = \text{sign}\left[\sum_{m = 1}^M \alpha_m G_m(x)\right] =$

$= \text{sign}\left[1.386G_1(x) + 1.466G_2(x) + 0.934G_3(x) + 1.299G_4(x) + 1.126G_5(x)\right]$
:::

::: {.fragment}

```{python}
#| echo: false

# Plot for the aggregated classifier
# Create mesh grid
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                      np.arange(y_min, y_max, 0.01))

agg_pred = np.zeros_like(xx, dtype=float)
y_pred = np.zeros_like(y, dtype=float)
for a, stump in zip(alphas, stumps):
    agg_pred += a * stump.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    y_pred += a * stump.predict(X)
Z_agg = np.sign(agg_pred)
y_pred_agg = np.sign(y_pred)
overall_err = np.mean(y_pred_agg != y)

# Plot aggregated classifier decision boundary
plt.contourf(xx, yy, Z_agg, alpha=0.3, cmap=ListedColormap(['blue', 'red']))
for i in range(len(X)):
    if y[i] == -1:
        plt.scatter(X[i, 0], X[i, 1], color='blue', s=500*0.1, marker='o', edgecolor='black')
    else:
        plt.scatter(X[i, 0], X[i, 1], color='red', s=500*0.1, marker='s', edgecolor='black')
    # plt.text(X[i, 0], X[i, 1], f'{w[i]:.2f}', color='black', fontsize=9,
    #             verticalalignment='bottom', horizontalalignment='right')
plt.title(f'Aggregated Classifier (after {iteration} iterations)\noverall_err = {overall_err:.3f}')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)

plt.show()
```
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
ואם נניח שאנחנו עושים את זה 5 איטרציות, כלומר 5 סטאמפים, נעצור.

החיזוי הסופי שלנו לכל נקודה במרחב הזה יהיה פונקציית סיין, על הממוצע המשוקלל של הטרי סטאמפס, G1 עד G5.

אם נצייר את תחום ההחלטה הזה נראה שהוא כבר אחרי 5 איטרציות הגיע לשגיאה אפס, הוא מסווג נכון את כל התצפיות, ובאופן מרשים אדאבוסט לקח את הקלסיפייר החלש הזה שמסוגל למעט מאוד, ועשה לו בוסטינג בצורה אדאפטיבית ככה שהוא יכול לתאר גבול החלטה הרבה יותר מורכב.
:::
:::

---

### AdaBoost Guarantee

::: {.fragment}
::: {.callout-tip}
Theorem:

$$\overline{err} = \sum_{i = 1}^n \mathbb{I}\left[y_i \neq \hat{f}(x_i)\right] \le \left(1-4\gamma^2\right)^{M/2} \le e^{-2M\gamma^2}$$
:::
:::

::: {.incremental}
- This means for a large enough $M$ we can get $\overline{err}$ as low as we want (on training data)!

- Specifically, to make the upper bound $e^{-2M\gamma^2} < \frac{1}{n}$ (i.e. $\overline{err} = 0$) $\Rightarrow M > \frac{\log(n)}{2\gamma^2}$

- A weak classifier which can only guarantee up to 40\% error at each iteration ($\gamma = 0.1$):
  - after $M = 100$ iterations will be boosted to give $\overline{err} = (1 - 4 \cdot 0.1^2)^{(100/2)} = 0.96^{50} \approx 0.13$ error

  - if $n = 1000$, need $M > \log(1000) / 2 \cdot 0.1^2 \approx 345$ iterations to get $\overline{err} = 0$

- Really nice proof in Freund \& Schapire (1997)
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
כמובטח, אנחנו יכולים להביא את השגיאה על מדגם הלמידה להיות קטנה כרצוננו. ספציפית אנחנו יכולים לחסום אותה על-ידי הביטוי שמופיע כאן, 1 פחות ארבעה גאמא בריבוע בחזקת M חלקי 2. ואפשר להראות בקלות שחסם פשוט יותר הוא סדר גודל של אי בחזקת מינוס פעמיים מספר האיטרציות כפול גאמא בריבוע.

למשל, אם רוצים לחסום את הטעות שלא תהיה גדולה יותר מ1 חלקי n, מה שבפועל אומר שהיא חייבת להיות אפס, אנחנו לא מוכנים לעשות אפילו שגיאה אחת, אז אחרי קצת אלגברה אפשר לקבל חסם תחתון על מספר האיטרציות שאנחנו חייבים לבצע, שהוא סדר גודל של לוג-מספר התצפיות מוחלק בפרמטר גאמא בריבוע. שוב כל זה בהנחה שאנחנו מבטיחים שבכל איטרציה הקלסיפייר שלנו טועה עד כדי מרחק גאמא קטן מחצי, השגיאה האקראית.

אז אם לדוגמא יש לכם קלסיפייר די בינוני, הוא יכול לעשות שגיאה של עד 40 אחוז או גאמא שווה 0.1, אחרי 100 איטרציות, הוא הופך להיות קלסיפייר לא רע בכלל עם שגיאה לא גדולה יותר מ13 אחוז, פשוט מציבים בחסם.

ואם לדוגמא אנחנו יודעים שיש אלף תצפיות במדגם הלמידה, כדי להגיע לשגיאה אפס אנחנו יודעים שצריך לפחות 345 איטרציות.

לא נוכיח את זה כאן אבל מי שרוצה יכול לקרוא את אחד המאמרים המקוריים של פרוינד ושפייר, זה פשוט הרבה אלגברה מאוד מאוד נחמדה.
:::
:::

---

## AdaBoost as Additive Stagewise Modeling {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Questions from AdaBoost

- Does it optimize a specific loss? Can we plug-in a different loss?

- What about regression?

- What about $K$-class classification?

- Is there a probabilistic justification? A likelihood-based approach?

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
הרבה שאלות צריכות לעלות לנו כשאנחנו מסתכלים על אדאבוסט.

בחשיבה שלנו קודם מגדירים פונקצית הפסד ואז עושים לה מינימיזציה - האם אדאבוסט עושה מינימיזציה לאיזשהו הפסד, ואם כן האם אפשר לייצר אלגוריתמים דומים עם פונקציות הפסד אחרות?

למשל האם האלגוריתם טוב רק לקלסיפיקציה בינארית, או שאנחנו יכולים ליצור משהו דומה לרגרסיה או לקלסיפיקציה ליותר מ2 קלאסים?

ואם אמרנו שההוכחה שהאלגוריתם עובד היא פשוט הרבה אלגברה - האם יש לאלגוריתם הצדקה הסתברותית? זה מזכיר את העובדה שראינו כיצד רגרסיה ליניארית היא קודם כל פתרון סגור באלגברה, שאפשר להצדיק אותו גם מנקודת מבט סטטיסטית, עם הנחה של רעש שמתפלג נורמלית וכולי. יכול להיות שגם כאן נמצא נקודת מבט סטטיסטית?
:::
:::

---

### AdaBoost sure sounds familiar

::: {.fragment}
Recall: **Forward stagewise regression**
:::

::: {.fragment}
0. Standardize all features, input some $\tau_{thresh} \in (0, 1)$ and $\varepsilon > 0$ step size
1. Residual $\mathbf{r} = \mathbf{y} - \bar{y}$, $\beta_1, \dots, \beta_p = 0$
2. Find the predictor $\mathbf{x}_j$ most correlated with $\mathbf{r}$, and let $\tau = Corr(\mathbf{r}, \mathbf{x}_j)$
3. While $|\tau| > \tau_{thresh}$:
    i. Update $\beta_j \leftarrow \beta_j + \delta_j$, where $\delta_j = \varepsilon \cdot \text{sign}(\tau)$
    ii. Update $\mathbf{r} \leftarrow \mathbf{r} - \delta_j\mathbf{x}_j$
    iii. Find the predictor $\mathbf{x}_j$ most correlated with $\mathbf{r}$, and let $\tau = Corr(\mathbf{r}, \mathbf{x}_j)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
מסתבר שבאמת אפשר להכליל את אדאבוסט, למשפחה של אלגוריתמים, שאחד מהם ראינו והבטחתי שעוד נראה שוב.

כשדיברנו על שיטות לבחירת משתנים ברגרסיה ליניארית, ראינו שיטה בשם פורוורד סטייג'וויז רגרשן, שלאט לאט מכניסה משתנים למודל, במשקל קטן. משהו בה מאוד מזכיר את אדאבוסט.

ניזכר בפורוורד סטייג'וויז: אנחנו עושים סטנדרטיזציה למשתנים, ומחליטים על גודל צעד קטן אפסילון.

אנחנו בודקים מה השארית בין התצפיות Y לממוצע, מסמנים אותן בR. מאתחלים את המקדמים של כל המשתנים לאפס.

כעת מוצאים את המשתנה האחד שנמצא בקורלציה הגבוהה ביותר עם השארית, והוא המועמד להיכנס למודל. אם אכן הקורלציה גדולה מאיזשהו סף אנחנו מכניסים עוד קצת מהמשתנה למודל אבל רק אפסילון קטן מתוך הקורלציה הזאת.

מחשבים מחדש את השאריות, וחוזרים למצוא את המשתנה הJ הבא עם הכי הרבה קורלציה לשאריות, וכך הלאה, עד שאין משתנה יחיד עם קורלציה מספיק גדולה לשאריות.
:::
:::

---

### A general forward stagewise additive model (FSAM)

::: {.fragment}
1. Initialize $f_0(x) = 0$
2. For $m = 1$ to $M$:
    (a) Compute:
    $$(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + \beta b(x_i, \gamma)\right)$$
    (b) Set $f_m(x) = f_{m - 1}(x) + \beta_m b(x; \gamma_m)$
:::

::: {.fragment}
3. Output: $\hat{f}(x) = \sum_{m = 1}^M \beta_m b(x; \gamma_m)$
:::

::: {.fragment}
::: {.callout-note}
If $L$ is the squared loss, and finding $\gamma_m$ is finding the $j$-th predictor $\mathbf{x}_j$ most correlated with with the residual

$\Rightarrow$ this is forward stagewise regression!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
את האלגוריתם שכרגע ראינו קל לכתוב הרבה יותר בתור מודל פורוורד אדיטיבי בשלבים, או פורוורד סטייג'וויז אדיטיב מודל:

נתחיל בחיזוי אפס לכל התצפיות.

בכל איטרציה M ניקח איזשהו פונקציה פשוטה של הנתונים שנקרא לה B, שיש לה פרמטרים גאמא. ננסה לעדכן את המודל שיש לנו עד כאן עם הליכה של צעד בטא בכיוון הפונקציה הזאת, באופן שיקטין לנו הכי הרבה איזשהו לוס. או כמו שרשום כאן, נמצא את הפרמטרים בטא וגאמא שיביאו למינימום את הלוס של עדכון פשוט כזה.

נעדכן את המודל עם הפרמטרים שמצאנו ונמשיך לאיטרציה הבאה.

המודל הסופי הוא אכן מודל אדיטיבי, הוא סכום ממושקל של הרבה מודלים קטנים ופשוטים. בביטוי הזה אנחנו גם מייד רואים את הרמז לאדאבוסט אבל זה עוד לא לגמרי ברור.

מכל מקום, אם L הוא שגיאה ריבועית, וגאמא בכל איטרציה פירושה פשוט למצוא את המשתנה האחד עם הקורלציה הכי גדולה לשארית, מה שיש כאן זה בדיוק פורוורד סטייג'וויז רגרשן, מלבד הבדלים זניחים. שם אתחלנו את כל התצפיות בממוצע Y וכאן באפס, שם עשינו את זה עד שלא נמצא יותר משתנה עם מספיק קורלציה לשאריות וכאן אנחנו עושים את זה מספר קבוע של צעדים מראש.
:::
:::

---

### Claim: AdaBoost is also a FSAM

::: {.fragment}
- If $L$ is the exponential loss: $L(y, f(x)) = \exp(-yf(x))$
- And, $b(x; \gamma_m) = Gm(x)$
- We get AdaBoost!
:::

::: {.fragment}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

f_x = np.linspace(-2, 2, 100)
L = np.exp(-f_x)

f_x_disc = np.array([-1, 1])
L_disc = np.exp(-f_x_disc)

plt.figure(figsize=(4, 3))
plt.plot(f_x, L, label = 'f(x) continuous')
plt.bar(f_x_disc, L_disc, label = 'f(x) discrete', width = 0.1)
plt.xlabel('f(x)')
plt.ylabel('L(f(x)) = exp(-f(x))')
plt.title('Exponential loss for y = +1')
plt.legend()
plt.show()
```

:::

::: {.fragment}
::: {.callout-note}
Fast forward: plug-in *other* loss functions!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אז אני טוען, שגם אדאבוסט הוא מודל פורווד סטייג'וויז אדיטיב.

אני טוען שהלוס שהוא מביא למינימום בכל איטרציה הוא לוס אקספוננציאלי שלא ראינו עדיין, אקספוננט בחזקת מינוס Y שאני מזכיר שהוא מינוס או פלוס 1, כפול החיזוי f(X).

ואם הפונקציות הפשוטות שקראנו להן B הן הקלסיפיירים החלשים שלנו Gm בכל איטרציה -- נקבל בדיוק את אדאבוסט.

אז לפני שנראה את זה, נכיר את הלוס האקספוננציאלי ולמה הוא סביר לבעיה שלנו: נניח שY הוא פלוס 1. אם אנחנו חוזים f(X) בדיד, שהוא גם פלוס ומינוס אחת, אז אם אנחנו חוזים מינוס 1 וטועים אנחנו מקבלים שגיאה של אקספוננט בחזקת 1 כלומר 2.7 בערך, ואם אנחנו חוזים 1 וצודקים אנחנו מקבלים שגיאה של אקספוננט בחזקת מינוס 1, כלומר 0.3 בערך.

אפשר אבל גם להרציף את הטעות הזאת, לחזות f(X) רציף, ואז ככל שהוא שלילי הטעות גדולה יותר, וככל שהוא חיובי הטעות שואפת לאפס.

ועוד דבר קטן, שוב למה אנחנו טורחים כל כך, רק כי זה מעניין? לא, המטרה שלנו היא להכליל לעוד פונקציות הפסד, ולהיות מסוגלים לעשות אדאבוסט לרגרסיה ולבעיות אחרות. אז בואו ניגש למשימה.
:::
:::

---

### Showing AdaBoost is FSAM

::: {.incremental}
- Recall the goal in FSAM iteration $m$:
$$(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + \beta b(x_i, \gamma_)\right)$$

- For exponential loss:
$$(\beta_m, G_m) = \arg\min_{\beta, G} \sum_{i = 1}^n \exp\left[-y_i(f_{m-1}(x_i) + \beta G(x_i))\right]$$
:::
::: {.fragment}
$= \arg\min_{\beta, G} \sum_{i = 1}^n \exp\left[-y_if_{m-1}(x_i)\right]\exp\left[\beta G(x_i)\right]$
:::
::: {.fragment}
$= \arg\min_{\beta, G} \sum_{i = 1}^n w_i^{(m)}\exp\left[-\beta y_i G(x_i)\right]$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Getting $G_m$

::: {.fragment}
- Assuming $\beta > 0$, separate the criterion to "correct" + "incorrect" sums:
:::
::: {.fragment}
$\sum_{i = 1}^n w_i^{(m)}\exp\left[-\beta y_i G(x_i)\right] = e^{-\beta}\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i = G(x_i)\right] + e^{\beta}\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right]$
:::
::: {.fragment}
- Can also write as:

$= (e^{\beta} - e^{-\beta})\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right] + e^{-\beta}\sum_{i = 1}^n w_i^{(m)}$
:::
::: {.fragment}
- AdaBoost stage 2a: $G_m$ minimizing the criterion is minimizing weighted error rate:
$$G_m = \arg\min_G \sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right]$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Getting $\alpha_m$

::: {.fragment}
$\text{criterion} = (e^{\beta} - e^{-\beta})\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right] + e^{-\beta}\sum_{i = 1}^n w_i^{(m)}$
:::
::: {.fragment}
$\frac{\partial \text{criterion}}{\partial \beta} = (e^{\beta} + e^{-\beta})\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right] - e^{-\beta}\sum_{i = 1}^n w_i^{(m)} \to =0$
:::
::: {.fragment}
- Multiply by $e^\beta$:

$(1 + e^{2\beta}) = \frac{\sum_{i = 1}^n w_i^{(m)}}{\sum_{i = 1}^n w_i^{(m)}\mathbb{I}\left[y_i \neq G(x_i)\right]} = \frac{1}{err_m}$
:::
::: {.fragment}
- Take $ln$, we get AdaBoost stages 2b and 2c:

$\beta_m = \frac{1}{2}\ln\left[\frac{1-err_m}{err_m}\right] = \frac{1}{2}\alpha_m$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Getting $w_i$

::: {.fragment}
- FSAM says:

$f_m(x) = f_{m - 1}(x) + \beta_m b(x; \gamma_m) = f_{m - 1}(x) + \beta_m G_m(x) = f_{m - 1}(x) + \frac{1}{2}\alpha_m G_m(x)$
:::
::: {.fragment}
- Which means next iteration weights are:

$w_i^{(m + 1)} = \exp\left[-y_if_{m+1}(x_i)\right] = \exp\left[-y_i(f_{m}(x) + \frac{1}{2}\alpha_m G_m(x))\right] = w_i^{(m)}\cdot\exp\left[-\frac{1}{2}\alpha_m y_i G_m(x)\right]$
:::
::: {.fragment}
- Given that $-y_i G_m(x_i) = 2\cdot\mathbb{I}\left[y_i \neq G_m(x_i)\right] - 1$, we can write:

$w_i^{(m + 1)} = w_i^{(m)} \cdot \exp\left[\alpha_m \cdot \mathbb{I}\left[y_i \neq G_m(x_i)\right]\right] \cdot Const$
:::
::: {.fragment}
- Equivalent to AdaBoost stage 2d.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Classification losses

:::: {.columns}
::: {.column}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Define the range for y * f(x)
yf = np.linspace(-2, 2, 400)

# Define the loss functions
misclass_loss = np.where(yf > 0, 0, 1)
exp_loss = np.exp(-yf)
binomial_deviance = np.log(1 + np.exp(-2 * yf)) / np.log(2)
squared_error = (1 - yf) ** 2
hinge_loss = np.maximum(0, 1 - yf)

# Plot the losses
plt.figure(figsize=(5, 4))

# Misclassification Loss
plt.plot(yf, misclass_loss, label="Misclassification", color="black")

# Exponential Loss
plt.plot(yf, exp_loss, label="Exponential", color="cyan")

# Binomial Deviance
plt.plot(yf, binomial_deviance, label="Binomial NLL", color="orange")

# Squared Error
plt.plot(yf, squared_error, label="Squared Error", color="red")

# Support Vector (Hinge Loss)
plt.plot(yf, hinge_loss, label="Support Vector (Hinge)", color="green")

# Add labels and legend
plt.xlabel("y · f")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()
```

::: {.fragment}
::: {.callout-note}
The $f(x)$ minimizing exponential loss is also minimizing Binomial negative log-likelihood!
:::
:::

:::
::: {.column}
- Misclassification (0/1 loss):

   $L_{\text{misclass}}(y, f) = \begin{cases} 
   0 & \text{if } y \cdot f(x) > 0 \\
   1 & \text{otherwise}
   \end{cases}$

- Exponential loss:

   $L_{\text{exp}}(y, f) = \exp(-y \cdot f(x))$

- Binomial negative log-likelihood (cross-entropy):

   $L_{\text{bin}}(y, f) = \log(1 + \exp(-2 \cdot y \cdot f(x))) / \log(2)$

- Squared error loss:

   $L_{\text{sq}}(y, f) = (1 - y \cdot f(x))^2$

- Support vector (hinge Loss):

   $L_{\text{hinge}}(y, f) = \max(0, 1 - y \cdot f(x))$
:::
::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Boosting for Regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Recap

What the FSAM framework got us:

$$(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + \beta b(x_i, \gamma)\right)$$

- Classification:
  - $y \in \{-1, 1\}$, exponential loss, $b(x_i, \gamma)$ are weak learners $G(x)$ $\to$ AdaBoost
- Regression:
  - $y \in \mathbb{R}$, squared error loss, $b(x_i, \gamma)$ are single features $\mathbf{x}_j$ $\to$ Forward stagewise regression

::: {.fragment}
- What if we want to boost weak learners $G(x)$ for regression?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Boosted trees for regression

::: {.incremental}
- Specifically, let us focus on boosting trees: $f(x) = \sum_{m = 1}^M T(x, \Theta_m)$
- Where $T(x, \Theta) = \sum_{j = 1}^J \gamma_j\mathbb{I}\left(X \in R_j\right)$
- With parameters $\Theta = \{R_j, \gamma_j\}_1^J$
:::
::: {.fragment}
- With squared loss, no problem:

$\Theta_m = \arg\min_{\Theta} \sum_{i = 1}^n L\left(y_i, f_{m-1}(x_i) + T(x_i, \Theta)\right) = \arg\min_{\Theta} \sum_{i = 1}^n\left(y_i - f_{m-1}(x_i) - T(x_i, \Theta)\right)^2$
:::
::: {.fragment}
$= \arg\min_{\Theta} \sum_{i = 1}^n\left(r_{im} - T(x_i, \Theta)\right)^2$
:::
::: {.incremental}
- Because we know how to build such trees!
- That is, at each iteration $m$ build standard regression tree $T(x, \Theta)$ which best fits the current residuals $\mathbf{r}_m$
:::
::: {.fragment}
::: {.callout-note}
At high-level, similar to AdaBoost!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### What about more [robust]{style="color:red;"} losses?

:::: {.columns}
::: {.column}
```{python}
#| echo: false

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt

# Define the range for y - f(x)
residual = np.linspace(-3, 3, 400)

# Define the loss functions
squared_error = residual ** 2
absolute_error = np.abs(residual)

# Define Huber loss with delta = 1
delta = 1
huber_loss = np.where(np.abs(residual) <= delta,
                      residual ** 2,
                      2 * delta * np.abs(residual) - delta**2)

# Plot the losses
plt.figure(figsize=(5, 4))

# Squared Error Loss
plt.plot(residual, squared_error, label="Squared Error", color="orange")

# Absolute Error Loss
plt.plot(residual, absolute_error, label="Absolute Error", color="cyan")

# Huber Loss
plt.plot(residual, huber_loss, label="Huber", color="green")

# Add vertical lines at -delta and delta for Huber loss
plt.axvline(x=-delta, color='black', linestyle='dotted')
plt.axvline(x=delta, color='black', linestyle='dotted')

# Add labels and legend
plt.xlabel("y - f")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()
```

:::
::: {.column}
- Squared Error Loss:
   
  $L_{\text{sq}}(y, f) = (y - f(x))^2$

- Absolute Error Loss:
  
  $L_{\text{abs}}(y, f) = |y - f(x)|$

- Huber Loss:
  
  $L_{\text{H}}(y, f) = \begin{cases} 
   (y - f(x))^2 & |y - f(x)| \leq \delta \\
   2\delta |y - f(x)| - \delta^2 & \text{otherwise.}
   \end{cases}$
:::
::::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Gradient Boosting {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Gradient descent algorithms

Minimize a function $J(\theta)$ by moving in the opposite direction of the gradient:
$$\hat\theta_{i+1} = \hat\theta_i - \varepsilon \frac{\partial J(\theta)}{\partial \theta}$$

```{python}
#| echo: false

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt

# Define the function J(theta) = theta_1^2 + 0.5 * theta_2^2 (quadratic function)
def J(theta):
    return theta[0]**2 + 0.5 * theta[1]**2

# Define the gradient of J(theta)
def grad_J(theta):
    return np.array([2 * theta[0], 1 * theta[1]])

# Perform gradient descent
def gradient_descent(initial_theta, learning_rate, num_iterations):
    theta = initial_theta
    path = [theta]
    for i in range(num_iterations):
        theta = theta - learning_rate * grad_J(theta)
        path.append(theta)
    return np.array(path)

# Set parameters for gradient descent
initial_theta = np.array([-3.0, -3.0])
learning_rate = 0.3
num_iterations = 5

# Run gradient descent and get the path
path = gradient_descent(initial_theta, learning_rate, num_iterations)

# Create a grid of points for the contour plot
theta1_vals = np.linspace(-4, 4, 400)
theta2_vals = np.linspace(-4, 4, 400)
theta1, theta2 = np.meshgrid(theta1_vals, theta2_vals)
Z = J([theta1, theta2])

# Plot the contour map
plt.figure(figsize=(5, 4))
plt.contour(theta1, theta2, Z, levels=20, colors='blue')

# Plot the path of gradient descent
plt.plot(path[:, 0], path[:, 1], marker='x', color='red', markersize=10, label="Gradient Descent Path")
plt.quiver(path[:-1, 0], path[:-1, 1], path[1:, 0] - path[:-1, 0], path[1:, 1] - path[:-1, 1], 
           scale_units='xy', angles='xy', scale=1, color='red')

# Annotate the points
for i in range(len(path)):
    plt.text(path[i, 0], path[i, 1], r"$\hat\theta_{}$".format(i), color="black", fontsize=12)

# Set labels and title
plt.xlabel(r'$\theta_1$')
plt.ylabel(r'$\theta_2$')
plt.grid(True)
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Gradient Boosting Machines

::: {.incremental}
- Consider any loss $L(y, \mathbf{f})$ a function of $n$ data points $\mathbf{f} = (f(x_1), \dots, f(x_n))$
- At each iteration $m$, calculate the gradient of $L$ by $\mathbf{f}$, evaluated at $\mathbf{f}_{m - 1}$:
$$\mathbf{g}_m = \frac{\partial L(y, \mathbf{f})}{\partial \mathbf{f}}\Bigr|_{\mathbf{f} = \mathbf{f}_{m - 1}}$$
- Move a small step $\varepsilon$ down this gradient:
$$\mathbf{f}_m = \mathbf{f}_{m - 1} - \varepsilon\mathbf{g}_m$$
- Technically, $\varepsilon$ can also be optimized at each iteration $m$ to minimize
$$\varepsilon_m = \arg\min_\varepsilon L(y, \mathbf{f}_{m - 1} - \varepsilon\mathbf{g}_m)$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Gradient Boosting Machines

::: {.incremental}
- Eventually we would get:
$$\mathbf{f}_M = \sum_{i = 1}^M\varepsilon_m(-\mathbf{g}_m)$$
- But we are not interested in $\mathbf{f}_M$ for our training data, we wanted a model!
  - E.g. $f(x) = \sum_{m = 1}^M T(x, \Theta_m)$

- Solution: at each iteartion $m$ approximate the negative gradient $-\mathbf{g}_m$ with a weak learner or regression tree!
$$\Theta_m = \arg\min_\Theta \sum_{i = 1}^n (-g_{im} - T(x_i, \Theta))^2$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Gradients of common loss functions

| setting         | loss                          | gradient                                     |
|-----------------|-------------------------------|----------------------------------------------|
| Regression      | $\frac{1}{2}(y_i - f(x_i))^2$ | $y_i - f(x)$                                 |
| Regression      | $|y_i - f(x_i)|$              | $\text{sign}(y_i - f(x))$                    |
| Regression      | Huber                         | $\begin{cases} y_i - f(x) & \text{if } |y - f(x)| \leq \delta \\ \delta\text{sign}(y_i - f(x)) & \text{otherwise} \end{cases}$ |
| Classification* | NLL                           | $\mathbb{I}\left[y_i = 1\right] - \hat{p}_i$ |

*Here $y \in \{0, 1\}$ and $\hat{p}_i = \frac{\exp(\hat{y}_i)}{1 + \exp(\hat{y}_i)} = \frac{\exp(f_{m - 1}(x_i))}{1 + \exp(f_{m - 1}(x_i))}$

::: {.fragment}
::: {.callout-note}
Notice for (half) squared loss the gradient is the residuals, as we got earlier.
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Gradient Boosting Trees (simplified)

1. Initialize $f_0(x) = \arg\min_\gamma \sum_{i = 1}^n L(y_i, \gamma)$ (e.g. $f_0(x) = \bar{y}$)
2. For $m = 1$ to $M$:
    (a) Compute pseudo-residuals:
    $$\mathbf{r}_m = -\left[\frac{\partial L(y, \mathbf{f})}{\partial \mathbf{f}}\right]_{\mathbf{f} = \mathbf{f}_{m - 1}}$$
    (b) Fit a **regression tree** to data $(\mathbf{x}, \mathbf{r}_m)$, giving $T(x, \Theta_m)$
    (c) Update: $f_m(x) = f_{m - 1}(x) + \varepsilon T(x, \Theta_m)$
3. Output: $\hat{f}(x) = \sum_{m = 1}^M \varepsilon T(x, \Theta_m)$

::: {.fragment}
::: {.callout-note}
What parameters need tuning?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Example: credit data

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score, KFold

# Load the dataset
df = pd.read_csv('../datasets/ISLP_credit_data.csv')

# One-hot encoding for categorical features
df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
df['Student'] = df['Student'].apply(lambda x: 1 if x == 'Yes' else 0)
df['Married'] = df['Married'].apply(lambda x: 1 if x == 'Yes' else 0)
df = pd.get_dummies(df, columns=['Ethnicity'], drop_first=True)

# Drop the unnecessary 'Unnamed: 0' column
df = df.drop(columns=['Unnamed: 0'])

# Define the dependent variable
y = df['Balance']

# Define the independent variables
X = df.drop(columns=['Balance'])
# Assuming X, y are already defined
n_splits = 5  # Number of cross-validation folds
n_trees = range(1, 101, 10)  # Range of trees to evaluate in GBT

train_errors_gb = []
test_errors_gb = []
train_errors_gb_std = []
test_errors_gb_std = []

# Cross-validation setup
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

for n_tree in n_trees:
    train_scores = []
    test_scores = []
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        # Initialize the GBT Regressor
        gb_regressor = GradientBoostingRegressor(n_estimators=n_tree, random_state=42)
        gb_regressor.fit(X_train, y_train)
        
        # Evaluate train and test MSE
        train_scores.append(np.mean((gb_regressor.predict(X_train) - y_train) ** 2))
        test_scores.append(np.mean((gb_regressor.predict(X_test) - y_test) ** 2))
    
    # Store mean and standard deviation of MSE across folds
    train_errors_gb.append(np.mean(train_scores))
    test_errors_gb.append(np.mean(test_scores))
    train_errors_gb_std.append(np.std(train_scores)/np.sqrt(5))
    test_errors_gb_std.append(np.std(test_scores)/np.sqrt(5))

# Compute train/test MSE for Linear Regression and Single Decision Tree
linear_regressor = LinearRegression()
tree_regressor = DecisionTreeRegressor(max_depth=9)

# Single Decision Tree MSE
train_mse_tree = -cross_val_score(tree_regressor, X, y, cv=kf, scoring='neg_mean_squared_error').mean()
test_mse_tree = train_mse_tree  # Same for train and test in cross-validation

# Plotting the results
# plt.figure(figsize=(12, 8))

# Plot GBT MSE vs. Number of Trees
plt.errorbar(n_trees, train_errors_gb, yerr=train_errors_gb_std, label='GBT Train MSE')
plt.errorbar(n_trees, test_errors_gb, yerr=test_errors_gb_std, label='GBT Test MSE')

# Plot horizontal lines for a Single Decision Tree
plt.axhline(y=train_mse_tree, color='black', linestyle='-.', label='Single Tree MSE')

plt.xlabel('Number of trees')
plt.ylabel('MSE')
plt.ylim(0, 40000)
plt.legend()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
