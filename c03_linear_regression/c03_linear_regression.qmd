---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Linear Regression"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Linear Regression - Class 2

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Simple linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Why learn linear regression?

- Linear regression is still a highly useful modeling tool
- It can be used in many non-linear cases by using transformed variables / transformed goals
- Its probabilistic aspect is fully understood, so inference questions can be answered exactly (under the regression assumptions)
- Many newer methods can be seen as a generalization for linear regression

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Simple linear regression

- Assume: $y \approx \beta_0 + \beta_1x$
- This is the *unobserved* [population regression]{style="color:red;"} line
- We look for the values of $\beta_0, \beta_1$ through a sample $\{(x_1, y_1), \dots, (x_n, y_n)\}$
- In order to estimate the parameters we need to define a measure of error
- By far the most common measure is:

$RSS = \sum_{i = 1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2$ or its scaled version: $MSE = \frac{RSS}{n}$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### From line to RSS surface

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from mpl_toolkits.mplot3d import Axes3D

# Set the random seed for reproducibility
np.random.seed(42)

# Number of samples
n = 100

# Generate X values
X = np.random.rand(n, 1)

# Generate noise
e = np.random.randn(n, 1)

# True population line parameters
beta_0_true = 2
beta_1_true = 3

# Generate Y values
Y = beta_0_true + beta_1_true * X + e

# Fit the least squares regression line
model = LinearRegression().fit(X, Y)
Y_pred = model.predict(X)

# Create a meshgrid for beta_0 and beta_1
beta_0_range = np.linspace(beta_0_true - 2, beta_0_true + 2, 100)
beta_1_range = np.linspace(beta_1_true - 2, beta_1_true + 2, 100)
beta_0, beta_1 = np.meshgrid(beta_0_range, beta_1_range)

# Calculate the RSS for each combination of beta_0 and beta_1
RSS = np.zeros(beta_0.shape)
for i in range(beta_0.shape[0]):
    for j in range(beta_0.shape[1]):
        Y_pred_mesh = beta_0[i, j] + beta_1[i, j] * X
        RSS[i, j] = np.sum((Y - Y_pred_mesh) ** 2)

# Prepare for plotting
fig = plt.figure(figsize=(5 * 3, 3.5))

ax1 = fig.add_subplot(131)
# Plot the sample points
ax1.scatter(X, Y, color='blue', label='Sample points')
ax1.set_xlabel(r'$x$')
ax1.set_ylabel(r'$y$')
# Plot the least squares regression line
ax1.plot(X, Y_pred, color='red', linestyle='--', label='Least squares line', linewidth=2)
# Plot vertical lines from each sample point to the fitted line
for i in range(n):
    ax1.plot([X[i], X[i]], [Y[i], Y_pred[i]], color='gray', linestyle=':', linewidth=1)
ax1.set_title(r'Residuals for specific $\hat{\beta}_0, \hat{\beta}_1$')

# Contour plot on the left
ax2 = fig.add_subplot(132)
contour = ax2.contour(beta_0, beta_1, RSS, levels=50, cmap='viridis')
ax2.set_xlabel(r'$\beta_0$')
ax2.set_ylabel(r'$\beta_1$')
ax2.set_title('Contour plot of RSS')
fig.colorbar(contour, ax=ax2, label='RSS')

# Add a red cross at the true beta values
ax2.plot(beta_0_true, beta_1_true, 'rx', markersize=12, markeredgewidth=3)
# Add dashed lines from the cross to the true beta_0 and beta_1 axes
ax2.plot([beta_0_true, beta_0_true], [beta_1_range.min(), beta_1_true], 'r--')
ax2.plot([beta_0_range.min(), beta_0_true], [beta_1_true, beta_1_true], 'r--')

# 3D plot on the right
ax3 = fig.add_subplot(133, projection='3d')
ax3.plot_surface(beta_0, beta_1, RSS, cmap='viridis', edgecolor='none')
ax3.set_xlabel(r'$\beta_0$')
ax3.set_ylabel(r'$\beta_1$')
ax3.set_zlabel('RSS')
ax3.set_title('3D plot of RSS surface')

# Add a red cross at the true beta values
ax3.plot([beta_0_true], [beta_1_true], [np.min(RSS)], 'rx', markersize=12, markeredgewidth=3, zorder=10)

# Add dashed lines from the cross to the true beta_0 and beta_1 axes
ax3.plot([beta_0_true, beta_0_true], [0, beta_1_true], [np.min(RSS), np.min(RSS)], 'r--', zorder=10)
ax3.plot([4, beta_0_true], [beta_1_true, beta_1_true], [np.min(RSS), np.min(RSS)], 'r--', zorder=10)

# plt.tight_layout()
plt.show()
```

$$RSS = \sum_{i = 1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Simple linear regression

- A simple derivation gives:

$\hat{\beta}_1 = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} = \frac{\widehat{Cov(X,Y)}}{\widehat{Var(X)}},\;\;\;\;\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$

- This is the [least squares]{style="color:red;"} line (OLS), the *best* linear predictor for the population regression line
- Replacing the sample averages by the population means (or having a huge sample) should get us to the true line
- Using $\hat{\beta}_0, \hat{\beta}_1$ we can easily perform prediction

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Average of many OLS lines

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Set the random seed for reproducibility
np.random.seed(42)

# Number of samples
n = 100

# Generate X values
X = np.random.rand(n, 1)

# Generate noise
e = np.random.randn(n, 1)

# True population line parameters
beta_0 = 2
beta_1 = 3

# Generate Y values
Y = beta_0 + beta_1 * X + e

# Fit the least squares regression line
model = LinearRegression().fit(X, Y)
Y_pred = model.predict(X)

# Prepare for plotting
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Plot on the left
axs[0].scatter(X, Y, color='blue', label='Sample points')
axs[0].plot(X, beta_0 + beta_1 * X, color='green', label='True population line', linewidth=2)
axs[0].plot(X, Y_pred, color='red', label='Least squares line', linewidth=2)
axs[0].set_title('Single Sample Regression')
axs[0].legend()

# Plot on the right
# axs[1].scatter(X, Y, color='blue', label='Sample points')
axs[1].plot(X, beta_0 + beta_1 * X, color='green', label='True population line', linewidth=2)

# Plot 10 additional estimated regression lines
for _ in range(10):
    X_sample = np.random.rand(n, 1)
    e_sample = np.random.randn(n, 1)
    Y_sample = beta_0 + beta_1 * X_sample + e_sample
    model_sample = LinearRegression().fit(X_sample, Y_sample)
    Y_sample_pred = model_sample.predict(X)
    axs[1].plot(X, Y_sample_pred, color='orange', linestyle='--', alpha=0.6)

# Also plot the estimated line from the original sample
axs[1].plot(X, Y_pred, color='red', label='Original sample estimated line', linewidth=2)
axs[1].set_title('Multiple Sample Regressions')
axs[1].legend()

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Probabilistic View of Linear Regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Probabilistic justification for OLS

- Assume the data's true model is:
  - $y_i = \beta_0 + \beta_1x_i + \varepsilon_i$
  - $\varepsilon_i$ are i.i.d, specifically: $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$
  - $(x_i, \varepsilon_i)$ are independent
- *Now* we can write the log likelihood and maximize it

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Simple linear regression maximum likelihood

Since the errors are normal and the model is linear we get:

$y_i = \beta_0 + \beta_1x_i + \varepsilon_i  \to y_i \sim \mathcal{N}(\beta_0 + \beta_1x_i, \sigma^2) \to$

$$L(\beta, \sigma^2 | x, y) = \prod_{i = 1}^n f(y_i) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(y_i - \beta_0 - \beta_1x_i)^2}{2\sigma^2}\right]$$
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_i)^2\right]$$

Calculating the log-likelihood we get:

$\ell(\beta, \sigma^2 | x, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}$[$\sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_i)^2$]{style="color:red;"}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Multiple linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### From simple to multiple regression

<br></br>

$y_i = \beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip} + \varepsilon_i \to y_i = x_i'\beta + \varepsilon_i$, $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$

Or even more concisely:

$y = \begin{pmatrix}y_{1} \\ \vdots \\ y_{n}\end{pmatrix}$, $X = \begin{pmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p} \\
1 & x_{31} & x_{32} & \cdots & x_{3p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{np}
\end{pmatrix}$, $\beta = \begin{pmatrix}\beta_0 \\ \beta_{1} \\ \vdots \\ \beta_{p}\end{pmatrix}$, $\varepsilon = \begin{pmatrix}\varepsilon_{1} \\ \vdots \\ \varepsilon_{n}\end{pmatrix}$

::: {.fragment}
$$\to y = X\beta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(\textbf{0}, \sigma^2I_n)$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### From line to (hyper)plane
```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from matplotlib.patches import Patch
from matplotlib.lines import Line2D

df = pd.read_csv('../datasets/ISLP_income_data.csv')
X1 = df['Education']
X2 = df['Seniority']
X = np.column_stack((X1, X2))
Y = df['Income']

# Plotting
fig, ax = plt.subplots(1, 1, figsize=(14, 7), subplot_kw={'projection': '3d'})

# Create a grid of values for X1 and X2
X1_grid, X2_grid = np.meshgrid(np.linspace(min(X1), max(X1), 200),
                               np.linspace(min(X2), max(X2), 200))

# Flatten the grid to pass into the model
X_grid = np.column_stack((X1_grid.ravel(), X2_grid.ravel()))

model = LinearRegression()

model.fit(X, Y)

# Predict Y values
Y_pred = model.predict(X_grid).reshape(X1_grid.shape)

# Plot the fitted plane
ax.scatter(X1, X2, Y, color='red', label='Data')
ax.plot_surface(X1_grid, X2_grid, Y_pred, color='blue', alpha=0.5)

# Add vertical lines from data points to the surface
Y_surface = model.predict(X).reshape(Y.shape)  # Predicted Y values for the actual data points
for x1, x2, y, y_surface in zip(X1, X2, Y, Y_surface):
    ax.plot([x1, x1], [x2, x2], [y, y_surface], color='black')

# Labels
ax.set_xlabel('Education')
ax.set_ylabel('Seniority')
ax.set_zlabel('Income')
# ax.set_title(title)

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Multiple linear regression ML

$y_i = x_i'\beta + \varepsilon_i  \to y \sim \mathcal{N}(x_i'\beta, \sigma^2) \to$

$$L(\beta, \sigma^2 | X, y) = \prod_{i = 1}^n f(y_i) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(y_i - x_i'\beta)^2}{2\sigma^2}\right]$$
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - x_i'\beta)^2\right]$$
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta)\right]$$

Calculating the log-likelihood we get:

$\ell(\beta, \sigma^2 | X, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}$[$(y - X\beta)'(y - X\beta)$]{style="color:red;"}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Linear regression MLE

$\ell(\beta, \sigma^2 | X, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta)$

::: {.fragment}
$= \mathcal{C} - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y'y -2\beta'X'y + \beta'X'X\beta)$
:::

<br></br>

::: {.fragment}
$\frac{\partial l}{\partial \beta} = -\frac{1}{2\sigma^2}(2X'y - 2X'X\beta)$
:::

::: {.fragment}
$\frac{1}{\sigma^2}(X'y - X'X\beta) = 0$
:::
::: {.fragment}
$X'X\beta = X'y$
:::
::: {.fragment}
[$\hat{\beta} = (X'X)^{-1}X'y$]{style="color:red;"}
:::

<br></br>

::: {.fragment}
Similarly:

$\frac{\partial l}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}(y - X\beta)'(y - X\beta)$ $\to$ [$\hat{\sigma}^2 = \frac{1}{n}(y - X\beta)'(y - X\beta)$]{style="color:red;"}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Distribution of OLS estimators {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Distribution of the OLS solution

::: {.incremental}
- What we know: 
$$(a)\; E(Y) = X\beta,\;\;\;\; (b)\; Cov(Y) = \sigma^2 I_n ,\;\;\;\;(c)\; \hat{\beta} = (X'X)^{-1} X' Y$$

- Mean: 
$$E(\hat{\beta}) \stackrel{(c)}{=} (X'X)^{-1} X' E(Y) \stackrel{(a)}{=} (X'X)^{-1} X' X\beta = \beta.$$

- Covariance matrix: 
$$Cov(\hat{\beta}) \stackrel{(c)}{=} (X'X)^{-1} X' Cov(Y) X (X'X)^{-1} \stackrel{(b)}{=} \sigma^2 (X'X)^{-1} (X' X) (X'X)^{-1} = \sigma^2 (X'X)^{-1}.$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
אנחנו רוצים למצוא את ההתפלגות של האומד בטא האט. בטא-האט הוא צירוף לינארי של משתנים נורמליים ולכן גם הוא, מתפלג נורמלית, עם איזשהו וקטור תוחלות ומטריצת שונויות (להדגים). השאלה היחידה היא מהו וקטור התוחלות ומטריצת השונויות האלה. 

מה ידוע לנו עד כה? התוחלת המותנית של Y היא X בטא. הYים בלתי תלויים, מטריצת השונויות שלהם היא אלכסונית עם סיגמא בריבוע על האלכסון, ניתן לסמן זאת כך. והאומד לבטא-האט נראה כך, הוא לא משתנה כאמור בעקבות ההנחה הסטטיסטית.

נחשב את התוחלת של בטא-האט: האיקסים הם קבועים, ומליניאריות התוחלת הם יוצאים החוצה ונשארת רק התוחלת של Y, שהיא כידוע X בטא, וכך אנחנו מגיעים לעבודה שהתוחלת של בטא-האט היא בטא עצמו, בטא-האט נקרא אומד חסר הטיה לבטא.

ומטריצת השונות או הקווריאנס של וקטור בטא-האט: כשמחשבים שונות של סקלאר כפול משתנה הסקלאר יוצא בריבוע. כשמחשבים מטריצת שונות של מטריצת קבועים כפול הוקטור שלנו, היא יוצאת בהכפלה משמאל ומימין. אבל מטריצת השונות של Y היא כאמור אלכסונית, וכל מה שנשאר זה הכפלה של הביטוי הזה בסיגמא בריבוע. דברים מצטמצמים ומגיעים לביטוי סופי, סיגמא בריבוע כפול המטריצה ההופכית של X'X.

נסכם: בטא-האט מתפלג נורמלית עם תוחלת בטא האמיתית, ושונות סיגמא בריבוע כפול מטריצת X'X.
:::
:::

---

### Variance of OLS estimators

- Again: $Cov(\hat{\beta}) = \sigma^2(X'X)^{-1}$

- For simple regression and scalar $\hat{\beta}_0, \hat{\beta}_1$, this amounts to the (squared) [Standard Errors]{style="color:red;"}:

$$SE(\beta_0)^2 = \sigma^2\left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i = 1}^n(x_i - \bar{x})^2}\right]; \quad SE(\beta_1)^2 = \frac{\sigma^2}{\sum_{i = 1}^n(x_i - \bar{x})^2}$$

- where, $\sigma^2$ would be replaced be its MLE or unbiased estimator the (squared) [Residual Standard Error]{style="color:red;"} (RSE): $RSE^2 = \frac{RSS}{n - p - 1}$

::: {.fragment}
::: {.callout-note}
Question: what happens to the SE when $x$ is more "spread out"?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### SE vs. $x$ spread

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Function to create sample data and perform linear regression
def generate_data(n, spread_x=False, seed=1):
    np.random.seed(seed)
    bound = 1
    if spread_x:
      bound = bound * 5
    X = np.random.uniform(-bound, bound, n)
    noise = np.random.normal(0, 1, n)
    Y = 2 * X + 3 + noise  # Y = 2X + 3 + noise
    return X, Y

def plot_regression(ax, X, Y, x_min, x_max, label, spread_x=False):
    ax.scatter(X, Y, label='Sample Data')
    
    # Perform linear regression
    model = LinearRegression()
    X_reshaped = X.reshape(-1, 1)
    model.fit(X_reshaped, Y)
    X_full_range = np.linspace(x_min, x_max, 100).reshape(-1, 1)
    Y_pred = model.predict(X_full_range)
    ax.plot(X_full_range, Y_pred, color='red', label='Least Squares Line')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    
    # Calculate standard error of the slope
    X_mean = np.mean(X)
    se = np.sqrt(np.sum((Y - model.predict(X_reshaped)) ** 2) / (len(Y) - 2)) / np.sqrt(np.sum((X - X_mean) ** 2))
    ax.annotate(f'SE(slope): {se:.2f}', xy=(0.05, 0.7), xycoords='axes fraction', fontsize=12, 
                bbox=dict(facecolor='white', alpha=0.6))
    
    # Plot a few other least squares lines from different samples
    for _ in range(5):
        new_X, new_Y = generate_data(len(X), spread_x, seed=np.random.randint(1000))
        new_X_reshaped = new_X.reshape(-1, 1)
        model.fit(new_X_reshaped, new_Y)
        new_Y_pred = model.predict(X_full_range)
        ax.plot(X_full_range, new_Y_pred, color='gray', linestyle='--', alpha=0.5)
    
    ax.set_title(label)

# Create figure and axes
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Generate data for both cases
X1, Y1 = generate_data(50)
X2, Y2 = generate_data(50, spread_x=True)

# Define the same X/Y axis limits for both plots
x_min, x_max = min(np.min(X1), np.min(X2)), max(np.max(X1), np.max(X2))
y_min, y_max = min(np.min(Y1), np.min(Y2)), max(np.max(Y1), np.max(Y2))

# Plot for the first case (normal spread)
plot_regression(axs[0], X1, Y1, x_min, x_max, 'Small spread of X')
axs[0].set_xlim(x_min, x_max)
axs[0].set_ylim(y_min, y_max)

# Plot for the second case (more spread out X)
plot_regression(axs[1], X2, Y2, x_min, x_max, 'Large spread of X', True)
axs[1].set_xlim(x_min, x_max)
axs[1].set_ylim(y_min, y_max)

plt.tight_layout()
plt.show()
```

::: {.fragment}
Closely related to a $x$ points' [leverage]{style="color:red;"}.
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Gauss-Markov Theorem

Under the assumptions of the previous slides: the linear regression $\hat{\beta}$ is the [best linear unbiased estimator]{style="color:red;"} (BLUE), i.e.: the unbiased linear estimator with the smallest variance.

<br></br>

::: {.fragment}
In other words, for any linear **unbiased** $\tilde{\beta} = Cy$:
$$Var(\tilde{\beta}) \succeq Var(\hat{\beta})$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Hypothesis testing and Feature Selection {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Hypothesis testing: simple

$$H0\text{: feature }x\text{ does not affect } y$$
$$H1\text{: feature }x\text{ does affect } y$$

::: {.fragment}
Translates to:
$$H0: \beta_1 = 0$$
$$H1: \beta_1 \neq 0$$
:::

::: {.fragment}
$t_{obs} = \frac{\hat{\beta}_1 - 0}{\hat{SE}(\hat{\beta}_1)} \sim T_{n - 2}$
:::

::: {.fragment}
$\to \text{P-value} = P(T_{n - 2} > |t_{obs}|)$

$\to CI_{0.95}(\beta_1) = \hat{\beta}_1 \pm T_{n - 2, 0.975} \cdot \hat{SE}(\hat{\beta}_1) \approx \hat{\beta}_1 \pm 2 \cdot \hat{SE}(\hat{\beta}_1)$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Hypothesis testing: multiple

$$H0\text{: features }X\text{ do not affect } y$$
$$H1\text{: features }X\text{ do affect } y$$

::: {.fragment}
Translates to:
$$H0: \beta_1 = \dots = \beta_p = 0$$
$$H1: \beta_j \neq 0 \text{ for at least one }j$$
:::

::: {.fragment}
$f_{obs} = \frac{(TSS - RSS)/p}{RSS / (n - p - 1)} \sim F_{p, n - p - 1}$
:::

::: {.fragment}
$\to \text{P-value} = P(F_{p, n - p - 1} > f_{obs})$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Example with `statsmodels`

```{python}
#| echo: false

import numpy as np
import pandas as pd
import statsmodels.api as sm

df = pd.read_csv('../datasets/ISLP_income_data.csv')
X_df = df[['Education', 'Seniority']]
Y = df['Income']

X_df1 = sm.add_constant(X_df)

model = sm.OLS(Y, X_df1)
res = model.fit()
print(res.summary())
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Choosing between nested models

- Checking whether a subset of $q$ features given the other $p - q$ features affect $y$:

$$H0: \beta_{p - q + 1} = \dots = \beta_p = 0$$
$$H1: \beta_j \neq 0 \text{ for at least one } j {out of } q { features}$$

- Run an additional regression *without* these $q$ features, reach $RSS_0$, then:

::: {.fragment}
$f_{obs} = \frac{(RSS_0 - RSS)/q}{RSS / (n - p - 1)} \sim F_{q, n - p - 1}$
:::

::: {.fragment}
- When $q = 1$, $f_{obs} = t^2_{obs}$, where $t_{obs} \sim T_{n - p - 1}$ (testing a single feature given others)
:::

::: {.fragment}
::: {.callout-note}
Question: why $F$-test? Why not test for each of the $p$ features with this $t_{obs}$ test?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Model Fit: plotting

When $p$ is low - draw!

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression

# Generate the dataset
np.random.seed(42)
n_samples = 100
X = np.random.normal(size=(n_samples, 2))  # Random values in 2D
X[:, 0] *= 3  # Scale the first feature
X[:, 1] *= 10  # Scale the second feature
noise = np.random.normal(size=(n_samples,)) * 0.1 # Noise to add to the output

# Non-linear function with an upward trend
y = np.sin(X[:, 0]) + 0.2*X[:, 1] + noise

# Fit a linear regression model
reg = LinearRegression()
reg.fit(X, y)
y_pred = reg.predict(X)

# Plot the data and the fitted plane
fig, axes = plt.subplots(1, 2, figsize=(12, 5), subplot_kw={'projection': '3d'})

# Scatter plot of the original data
axes[0].scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')
axes[1].scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')

# Create a grid for plotting the plane
x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
y_grid = reg.intercept_ + reg.coef_[0] * x1_grid + reg.coef_[1] * x2_grid

# Plot the linear regression plane
axes[0].plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')
axes[1].plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')

# Draw vertical lines from the plane to the data points
for i in range(n_samples):
    axes[0].plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')
    axes[1].plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')

# Adjust the viewing angle
axes[0].view_init(elev=30, azim=-60)
axes[1].view_init(elev=30, azim=90)

# Set labels and title
axes[0].set_xlabel('X1')
axes[1].set_xlabel('X1')
axes[0].set_ylabel('X2')
axes[1].set_ylabel('X2')
axes[0].set_zlabel('y')
axes[1].set_zlabel('y')
# ax.set_title('Non-linear Data with Linear Regression Plane')

plt.show()
```


::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Model fit: residuals plots

```{python}
#| echo: false

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].scatter(X[:, 0], y_pred - y)
axes[0].axhline(y=0, color='r', linestyle='--')
axes[0].set_xlabel('X1')
axes[0].set_ylabel('Residuals')
axes[1].scatter(X[:, 1], y_pred - y)
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('X2')
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Model Fit: $R$ squared

Measure of "explained variance":

$R^2 = 1 - \frac{\sum_{i = 1}^n (y_i - X\hat{\beta})^2}{\sum_{i = 1}^n (y_i - \bar{y})^2} = 1 - \frac{RSS}{TSS} = 1 - \frac{(y - X\hat{\beta})'(y - X\hat{\beta})}{(y - X\hat{\beta^*_0})'(y - X\hat{\beta^*_0})}$

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Generate sample data
np.random.seed(42)
X = np.linspace(0, 10, 15)
y = 2 * X + 1 + np.random.randn(15) * 2  # Linear relation with noise

# Perform linear regression
coeffs = np.polyfit(X, y, 1)
y_hat = np.polyval(coeffs, X)

# Choose a sample point far from the mean
x_i = 7
y_i = 2 * x_i + 1 + 3  # Simulated data point
y_hat_i = np.polyval(coeffs, x_i)

# Mean line
y_mean = np.mean(y)

# Create the plot
fig, ax = plt.subplots(figsize=(7, 4))

# Plot data points
ax.scatter(X, y, color='red', label='Data points')
ax.scatter(x_i, y_i, color='red')  # Highlighted data point

# Plot regression line
ax.plot(X, y_hat, color='blue', label='Regression line')

# Plot mean line
ax.axhline(y=y_mean, color='blue', linestyle='--', label='Mean line ($\overline{y}$)')

# Plot fitted point
ax.scatter(x_i, y_hat_i, color='blue', zorder=5)  # Fitted point

# Draw vertical lines from the points to the lines
ax.vlines(x_i, y_i, y_mean, color='black', linestyle='--')
ax.vlines(x_i, y_i, y_hat_i, color='black', linestyle='--')

# Setting labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.legend()

plt.show()
```

::: {.fragment}
::: {.callout-note}
Careful: $R^2$ is monotone increasing in $p$ for the training data!
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Intro. to feature selection

How can we choose a subset of the variables?

::: {.incremental}
- All subsets: what's wrong with that?
- Forward: what's wrong with that?
- Backward
- Hybrid
- Other more advanced methods
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Extending the linear model {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Categorical features with $k$ levels

When $k = 2$:

::: {.incremental}
- If $x$ indicates whether person "has tattoos" or "no tattoos"
- Define: $x_i =
\begin{cases} 
1 & \text{if person } i \text{ has tattoos,} \\
0 & \text{otherwise.}
\end{cases}$
- Model: $y_i = \beta_0 + \beta_1x_i + \varepsilon_i$
- Meaning: $\beta_1 = E(y|\text{has tattoos}) - E(y|\text{no tattoos})$
:::

::: {.fragment}
::: {.callout-note}
Question: what if $x_i =
\begin{cases} 
1 & \text{if person } i \text{ has tattoos} \\
-1 & \text{otherwise.}
\end{cases}$
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

#### Categorical features with $k$ levels: $k > 2$

::: {.incremental}
- If $x$ is a person's profession (carpenter, gardener, or teacher)

::: {.fragment}
::: {.callout-note}
Question: what if we just had $x_{i} =
\begin{cases} 
0 & \text{if person } i \text{ is a teacher} \\
1 & \text{if person } i \text{ is a carpenter} \\
2 & \text{if person } i \text{ is a gardener}
\end{cases}$
:::
:::

- Define $k - 1$ indicators:

::: {.fragment}
$x_{i1} =
\begin{cases} 
1 & \text{if person } i \text{ is a carpenter,} \\
0 & \text{otherwise.}
\end{cases}$, $x_{i2} =
\begin{cases} 
1 & \text{if person } i \text{ is a gardener,} \\
0 & \text{otherwise.}
\end{cases}$
:::

- Why $k - 1$ indicators?
- Model: $y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \varepsilon_i$
- Meaning: $\beta_1 = E(y|\text{carpenter}) - E(y|\text{teacher}); \quad \beta_2 = E(y|\text{gardener}) - E(y|\text{teacher})$
:::

::: {.fragment}
::: {.callout-note}
Question: how would you test if "profession" has effect on $y$?
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Interactions

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression

# Generate the dataset
np.random.seed(42)
n_samples = 100
X = np.random.normal(size=(n_samples, 2))  # Random values in 2D
X[:, 0] *= 3  # Scale the first feature
X[:, 1] *= 3  # Scale the second feature
noise = np.random.normal(size=(n_samples,)) * 0.1 # Noise to add to the output

# Non-linear function with an upward trend
y = 0.5*X[:, 0] + 0.2*X[:, 1] + 0.3*X[:, 0] * X[:, 1] + noise

# Fit a linear regression model
reg = LinearRegression()
reg.fit(X, y)
y_pred = reg.predict(X)

# Plot the data and the fitted plane
fig = plt.figure(figsize=(14, 7))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot of the original data
ax.scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')

# Create a grid for plotting the plane
x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
y_grid = reg.intercept_ + reg.coef_[0] * x1_grid + reg.coef_[1] * x2_grid

# Plot the linear regression plane
ax.plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')

# Draw vertical lines from the plane to the data points
for i in range(n_samples):
    ax.plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

# Set labels and title
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('y')

plt.show()
```

::: {.fragment}
A [multiplicative]{style="color:red;"} effect?
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Interactions: residuals plots

```{python}
#| echo: false

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].scatter(X[:, 0], y_pred - y)
axes[0].axhline(y=0, color='r', linestyle='--')
axes[0].set_xlabel('X1')
axes[0].set_ylabel('Residuals')
axes[1].scatter(X[:, 1], y_pred - y)
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('X2')
plt.show()
```

::: {.fragment}
Consider adding an interaction term to the model: $x_1 \cdot x_2$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Interactions: adding an interaction term

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression

# Generate the dataset
np.random.seed(42)
n_samples = 100
X = np.random.normal(size=(n_samples, 2))  # Random values in 2D
X[:, 0] *= 3  # Scale the first feature
X[:, 1] *= 3  # Scale the second feature
X = np.hstack((X, (X[:, 0] * X[:, 1]).reshape(-1, 1)))  # Add a new interaction feature
noise = np.random.normal(size=(n_samples,)) * 0.1 # Noise to add to the output

# Non-linear function with an upward trend
y = 0.5*X[:, 0] + 0.2*X[:, 1] + 0.3*X[:, 0] * X[:, 1] + noise

# Fit a linear regression model
reg = LinearRegression()
reg.fit(X, y)
y_pred = reg.predict(X)

# Plot the data and the fitted plane
fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot of the original data
ax.scatter(X[:, 0], X[:, 1], y, color='r', label='Original data')

# Create a grid for plotting the plane
x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
y_grid = reg.intercept_ + reg.coef_[0] * x1_grid + reg.coef_[1] * x2_grid + reg.coef_[2] * x1_grid * x2_grid

# Plot the linear regression plane
ax.plot_surface(x1_grid, x2_grid, y_grid, color='b', alpha=0.5, label='Fitted plane')

# Draw vertical lines from the plane to the data points
for i in range(n_samples):
    ax.plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], y_pred[i]], color='k')

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

# Set labels and title
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('y')

plt.show()
```

$y_i = \beta_0 + \beta_1x_{i1} + \beta_2 x_{i1} + \beta_3 x_{i1} \cdot x_{i2}+ \varepsilon_i$

::: {.fragment}
::: {.callout-note}
Note: in multiplicative models we usually add the features without interactions to the model ("main effects"), even if their P-value is not small.
:::
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Non-linear relations

```{python}
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression

df = pd.read_csv('../datasets/mtcars.csv')

model = LinearRegression().fit(df[['hp']], df['mpg'])
y_pred = model.predict(df[['hp']])

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.scatterplot(data=df, x='hp', y='mpg', ax=axes[0])
axes[0].plot(df['hp'], y_pred, color='red')
axes[0].set_xlabel('Horsepower')
axes[0].set_ylabel('Miles per Gallon')

sns.scatterplot(x=df['hp'], y=y_pred - df['mpg'], ax=axes[1])
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('Horsepower')
axes[1].set_ylabel('Residuals')

plt.show()
```

::: {.fragment}
Consider adding polynomial terms $x^2_1, x^3_1$, ...
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Plynomial regression: Beware 1

```{python}
#| echo: false

x_hp = df[['hp']].values
X2 = np.hstack((x_hp, x_hp**2))
X5 = np.hstack((x_hp, x_hp**2, x_hp**3, x_hp**4, x_hp**5))
model1 = LinearRegression().fit(x_hp, df['mpg'])
model2 = LinearRegression().fit(X2, df['mpg'])
model5 = LinearRegression().fit(X5, df['mpg'])

X1_range = np.linspace(df['hp'].min(), df['hp'].max(), 100).reshape(-1, 1)
X2_range = np.hstack((X1_range, X1_range**2))
X5_range = np.hstack((X1_range, X1_range**2, X1_range**3, X1_range**4, X1_range**5))
y_pred1 = model1.predict(X1_range)
y_pred2 = model2.predict(X2_range)
y_pred5 = model5.predict(X5_range)

fig = plt.figure(figsize=(6, 5))
sns.scatterplot(data=df, x='hp', y='mpg')
plt.plot(X1_range, y_pred1, color='red', label = 'Linear')
plt.plot(X1_range, y_pred2, color='orange', label = 'Degree 2')
plt.plot(X1_range, y_pred5, color='green', label = 'Degree 5')
plt.xlabel('Horsepower')
plt.ylabel('Miles per Gallon')

plt.legend()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Plynomial regression: Beware 1

Quadratic:

```{python}
#| echo: false

import statsmodels.api as sm

X2 = sm.add_constant(X2)
model = sm.OLS(df['mpg'], X2).fit()
model.summary().tables[1]
```

Cubic:

```{python}
#| echo: false

X3 = np.hstack((x_hp, x_hp**2, x_hp**3))
X3 = sm.add_constant(X3)
model = sm.OLS(df['mpg'], X3).fit()
model.summary().tables[1]
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Plynomial regression: Beware 2

```{python}
#| echo: false

X1_range = np.linspace(df['hp'].min() - 100, df['hp'].max() + 100, 1000).reshape(-1, 1)
X2_range = np.hstack((X1_range, X1_range**2))
X5_range = np.hstack((X1_range, X1_range**2, X1_range**3, X1_range**4, X1_range**5))
y_pred1 = model1.predict(X1_range)
y_pred2 = model2.predict(X2_range)
y_pred5 = model5.predict(X5_range)

fig = plt.figure(figsize=(6, 5))
sns.scatterplot(data=df, x='hp', y='mpg')
plt.plot(X1_range, y_pred1, color='red', label = 'Linear')
plt.plot(X1_range, y_pred2, color='orange', label = 'Degree 2')
plt.plot(X1_range, y_pred5, color='green', label = 'Degree 5')
plt.xlabel('Horsepower')
plt.ylabel('Miles per Gallon')

plt.legend()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Other issues with linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Collinearity

::: {.incremental}
- [Collinearity]{style="color:red;"} is when a feature is in the span of other features
- Usually it is not exactly in the span but near the span
- In the case of exact collinearity: the matrix $X'X$ is singular, no unique solution
- In the case of approximate collinearity:
  - the solution is not numerically stable
  - $SE(\hat{\beta})$ grows, $t_{obs}$ value decreases, feature is "masked" in inference
- Explore your data with a correlation matrix, many other metrics
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### High-leverage and Outlier observations

::: {.incremental}
- An observation with high [leverage]{style="color:red;"} has an extreme $x$ value

- For simple linear regression: $h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum (x_{i'} - \bar{x})^2}$

- Observations with large $h_i$ $\to$ large effect on $SE(\hat{\beta})$

- If such an observation is also an [outlier]{style="color:red;"} $\to$ the model changes a lot if removed

:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### High-leverage and Outlier observations

```{python}
#| echo: false

model = LinearRegression().fit(df[['hp']], df['mpg'])
y_pred = model.predict(df[['hp']])

SSx = np.sum((x_hp - np.mean(x_hp))**2)
h = 1 / x_hp.shape[0] + (x_hp - np.mean(x_hp))**2 / SSx

highlight_point1 = {'X1': df['hp'].values[-2], 'Y': df['mpg'].values[-2]}
highlight_point2 = {'X1': h[-2], 'Y': (y_pred - df['mpg'].values)[-2]}

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.scatterplot(data=df, x='hp', y='mpg', ax=axes[0])
axes[0].scatter(highlight_point1['X1'], highlight_point1['Y'], color='red', s=100)
axes[0].plot(df['hp'], y_pred, color='red')
axes[0].set_xlabel('Horsepower')
axes[0].set_ylabel('Miles per Gallon')

sns.scatterplot(x=h.flatten(), y=y_pred - df['mpg'], ax=axes[1])
axes[1].scatter(highlight_point2['X1'], highlight_point2['Y'], color='red', s=100)
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('Leverage')
axes[1].set_ylabel('Residuals')

plt.show()
```

::: {.fragment}
Consider removing (careful!).
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

#### Excluding high-leverage + outlier

```{python}
#| echo: false

df2 = df.drop(x_hp.shape[0] - 2, axis=0)
model = LinearRegression().fit(df2[['hp']], df2['mpg'])
y_pred = model.predict(df2[['hp']])

x_hp2 = np.delete(x_hp, -2)
SSx = np.sum((x_hp2 - np.mean(x_hp2))**2)
h = 1 / x_hp2.shape[0] + (x_hp2 - np.mean(x_hp2))**2 / SSx

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.scatterplot(data=df2, x='hp', y='mpg', ax=axes[0])
axes[0].plot(df2['hp'], y_pred, color='red')
axes[0].set_xlabel('Horsepower')
axes[0].set_ylabel('Miles per Gallon')

sns.scatterplot(x=h.flatten(), y=y_pred - df2['mpg'], ax=axes[1])
axes[1].axhline(y=0, color='r', linestyle='--')
axes[1].set_xlabel('Leverage')
axes[1].set_ylabel('Residuals')

plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::
