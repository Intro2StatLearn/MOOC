---
format:
  revealjs:
    slide-number: true
    chalkboard: true
    fig-width: 6
    fig-asp: 0.618
    template-partials:
      - "../title-slide.html"
css: "../slides_quarto.css"
standalone: false
include-in-header: "../header_quarto.html"
logo: "../Intro2SL_logo_white.jpg"
pagetitle: "Linear Regression"
callout-appearance: simple
smaller: true
execute:
  eval: true
  echo: true
code-line-numbers: false
code-block-border-left: true
highlight-style: github
footer: "[Intro to Statistical Learning](https://intro2statlearn.github.io/mooc/){target='_blank'}"
---

## {.logo-slide}

## Introduction to Statistical Learning {.title-slide}

### Linear Regression - Class 2

### Giora Simchoni

#### `gsimchoni@gmail.com` and add `#intro2sl` in subject

### Stat. and OR Department, TAU

---

## Simple linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Why learn linear regression?

- Linear regression is still a highly useful modeling tool
- It can be used in many non-linear cases by using transformed variables / transformed goals
- Its probabilistic aspect is fully understood, so inference questions can be answered exactly (under the regression assumptions)
- Many newer methods can be seen as a generalization for linear regression

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Simple linear regression

- Assume: $y \approx \beta_0 + \beta_1x$
- This is the *unobserved* [population regression]{style="color:red;"} line
- We look for the values of $\beta_0, \beta_1$ through a sample $\{(x_1, y_1), \dots, (x_n, y_n)\}$
- In order to estimate the parameters we need to define a measure of error
- By far the most common measure is:

$RSS = \sum_{i = 1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2$ or its scaled version: $MSE = \frac{RSS}{n}$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### From line to RSS surface

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from mpl_toolkits.mplot3d import Axes3D

# Set the random seed for reproducibility
np.random.seed(42)

# Number of samples
n = 100

# Generate X values
X = np.random.rand(n, 1)

# Generate noise
e = np.random.randn(n, 1)

# True population line parameters
beta_0_true = 2
beta_1_true = 3

# Generate Y values
Y = beta_0_true + beta_1_true * X + e

# Fit the least squares regression line
model = LinearRegression().fit(X, Y)
Y_pred = model.predict(X)

# Create a meshgrid for beta_0 and beta_1
beta_0_range = np.linspace(beta_0_true - 2, beta_0_true + 2, 100)
beta_1_range = np.linspace(beta_1_true - 2, beta_1_true + 2, 100)
beta_0, beta_1 = np.meshgrid(beta_0_range, beta_1_range)

# Calculate the RSS for each combination of beta_0 and beta_1
RSS = np.zeros(beta_0.shape)
for i in range(beta_0.shape[0]):
    for j in range(beta_0.shape[1]):
        Y_pred_mesh = beta_0[i, j] + beta_1[i, j] * X
        RSS[i, j] = np.sum((Y - Y_pred_mesh) ** 2)

# Prepare for plotting
fig = plt.figure(figsize=(5 * 3, 3.5))

ax1 = fig.add_subplot(131)
# Plot the sample points
ax1.scatter(X, Y, color='blue', label='Sample points')
ax1.set_xlabel(r'$x$')
ax1.set_ylabel(r'$y$')
# Plot the least squares regression line
ax1.plot(X, Y_pred, color='red', linestyle='--', label='Least squares line', linewidth=2)
# Plot vertical lines from each sample point to the fitted line
for i in range(n):
    ax1.plot([X[i], X[i]], [Y[i], Y_pred[i]], color='gray', linestyle=':', linewidth=1)
ax1.set_title(r'Residuals for specific $\hat{\beta}_0, \hat{\beta}_1$')

# Contour plot on the left
ax2 = fig.add_subplot(132)
contour = ax2.contour(beta_0, beta_1, RSS, levels=50, cmap='viridis')
ax2.set_xlabel(r'$\beta_0$')
ax2.set_ylabel(r'$\beta_1$')
ax2.set_title('Contour plot of RSS')
fig.colorbar(contour, ax=ax2, label='RSS')

# Add a red cross at the true beta values
ax2.plot(beta_0_true, beta_1_true, 'rx', markersize=12, markeredgewidth=3)
# Add dashed lines from the cross to the true beta_0 and beta_1 axes
ax2.plot([beta_0_true, beta_0_true], [beta_1_range.min(), beta_1_true], 'r--')
ax2.plot([beta_0_range.min(), beta_0_true], [beta_1_true, beta_1_true], 'r--')

# 3D plot on the right
ax3 = fig.add_subplot(133, projection='3d')
ax3.plot_surface(beta_0, beta_1, RSS, cmap='viridis', edgecolor='none')
ax3.set_xlabel(r'$\beta_0$')
ax3.set_ylabel(r'$\beta_1$')
ax3.set_zlabel('RSS')
ax3.set_title('3D plot of RSS surface')

# Add a red cross at the true beta values
ax3.plot([beta_0_true], [beta_1_true], [np.min(RSS)], 'rx', markersize=12, markeredgewidth=3, zorder=10)

# Add dashed lines from the cross to the true beta_0 and beta_1 axes
ax3.plot([beta_0_true, beta_0_true], [0, beta_1_true], [np.min(RSS), np.min(RSS)], 'r--', zorder=10)
ax3.plot([4, beta_0_true], [beta_1_true, beta_1_true], [np.min(RSS), np.min(RSS)], 'r--', zorder=10)

# plt.tight_layout()
plt.show()
```

$$RSS = \sum_{i = 1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2$$

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Simple linear regression

- A simple derivation gives:

$\hat{\beta}_1 = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} = \frac{\widehat{Cov(X,Y)}}{\widehat{Var(X)}},\;\;\;\;\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$

- This is the [least squares]{style="color:red;"} line (OLS), the *best* linear predictor for the population regression line
- Replacing the sample averages by the population means (or having a huge sample) should get us to the true line
- Using $\hat{\beta}_0, \hat{\beta}_1$ we can easily perform prediction

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Average of many OLS lines

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Set the random seed for reproducibility
np.random.seed(42)

# Number of samples
n = 100

# Generate X values
X = np.random.rand(n, 1)

# Generate noise
e = np.random.randn(n, 1)

# True population line parameters
beta_0 = 2
beta_1 = 3

# Generate Y values
Y = beta_0 + beta_1 * X + e

# Fit the least squares regression line
model = LinearRegression().fit(X, Y)
Y_pred = model.predict(X)

# Prepare for plotting
fig, axs = plt.subplots(1, 2, figsize=(10, 5))

# Plot on the left
axs[0].scatter(X, Y, color='blue', label='Sample points')
axs[0].plot(X, beta_0 + beta_1 * X, color='green', label='True population line', linewidth=2)
axs[0].plot(X, Y_pred, color='red', label='Least squares line', linewidth=2)
axs[0].set_title('Single Sample Regression')
axs[0].legend()

# Plot on the right
# axs[1].scatter(X, Y, color='blue', label='Sample points')
axs[1].plot(X, beta_0 + beta_1 * X, color='green', label='True population line', linewidth=2)

# Plot 10 additional estimated regression lines
for _ in range(10):
    X_sample = np.random.rand(n, 1)
    e_sample = np.random.randn(n, 1)
    Y_sample = beta_0 + beta_1 * X_sample + e_sample
    model_sample = LinearRegression().fit(X_sample, Y_sample)
    Y_sample_pred = model_sample.predict(X)
    axs[1].plot(X, Y_sample_pred, color='orange', linestyle='--', alpha=0.6)

# Also plot the estimated line from the original sample
axs[1].plot(X, Y_pred, color='red', label='Original sample estimated line', linewidth=2)
axs[1].set_title('Multiple Sample Regressions')
axs[1].legend()

plt.tight_layout()
plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Probabilistic View of Linear Regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Probabilistic justification for OLS

- Assume the data's true model is:
  - $y_i = \beta_0 + \beta_1x_i + \varepsilon_i$
  - $\varepsilon_i$ are i.i.d, specifically: $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$
  - $(x_i, \varepsilon_i)$ are independent
- *Now* we can write the log likelihood and maximize it

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Simple linear regression maximum likelihood

Since the errors are normal and the model is linear we get:

$y_i = \beta_0 + \beta_1x_i + \varepsilon_i  \to y_i \sim \mathcal{N}(\beta_0 + \beta_1x_i, \sigma^2) \to$

$$L(\beta, \sigma^2 | x, y) = \prod_{i = 1}^n f(y_i) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(y_i - \beta_0 - \beta_1x_i)^2}{2\sigma^2}\right]$$
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_i)^2\right]$$

Calculating the log-likelihood we get:

$\ell(\beta, \sigma^2 | x, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}$[$\sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_i)^2$]{style="color:red;"}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Multiple linear regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### From simple to multiple regression

<br></br>

$y_i = \beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip} + \varepsilon_i \to y_i = x_i'\beta + \varepsilon_i$, $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$

Or even more concisely:

$y = \begin{pmatrix}y_{1} \\ \vdots \\ y_{n}\end{pmatrix}$, $X = \begin{pmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p} \\
1 & x_{31} & x_{32} & \cdots & x_{3p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{np}
\end{pmatrix}$, $\beta = \begin{pmatrix}\beta_0 \\ \beta_{1} \\ \vdots \\ \beta_{p}\end{pmatrix}$, $\varepsilon = \begin{pmatrix}\varepsilon_{1} \\ \vdots \\ \varepsilon_{n}\end{pmatrix}$

::: {.fragment}
$$\to y = X\beta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(\textbf{0}, \sigma^2I_n)$$
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### From line to (hyper)plane
```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from matplotlib.patches import Patch
from matplotlib.lines import Line2D

df = pd.read_csv('../datasets/ISLP_income_data.csv')
X1 = df['Education']
X2 = df['Seniority']
X = np.column_stack((X1, X2))
Y = df['Income']

# Plotting
fig, ax = plt.subplots(1, 1, figsize=(14, 7), subplot_kw={'projection': '3d'})

# Create a grid of values for X1 and X2
X1_grid, X2_grid = np.meshgrid(np.linspace(min(X1), max(X1), 200),
                               np.linspace(min(X2), max(X2), 200))

# Flatten the grid to pass into the model
X_grid = np.column_stack((X1_grid.ravel(), X2_grid.ravel()))

model = LinearRegression()

model.fit(X, Y)

# Predict Y values
Y_pred = model.predict(X_grid).reshape(X1_grid.shape)

# Plot the fitted plane
ax.scatter(X1, X2, Y, color='red', label='Data')
ax.plot_surface(X1_grid, X2_grid, Y_pred, color='blue', alpha=0.5)

# Add vertical lines from data points to the surface
Y_surface = model.predict(X).reshape(Y.shape)  # Predicted Y values for the actual data points
for x1, x2, y, y_surface in zip(X1, X2, Y, Y_surface):
    ax.plot([x1, x1], [x2, x2], [y, y_surface], color='black')

# Labels
ax.set_xlabel('Education')
ax.set_ylabel('Seniority')
ax.set_zlabel('Income')
# ax.set_title(title)

# Adjust the viewing angle
ax.view_init(elev=30, azim=-60)

plt.show()
```

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Multiple linear regression ML

$y_i = x_i'\beta + \varepsilon_i  \to y \sim \mathcal{N}(x_i'\beta, \sigma^2) \to$

$$L(\beta, \sigma^2 | X, y) = \prod_{i = 1}^n f(y_i) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(y_i - x_i'\beta)^2}{2\sigma^2}\right]$$
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - x_i'\beta)^2\right]$$
$$= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta)\right]$$

Calculating the log-likelihood we get:

$\ell(\beta, \sigma^2 | X, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}$[$(y - X\beta)'(y - X\beta)$]{style="color:red;"}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

### Linear regression MLE

$\ell(\beta, \sigma^2 | X, y) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta)$

::: {.fragment}
$= \mathcal{C} - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y'y -2\beta'X'y + \beta'X'X\beta)$
:::

<br></br>

::: {.fragment}
$\frac{\partial l}{\partial \beta} = -\frac{1}{2\sigma^2}(2X'y - 2X'X\beta)$
:::

::: {.fragment}
$\frac{1}{\sigma^2}(X'y - X'X\beta) = 0$
:::
::: {.fragment}
$X'X\beta = X'y$
:::
::: {.fragment}
[$\hat{\beta} = (X'X)^{-1}X'y$]{style="color:red;"}
:::

<br></br>

::: {.fragment}
Similarly:

$\frac{\partial l}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}(y - X\beta)'(y - X\beta)$ $\to$ [$\hat{\sigma}^2 = \frac{1}{n}(y - X\beta)'(y - X\beta)$]{style="color:red;"}
:::

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---

## Properties of Linear Regression {.title-slide}

::: {.notes}
::: {style="direction:rtl; font-size:16px"}
:::
:::

---
